PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Xie, EZ; Wang, WH; Ding, MY; Zhang, RM; Luo, P				Xie, Enze; Wang, Wenhai; Ding, Mingyu; Zhang, Ruimao; Luo, Ping			PolarMask plus plus : Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pipelines; Image segmentation; Object detection; Feature extraction; Detectors; Tensors; Proposals; Instance segmentation; object detection; polar representation; fully convolutional network		Reducing the complexity of the pipeline of instance segmentation is crucial for real-world applications. This work addresses this issue by introducing an anchor-box free and single-shot instance segmentation framework, termed PolarMask, which reformulates the instance segmentation problem as predicting the contours of objects in the polar coordinate, with several appealing benefits. (1) The polar representation unifies instance segmentation (masks) and object detection (bounding boxes) into a single framework, reducing the design and computational complexity. (2) Two modules are carefully designed (i.e., soft polar centerness and polar IoU loss) to sample high-quality center examples and optimize polar contour regression, making the performance of PolarMask does not depend on the bounding box prediction results and thus becomes more efficient in training. (3) PolarMask is fully convolutional and can be easily embedded into most off-the-shelf detection methods. To further improve the accuracy of the framework, a Refined Feature Pyramid is introduced to further improve the feature representation at different scales, termed PolarMask++. Extensive experiments demonstrate the effectiveness of both PolarMask and PolarMask++, which achieve competitive results on instance segmentation in the challenging COCO dataset with single-model and single-scale training and testing, as well as new state-of-the-art results on rotate text detection and cell segmentation. We hope the proposed polar representation can provide a new perspective for designing algorithms to solve single-shot instance segmentation. The codes and models are available at: github.com/xieenze/PolarMask.	[Xie, Enze; Ding, Mingyu; Luo, Ping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China; [Wang, Wenhai] Nanjing Univ, Dept Comp Sci, Nanjing 210023, Peoples R China; [Zhang, Ruimao] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China; [Zhang, Ruimao] Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China	University of Hong Kong; Nanjing University; Chinese University of Hong Kong, Shenzhen	Xie, EZ; Luo, P (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.	xieenze@hku.hk; wangwenhai362@163.com; mingyuding@hku.hk; r.m.zhang1989@gmail.com; pluo@cshku.hk	Luo, Ping/HGE-7623-2022; wang, wenhai/GUX-3226-2022	Luo, Ping/0000-0002-6685-7950; 	RGC General Research Fund of HK [27208720]; HKU Seed Fund for Basic Research; SenseTime through Start-up Fund and Research Donation	RGC General Research Fund of HK; HKU Seed Fund for Basic Research; SenseTime through Start-up Fund and Research Donation	This work was supported in part by the RGC General Research Fund of HK under Grant 27208720, in part by HKU Seed Fund for Basic Research, and in part by the SenseTime through Start-up Fund and Research Donation. Enze Xie and Wenhai Wang contributed equally to this work.	[Anonymous], 1999, 10153 BS EN; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bi Y., 2020, PROC ASIAN C COMPUT, P304; Bo Liu, 2019, Arxiv, DOI arXiv:1911.09199; Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925; Chen H, 2016, PROC CVPR IEEE, P2487, DOI 10.1109/CVPR.2016.273; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215; Cheng D, 2019, PROC CVPR IEEE, P7423, DOI 10.1109/CVPR.2019.00761; Chuhui Xue, 2019, Arxiv, DOI arXiv:1901.02596; Cong Yao, 2018, Arxiv, DOI arXiv:1802.08948; Dai JF, 2016, ADV NEUR IN, V29; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Deng D., P 32 AAAI C ART INT, V44; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denzler J, 1999, REAL-TIME IMAGING, V5, P203, DOI 10.1006/rtim.1997.0116; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221; Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917; Fu C., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1701.06659; Fuchun Sun, 2020, Arxiv, DOI arXiv:1904.03797; Girshick R., 2014, PROC IEEE C COMPUT V; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824; He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331; He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87; Hirsch P., 2020, ARXIV; Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Kaggle, 2018, KAGGLE 2018 DATA SCI; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472; Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619; Liao MH, 2017, AAAI CONF ARTIF INTE, P4161; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu ZC, 2018, PROC CVPR IEEE, P6936, DOI 10.1109/CVPR.2018.00725; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2; Lv GF, 2019, 2019 2ND IEEE INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND SIGNAL PROCESSING (ICICSP), P357, DOI 10.1109/ICICSP48821.2019.8958541; Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020; Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091; Pengbo Zhao, 2020, Arxiv, DOI arXiv:2010.08720; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O., 2015, P INT C MED IM COMP; Schmidt U, 2018, LECT NOTES COMPUT SC, V11071, P265, DOI 10.1007/978-3-030-00934-2_30; Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371; Sida Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8530, DOI 10.1109/CVPR42600.2020.00856; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4; Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853; Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xu WQ, 2019, IEEE I CONF COMP VIS, P5167, DOI 10.1109/ICCV.2019.00527; Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589; Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787; Yi JR, 2019, LECT NOTES COMPUT SC, V11764, P369, DOI 10.1007/978-3-030-32239-7_41; Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274; Zhou X., 2019, ARXIV; Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283	75	5	5	11	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5385	5400		10.1109/TPAMI.2021.3080324	http://dx.doi.org/10.1109/TPAMI.2021.3080324			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33989151	Green Submitted			2022-12-18	WOS:000836666600063
J	Zhang, JP; Jia, XP; Hu, JK; Tan, K				Zhang, Junpeng; Jia, Xiuping; Hu, Jiankun; Tan, Kun			Moving Vehicle Detection for Remote Sensing Video Surveillance With Nonstationary Satellite Platform	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Satellites; Matrix decomposition; Optimization; Optical sensors; Cameras; Sparse matrices; Remote sensing; Moving object detection; remote sensing video surveillance; satellite video; low rank matrix decomposition; moving satellite platform	LOW-RANK; OBJECT DETECTION	With satellite platforms gazing at a target territory, the captured satellite videos exhibit local misalignment and local intensity variation on some stationary objects that can be mistakenly extracted as moving objects and increase false alarm rates. Typical approaches for mitigating the effect of moving cameras in moving object detection (MOD) follow domain transformation technique, where the misalignment between consecutive frames is restricted to the image planar. However, such technique cannot properly handle satellite videos, as the local misalignment on them is caused by the varying projections from the 3D objects on the Earth's surface to 2D image planar. In order to suppress the effect of moving satellite platform in MOD, we propose a Moving-Confidence-Assisted Matrix Decomposition (MCMD) model, where foreground regularization is designed to promote real moving objects and ignore system movements with the assistance of a moving-confidence score estimated from dense optical flows. For solving the convex optimization problem in MCMD, both batch processing and online solutions are developed in this study, by adopting the alternating direction method and the stochastic optimization strategy, respectively. Experimental results on the videos captured by SkySat and Jilin-1 show that MCMD outperforms the state-of-the-art techniques with improved precision by suppressing effect of nonstationary satellite platforms.	[Zhang, Junpeng; Jia, Xiuping; Hu, Jiankun] Univ New South Wales, Sch Engn & Informat Technol SEIT, Canberra, NSW 2600, Australia; [Tan, Kun] East China Normal Univ, Key Lab Geog Informat Sci, Shanghai 200241, Peoples R China	University of New South Wales Sydney; East China Normal University	Jia, XP (corresponding author), Univ New South Wales, Sch Engn & Informat Technol SEIT, Canberra, NSW 2600, Australia.	zhangjunpeng9354@126.com; x.jia@adfa.edu.au; J.Hu@adfa.edu.au; tankuncu@gmail.com	Tan, Kun/AAA-6254-2019	Tan, Kun/0000-0001-6353-0146; Hu, Jiankun/0000-0003-0230-1432	Australia Research Council (ARC) [LP180100663, DP190103662, DP190103660]	Australia Research Council (ARC)(Australian Research Council)	This work was supported by Australia Research Council (ARC) grants with projects IDs: LP180100663, DP190103662, and DP190103660. The authors would like to thank Planet Team for providing the data in this research [1].	Aires KRT, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1607; [Anonymous], 2017, PLANET APPL PROGRAM; Ao W, 2020, IEEE T IMAGE PROCESS, V29, P1944, DOI 10.1109/TIP.2019.2944097; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001; Bouwmans T, 2018, P IEEE, V106, P1427, DOI 10.1109/JPROC.2018.2853589; Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009; Boyd S, 2004, CONVEX OPTIMIZATION; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Cao XC, 2016, IEEE T CYBERNETICS, V46, P1014, DOI 10.1109/TCYB.2015.2419737; Ding P, 2018, ISPRS J PHOTOGRAMM, V141, P208, DOI 10.1016/j.isprsjprs.2018.05.005; Ebadi SE, 2018, IEEE T PATTERN ANAL, V40, P2273, DOI 10.1109/TPAMI.2017.2745573; Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50; Gao Z, 2012, LECT NOTES COMPUT SC, V7576, P690, DOI 10.1007/978-3-642-33715-4_50; Gilman K, 2019, IEEE INT CONF COMP V, P643, DOI 10.1109/ICCVW.2019.00078; He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848; Jenatton R, 2011, J MACH LEARN RES, V12, P2777; Jenatton R, 2011, J MACH LEARN RES, V12, P2297; Li K, 2018, IEEE T GEOSCI REMOTE, V56, P2337, DOI 10.1109/TGRS.2017.2778300; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Liu WC, 2018, IEEE GEOSCI REMOTE S, V15, P937, DOI 10.1109/LGRS.2018.2813094; Liu X, 2015, IEEE T IMAGE PROCESS, V24, P2502, DOI 10.1109/TIP.2015.2419084; Long Y, 2017, IEEE T GEOSCI REMOTE, V55, P2486, DOI 10.1109/TGRS.2016.2645610; Luo YM, 2017, IEEE GEOSCI REMOTE S, V14, P2398, DOI 10.1109/LGRS.2017.2766204; Mairal J., 2010, ADV NEURAL INFORM PR, P1558, DOI [DOI 10.5555/2997046.2997070, 10.5555/2997046.2997070]; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Reilly V, 2010, LECT NOTES COMPUT SC, V6313, P186; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Saleemi I, 2013, INT J COMPUT VISION, V104, P198, DOI 10.1007/s11263-013-0624-1; Shakeri M, 2016, COMPUT VIS IMAGE UND, V146, P27, DOI 10.1016/j.cviu.2016.02.009; Shen J, 2016, PR MACH LEARN RES, V48; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Wright Y., 2009, ADV NEURAL INFORM PR, V22, DOI DOI 10.5555/2984093.2984326; Xin B, 2015, PROC CVPR IEEE, P4676, DOI 10.1109/CVPR.2015.7299099; Xu J, 2013, IEEE I CONF COMP VIS, P3376, DOI 10.1109/ICCV.2013.419; Xu Y, 2018, IEEE T GEOSCI REMOTE, V56, P1680, DOI 10.1109/TGRS.2017.2766094; Xu YC, 2021, IEEE T PATTERN ANAL, V43, P1452, DOI 10.1109/TPAMI.2020.2974745; Zhang JP, 2020, IEEE T GEOSCI REMOTE, V58, P6420, DOI 10.1109/TGRS.2020.2976855; Zhang JP, 2020, IEEE T GEOSCI REMOTE, V58, P2659, DOI 10.1109/TGRS.2019.2953181; Zhou TB, 2011, INT J NEPHROL, V2011, DOI 10.4061/2011/360357; Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132	45	5	5	12	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5185	5198		10.1109/TPAMI.2021.3066696	http://dx.doi.org/10.1109/TPAMI.2021.3066696			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33729927				2022-12-18	WOS:000836666600051
J	Xie, MK; Huang, SJ				Xie, Ming-Kun; Huang, Sheng-Jun			Partial Multi-Label Learning With Noisy Label Identification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Multi-lable learning; partial multi-label learning; candidate label set; noisy label identification; multi-instance multi-label learning		Partial multi-label learning (PML) deals with problems where each instance is assigned with a candidate label set, which contains multiple relevant labels and some noisy labels. Recent studies usually solve PML problems with the disambiguation strategy, which recovers ground-truth labels from the candidate label set by simply assuming that the noisy labels are generated randomly. In real applications, however, noisy labels are usually caused by some ambiguous contents of the example. Based on this observation, we propose a partial multi-label learning approach to simultaneously recover the ground-truth information and identify the noisy labels. The two objectives are formalized in a unified framework with trace norm and l(1) norm regularizers. Under the supervision of the observed noisecorrupted label matrix, the multi-label classifier and noisy label identifier are jointly optimized by incorporating the label correlation exploitation and feature-induced noise model. Furthermore, by mapping each bag to a feature vector, we extend PML-NI method into multi-instance multi-label learning by identifying noisy labels based on ambiguous instances. A theoretical analysis of generalization bound and extensive experiments on multiple data sets from various real-world tasks demonstrate the effectiveness of the proposed approach.	[Xie, Ming-Kun; Huang, Sheng-Jun] Nanjing Univ Aeronaut & Astronaut, MIIT Key Lab Pattern Anal & Machine Intelligence, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 211106, Peoples R China	Nanjing University of Aeronautics & Astronautics	Huang, SJ (corresponding author), Nanjing Univ Aeronaut & Astronaut, MIIT Key Lab Pattern Anal & Machine Intelligence, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 211106, Peoples R China.	mkxie@nuaa.edu.cn; huangsj@nuaa.edu.cn			National Key R&D Program of China [2020AAA0107000]; NSFC [62076128]; China University S&T Innovation Plan Guided by the Ministry of Education	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); China University S&T Innovation Plan Guided by the Ministry of Education	This work was supported by the National Key R&D Program of China (2020AAA0107000), NSFC (62076128) and the China University S&T Innovation Plan Guided by the Ministry of Education.	[Anonymous], EUR J NEUROSCI, DOI [10.1016/j.medj.2022.03.004, DOI 10.1016/0735-1933(85)90003-X]; Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009; Briggs F., 2012, P 18 ACM SIGKDD INT, P534, DOI DOI 10.1145/2339530.2339616; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Nguyen CT, 2014, AAAI CONF ARTIF INTE, P2013; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen Y., 2012, ADV NEURAL INFORM PR, V25, P1529; Chen ZS, 2020, AAAI CONF ARTIF INTE, V34, P3553; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Cour T, 2011, J MACH LEARN RES, V12, P1501; Demsar J, 2006, J MACH LEARN RES, V7, P1; Fang J-P, 2019, 33 AAAI C ART INT; Feng J, 2017, AAAI CONF ARTIF INTE, P1884; Feng L, 2019, AAAI CONF ARTIF INTE, P3542; FREY PW, 1991, MACH LEARN, V6, P161, DOI 10.1023/A:1022606404104; Grandvalet Y., 2004, CIRANO WORKING PAPER; Guo Y., 2011, PROC 22 INT JOINT C, P1300, DOI DOI 10.5591/978-1-57735-516-8/IJCA111-220; He S, 2019, IEEE DATA MINING, P280, DOI 10.1109/ICDM.2019.00038; Huang Sheng Jun, 2012, P 26 AAAI C ART INT, P949; Huang SJ, 2019, IEEE T PATTERN ANAL, V41, P2614, DOI 10.1109/TPAMI.2018.2861732; Hullermeier E, 2006, INTELL DATA ANAL, V10, P419, DOI 10.3233/IDA-2006-10503; JIN R, 2002, PROC ANN C NEURAL IN, V2, P897; Li Y.F., 2012, AAAI, P1012; Li YC, 2017, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2017.199; Lin J., 2018, P C EMP METH NAT LAN; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; Mohri M., 2018, FDN MACHINE LEARNING; Pei YL, 2014, PATTERN RECOGN LETT, V37, P107, DOI 10.1016/j.patrec.2013.07.002; Pham AT, 2017, IEEE T PATTERN ANAL, V39, P2381, DOI 10.1109/TPAMI.2017.2647944; Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; Sun LJ, 2019, AAAI CONF ARTIF INTE, P5016; Trohidis K., 2008, ISMIR, P325; Wang HB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3691; Winn J, 2005, IEEE I CONF COMP VIS, P1800; Wu JS, 2014, IEEE ACM T COMPUT BI, V11, P891, DOI 10.1109/TCBB.2014.2323058; Xie MK, 2020, AAAI CONF ARTIF INTE, V34, P6454; Xie MK, 2020, IEEE DATA MINING, P691, DOI 10.1109/ICDM50108.2020.00078; Xie MK, 2018, AAAI CONF ARTIF INTE, P4302; Yan Y., 2019, ARXIV 190906717; Yan Y, 2020, AAAI CONF ARTIF INTE, V34, P6575; Yang S. H., 2009, ADV NEURAL INFORM PR, P2143; Yu F, 2017, MACH LEARN, V106, P573, DOI 10.1007/s10994-016-5606-4; Yu GX, 2018, IEEE DATA MINING, P1398, DOI 10.1109/ICDM.2018.00192; Zha ZJ, 2008, PROC CVPR IEEE, P333; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhang ML, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1335, DOI 10.1145/2939672.2939788; Zhang ML, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4048; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang ML, 2010, PROC INT C TOOLS ART, P207, DOI 10.1109/ICTAI.2010.102; Zhang ML, 2009, NEUROCOMPUTING, V72, P3951, DOI 10.1016/j.neucom.2009.07.008; Zhang QW, 2018, AAAI CONF ARTIF INTE, P4446; Zhou Z.H., 2007, NIPS 19, P1609, DOI DOI 10.1016/J.PATCOG.2006.12.019; Zhou ZH, 2012, ARTIF INTELL, V176, P2291, DOI 10.1016/j.artint.2011.10.002; Zhu Y, 2017, AAAI CONF ARTIF INTE, P2977	56	5	5	13	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3676	3687		10.1109/TPAMI.2021.3059290	http://dx.doi.org/10.1109/TPAMI.2021.3059290			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33587695				2022-12-18	WOS:000805820500025
J	Liu, YL; Chen, G; Knoll, A				Liu, Yinlong; Chen, Guang; Knoll, Alois			Globally Optimal Vertical Direction Estimation in Atlanta World	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Estimation; Robustness; Three-dimensional displays; Optimization; Clustering algorithms; Inference algorithms; Search problems; Global optimization; branch-and-bound; rotation search; imaging geometry	CALIBRATION; SPACE	In man-made environments, most of the objects and structures are organized in the form of orthogonal and parallel planes. These planes can be approximated by an Atlanta world assumption, in which the normals of planes can be represented by Atlanta frames. The Atlanta world assumption has one vertical frame and multiple horizontal frames. Conventionally, given a set of inputs such as surface normals, the Atlanta frame estimation problem can be solved by a branch-and-bound (BnB) algorithm. However, the runtime of the BnB algorithm will increase greatly when the dimensionality (i.e., the number of horizontal frames) increases. In this paper, we estimate only the vertical direction, instead of all Atlanta frames at once. Accordingly, we propose a vertical direction estimation method by considering the relationship between the vertical frame and horizontal frames. Concretely, our approach employs a BnB algorithm to search the vertical direction, thereby guaranteeing global optimality without requiring prior knowledge of the number of Atlanta frames. In order to guarantee convergence, four novel bounds are investigated, by mapping a 3D hemisphere to a 2D region. We verify the feasibility of the proposed method using various challenging synthetic and real-world data.	[Liu, Yinlong; Knoll, Alois] Tech Univ Munich, Dept Informat, D-85748 Munich, Germany; [Chen, Guang] Tongji Univ, Shanghai 200092, Peoples R China; [Chen, Guang] Tech Univ Munich, D-85748 Munich, Germany	Technical University of Munich; Tongji University; Technical University of Munich	Chen, G (corresponding author), Tongji Univ, Shanghai 200092, Peoples R China.	Yinlong.Liu@tum.de; guangchen@tongji.edu.cn; knoll@in.tum.de			European Union [945539]; National Natural Science Foundation of China [61906138]; Shanghai AI Innovative Development Project 2018; Chinese Scholarship Council (CSC)	European Union(European Commission); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai AI Innovative Development Project 2018; Chinese Scholarship Council (CSC)(China Scholarship Council)	This work was supported by the European Union's Horizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreement No. 945539 (Human Brain Project SGA3), from the National Natural Science Foundation of China (No. 61906138), and from the Shanghai AI Innovative Development Project 2018. The work of Yinlong Liu was supported by Chinese Scholarship Council (CSC).	Abbaspour H., 2007, BASIC LIE THEORY; Amayo P, 2018, PROC CVPR IEEE, P8138, DOI 10.1109/CVPR.2018.00849; Antunes M, 2013, PROC CVPR IEEE, P1336, DOI 10.1109/CVPR.2013.176; Barath D, 2018, LECT NOTES COMPUT SC, V11220, P229, DOI 10.1007/978-3-030-01270-0_14; Bazin JC, 2012, PROC CVPR IEEE, P638, DOI 10.1109/CVPR.2012.6247731; Bazin Jean-Charles, 2012, P AS C COMP VIS, P2; Bishop C.M, 2006, PATTERN RECOGN; Bustos AP, 2018, IEEE T PATTERN ANAL, V40, P2868, DOI 10.1109/TPAMI.2017.2773482; Bustos AP, 2016, IEEE T PATTERN ANAL, V38, P2227, DOI 10.1109/TPAMI.2016.2517636; Cai ZP, 2019, ISPRS J PHOTOGRAMM, V147, P118, DOI 10.1016/j.isprsjprs.2018.11.016; Campbell D, 2020, IEEE T PATTERN ANAL, V42, P328, DOI 10.1109/TPAMI.2018.2848650; Chin TJ, 2018, LECT NOTES COMPUT SC, V11216, P715, DOI 10.1007/978-3-030-01258-8_43; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fisher N.I., 1995, STAT ANAL CIRCULAR D; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411; Heller J, 2016, IEEE T PATTERN ANAL, V38, P1027, DOI 10.1109/TPAMI.2015.2469299; Johnson D. S., 1977, THEOR COMPUT SCI, V6, P93; Joo K, 2020, IEEE T PATTERN ANAL, V42, P2656, DOI 10.1109/TPAMI.2019.2909863; Joo K, 2018, PROC CVPR IEEE, P5726, DOI 10.1109/CVPR.2018.00600; Joo K, 2019, IEEE T PATTERN ANAL, V41, P682, DOI 10.1109/TPAMI.2018.2799944; Kim S, 2017, IEEE WINT CONF APPL, P11, DOI 10.1109/WACV.2017.9; Lee GH, 2017, INT CONF 3D VISION, P584, DOI 10.1109/3DV.2017.00072; Magri L, 2016, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2016.361; Magri L, 2014, PROC CVPR IEEE, P3954, DOI 10.1109/CVPR.2014.505; Morrison DR, 2016, DISCRETE OPTIM, V19, P79, DOI 10.1016/j.disopt.2016.01.005; Needham Tristan, 1998, VISUAL COMPLEX ANAL; Parra A. ~, 2016, THESIS U ADELAIDE AD, DOI [10.4225/55/5823d9e099879, DOI 10.4225/55/5823D9E099879]; Schindler G, 2004, PROC CVPR IEEE, P203; Seo Y, 2009, IEEE I CONF COMP VIS, P1173, DOI 10.1109/ICCV.2009.5459343; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sola J., 2020, ARXIV181201537; Straub J, 2018, IEEE T PATTERN ANAL, V40, P235, DOI 10.1109/TPAMI.2017.2662686; Straub J, 2017, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2017.258; Sunderhauf N, 2012, IEEE INT C INT ROBOT, P1879, DOI 10.1109/IROS.2012.6385590; Tardif JP, 2009, IEEE I CONF COMP VIS, P1250, DOI 10.1109/ICCV.2009.5459328; Taylor C., 2012, P ROB SCI SYST, P401; Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41; Wilkins D. R., MOBIUS TRANSFORMATIO; Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405; Yang JL, 2014, LECT NOTES COMPUT SC, V8689, P111, DOI 10.1007/978-3-319-10590-1_8; Yu L, 2007, IEEE INT C BIOINFORM, P9, DOI 10.1109/BIBM.2007.19; Zhou HZ, 2015, IEEE T VEH TECHNOL, V64, P1364, DOI 10.1109/TVT.2015.2388780	45	5	5	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					1949	1962		10.1109/TPAMI.2020.3027047	http://dx.doi.org/10.1109/TPAMI.2020.3027047			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	32986545	Green Submitted			2022-12-18	WOS:000764815300022
J	Ma, QL; Li, S; Cottrell, GW				Ma, Qianli; Li, Sen; Cottrell, Garrison W.			Adversarial Joint-Learning Recurrent Neural Network for Incomplete Time Series Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Time series analysis; Recurrent neural networks; Data models; Training; Analytical models; Sensors; Computer science; Incomplete time series classification; recurrent neural networks; adversarial learning; exploding error	MODELS	Incomplete time series classification (ITSC) is an important issue in time series analysis since temporal data often has missing values in practical applications. However, integrating imputation (replacing missing data) and classification within a model often rapidly amplifies the error from imputed values. Reducing this error propagation from imputation to classification remains a challenge. To this end, we propose an adversarial joint-learning recurrent neural network (AJ-RNN) for ITSC, an end-to-end model trained in an adversarial and joint learning manner. We train the system to categorize the time series as well as impute missing values. To alleviate the error introduced by each imputation value, we use an adversarial network to encourage the network to impute realistic missing values by distinguishing real and imputed values. Hence, AJ-RNN can directly perform classification with missing values and greatly reduce the error propagation from imputation to classification, boosting the accuracy. Extensive experiments on 68 synthetic datasets and 4 real-world datasets from the expanded UCR time series archive demonstrate that AJ-RNN achieves state-of-the-art performance. Furthermore, we show that our model can effectively alleviate the accumulating error problem through qualitative and quantitative analysis based on the trajectory of the dynamical system learned by the RNN. We also provide an analysis of the model behavior to verify the effectiveness of our approach.	[Ma, Qianli; Li, Sen] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China; [Cottrell, Garrison W.] Univ Calif San Diego, Dept Comp Sci & Engn, San Diego, CA 92093 USA	South China University of Technology; University of California System; University of California San Diego	Ma, QL (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.	qianlima@scut.edu.cn; awslee@foxmail.com; gary@ucsd.edu			National Natural Science Foundation of China [61502174, 61872148]; Natural Science Foundation of Guangdong Province [2017A030313355, 2019A1515010768]; Guangzhou Science and Technology Planning Project [201704030051, 201902010020]; Key R&D Program of Guangdong Province [2018B010107002]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); Guangzhou Science and Technology Planning Project; Key R&D Program of Guangdong Province	This work was supported by the National Natural Science Foundation of China (Grants 61502174 and 61872148), the Natural Science Foundation of Guangdong Province (Grants 2017A030313355, 2019A1515010768), the Guangzhou Science and Technology Planning Project (Grants 201704030051 and 201902010020), and the Key R&D Program of Guangdong Province (Grant 2018B010107002). The authors would like to thank the anonymous reviewers for their helpful feedbacks. They would also thank Kejian Shi and Chuxin Chen for their experimental support on this work. Qianli Ma and Sen Li equally contributed to this work.	Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/2951913.2976746, 10.1145/3022670.2976746]; Anwar T, 2018, IEEE T KNOWL DATA EN, V30, P1426, DOI 10.1109/TKDE.2018.2795001; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Berndt D. J., 1994, P 3 INT C KNOWL DISC, P359; Cai ZW, 2000, J AM STAT ASSOC, V95, P941, DOI 10.2307/2669476; Cao W., 2018, ADV NEURAL INFORM PR, V31, P6774; Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9; Cheema JR, 2014, REV EDUC RES, V84, P487, DOI 10.3102/0034654314532697; Cho K., 2014, P 2014 C EMP METH NA, P1724; de Souto MCP, 2015, BMC BIOINFORMATICS, V16, DOI 10.1186/s12859-015-0494-3; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Demsar J, 2006, J MACH LEARN RES, V7, P1; Fedus William, 2018, INT C LEARN REPR; Ganin Y, 2016, J MACH LEARN RES, V17; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Dau HA, 2019, IEEE-CAA J AUTOMATIC, V6, P1293, DOI 10.1109/JAS.2019.1911747; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hou CP, 2017, IEEE T KNOWL DATA EN, V29, P1998, DOI 10.1109/TKDE.2017.2681670; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Kingma D.P, P 3 INT C LEARNING R; Laptev N., 2017, INT C MACH LEARN TIM, V34, P1; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lei H, 2016, ANN STAT, V44, P1618, DOI 10.1214/15-AOS1430; Li J, 2021, IEEE T DEPEND SECURE, V18, P460, DOI [10.1109/TPAMI.2019.2929036, 10.1109/TDSC.2019.2894411]; Li L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507; Li S., 2015, ARXIV151003164; Li Shuai, 2016, ARXIV160500596; Lin SD, 2018, IEEE T NEUR NET LEAR, V29, P4709, DOI 10.1109/TNNLS.2017.2772336; Little RJA, 1987, STAT ANAL MISSING DA; Luo YH, 2018, ADV NEUR IN, V31; Ma QL, 2020, IEEE T CYBERNETICS, V50, P4908, DOI 10.1109/TCYB.2019.2906426; Marjaninejad A, 2017, IEEE ENG MED BIO, P986, DOI 10.1109/EMBC.2017.8036991; Rakthanmanon T, 2013, ACM T KNOWL DISCOV D, V7, DOI 10.1145/2500489; RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.2307/2335739; Schneider T, 2001, J CLIMATE, V14, P853, DOI 10.1175/1520-0442(2001)014<0853:AOICDE>2.0.CO;2; Shang C, 2017, IEEE INT CONF BIG DA, P766, DOI 10.1109/BigData.2017.8257992; Shen L., 2018, ASIAN C MACHINE LEAR, P248; Shi H, 2018, IEEE T SMART GRID, V9, P5271, DOI 10.1109/TSG.2017.2686012; Shi L, 2019, IEEE T KNOWL DATA EN, V31, P1094, DOI 10.1109/TKDE.2018.2854193; Silva I, 2012, COMPUT CARDIOL CONF, V39, P245; Spiotta M, 2017, IEEE T KNOWL DATA EN, V29, P2567, DOI 10.1109/TKDE.2017.2734084; TJOSTHEIM D, 1994, J AM STAT ASSOC, V89, P1398, DOI 10.2307/2291002; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422; Yoon J, 2018, PR MACH LEARN RES, V80; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852	49	5	5	19	45	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					1765	1776		10.1109/TPAMI.2020.3027975	http://dx.doi.org/10.1109/TPAMI.2020.3027975			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	32997624				2022-12-18	WOS:000764815300010
J	Ding, CX; Wang, K; Wang, PF; Tao, DC				Ding, Changxing; Wang, Kan; Wang, Pengfei; Tao, Dacheng			Multi-Task Learning With Coarse Priors for Robust Part-Aware Person Re-Identification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Training; Task analysis; Robustness; Testing; Tools; Electronic mail; Person re-identification; part-based models; misalignment; multi-task learning	NETWORK	Part-level representations are important for robust person re-identification (ReID), but in practice feature quality suffers due to the body part misalignment problem. In this paper, we present a robust, compact, and easy-to-use method called the Multi-task Part-aware Network (MPN), which is designed to extract semantically aligned part-level features from pedestrian images. MPN solves the body part misalignment problem via multi-task learning (MTL) in the training stage. More specifically, it builds one main task (MT) and one auxiliary task (AT) for each body part on the top of the same backbone model. The ATs are equipped with a coarse prior of the body part locations for training images. ATs then transfer the concept of the body parts to the MTs via optimizing the MT parameters to identify part-relevant channels from the backbone model. Concept transfer is accomplished by means of two novel alignment strategies: namely, parameter space alignment via hard parameter sharing and feature space alignment in a class-wise manner. With the aid of the learned high-quality parameters, MTs can independently extract semantically aligned part-level features from relevant channels in the testing stage. MPN has three key advantages: 1) it does not need to conduct body part detection in the inference stage; 2) its model is very compact and efficient for both training and testing; 3) in the training stage, it requires only coarse priors of body part locations, which are easy to obtain. Systematic experiments on four large-scale ReID databases demonstrate that MPN consistently outperforms state-of-the-art approaches by significant margins.	[Ding, Changxing; Wang, Kan; Wang, Pengfei] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Guangdong, Peoples R China; [Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia	South China University of Technology; University of Sydney	Ding, CX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Guangdong, Peoples R China.	chxding@scut.edu.cn; eekan.wang@mail.scut.edu.cn; mswangpengfei@mail.scut.edu.cn; dacheng.tao@sydney.edu.au		Wang, Kan/0000-0003-3133-8891; Wang, Pengfei/0000-0002-3675-9508	National Natural Science Foundation of China [61702193, 62076101]; Science and Technology Program of Guangzhou [201804010272]; Program for Guangdong Introducing Innovative and Entrepreneurial Teams [2017ZT07X183]; Fundamental Research Funds for the Central Universities of China [2019JQ01]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Program of Guangzhou; Program for Guangdong Introducing Innovative and Entrepreneurial Teams; Fundamental Research Funds for the Central Universities of China(Fundamental Research Funds for the Central Universities)	This work was supported by the National Natural Science Foundation of China under Grants 61702193 and 62076101, the Science and Technology Program of Guangzhou under Grant 201804010272, the Program for Guangdong Introducing Innovative and Entrepreneurial Teams under Grant 2017ZT07X183, and the Fundamental Research Funds for the Central Universities of China under Grant 2019JQ01. Changxing Ding and Kan Wang contributed equally to this work.	Alemu LT, 2019, IEEE I CONF COMP VIS, P9854, DOI 10.1109/ICCV.2019.00995; Bai S, 2017, AAAI CONF ARTIF INTE, P1281; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902; Chen WH, 2017, AAAI CONF ARTIF INTE, P3988; Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Chi C, 2020, AAAI CONF ARTIF INTE, V34, P10639; Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Fu Y, 2019, AAAI CONF ARTIF INTE, P8295; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854; He XM, 2020, IEEE T EMERG TOP COM, V8, P781, DOI 10.1109/TETC.2018.2805718; Hermans Alexander, 2017, ARXIV170307737; Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954; Hu J., 2018, PROC CVPR IEEE, P7132, DOI [DOI 10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Ioffe S., 2015, INT C MACH LEARN, P448, DOI [10.5555/3045118.3045167, DOI 10.5555/3045118.3045167]; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Karanam S, 2019, IEEE T PATTERN ANAL, V41, P523, DOI 10.1109/TPAMI.2018.2807450; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li J., 2019, IEEE T PATTERN ANAL, P8649; Li K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1492, DOI 10.1145/3240508.3240674; Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058; Li S, 2018, IEEE T PATTERN ANAL, V40, P2963, DOI 10.1109/TPAMI.2017.2764893; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Liao YY, 2016, IEEE INT CONF ROBOT, P2318, DOI 10.1109/ICRA.2016.7487381; Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055; Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46; Long MS, 2015, PR MACH LEARN RES, V37, P97; Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508; Matsukawa T., 2019, IEEE T PATTERN ANAL, P1363; McLaughlin N, 2017, IEEE T CIRC SYST VID, V27, P525, DOI 10.1109/TCSVT.2016.2619498; Nair V., 2010, PROC INT C MACH LEAR, P803; Qian XL, 2020, IEEE T PATTERN ANAL, V42, P371, DOI 10.1109/TPAMI.2019.2928294; Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385; Ren P., 2018, ARXIV181108073; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Ro Y, 2019, AAAI CONF ARTIF INTE, P8859; Ruder S., 2017, ARXIV; Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Su C, 2018, IEEE T PATTERN ANAL, V40, P1167, DOI 10.1109/TPAMI.2017.2679002; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23; Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552; Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242; Wang K, 2020, IEEE T IMAGE PROCESS, V29, P3416, DOI 10.1109/TIP.2019.2959923; Wang Y, 2018, PROC CVPR IEEE, P8042, DOI 10.1109/CVPR.2018.00839; Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226; Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30; Zhang X, 2017, ARXIV170804896; Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325; Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076; Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26; Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871; Zheng L., 2016, ARXIV PREPRINT ARXIV; Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414; Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; Zhou SP, 2019, IEEE T IMAGE PROCESS, V28, P4671, DOI 10.1109/TIP.2019.2908065; Zhu FQ, 2017, IEEE T IMAGE PROCESS, V26, P4806, DOI 10.1109/TIP.2017.2695101	86	5	5	4	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1474	1488		10.1109/TPAMI.2020.3024900	http://dx.doi.org/10.1109/TPAMI.2020.3024900			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32946381	Green Accepted			2022-12-18	WOS:000752018000029
J	Hinz, T; Heinrich, S; Wermter, S				Hinz, Tobias; Heinrich, Stefan; Wermter, Stefan			Semantic Object Accuracy for Generative Text-to-Image Synthesis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Layout; Semantics; Gallium nitride; Measurement; Generators; Image resolution; Image quality; Text-to-image synthesis; generative adversarial network (GAN); evaluation of generative models; generative models		Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g., whether an image generated from "a car driving down the street" contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.	[Hinz, Tobias; Heinrich, Stefan; Wermter, Stefan] Univ Hamburg, Knowledge Technol Grp, D-22527 Hamburg, Germany	University of Hamburg	Hinz, T (corresponding author), Univ Hamburg, Knowledge Technol Grp, D-22527 Hamburg, Germany.	hinz@informatik.uni-hamburg.de; heinrich@informatik.uni-hamburg.de; wermter@informatik.uni-hamburg.de		Wermter, Stefan/0000-0003-1343-4775; Hinz, Tobias/0000-0003-1354-1562	German Research Foundation DFG under project CML [TRR 169]; NVIDIA Corporation through the GPU Grant Program	German Research Foundation DFG under project CML(German Research Foundation (DFG)); NVIDIA Corporation through the GPU Grant Program	The authors would like to thank the partial support from the German Research Foundation DFG under project CML (TRR 169). They would also like to thank the NVIDIA Corporation for their support through the GPU Grant Program.	Agarwal P., 2020, PROC ICLR WORKSHOP M; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Barratt S., 2018, ARXIV180101973; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877; Cartillier V., 2017, PROC ADV NEURAL INF; Cha M, 2019, AAAI CONF ARTIF INTE, P3272; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Cheng Y, 2018, ARXIV 181208352; Deng ZW, 2018, ADV NEUR IN, V31; EFRON B, 1979, ANN STAT, V7, P1, DOI 10.1214/aos/1176344552; El-Nouby A, 2019, IEEE I CONF COMP VIS, P10303, DOI 10.1109/ICCV.2019.01040; Geirhos R., 2019, PROC INT C LEARN REP; Gimenez J., 2007, P ACL WORKSH, P256; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hensel M, 2017, ADV NEUR IN, V30; Hinz T., 2019, P INT C LEARN REPR; Hong S., 2018, PROC ADV NEURAL INF, P2712; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; Huang W., 2019, ASIAN C MACHINE LEAR, P284; Huang X, 2019, 2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019), P73, DOI 10.1109/CRV.2019.00018; Jaderberg M, 2015, ADV NEUR IN, V28; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Jyothi AA, 2019, IEEE I CONF COMP VIS, P9894, DOI 10.1109/ICCV.2019.00999; Karacan L., 2016, ARXIV161200215; Kilickaya M, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P199; Lee D., 2018, PROC ADV NEURAL INF, P423; Li BR, 2019, IEEE I CONF COMP VIS, P7434, DOI 10.1109/ICCV.2019.00753; Li BW, 2019, ADV NEUR IN, V32; LI J., 2019, INT C LEARN REPR; Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514; Li YK, 2019, ADV NEUR IN, V32; Li YT, 2019, PROC CVPR IEEE, P6322, DOI 10.1109/CVPR.2019.00649; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu XH, 2019, ADV NEUR IN, V32; Madhyastha P, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6539; Mirza M., 2014, ARXIV PREPRINT ARXIV; Mittal G., 2019, PROC INT C LEARN REP; Nam S, 2018, ADV NEUR IN, V31; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Qiao TT, 2019, ADV NEUR IN, V32; Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160; Ravuri S., 2019, PROC ADV NEURAL INF, P1268; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2016, PR MACH LEARN RES, V48; Sah S, 2018, IEEE IMAGE PROC, P3783, DOI 10.1109/ICIP.2018.8451656; Salimans T., 2016, ADV NEUR IN, P2234; Sharma S., 2018, PROC INT C LEARN REP; Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14; Sun W, 2019, IEEE I CONF COMP VIS, P10530, DOI 10.1109/ICCV.2019.01063; Theis L., 2016, PROC INT C LEARN REP; van den Oord A., 2016, GENERATING INTERPRET; Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640; Vo D. M, 2019, ARXIV 190801741; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243; Yu SM, 2019, IEEE IMAGE PROC, P734, DOI 10.1109/ICIP.2019.8804285; Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649; Zhao B, 2019, IEEE COMPUT SOC CONF, P398, DOI [10.1109/CVPRW.2019.00053, 10.1109/CVPR.2019.00878]; Zhou XR, 2019, PROC CVPR IEEE, P3658, DOI 10.1109/CVPR.2019.00378; Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595	65	5	5	4	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1552	1565		10.1109/TPAMI.2020.3021209	http://dx.doi.org/10.1109/TPAMI.2020.3021209			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32877332	hybrid, Green Submitted			2022-12-18	WOS:000752018000034
J	Chen, X; Chen, SH; Yao, JC; Zheng, HJ; Zhang, Y; Tsang, IW				Chen, Xu; Chen, Siheng; Yao, Jiangchao; Zheng, Huangjie; Zhang, Ya; Tsang, Ivor W.			Learning on Attribute-Missing Graphs	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						graph neural network; attribute-missing graphs; distribution matching; node attribute completion; node classification; link prediction		Graphs with complete node attributes have been widely explored recently. While in practice, there is a graph where attributes of only partial nodes could be available and those of the others might be entirely missing. This attribute-missing graph is related to numerous real-world applications and there are limited studies investigating the corresponding learning problems. Existing graph learning methods including the popular GNN cannot provide satisfied learning performance since they are not specified for attribute-missing graphs. Thereby, designing a new GNN for these graphs is a burning issue to the graph learning community. In this article, we make a shared-latent space assumption on graphs and develop a novel distribution matching-based GNN called structure-attribute transformer (SAT) for attribute-missing graphs. SAT leverages structures and attributes in a decoupled scheme and achieves the joint distribution modeling of structures and attributes by distribution matching techniques. It could not only perform the link preciction task but also the newly introduced node attnbute completion task. Furthermore, practical measures are introduced to quantify the performance of node attribute completion. Extensive experiments on seven real-world datasets indicate SAT shows better performance than other methods on both link prediction and node attribute completion tasks.	[Chen, Xu; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China; [Chen, Xu; Zhang, Ya] Shanghai Jiao Tong Univ, Shanghai Key Lab Multimedia Proc & Transmiss, Shanghai 200240, Peoples R China; [Chen, Xu] Univ Technol Sydney, Australian Artificial Intelligence Inst AAII, 15 Broadway, Sydney, Australia; [Chen, Siheng] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA; [Yao, Jiangchao] Alibaba Grp, Hangzhou, Peoples R China; [Zheng, Huangjie] Texas Univ, Austin, TX 78712 USA; [Tsang, Ivor W.] Univ Technol Sydney, 15 Broadway, Sydney, NSW 2007, Australia; [Tsang, Ivor W.] Australian Artificial Intelligence Inst AAII, Sydney, NSW, Australia	Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of Technology Sydney; Alibaba Group; University of Technology Sydney	Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.; Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Shanghai Key Lab Multimedia Proc & Transmiss, Shanghai 200240, Peoples R China.	xuchen2016@sjtu.edu.cn; schen@merl.com; jiangchao.yjc@alibaba-inc.com; huangjie.zheng@utexas.edu; ya_zhang@sjtu.edu.cn; ivor.tsang@uts.edu.au	Zheng, Huangjie/AAD-9157-2022; Tsang, Ivor/E-8653-2011	Zheng, Huangjie/0000-0003-0508-5034; Tsang, Ivor/0000-0003-2211-8176; Chen, Xu/0000-0001-5299-7074	National Key Research and Development Program of China [2019YFB1804304]; SHEITC [2018-RGZN-02046]; 111 plan [BP0719010]; STCSM [18DZ2270700]; State Key Laboratory of UHD Video and Audio Production and Presentation; ARC of Australia [DP180100106, DP200101328]	National Key Research and Development Program of China; SHEITC; 111 plan; STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); State Key Laboratory of UHD Video and Audio Production and Presentation; ARC of Australia(Australian Research Council)	This work was supported by the National Key Research and Development Program of China (under Grant No. 2019YFB1804304), SHEITC (under Grant No. 2018-RGZN-02046), 111 plan (under Grant No. BP0719010), and STCSM (under Grant No. 18DZ2270700), and State Key Laboratory of UHD Video and Audio Production and Presentation. This work was also supported by ARC DP180100106 and DP200101328 of Australia.	Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bishop C.M, 2006, PATTERN RECOGN; Bojchevski A, 2018, PR MACH LEARN RES, V80; Cao ZJ, 2018, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR.2018.00288; Chen X, 2016, ADV NEUR IN, V29; Chenane JL, 2020, POLIC SOC, V30, P720, DOI 10.1080/10439463.2019.1587436; De Cao Nicola, 2018, ARXIV180511973; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Ganin Y, 2016, J MACH LEARN RES, V17; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hu L, 2019, AAAI CONF ARTIF INTE, P3830; Huang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P732, DOI 10.1145/3292500.3330941; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jin WG, 2018, PR MACH LEARN RES, V80; Kingma D. P, 2014, ARXIV13126114; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Lee J, 2019, PR MACH LEARN RES, V97; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371; Lindvall T., 2002, LECT COUPLING METHOD; Liu MY, 2017, ADV NEUR IN, V30; Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685; Lu LY, 2015, P NATL ACAD SCI USA, V112, P2325, DOI 10.1073/pnas.1424644112; Makhzani A., 2018, ARXIV180509804; Makhzani Alireza, 2016, ICLR WORKSH; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988; Mescheder L, 2017, PR MACH LEARN RES, V70; Mikolov T., 2013, 9 INT C LEARN REPR, P1; Monti F, 2017, ADV NEUR IN, V30; Namata Galileo, 2012, P 10 INT WORKSHOP MI, V8, P1; Nowozin S, 2016, ADV NEUR IN, V29; Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732; Pitner G, 2020, INT EL DEVICES MEET, DOI 10.1109/IEDM13553.2020.9371899; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rosca M., 2018, ARXIV PREPRINT ARXIV; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shchur O., 2018, ARXIV181105868; Simsek O, 2008, P NATL ACAD SCI USA, V105, P12758, DOI 10.1073/pnas.0800497105; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Taheri A., 2018, KDD DEEP LEARNING DA, P1; Tomczak JM, 2018, PR MACH LEARN RES, V84; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van der Merwe R, 2019, ARCH REC, V40, P239, DOI 10.1080/23257962.2017.1388224; Velickovic P., 2018, P INT C LEARN REPR; Wang HW, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P417, DOI 10.1145/3269206.3271739; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989; Wu F, 2019, PR MACH LEARN RES, V97; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang Q., 2019, ADV KNOWLEDGE DISCOV, V11441; Yin MZ, 2018, PR MACH LEARN RES, V80; Ying R, 2018, ADV NEUR IN, V31; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890; You JX, 2018, PR MACH LEARN RES, V80; Zhao SJ, 2019, AAAI CONF ARTIF INTE, P5885; Zheng H., 2018, ARXIV180206677; Zheng HJ, 2019, AAAI CONF ARTIF INTE, P5917; Zhou J. T., 2019, IEEE T PATTERN ANAL; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	67	5	5	9	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					740	757		10.1109/TPAMI.2020.3032189	http://dx.doi.org/10.1109/TPAMI.2020.3032189			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	33074805	Green Submitted			2022-12-18	WOS:000740006100016
J	Santo, H; Samejima, M; Sugano, Y; Shi, BX; Matsushita, Y				Santo, Hiroaki; Samejima, Masaki; Sugano, Yusuke; Shi, Boxin; Matsushita, Yasuyuki			Deep Photometric Stereo Networks for Determining Surface Normal and Reflectances	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Photometric stereo; surface normal; bidirectional reflectance distribution functions (BRDFs); deep learning	NEURAL-NETWORKS; RECONSTRUCTION; REFLECTION; SHAPE	This article presents a photometric stereo method based on deep learning. One of the major difficulties in photometric stereo is designing an appropriate reflectance model that is both capable of representing real-world reflectances and computationally tractable for deriving surface normal. Unlike previous photometric stereo methods that rely on a simplified parametric image formation model, such as the Lambert's model, the proposed method aims at establishing a flexible mapping between complex reflectance observations and surface normal using a deep neural network. In addition, the proposed method predicts the reflectance, which allows us to understand surface materials and to render the scene under arbitrary lighting conditions. As a result, we propose a deep photometric stereo network (DPSN) that takes reflectance observations under varying light directions and infers the surface normal and reflectance in a per-pixel manner. To make the DPSN applicable to real-world scenes, a dataset of measured BRDFs (MERL BRDF dataset) has been used for training the network. Evaluation using simulation and real-world scenes shows the effectiveness of the proposed approach in estimating both surface normal and reflectances.	[Santo, Hiroaki; Samejima, Masaki; Matsushita, Yasuyuki] Osaka Univ, Grad Sch Informat Sci & Technol, Osaka 5650871, Japan; [Sugano, Yusuke] Univ Tokyo, Inst Ind Sci, Tokyo 1138654, Japan; [Shi, Boxin] Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Shi, Boxin] Peking Univ, Inst Artificial Intelligence, Beijing 100871, Peoples R China	Osaka University; University of Tokyo; Peking University; Peking University	Santo, H (corresponding author), Osaka Univ, Grad Sch Informat Sci & Technol, Osaka 5650871, Japan.	santo.hiroaki@ist.osaka-u.ac.jp; samejima@ist.osaka-u.ac.jp; sugano@iis.u-tokyo.ac.jp; shiboxin@pku.edu.cn; yasumat@ist.osaka-u.ac.jp		Santo, Hiroaki/0000-0003-2891-5993; Samejima, Masaki/0000-0002-9704-2711; Matsushita, Yasuyui/0000-0002-1935-4752	JSPS KAKENHI [JP19H01123]; Japan Society for the Promotion of Science [JP19J10326]; National Natural Science Foundation of China [61872012]; National Key R&D Program of China [2019YFF0302902]; Beijing Academy of Artificial Intelligence (BAAI)	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Japan Society for the Promotion of Science(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; Beijing Academy of Artificial Intelligence (BAAI)	This work was supported by JSPS KAKENHI Grant JP19H01123. Hiroaki Santo was grateful for support through a JSPS research fellowship for young scientists by the Japan Society for the Promotion of Science (JP19J10326). Boxin Shi was supported by the National Natural Science Foundation of China under Grants 61872012, National Key R&D Program of China (2019YFF0302902), and Beijing Academy of Artificial Intelligence (BAAI).	Alldrin N, 2008, PROC CVPR IEEE, P2447; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1; Chen L, 2017, IEEE I CONF COMP VIS, P3181, DOI 10.1109/ICCV.2017.343; Cheng WC, 2006, IEEE IJCNN, P404; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378; Ding Y, 2009, LECT NOTES ARTIF INT, V5712, P705, DOI 10.1007/978-3-642-04592-9_87; Elizondo D, 2008, LECT NOTES COMPUT SC, V5163, P857, DOI 10.1007/978-3-540-87536-9_88; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Georghiades AS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P816; Goldman D. B., 2009, IEEE T PATTERN ANAL, V32, P1060, DOI DOI 10.1109/TPAMI.2009.102; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084; Holroyd M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409086; Hui Z, 2017, IEEE T PATTERN ANAL, V39, P2060, DOI 10.1109/TPAMI.2016.2623613; Ikehata S, 2018, LECT NOTES COMPUT SC, V11219, P3, DOI 10.1007/978-3-030-01267-0_1; Ikehata S, 2014, PROC CVPR IEEE, P2187, DOI 10.1109/CVPR.2014.280; Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691; IWAHORI Y, 1993, IEEE IJCNN, P1181; Janner M, 2017, ADV NEUR IN, V30; Johnson M. K., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2553, DOI 10.1109/CVPR.2011.5995510; Kim K, 2017, IEEE I CONF COMP VIS, P20, DOI 10.1109/ICCV.2017.12; Kingma D.P, P 3 INT C LEARNING R; Lambert J. H., 1760, PHOTOMETRIA; Li JX, 2019, PROC CVPR IEEE, P7560, DOI 10.1109/CVPR.2019.00775; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; Miyazaki D, 2010, INT J COMPUT VISION, V86, P229, DOI 10.1007/s11263-009-0262-9; Mukaigawa Y, 2007, J OPT SOC AM A, V24, P3326, DOI 10.1364/JOSAA.24.003326; Nielsen JB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818085; Papadhimitri T, 2014, INT J COMPUT VISION, V107, P139, DOI 10.1007/s11263-013-0665-5; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037; Rematas K, 2016, PROC CVPR IEEE, P4508, DOI 10.1109/CVPR.2016.488; Ruiters R, 2009, COMPUT GRAPH FORUM, V28, P513, DOI 10.1111/j.1467-8659.2009.01390.x; Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66; Sato Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P379, DOI 10.1145/258734.258885; Shi BX, 2016, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2016.403; Shi BX, 2012, LECT NOTES COMPUT SC, V7574, P455, DOI 10.1007/978-3-642-33712-3_33; Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196; Silver W.M., 1980, THESIS MIT; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740; Taniai T, 2018, PR MACH LEARN RES, V80; Tominaga S, 2000, IEEE COMPUT GRAPH, V20, P58, DOI 10.1109/38.865881; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Wang Oliver, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2805, DOI 10.1109/CVPRW.2009.5206558; Wang TC, 2016, LECT NOTES COMPUT SC, V9907, P121, DOI 10.1007/978-3-319-46487-9_8; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Wu TP, 2010, IEEE T PATTERN ANAL, V32, P546, DOI 10.1109/TPAMI.2009.15; Xu ZX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982396; Zhou ZL, 2013, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2013.195	55	5	5	4	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					114	128		10.1109/TPAMI.2020.3005219	http://dx.doi.org/10.1109/TPAMI.2020.3005219			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750795	hybrid			2022-12-18	WOS:000728561300010
J	Su, R; Xu, D; Zhou, LP; Ouyang, WL				Su, Rui; Xu, Dong; Zhou, Luping; Ouyang, Wanli			Progressive Cross-Stream Cooperation in Spatial and Temporal Domain for Action Localization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Proposals; Detectors; Feature extraction; Task analysis; Training; Motion segmentation; Electron tubes; Action localization; spatio-temporal action localization; two-stream cooperation		Spatio-temporal action localization consists of three levels of tasks: spatial localization, action classification, and temporal localization. In this work, we propose a new progressive cross-stream cooperation (PCSC) framework that improves all three tasks above. The basic idea is to utilize both spatial region (resp., temporal segment proposals) and features from one stream (i.e., the Flow/RGB stream) to help another stream (i.e., the RGB/Flow stream) to iteratively generate better bounding boxes in the spatial domain (resp., temporal segments in the temporal domain). In this way, not only the actions could be more accurately localized both spatially and temporally, but also the action classes could be predicted more precisely. Specifically, we first combine the latest region proposals (for spatial detection) or segment proposals (for temporal localization) from both streams to form a larger set of labelled training samples to help learn better action detection or segment detection models. Second, to learn better representations, we also propose a new message passing approach to pass information from one stream to another stream, which also leads to better action detection and segment detection models. By first using our newly proposed PCSC framework for spatial localization at the frame-level and then applying our temporal PCSC framework for temporal localization at the tube-level, the action localization results are progressively improved at both the frame level and the video level. Comprehensive experiments on two benchmark datasets UCF-101-24 and J-HMDB demonstrate the effectiveness of our newly proposed approaches for spatio-temporal action localization in realistic scenarios.	[Su, Rui; Xu, Dong; Zhou, Luping; Ouyang, Wanli] Univ Sydney, Sch Elect & Informat Engn, Camperdown, NSW 2006, Australia	University of Sydney	Xu, D (corresponding author), Univ Sydney, Sch Elect & Informat Engn, Camperdown, NSW 2006, Australia.	rui.su@sydney.edu.au; dong.xu@sydney.edu.au; luping.zhou@sydney.edu.au; wanli.ouyang@sydney.edu.au			Australian Research Council (ARC) Future Fellowship [FT180100116]; ARC [DP200103223]	Australian Research Council (ARC) Future Fellowship(Australian Research Council); ARC(Australian Research Council)	This work was supported by the Australian Research Council (ARC) Future Fellowship under Grant FT180100116 and ARC DP200103223.	Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Cheron G., 2018, ABS180611008 CORR; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472; Li D, 2018, LECT NOTES COMPUT SC, V11210, P306, DOI 10.1007/978-3-030-01231-1_19; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849; Ouyang WL, 2017, IEEE I CONF COMP VIS, P1956, DOI 10.1109/ICCV.2017.214; Ouyang WL, 2017, IEEE T PATTERN ANAL, V39, P1320, DOI 10.1109/TPAMI.2016.2587642; Ouyang WL, 2013, PROC CVPR IEEE, P3198, DOI 10.1109/CVPR.2013.411; Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45; Pigou L, 2018, INT J COMPUT VISION, V126, P430, DOI 10.1007/s11263-016-0957-7; Ren S., 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.169; Saha S., 2016, BMVC, P581; Simonyan K, 2014, ADV NEUR IN, V27; Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216; Singh G, 2017, IEEE I CONF COMP VIS, P3657, DOI 10.1109/ICCV.2017.393; Soomro K., 2012, ARXIV; Su R, 2019, PROC CVPR IEEE, P12008, DOI 10.1109/CVPR.2019.01229; Sun C, 2018, LECT NOTES COMPUT SC, V11215, P335, DOI 10.1007/978-3-030-01252-6_20; Sun SY, 2018, ADV NEUR IN, V31; Wang LM, 2016, PROC CVPR IEEE, P2708, DOI 10.1109/CVPR.2016.296; Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362; Yang Z., 2017, BMVC; Ye YC, 2019, J VIS COMMUN IMAGE R, V58, P515, DOI 10.1016/j.jvcir.2018.12.019; Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400; Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316	36	5	5	3	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4477	4490		10.1109/TPAMI.2020.2997860	http://dx.doi.org/10.1109/TPAMI.2020.2997860			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750775				2022-12-18	WOS:000714203900024
J	Wever, M; Tornede, A; Mohr, F; Hullermeier, E				Wever, Marcel; Tornede, Alexander; Mohr, Felix; Huellermeier, Eyke			AutoML for Multi-Label Classification: Overview and Empirical Evaluation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Tools; Pipelines; Machine learning; Loss measurement; Search problems; Complexity theory; Training; Automated machine learning; multi-label classification; hierarchical planning; Bayesian optimization		Automated machine learning (AutoML) supports the algorithmic construction and data-specific customization of machine learning pipelines, including the selection, combination, and parametrization of machine learning algorithms as main constituents. Generally speaking, AutoML approaches comprise two major components: a search space model and an optimizer for traversing the space. Recent approaches have shown impressive results in the realm of supervised learning, most notably (single-label) classification (SLC). Moreover, first attempts at extending these approaches towards multi-label classification (MLC) have been made. While the space of candidate pipelines is already huge in SLC, the complexity of the search space is raised to an even higher power in MLC. One may wonder, therefore, whether and to what extent optimizers established for SLC can scale to this increased complexity, and how they compare to each other. This paper makes the following contributions: First, we survey existing approaches to AutoML for MLC. Second, we augment these approaches with optimizers not previously tried for MLC. Third, we propose a benchmarking framework that supports a fair and systematic comparison. Fourth, we conduct an extensive experimental study, evaluating the methods on a suite of MLC problems. We find a grammar-based best-first search to compare favorably to other optimizers.	[Wever, Marcel; Tornede, Alexander; Huellermeier, Eyke] Paderborn Univ, Heinz Nixdorf Inst, Dept Comp Sci, D-33098 Paderborn, Germany; [Mohr, Felix] Univ La Sabana, Fac Engn, Chia 250007, Cundinamarca, Colombia	University of Paderborn; Universidad de La Sabana	Wever, M (corresponding author), Paderborn Univ, Heinz Nixdorf Inst, Dept Comp Sci, D-33098 Paderborn, Germany.	marcel.wever@upb.de; alexander.tornede@upb.de; felix.mohr@unisabana.edu.co; eyke@upb.de		Tornede, Alexander/0000-0002-2415-2186	German Research Foundation (DFG) within the Collaborative Research Center "On-The-Fly Computing" [SFB 901/3, 160364472]	German Research Foundation (DFG) within the Collaborative Research Center "On-The-Fly Computing"(German Research Foundation (DFG))	This work was supported by the German Research Foundation (DFG) within the Collaborative Research Center "On-The-Fly Computing" (SFB 901/3 project no. 160364472). The authors gratefully acknowledge the support of this project by the Paderborn Center for Parallel Computing (PC2), which provided computational resources and computing time.	Balaji A, 2018, ARXIV180806492; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Chen BY, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P402, DOI 10.1145/3205455.3205586; das Dores SN, 2018, 2018 7TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS), P121, DOI 10.1109/BRACIS.2018.00029; de Sa AGC, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE COMPANION (GECCO'17 COMPANION), P1125, DOI 10.1145/3067695.3082053; de Sa AGC, 2018, LECT NOTES COMPUT SC, V11102, P308, DOI 10.1007/978-3-319-99259-4_25; de Sa AGC, 2017, LECT NOTES COMPUT SC, V10196, P246, DOI 10.1007/978-3-319-55696-3_16; Drori I, 2018, P AUT MACH LEARN WOR; Eibe F, 2016, WEKA WORKBENCH ONLIN; Elshawi R., 2019, ARXIV PREPRINT ARXIV; Erickson N, ARXIV200306505, P2020; Falkner Stefan, 2018, P MACHINE LEARNING R, V80, P1437; Feurer M., 2018, P INT WORKSH AUT MAC; Feurer Matthias, 2015, ADV NEURAL INFORM PR, P2962; Frazier P.I., 2018, ARXIVPREPRINTARXIV18; Frazier PI, 2008, SIAM J CONTROL OPTIM, V47, P2410, DOI 10.1137/070693424; Ghallab Malik, 2004, AUTOMATED PLANNING T; Gijsbers P., 2019, ARXIV PREPRINT ARXIV; Gijsbers P., 2019, J OPEN SOURCE SOFTW, V4, P1132, DOI 10.21105/joss.01132; He Xin, 2019, ARXIV190800709; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hoffman MW, 2014, JMLR WORKSH CONF PRO, V33, P365; Hutter F, 2019, SPRING SER CHALLENGE, P1, DOI 10.1007/978-3-030-05318-5; Jamieson K, 2016, JMLR WORKSH CONF PRO, V51, P240; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Kietz JU, 2012, FRONT ARTIF INTEL AP, V242, P1011, DOI 10.3233/978-1-61499-098-7-1011; Kocev D, 2007, LECT NOTES ARTIF INT, V4701, P624; Komer B, 2019, SPRING SER CHALLENGE, P97, DOI 10.1007/978-3-030-05318-5_5; Kotthoff L., 2017, J MACH LEARN RES, V18, P826; LeDell E., 2020, P AUT MACH LEARN WOR, V2020; McKay RI, 2010, GENET PROGRAM EVOL M, V11, P365, DOI 10.1007/s10710-010-9109-y; Mitchell M, 1996, INTRO GENETIC ALGORI; Mohr F., 2018, ARXIV180900486; Mohr F, 2018, P IEEE I C SERV COMP, P241, DOI 10.1109/SCC.2018.00039; Mohr F, 2018, MACH LEARN, V107, P1495, DOI 10.1007/s10994-018-5735-z; Mokus J, 1975, OPTIMIZATION TECHNIQ, P400; Nam J, 2019, PR MACH LEARN RES, V97; Olson RS, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P485, DOI 10.1145/2908812.2908918; Pakrashi A, 2019, LECT NOTES ARTIF INT, V11927, P3, DOI 10.1007/978-3-030-34885-4_1; Pappa G. L., 2018, ARXIV181111353; Rakotoarison H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3296; Read J, 2016, J MACH LEARN RES, V17; Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5; Rivolli A, 2020, MACH LEARN, V109, P1509, DOI 10.1007/s10994-020-05879-3; Senge R, 2014, STUD CLASS DATA ANAL, P163, DOI 10.1007/978-3-319-01595-8_18; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Sun X., 2019, P JOINT EUR C MACH L, P68; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Tornede Alexander, 2020, Discovery Science. 23rd International Conference, DS 2020. Proceedings. Lecture Notes in Artificial Intelligence. Subseries of Lecture Notes in Computer Science (LNAI 12323), P309, DOI 10.1007/978-3-030-61527-7_21; Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Waegeman W, 2019, DATA MIN KNOWL DISC, V33, P293, DOI 10.1007/s10618-018-0595-5; Wever M., 2018, P ICML AUT MACH LEAR; Wever M., 2018, ARXIV181104060; Wever M., 2019, P AUT MACH LEARN WOR, V2020; Wever M., 2017, P 27 WORKSH COMP INT; Wever M, 2018, P 1 ICAPS WORKSH HIE, P31; Wever M, 2020, LECT NOTES COMPUT SC, V12080, P561, DOI 10.1007/978-3-030-44584-3_44; Yao Quanming, 2018, ARXIV181013306; Zhang ML, 2018, FRONT COMPUT SCI-CHI, V12, P191, DOI 10.1007/s11704-017-7031-7; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zoller MA, 2019, ARXIV190412054	66	5	5	1	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3037	3054		10.1109/TPAMI.2021.3051276	http://dx.doi.org/10.1109/TPAMI.2021.3051276			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	33439834	hybrid, Green Published			2022-12-18	WOS:000681124300015
J	Zheng, XW; Ji, RR; Chen, YH; Wang, Q; Zhang, BC; Chen, J; Ye, QX; Huang, FY; Tian, YH				Zheng, Xiawu; Ji, Rongrong; Chen, Yuhang; Wang, Qiang; Zhang, Baochang; Chen, Jie; Ye, Qixiang; Huang, Feiyue; Tian, Yonghong			MIGO-NAS: Towards Fast and Generalizable Neural Architecture Search	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Neural architecture search; multivariate information-geometric optimization; dynamic programming		Neural architecture search (NAS) has achieved unprecedented performance in various computer vision tasks. However, most existing NAS methods are defected in search efficiency and model generalizability. In this paper, we propose a novel NAS framework, termed MIGO-NAS, with the aim to guarantee the efficiency and generalizability in arbitrary search spaces. On the one hand, we formulate the search space as a multivariate probabilistic distribution, which is then optimized by a novel multivariate information-geometric optimization (MIGO). By approximating the distribution with a sampling, training, and testing pipeline, MIGO guarantees the memory efficiency, training efficiency, and search flexibility. Besides, MIGO is the first time to decrease the estimation error of natural gradient in multivariate distribution. On the other hand, for a set of specific constraints, the neural architectures are generated by a novel dynamic programming network generation (DPNG), which significantly reduces the training cost under various hardware environments. Experiments validate the advantages of our approach over existing methods by establishing a superior accuracy and efficiency i.e., 2.39 test error on CIFAR-10 benchmark and 21.7 on ImageNet benchmark, with only 1.5 GPU hours and 96 GPU hours for searching, respectively. Besides, the searched architectures can be well generalize to computer vision tasks including object detection and semantic segmentation, i.e., 25xFLOPs compression, with 6.4 mAP gain over Pascal VOC dataset, and 29:9xFLOPs compression, with only 1.41 percent performance drop over Cityscapes dataset. The code is publicly available.	[Zheng, Xiawu; Ji, Rongrong; Chen, Yuhang; Wang, Qiang] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China; [Ji, Rongrong] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China; [Zhang, Baochang] Beihang Univ, Inst Artificial Intelligence, Beijing 100083, Peoples R China; [Chen, Jie] Peking Univ, Sch Elect & Comp Engn, Beijing 100871, Peoples R China; [Ye, Qixiang; Tian, Yonghong] Peng Cheng Lab, Shenzhen 518066, Peoples R China; [Ye, Qixiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Huang, Feiyue] Tencent Co Ltd, YouTu Lab, Shanghai 518064, Peoples R China; [Tian, Yonghong] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol NELVT, Beijing 100871, Peoples R China	Xiamen University; Xiamen University; Beihang University; Peking University; Peng Cheng Laboratory; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Tencent; Peking University	Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.	zhengxiawu@stu.xmu.edu.cn; rrji@xmu.edu.cn; johsnows@gmail.com; wangqiang@stu.xmu.edu.cn; bczhang@buaa.edu.cn; chenj@pcl.ac.cn; qxye@ucas.ac.cn; garyhuang@tencent.com; yhtian@pku.edu.cn		ye, qi xiang/0000-0003-1215-6259	National Science Fund for Distinguished Young Scholars [62025603]; National Natural Science Foundation of China [62072386, 62072387, 62072389, 62002305, 61772443, 61802324, 61702136, 61972217, 62081360152]; Guangdong Basic and Applied Basic Research Foundation [2019B1515120049]; Guangdong Science and Technology Department [2020B1111340056]	National Science Fund for Distinguished Young Scholars(National Natural Science Foundation of China (NSFC)National Science Fund for Distinguished Young Scholars); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Guangdong Basic and Applied Basic Research Foundation; Guangdong Science and Technology Department	This work was supported by the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No.U1705262, No. 62072386, No. 62072387, No. 62072389, No. 62002305, No. 61772443, No. 61802324, and No. 61702136, No. 61972217, and No. 62081360152), Guangdong Basic and Applied Basic Research Foundation (No. 2019B1515120049), and Guangdong Science and Technology Department (No. 2020B1111340056).	Akimoto Y, 2019, PR MACH LEARN RES, V97; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Back T, 1996, EVOLUTIONARY ALGORIT; BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34; Bender G, 2018, PR MACH LEARN RES, V80; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Cai H., 2020, ICLR, P1; Cai H, 2018, PR MACH LEARN RES, V80; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai Han, 2019, INT C LEARN REPR; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Chen Y., 2019, DETNAS NEURAL ARCHIT, P6638; Chu Xiangxiang, 2021, ARXIV190701845, P12239; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; DeVries T., 2017, P 2017 COMPUTER VISI; Dong X., 2019, P INT C LEARN REPR; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Elsken T, 2019, J MACH LEARN RES, V20; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gupta O., 2017, P INT C LEARN REPR; Hammersley J, 2013, MONTE CARLO METHODS; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HOFFMEISTER F, 1991, LECT NOTES COMPUT SC, V496, P455; Howard A.G, 2017, ARXIV170404861; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky A., 2009, COMPUT SCI DEP, V1, P1; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Liam, 2020, ARXIV200407802; Li Liam, 2019, ABS190207638 CORR; Liang H., ARXIV190906035; Lindauer M., 2019, ARXIV190902453; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941; Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34; Mei J., 2019, P INT C LEARN REPR; Ollivier Y, 2017, J MACH LEARN RES, V18; Park H., 2018, ARXIV181204920; Pham H, 2018, PR MACH LEARN RES, V80; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shirakawa S, 2018, AAAI CONF ARTIF INTE, P4074; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Stamoulis D, 2020, IEEE J-STSP, V14, P609, DOI 10.1109/JSTSP.2020.2971421; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xiao H., 2017, ARXIV 170807747; Xiawu Zheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11353, DOI 10.1109/CVPR42600.2020.01137; Xie Sirui, 2019, INT C LEARN REPR; Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675; Xu Y, 2020, ICLR; Yang TJ, 2018, LECT NOTES COMPUT SC, V11214, P289, DOI 10.1007/978-3-030-01249-6_18; Ying C, 2019, PR MACH LEARN RES, V97; Yu J., 2019, AUTOSLIM ONE SHOT AR; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zela A., 2019, P INT C LEARN REPR; Zheng X., ARXIV190513543; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	73	5	6	5	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					2936	2952		10.1109/TPAMI.2021.3065138	http://dx.doi.org/10.1109/TPAMI.2021.3065138			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	33710952				2022-12-18	WOS:000681124300009
J	Lao, YZ; Ait-Aider, O				Lao, Yizhen; Ait-Aider, Omar			Rolling Shutter Homography and its Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Transmission line matrix methods; Geometry; Computer vision; Pose estimation; Kinematics; Delays; Rolling shutter; homography; relative pose estimation; image stitching	IMAGE; POSE	In this article we study the adaptation of the concept of homography to Rolling Shutter (RS) images. This extension has never been clearly adressed despite the many roles played by the homography matrix in multi-view geometry. We first show that a direct point-to-point relationship on a RS pair can be expressed as a set of 3 to 8 atomic 3x3 matrices depending on the kinematic model used for the instantaneous-motion during image acquisition. We call this group of matrices the RS Homography. We then propose linear solvers for the computation of these matrices using point correspondences. Finally, we derive linear and closed form solutions for two famous problems in computer vision in the case of RS images: image stitching and plane-based relative pose computation. Extensive experiments with both synthetic and real data from public benchmarks show that the proposed methods outperform state-of-art techniques.	[Lao, Yizhen; Ait-Aider, Omar] Univ Clermont Auvergne, CNRS, Inst Pascal, F-63000 Clermont Ferrand, France	Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)	Lao, YZ (corresponding author), Univ Clermont Auvergne, CNRS, Inst Pascal, F-63000 Clermont Ferrand, France.	lyz91822@gmail.com; omar.ait-aider@uca.fr			French Government research program "Investissements d'Avenir" through the IDEX-ISITE initiative [16-IDEX-0001 (CAP 20-25)]; IMobS3 Laboratory of Excellence [ANR-10-LABX-16-01]; RobotEx Equipment of Excellence [ANR-10-EQPX-44]; European Union through the Regional Competitiveness and Employment program -2014-2020-(ERDF -AURAregion); AURA region	French Government research program "Investissements d'Avenir" through the IDEX-ISITE initiative(French National Research Agency (ANR)); IMobS3 Laboratory of Excellence; RobotEx Equipment of Excellence; European Union through the Regional Competitiveness and Employment program -2014-2020-(ERDF -AURAregion); AURA region	This work was supported by the French Government research program "Investissements d'Avenir" through the IDEX-ISITE initiative 16-IDEX-0001 (CAP 20-25), the IMobS3 Laboratory of Excellence (ANR-10-LABX-16-01) and the RobotEx Equipment of Excellence (ANR-10-EQPX-44). This research was also financed by the European Union through the Regional Competitiveness and Employment program -2014-2020-(ERDF -AURAregion) and by the AURA region.	Ait-Aider O, 2006, LECT NOTES COMPUT SC, V3952, P56; Ait-Aider O, 2009, IEEE I CONF COMP VIS, P1835, DOI 10.1109/ICCV.2009.5459408; Albl C, 2020, IEEE T PATTERN ANAL, V42, P1439, DOI 10.1109/TPAMI.2019.2894395; Albl C, 2016, LECT NOTES COMPUT SC, V9909, P36, DOI 10.1007/978-3-319-46454-1_3; [Anonymous], 2005, GROBNER BASIS METHOD; [Anonymous], 2012, INVITATION 3 D VISIO; [Anonymous], 2016, IMAG COMP EDIT MIC; Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247; Cho W.-H., 2012, P IEEE 16 INT S CONS, P1; Dai YC, 2016, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2016.448; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Forssen PE, 2010, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2010.5540173; Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599; Geyer Christopher, 2005, 6 OMNIVIS WS, V1, P4; Grundmann M., 2012, 2012 IEEE INT C COMP, P1, DOI [10.1109/ICCPhot.2012.6215213, DOI 10.1109/ICCPHOT.2012.6215213]; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hedborg J, 2012, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2012.6247831; Ito E, 2017, PROC CVPR IEEE, P4512, DOI 10.1109/CVPR.2017.480; Jia C, 2012, IEEE INT WORKSH MULT, P203, DOI 10.1109/MMSP.2012.6343441; Klingner B, 2013, IEEE I CONF COMP VIS, P953, DOI 10.1109/ICCV.2013.122; Kukelova Z, 2008, LECT NOTES COMPUT SC, V5304, P302, DOI 10.1007/978-3-540-88690-7_23; Lao YZ, 2018, PROC CVPR IEEE, P4795, DOI 10.1109/CVPR.2018.00504; Lao YZ, 2018, LECT NOTES COMPUT SC, V11206, P477, DOI 10.1007/978-3-030-01216-8_29; Lao YZ, 2018, PATTERN RECOGN LETT, V111, P1, DOI 10.1016/j.patrec.2018.04.004; Larsson V, 2017, PROC CVPR IEEE, P2383, DOI 10.1109/CVPR.2017.256; Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719; Linkemann J., 2014, CISC VIS NETW IND GL; Liu S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461995; Malis E., 2007, DEEP UND HOM DEC VIS; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Purkait P, 2018, IEEE WINT CONF APPL, P903, DOI 10.1109/WACV.2018.00104; Purkait P, 2017, IEEE I CONF COMP VIS, P882, DOI 10.1109/ICCV.2017.101; Rengarajan V, 2017, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2017.252; Rengarajan V, 2016, PROC CVPR IEEE, P2773, DOI 10.1109/CVPR.2016.303; Saurer O, 2017, IEEE T PATTERN ANAL, V39, P327, DOI 10.1109/TPAMI.2016.2545663; Saurer O, 2016, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2016.363; Saurer O, 2015, IEEE INT C INT ROBOT, P1328, DOI 10.1109/IROS.2015.7353540; Saurer O, 2013, IEEE I CONF COMP VIS, P465, DOI 10.1109/ICCV.2013.64; Vasu S, 2018, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2018.00073; Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247; Zhou ZH, 2012, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2012.6247837; Zhuang BB, 2017, IEEE I CONF COMP VIS, P948, DOI 10.1109/ICCV.2017.108	44	5	5	1	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG 1	2021	43	8					2780	2793		10.1109/TPAMI.2020.2977644	http://dx.doi.org/10.1109/TPAMI.2020.2977644			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TF2YV	32142425	Green Submitted			2022-12-18	WOS:000670578800019
J	Slavcheva, M; Baust, M; Ilic, S				Slavcheva, Miroslava; Baust, Maximilian; Ilic, Slobodan			Variational Level Set Evolution for Non-Rigid 3D Reconstruction From a Single Depth Camera	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Eigenvalues and eigenfunctions; Level set; Cameras; Image reconstruction; Laplace equations; Surface reconstruction; Non-rigid 3D reconstruction; signed distance field evolution; Laplacian eigenfunctions	FLOW ESTIMATION; VECTOR-FIELDS; SCENE FLOW; SEGMENTATION; REGISTRATION; SPEED	We present a framework for real-time 3D reconstruction of non-rigidly moving surfaces captured with a single RGB-D camera. Based on the variational level set method, it warps a given truncated signed distance field (TSDF) to a target TSDF via gradient flow without explicit correspondence search. We optimize an energy that contains a data term which steers towards voxel-wise alignment. To ensure geometrically consistent reconstructions, we develop and compare different strategies, namely an approximately Killing vector field regularizer, gradient flow in Sobolev space and newly devised accelerated optimization. The underlying TSDF evolution makes our approach capable of capturing rapid motions, topological changes and interacting agents, but entails loss of data association. To recover correspondences, we propose to utilize the lowest-frequency Laplacian eigenfunctions of the TSDFs, which encode inherent deformation patterns. For moderate motions we are able to obtain implicit associations via a term that imposes voxel-wise eigenfunction alignment. This is not sufficient for larger motions, so we explicitly estimate voxel correspondences via signature matching of lower-dimensional eigenfunction embeddings. We carry out qualitative and quantitative evaluation of our geometric reconstruction fidelity and voxel correspondence accuracy, demonstrating advantages over related techniques in handling topological changes and fast motions.	[Slavcheva, Miroslava; Baust, Maximilian; Ilic, Slobodan] TUM CAMP, D-85748 Garching, Germany; [Slavcheva, Miroslava; Ilic, Slobodan] Siemens CT, D-81739 Munich, Germany; [Baust, Maximilian] NVIDIA, D-81677 Munich, Germany	Siemens AG; Siemens Germany	Slavcheva, M (corresponding author), TUM CAMP, D-85748 Garching, Germany.; Slavcheva, M (corresponding author), Siemens CT, D-81739 Munich, Germany.	mira.slavcheva@tum.de; maximilian.baust@tum.de; slobodan.ilic@siemens.com						Angelini ED, 2005, HDB BIOMEDICAL IMAGE, V3; Baust M., 2014, P BRIT MACH VIS C, DOI [10.5244/C.28.39, DOI 10.5244/C.28.39]; Belkin M, 2002, ADV NEUR IN, V14, P585; Ben-Chen M, 2010, COMPUT GRAPH FORUM, V29, P1701, DOI 10.1111/j.1467-8659.2010.01779.x; Benyamin M, 2020, J MATH IMAGING VIS, V62, P10, DOI 10.1007/s10851-019-00910-2; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bogo F, 2015, IEEE I CONF COMP VIS, P2300, DOI 10.1109/ICCV.2015.265; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Cagniart C, 2010, LECT NOTES COMPUT SC, V6314, P326, DOI 10.1007/978-3-642-15561-1_24; Calder J, 2010, SIAM J IMAGING SCI, V3, P981, DOI 10.1137/090771260; Calder J, 2019, RES MATH SCI, V6, DOI 10.1007/s40687-019-0197-x; Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195; Cohen-Or D, 1998, ACM T GRAPHIC, V17, P116, DOI 10.1145/274363.274366; Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697; Dou MS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130801; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647; Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566645, 10.1145/566570.566581]; Frank A., 2004, 200414 EG RES GROUP; Frisken S.F., 2006, ACM SIGGRAPH 2006 CO, P60; Fujiwara K, 2011, IEEE I CONF COMP VIS, P1527, DOI 10.1109/ICCV.2011.6126411; Guo KW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083722; Ho S, 2002, INT C PATT RECOG, P532, DOI 10.1109/ICPR.2002.1044788; Huang CH, 2016, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2016.419; Huguet F, 2007, IEEE I CONF COMP VIS, P1342, DOI 10.1109/iccv.2007.4409000; Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22; Jaimez Mariano, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3992, DOI 10.1109/ICRA.2017.7989459; Jain V, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P118; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Kahler O, 2015, IEEE T VIS COMPUT GR, V21, P1241, DOI 10.1109/TVCG.2015.2459891; Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650; Kruskal Joseph B., 1989, MULTIWAY DATA ANAL P, V4; Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66; Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690; Li CM, 2005, PROC CVPR IEEE, P430; Li H, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1618521, 10.1145/1618452.1618503]; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas BC, 2013, IEEE T VIS COMPUT GR, V19, P852, DOI 10.1109/TVCG.2012.162; Lucas BC, 2011, LECT NOTES COMPUT SC, V6892, P442, DOI 10.1007/978-3-642-23629-7_54; Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8; Mateus D., 2008, PROC CVPR IEEE, P1, DOI DOI 10.1109/CVPR.2008.4587538; Mihalef V, 2007, COMPUT GRAPH FORUM, V26, P457, DOI 10.1111/j.1467-8659.2007.01068.x; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Neuberger J., 2009, SOBOLEV GRADIENTS DI; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Osher S., 2003, APPL MATH SCI, V153; Paragios N, 2003, COMPUT VIS IMAGE UND, V89, P142, DOI 10.1016/S1077-3142(03)00010-9; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Pons JP, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P894; Quiroga J, 2014, LECT NOTES COMPUT SC, V8695, P567, DOI 10.1007/978-3-319-10584-0_37; Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011; Runz Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4471, DOI 10.1109/ICRA.2017.7989518; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Schroers Christopher, 2012, Pattern Recognition. Proceedings Joint 34th DAGM and 36th OAGM Symposium, P73, DOI 10.1007/978-3-642-32717-9_8; Slavcheva M, 2018, PROC CVPR IEEE, P2646, DOI 10.1109/CVPR.2018.00280; Slavcheva M, 2017, IEEE INT CONF COMP V, P833, DOI 10.1109/ICCVW.2017.103; Slavcheva M, 2017, PROC CVPR IEEE, P5474, DOI 10.1109/CVPR.2017.581; Slavcheva M, 2016, LECT NOTES COMPUT SC, V9905, P680, DOI 10.1007/978-3-319-46448-0_41; Solomon J, 2011, COMPUT GRAPH FORUM, V30, P1543, DOI 10.1111/j.1467-8659.2011.02028.x; Sorkine Olga, 2007, P EUROGRAPHICS ACM S, V4, P109, DOI 10.1145/1281991.1282006; Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531; Sundaramoorthi G., 2018, P 32 INT C NEUR INF, P3797; Sundaramoorthi G., 2018, ARXIV180402307; Sundaramoorthi G, 2008, IEEE T PATTERN ANAL, V30, P851, DOI 10.1109/TPAMI.2007.70751; Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tan DJ, 2016, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR.2016.605; Tao M, 2016, COMPUT GRAPH FORUM, V35, P65, DOI 10.1111/cgf.12964; Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965; Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056; Tung-Ying Lee, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563084; Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580; UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778; Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Vogel C, 2013, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2013.174; Wedel A, 2008, LECT NOTES COMPUT SC, V5302, P739, DOI 10.1007/978-3-540-88682-2_56; Weng YL, 2013, COMPUT GRAPH FORUM, V32, P381, DOI 10.1111/cgf.12246; Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Whitaker RT, 1998, INT J COMPUT VISION, V29, P203, DOI 10.1023/A:1008036829907; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Wulff J, 2015, PROC CVPR IEEE, P120, DOI 10.1109/CVPR.2015.7298607; Xie XH, 2011, IMAGE VISION COMPUT, V29, P167, DOI 10.1016/j.imavis.2010.08.011; Yezzi A., 2017, ARXIV171109867; Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761; Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104; Zach C., 2007, P 11 IEEE INT C COMP, P1, DOI DOI 10.1109/ICCV.2007.4408983; Zhao HK, 1996, J COMPUT PHYS, V127, P179, DOI 10.1006/jcph.1996.0167; Zhou QY, 2013, IEEE I CONF COMP VIS, P473, DOI 10.1109/ICCV.2013.65; Zollhofer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386; Zollhofer M., 2014, ACM T GRAPHIC, V33, p[2, 8]	98	5	5	4	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG 1	2021	43	8					2838	2850		10.1109/TPAMI.2020.2976065	http://dx.doi.org/10.1109/TPAMI.2020.2976065			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TF2YV	32091994				2022-12-18	WOS:000670578800023
J	Chen, DD; Yuan, L; Liao, J; Yu, NH; Hua, G				Chen, Dongdong; Yuan, Lu; Liao, Jing; Yu, Nenghai; Hua, Gang			Explicit Filterbank Learning for Neural Image Style Transfer and Image Processing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Convolution; Decoding; Neural networks; Feature extraction; Fuses; Image processing and computer vision; style transfer	TEXTURE SYNTHESIS; MODEL	Image style transfer is to re-render the content of one image with the style of another. Most existing methods couple content and style information in their network structures and hyper-parameters, and learn it as a black-box. For better understanding, this paper aims to provide a new explicit decoupled perspective. Specifically, we propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style. To transfer an image to a specific style, the corresponding filter bank is operated on the intermediate feature produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt in such a way that the auto-encoder does not encode any style information. This explicit representation also enables us to conduct incremental learning to add a new style and fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and provides new understanding on neural style transfer. We further apply this general filterbank learning idea to two different multi-parameter image processing tasks: edge-aware image smoothing and denoising. Experiments demonstrate that it can achieve comparable results to its single parameter setting counterparts.	[Chen, Dongdong; Yu, Nenghai] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Anhui, Peoples R China; [Yuan, Lu] Microsoft Res, Redmond, WA 98052 USA; [Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China; [Hua, Gang] Wormpex Res LLC, Bellevue, WA 98004 USA	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft; City University of Hong Kong	Liao, J (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.; Hua, G (corresponding author), Wormpex Res LLC, Bellevue, WA 98004 USA.	cd722522@mail.ustc.edu.cn; luyuan@microsoft.com; jingliao@cityu.edu.hk; ynh@ustc.edu.cn; ganghua@gmail.com		LIAO, Jing/0000-0001-7014-5377; Chen, Dongdong/0000-0002-4642-4373	National Key R&D Program of China [2018AAA0101400]; NSFC [61629301, U163620]	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC))	This work was supported partly by the National Key R&D Program of China Grant 2018AAA0101400, and NSFC Grant 61629301, U163620.	Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696; Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126; Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296; Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Di Lin, 2016, Arxiv, DOI arXiv:1604.05144; Dumoulin Vincent, 2017, ICLR, P2; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Elad M., 2016, ARXIV160903057; Everingham M., 2012, PASCAL VISUAL OBJECT; Fan QN, 2018, LECT NOTES COMPUT SC, V11217, P455, DOI 10.1007/978-3-030-01261-8_27; Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351; Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jing Y., 2017, ABS170504058 CORR; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; King DB, 2015, ACS SYM SER, V1214, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lempitsky V., 2016, ARXIV160708022V3; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Li Y, 2002, ACM T GRAPHIC, V21, P465; Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36; Li Yijun, 2017, ARXIV170508086; Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu Y, 2015, ARXIV150908379; Malik J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P918, DOI 10.1109/ICCV.1999.790346; Mordvintsev A., 2015, INCEPTIONISM GOING D, V20, P5; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Ulyanov D, 2016, PR MACH LEARN RES, V48; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; Xu L, 2015, PR MACH LEARN RES, V37, P1669; Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158; Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang K, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP); Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1	61	5	5	5	26	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2373	2387		10.1109/TPAMI.2020.2964205	http://dx.doi.org/10.1109/TPAMI.2020.2964205			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	31905133				2022-12-18	WOS:000692540900015
J	Dong, XH; Dong, JY; Chantler, MJ				Dong, Xinghui; Dong, Junyu; Chantler, Mike J.			Perceptual Texture Similarity Estimation: An Evaluation of Computational Features	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Evaluation; features; perceptual similarity; similarity measures; texture similarity	FEATURE-EXTRACTION; IMAGE DESCRIPTION; CLASSIFICATION; SEGMENTATION; MODEL; DISCRIMINATION; REPRESENTATION; STATISTICS; DIFFERENCE; ERROR	Estimation of texture similarity is fundamental to many material recognition tasks. This study uses fine-grained human perceptual similarity ground-truth to provide a comprehensive evaluation of 51 texture feature sets. We conduct two types of evaluation and both show that these features do not estimate similarity well when compared against human agreement rates, but that performances are improved when the features are combined using a Random Forest. Using a simple two-stage statistical model we show that few of the features capture long-range aperiodic relationships. We perform two psychophysical experiments which indicate that long-range interactions do provide humans with important cues for estimating texture similarity. This motivates an extension of the study to include Convolutional Neural Networks (CNNs) as they enable arbitrary features of large spatial extent to be learnt. Our conclusions derived from the use of two pre-trained CNNs are: that the large spatial extent exploited by the networks' top convolutional and first fully-connected layers, together with the use of large numbers of filters, confers significant advantage for estimation of perceptual texture similarity.	[Dong, Xinghui] Univ Manchester, Ctr Imaging Sci, Manchester M13 9PT, Lancs, England; [Dong, Junyu] Ocean Univ China, Dept Comp Sci, Qingdao 266071, Shandong, Peoples R China; [Chantler, Mike J.] Heriot Watt Univ, Sch Mathemat & Comp Sci, Texture Lab, Edinburgh EH14 4AS, Midlothian, Scotland	University of Manchester; Ocean University of China; Heriot Watt University	Dong, XH (corresponding author), Univ Manchester, Ctr Imaging Sci, Manchester M13 9PT, Lancs, England.	dongxinghui@gmail.com; dongjunyu@ouc.edu.cn; m.j.chantler@hw.ac.uk		Chantler, Mike/0000-0002-8381-1751; Dong, Junyu/0000-0001-7012-2087	Heriot-Watt University	Heriot-Watt University	X. Dong completed the majority of this research when he worked with the Texture Lab, Heriot-Watt University, Edinburgh, UK. The authors would like to thank the editor and anonymous reviewers for their extensive comments and help in improving this paper. They acknowledge the support of Heriot-Watt University.	Abbadeni N, 2011, IEEE T IMAGE PROCESS, V20, P236, DOI 10.1109/TIP.2010.2060345; ADE F, 1983, SIGNAL PROCESS, V5, P451, DOI 10.1016/0165-1684(83)90008-7; Ahonen T, 2009, LECT NOTES COMPUT SC, V5575, P61, DOI 10.1007/978-3-642-02230-2_7; Ahonen T, 2009, PATTERN RECOGN LETT, V30, P368, DOI 10.1016/j.patrec.2008.10.012; AMADASUN M, 1989, IEEE T SYST MAN CYB, V19, P1264, DOI 10.1109/21.44046; [Anonymous], 1985, PERCEPTUAL ORG VISUA; Bar-Ilan J, 2007, J AM SOC INF SCI TEC, V58, P1254, DOI 10.1002/asi.20608; Berger G., ARXIV160601286V2; Bergmann U, 2017, PR MACH LEARN RES, V70; Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727; BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384; Box J. F., 1987, STAT SCI, V2, P45, DOI [DOI 10.1214/SS/1177013437, 10.1214/ss/1177013437]; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; BURT PJ, 1981, COMPUT VISION GRAPH, V16, P20, DOI 10.1016/0146-664X(81)90092-7; Cadena SA, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006897; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chantler M, 2005, INT J COMPUT VISION, V62, P83, DOI 10.1007/s11263-005-4636-3; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; CHAUDHURI BB, 1993, IEE PROC-E, V140, P233, DOI 10.1049/ip-e.1993.0034; CHELLAPPA R, 1985, IEEE T ACOUST SPEECH, V33, P959, DOI 10.1109/TASSP.1985.1164641; Cho RY, 2000, PERCEPT PSYCHOPHYS, V62, P735, DOI 10.3758/BF03206920; Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3; Clarke A. D. F., 2012, P 3 INT C APP, P17; Clarke ADF, 2011, SYMMETRY-BASEL, V3, P246, DOI 10.3390/sym3020246; COGGINS JM, 1985, PATTERN RECOGN LETT, V3, P195, DOI 10.1016/0167-8655(85)90053-4; Crosier M, 2010, INT J COMPUT VISION, V88, P447, DOI 10.1007/s11263-009-0315-0; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; David, 1988, METHOD PAIRED COMP; Dong X., 2014, P ACM INT C MULT RET, P281; Dong X., 2014, THESIS HERIOT WATT U; Dong XH, 2013, LECT NOTES COMPUT SC, V8047, P425, DOI 10.1007/978-3-642-40261-6_51; Dosovitskiy Alexey, 2016, NEURIPS; Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266; Emrith K, 2010, J OPT SOC AM A, V27, P1232, DOI 10.1364/JOSAA.27.001232; Fagin R, 2003, SIAM PROC S, P28; Fan SJ, 2014, PROC CVPR IEEE, P4201, DOI 10.1109/CVPR.2014.535; Field A., 2018, DISCOVERING STAT USI, DOI 10.1348/000709906X100611; FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q; Filip J, 2009, IEEE T PATTERN ANAL, V31, P1921, DOI 10.1109/TPAMI.2008.246; FOGEL I, 1989, BIOL CYBERN, V61, P103, DOI 10.1007/BF00204594; Fraley C, 1998, COMPUT J, V41, P578, DOI 10.1093/comjnl/41.8.578; Fujii K, 2003, PSYCHOL RES-PSYCH FO, V67, P197, DOI 10.1007/s00426-002-0113-6; Galloway MM., 1975, COMPUT GRAPHICS IMAG, V4, DOI DOI 10.1016/S0146-664X(75)80008-6; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; GREENHOUSE SW, 1959, PSYCHOMETRIKA, V24, P95, DOI 10.1007/BF02289823; Halley F., 2011, THESIS HERIOT WATT U; Hallum LE, 2011, J NEUROPHYSIOL, V105, P2121, DOI 10.1152/jn.01007.2010; HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Hariri N, 2011, ONLINE INFORM REV, V35, P598, DOI 10.1108/14684521111161954; HARWOOD D, 1995, PATTERN RECOGN LETT, V16, P1, DOI 10.1016/0167-8655(94)00061-7; Heaps C, 1999, J EXP PSYCHOL HUMAN, V25, P299, DOI 10.1037/0096-1523.25.2.299; JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; Kadyrov A, 2001, IEEE T PATTERN ANAL, V23, P811, DOI 10.1109/34.946986; Khelifi F, 2011, IEEE T IMAGE PROCESS, V20, P293, DOI 10.1109/TIP.2010.2052277; Kim JK, 1999, IEEE T MED IMAGING, V18, P231, DOI 10.1109/42.764896; Kohler PJ, 2016, J NEUROSCI, V36, P714, DOI 10.1523/JNEUROSCI.2962-15.2016; Kolmogorov A., 1933, GIORN I ITAL DEGLI A, V4; Landy M., 2004, VISUAL NEUROSCIENCES, P1106; Laws K. I., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V238, P376; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; LIN WC, 2006, P IEEE C COMP VIS PA, P427; Lin WC, 2007, IEEE T PATTERN ANAL, V29, P777, DOI 10.1109/TPAMI.2007.1053; Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z; Liu YX, 2005, INT J COMPUT VISION, V62, P145, DOI 10.1007/s11263-005-4639-0; Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731; Liu ZQ, 1996, APPL OPTICS, V35, P848, DOI 10.1364/AO.35.000848; Long H. Z., 2000, P INT C MULT MOD, P167; MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923; Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803; MAO JC, 1992, PATTERN RECOGN, V25, P173, DOI 10.1016/0031-3203(92)90099-5; Matthews T, 2013, PROC CVPR IEEE, P1248, DOI 10.1109/CVPR.2013.165; Mauchly JW, 1940, ANN MATH STAT, V11, P204, DOI 10.1214/aoms/1177731915; McNemar Q, 1947, PSYCHOMETRIKA, V12, P153, DOI 10.1007/BF02295996; Medioni G., 2018, COMPUT TEXTURE PATTE; Metaxas PT, 2009, LECT NOTES BUS INF, V18, P278; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; NG I, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL III, P627, DOI 10.1109/ICPR.1992.202065; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Ojansivu V., 2008, ROTATION INVARIANT L, P1, DOI DOI 10.1109/ICPR.2008.4761377; Olshausen BA, 1996, NETWORK-COMP NEURAL, V7, P333, DOI 10.1088/0954-898X/7/2/014; OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022; Pappas TN, 2013, P IEEE, V101, P2044, DOI 10.1109/JPROC.2013.2262912; Parseval M. A., 1806, SCI MATH PHYS, V1, P638; Payne JS, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P793, DOI 10.1109/MMCS.1999.778587; Polat U, 1999, SPATIAL VISION, V12, P143, DOI 10.1163/156856899X00094; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Rahtu E, 2005, IEEE T PATTERN ANAL, V27, P908, DOI 10.1109/TPAMI.2005.111; Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261; RAO AR, 1993, CVGIP-GRAPH MODEL IM, V55, P218, DOI 10.1006/cgip.1993.1016; REED TR, 1993, CVGIP-IMAG UNDERSTAN, V57, P359, DOI 10.1006/ciun.1993.1024; Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428; Schmid C, 2001, PROC CVPR IEEE, P39; Sharan L, 2013, INT J COMPUT VISION, V103, P348, DOI 10.1007/s11263-013-0609-0; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; SMIRNOV N, 1948, ANN MATH STAT, V19, P279, DOI 10.1214/aoms/1177730256; Smith G, 1997, PATTERN RECOGN LETT, V18, P1495, DOI 10.1016/S0167-8655(97)00132-3; Sobel, 1990, MACHINE VISION 3 DIM, P376, DOI [DOI 10.13140/RG.2.1.1912.4965, 10.13140/RG.2.1.1912.4965]; Spillmann L, 1996, TRENDS NEUROSCI, V19, P428; TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Toet A, 2011, IEEE T PATTERN ANAL, V33, P2131, DOI 10.1109/TPAMI.2011.53; Tuceryan M., 1993, HDB PATTERN RECOGNIT, P235, DOI DOI 10.1142/9789814343138_0010; Ulyanov D, 2016, PR MACH LEARN RES, V48; UNSER M, 1986, IEEE T PATTERN ANAL, V8, P118, DOI 10.1109/TPAMI.1986.4767760; Ustyuzhaninov I., 2016, WHAT DOES IT TAKE GE; VANGOOL L, 1985, COMPUT VISION GRAPH, V29, P336, DOI 10.1016/0734-189X(85)90130-6; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4; Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182; VILNROTTER FM, 1986, IEEE T PATTERN ANAL, V8, P76, DOI 10.1109/TPAMI.1986.4767754; Wang Xinli, 1994, Proceedings of IAPR Workshop on Machine Vision Applications, P375; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wenger R, 1997, LEONARDO, V30, P35, DOI 10.2307/1576374; WESZKA JS, 1976, IEEE T SYST MAN CYB, V6, P269, DOI 10.1109/TSMC.1976.5408777; Xie X., 2009, HDB TEXTUREANALYSIS; Ying L, 2006, WILEY ENCY BIOMEDICA, V6, P1; Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882; Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zujovic J., 2011, THESIS NW U EVANSTON	129	5	5	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2429	2448		10.1109/TPAMI.2020.2964533	http://dx.doi.org/10.1109/TPAMI.2020.2964533			20	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	UL3FK	31944946				2022-12-18	WOS:000692540900019
J	Sun, WJ; Chen, ZZ; Wu, F				Sun, Wanjie; Chen, Zhenzhong; Wu, Feng			Visual Scanpath Prediction Using IOR-ROI Recurrent Mixture Density Network	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Predictive models; Computational modeling; Feature extraction; Hidden Markov models; Solid modeling; Semantics; Visual scanpath prediction; fixation duration prediction; inhibition of return; LSTM; mixture density network	EYE-MOVEMENTS; ATTENTION; MODEL; SALIENCY; INHIBITION	A visual scanpath represents the human eye movements when scanning the visual field for acquiring and receiving visual information. Predicting visual scanpaths when a certain stimulus is presented plays an important role in modeling overt human visual attention and search behavior. In this paper, we presented an 'Inhibition of Return - Region of Interest' (IOR-ROI) recurrent mixture density network based framework learning to produce human-like visual scanpaths under task-free viewing conditions. The proposed model simultaneously predicts a sequence of ordered fixation positions and their corresponding fixation durations. Our model integrates bottom-up features and semantic features extracted by convolutional neural networks. Then the integrated feature maps are fed into the IOR-ROI Long Short-Term Memory (LSTM) which is the core component of the proposed model. The IOR-ROI LSTM is a dual LSTM unit, i.e., the IOR-LSTM and the ROI-LSTM, capturing IOR dynamics and gaze shift behavior simultaneously. IOR-LSTM simulates the visual working memory to adaptively maintain and update visual information regarding previously fixated regions. ROI-LSTM is responsible for predicting the next possible ROIs given the spatially inhibited image feature maps on the feature-wise basis. Fixation duration is predicted by a regression neural network given the viewing history and image feature maps corresponding to currently fixated ROI. Considering the eye movement pattern variations among subjects, a mixture density network is adopted to model the next fixation distribution as Gaussian mixtures and the fixation duration is also modeled using Gaussian distribution. Our model is evaluated on the OSIE and MIT low resolution eye-tracking datasets and experimental results indicate that the proposed method can achieve superior performance in predicting visual scanpaths. The code will be publicly available on URL: https://github.com/sunwj/scanpath.	[Sun, Wanjie; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Hubei, Peoples R China; [Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230052, Anhui, Peoples R China	Wuhan University; Chinese Academy of Sciences; University of Science & Technology of China, CAS	Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Hubei, Peoples R China.	sunwanjie@whu.edu.cn; zzchen@ieee.org; fengwu@ustc.edu.cn			National Key R&D Program of China [2017YFB1002202]; National Natural Science Foundation of China [61771348]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key R&D Program of China under contract No. 2017YFB1002202 and the National Natural Science Foundation of China under contractNo. 61771348.	Alvarez GA, 2004, PSYCHOL SCI, V15, P106, DOI 10.1111/j.0963-7214.2004.01502006.x; Anderson NC, 2015, BEHAV RES METHODS, V47, P1377, DOI 10.3758/s13428-014-0550-3; Arabadzhiyska E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073642; Assens M., 2018, P EUR C COMP VIS WOR; Assens M, 2018, SIGNAL PROCESS-IMAGE, V69, P8, DOI 10.1016/j.image.2018.06.006; Bazzani Loris, 2017, ICLR; Bishop C.M, 2006, PATTERN RECOGN; Bishop C.M., 1994, MIXTURE DENSITY NETW; Boccignone G, 2004, PHYSICA A, V331, P207, DOI 10.1016/j.physa.2003.09.011; Boccignone G., 2016, PROBABILISTIC TOUR V; Borji A, 2018, SALIENCY PREDICTION; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Brockmann D, 2000, NEUROCOMPUTING, V32, P643, DOI 10.1016/S0925-2312(00)00227-7; Buswell Guy Thomas, 1935, PEOPLE LOOK PICTURES; Cerf M., 2008, ADV NEURAL INFORM PR, V20, P241, DOI DOI 10.1145/2185520.2185525; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Chen ZZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P642; Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154; Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672; Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174; Coull JT, 2004, COGNITIVE BRAIN RES, V21, P216, DOI 10.1016/j.cogbrainres.2004.02.011; Cristino F, 2010, BEHAV RES METHODS, V42, P692, DOI 10.3758/BRM.42.3.692; Dewhurst R, 2012, BEHAV RES METHODS, V44, P1079, DOI 10.3758/s13428-012-0212-2; DUNCAN J, 1984, J EXP PSYCHOL GEN, V113, P501, DOI 10.1037/0096-3445.113.4.501; EGLY R, 1994, J EXP PSYCHOL GEN, V123, P161, DOI 10.1037/0096-3445.123.2.161; Ehmke C., 2007, P 21 BRIT HCI GROUP, V1, P119, DOI 10.14236/ewic/HCI2007.12; Engbert R, 2015, J VISION, V15, DOI 10.1167/15.1.14; Feng G, 2006, COGN SYST RES, V7, P70, DOI 10.1016/j.cogsys.2005.07.004; Galgani F, 2009, 2009 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DATA MINING, P195, DOI 10.1109/CIDM.2009.4938649; Ha David, 2018, ICLR; HARRIS CM, 1988, VISION RES, V28, P419, DOI 10.1016/0042-6989(88)90184-8; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hotta S, 2010, ARTIF LIFE ROBOT, V15, P129, DOI 10.1007/s10015-010-0778-7; Hou X., 2009, ADV NEURAL INFORM PR, P681; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jiang M, 2016, IEEE T NEUR NET LEAR, V27, P1241, DOI 10.1109/TNNLS.2015.2496306; Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710; Judd T, 2011, J VISION, V11, DOI 10.1167/11.4.14; Klein RM, 2000, TRENDS COGN SCI, V4, P138, DOI 10.1016/S1364-6613(00)01452-2; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Kummerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513; Le Meur O, 2017, IEEE T IMAGE PROCESS, V26, P4777, DOI 10.1109/TIP.2017.2722238; Le Meur O, 2016, VISION RES, V121, P72, DOI 10.1016/j.visres.2016.01.005; Le Meur O, 2015, VISION RES, V116, P152, DOI 10.1016/j.visres.2014.12.026; LeMeurand O, 2016, P SPIE IMAGE QUAL SY, P1; Li AQ, 2017, IEEE INT CON MULTI, P535, DOI 10.1109/ICME.2017.8019507; Li C, 2017, PROC CVPR IEEE, P388, DOI 10.1109/CVPR.2017.49; Liu HY, 2013, IEEE I CONF COMP VIS, P3232, DOI 10.1109/ICCV.2013.401; Lugtigheid A., 2007, THESIS ERASMUS U; NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4; NOTON D, 1971, SCIENCE, V171, P308, DOI 10.1126/science.171.3968.308; Nuthmann A, 2017, PSYCHON B REV, V24, P370, DOI 10.3758/s13423-016-1124-4; Nuthmann A, 2010, PSYCHOL REV, V117, P382, DOI 10.1037/a0018924; Pan Junting, 2017, ABS170101081 CORR; Pannasch S, 2011, ATTEN PERCEPT PSYCHO, V73, P1120, DOI 10.3758/s13414-011-0090-1; POSNER MI, 1984, ATTENTION PERFORM, V10, P531; Rensink RA, 2000, VISION RES, V40, P1469, DOI 10.1016/S0042-6989(00)00003-1; Shao X, 2017, LECT NOTES COMPUT SC, V10636, P3, DOI 10.1007/978-3-319-70090-8_1; Shen K, 2014, J VISION, V14, DOI 10.1167/14.14.11; Shi XJ, 2015, ADV NEUR IN, V28; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sun XS, 2014, IEEE T IMAGE PROCESS, V23, P4649, DOI 10.1109/TIP.2014.2337758; Tatler B.W., 2008, J EYE MOVEMENT RES, P1; Tatler BW, 2011, J VISION, V11, DOI 10.1167/11.5.5; Tavakoli HR, 2013, IMAGE VISION COMPUT, V31, P686, DOI 10.1016/j.imavis.2013.06.006; Ngo T, 2017, IEEE IMAGE PROC, P3435; Underwood G, 2006, Q J EXP PSYCHOL, V59, P1931, DOI 10.1080/17470210500416342; Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001; Wang W, 2011, PROC CVPR IEEE, P441, DOI 10.1109/CVPR.2011.5995423; Wang YX, 2017, COGN PROCESS, V18, P87, DOI 10.1007/s10339-016-0781-6; Wloka C, 2018, PROC CVPR IEEE, P3184, DOI 10.1109/CVPR.2018.00336; Wu Y, 2017, IEEE INT CON MULTI, P529, DOI 10.1109/ICME.2017.8019456; Xia C, 2017, IEEE INT CON MULTI, P1530, DOI 10.1109/ICME.2017.8019396; Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28; Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559; Yun K, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00917; Zanca D, 2017, ADV NEUR IN, V30; Zhang L., 2013, SELECTIVE VISUAL ATT, P1	80	5	5	3	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					2101	2118		10.1109/TPAMI.2019.2956930	http://dx.doi.org/10.1109/TPAMI.2019.2956930			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	SA8YQ	31796389				2022-12-18	WOS:000649590200020
J	Senocak, A; Oh, TH; Kim, J; Yang, MH; Kweon, IS				Senocak, Arda; Oh, Tae-Hyun; Kim, Junsik; Yang, Ming-Hsuan; Kweon, In So			Learning to Localize Sound Sources in Visual Scenes: Analysis and Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Videos; Task analysis; Correlation; Deep learning; Network architecture; Unsupervised learning; Audio-visual learning; sound localization; self-supervision; multi-modal learning; cross-modal retrieval	IDENTIFICATION; SEARCH	Visual events are usually accompanied by sounds in our daily lives. However, can the machines learn to correlate the visual scene and sound, as well as localize the sound source only by observing them like humans? To investigate its empirical learnability, in this work we first present a novel unsupervised algorithm to address the problem of localizing sound sources in visual scenes. In order to achieve this goal, a two-stream network structure which handles each modality with attention mechanism is developed for sound source localization. The network naturally reveals the localized response in the scene without human annotation. In addition, a new sound source dataset is developed for performance evaluation. Nevertheless, our empirical evaluation shows that the unsupervised method generates false conclusions in some cases. Thereby, we show that this false conclusion cannot be fixed without human prior knowledge due to the well-known correlation and causality mismatch misconception. To fix this issue, we extend our network to the supervised and semi-supervised network settings via a simple modification due to the general architecture of our two-stream network. We show that the false conclusions can be effectively corrected even with a small amount of supervision, i.e., semi-supervised setup. Furthermore, we present the versatility of the learned audio and visual embeddings on the cross-modal content alignment and we extend this proposed algorithm to a new application, sound saliency based automatic camera view panning in 360 degree videos.	[Senocak, Arda; Kim, Junsik; Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea; [Oh, Tae-Hyun] POSTECH, Dept Elect Engn, Pohang 37673, South Korea; [Yang, Ming-Hsuan] Univ Calif, Dept Elect Engn & Comp Sci, Merced, CA 95343 USA	Korea Advanced Institute of Science & Technology (KAIST); Pohang University of Science & Technology (POSTECH); University of California System; University of California Merced	Kweon, IS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.; Oh, TH (corresponding author), POSTECH, Dept Elect Engn, Pohang 37673, South Korea.	arda.senocak@gmail.com; taehyun@csail.mit.edu; mibastro@gmail.com; mhyang@ucmerced.edu; iskweon@kaist.ac.kr	; Yang, Ming-Hsuan/T-9533-2019; Oh, Tae-Hyun/D-7854-2016	Kim, Junsik/0000-0003-2555-5232; Senocak, Arda/0000-0001-9141-3270; Yang, Ming-Hsuan/0000-0003-4848-2304; Oh, Tae-Hyun/0000-0003-0468-1571	National Information Society Agency [2100-2131-305-10719]; NSF CAREER [1149783]	National Information Society Agency; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	A. Senocak, J. Kim, and I.S. Kweon were supported by the National Information Society Agency for construction of training data for artificial intelligence (2100-2131-305-10719). M.-H. Yang is supported in part by NSF CAREER (No. 1149783).	Abadi M, 2015, P 12 USENIX S OPERAT; Afouras T, 2018, INTERSPEECH, P3244; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27; Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73; Aytar Y., 2017, ABS170600932 CORR; Barzelay Z., 2007, P IEEE INT C COMP VI, P1; Bolia RS, 1999, HUM FACTORS, V41, P664, DOI 10.1518/001872099779656789; Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154; Chou S.-H., 2017, P AAAI, P6748; Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fisher III J. W., 2001, P 13 INT C NEUR INF, P742; Gao RH, 2019, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2019.00041; Gao RH, 2018, LECT NOTES COMPUT SC, V11207, P36, DOI 10.1007/978-3-030-01219-9_3; GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1; Harwath D, 2018, LECT NOTES COMPUT SC, V11210, P659, DOI 10.1007/978-3-030-01231-1_40; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153; Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476; JONES B, 1975, PERCEPT PSYCHOPHYS, V17, P241, DOI 10.3758/BF03203206; Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005; Kidron E, 2005, PROC CVPR IEEE, P88; Kim C., 2018, P AS C COMP VIS ACCV, P276; Kingma D.P, P 3 INT C LEARNING R; Kopf J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982405; Korbar B., 2018, P 32 INT C NEURAL IN, P7774; Majdak P, 2010, ATTEN PERCEPT PSYCHO, V72, P454, DOI 10.3758/APP.72.2.454; Movellan J. R, 1999, P 12 INT C NEUR INF; Murino V., 2019, ARXIV190407933; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Owens A, 2018, INT J COMPUT VISION, V126, P1120, DOI 10.1007/s11263-018-1083-5; Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48; Perrott DR, 1996, HUM FACTORS, V38, P702, DOI 10.1518/001872096778827260; Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; SHELTON BR, 1980, PERCEPT PSYCHOPHYS, V28, P589, DOI 10.3758/BF03198830; SKINNER BF, 1948, J EXP PSYCHOL, V38, P168, DOI 10.1037/h0055873; Stein BE, 2008, NAT REV NEUROSCI, V9, P255, DOI 10.1038/nrn2331; Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10; Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16; Van den Oord A., 2013, P ADV NEURAL INFORM, P2643; Van Trees H. L., 2002, OPTIMUM ARRAY PROC 4; Wang O, 2018, P 32 INT C NEUR INF, P360; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhao H, 2018, LECT NOTES COMPUT SC, V11205, P587, DOI 10.1007/978-3-030-01246-5_35; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374; Zunino A, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P693, DOI 10.1109/ICCVW.2015.95	55	5	5	2	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1605	1619		10.1109/TPAMI.2019.2952095	http://dx.doi.org/10.1109/TPAMI.2019.2952095			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	RJ3YD	31722472	Green Submitted			2022-12-18	WOS:000637533800009
J	Sun, Y; Li, XD; Ernst, A				Sun, Yuan; Li, Xiaodong; Ernst, Andreas			Using Statistical Measures and Machine Learning for Graph Reduction to Solve Maximum Weight Clique Problems	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Machine learning; Optimization; Machine learning algorithms; Search problems; Heuristic algorithms; Atmospheric measurements; Particle measurements; Combinatorial optimization; machine learning; data mining; statistics; problem reduction	ALGORITHM; HEURISTICS; SELECTION; LIBRARY; DESIGN	In this article, we investigate problem reduction techniques using stochastic sampling and machine learning to tackle large-scale optimization problems. These techniques heuristically remove decision variables from the problem instance, that are not expected to be part of an optimal solution. First we investigate the use of statistical measures computed from stochastic sampling of feasible solutions compared with features computed directly from the instance data. Two measures are particularly useful for this: 1) a ranking-based measure, favoring decision variables that frequently appear in high-quality solutions; and 2) a correlation-based measure, favoring decision variables that are highly correlated with the objective values. To take this further we develop a machine learning approach, called Machine Learning for Problem Reduction (MLPR), that trains a supervised learning model on easy problem instances for which the optimal solution is known. This gives us a combination of features enabling us to better predict the decision variables that belong to the optimal solution for a given hard problem. We evaluate our approaches using a typical optimization problem on graphs-the maximum weight clique problem. The experimental results show our problem reduction techniques are very effective and can be used to boost the performance of existing solution methods.	[Sun, Yuan; Li, Xiaodong] RMIT Univ, Sch Sci, Melbourne, Vic 3001, Australia; [Ernst, Andreas] Monash Univ, Sch Math Sci, Clayton, Vic 3800, Australia	Royal Melbourne Institute of Technology (RMIT); Monash University	Sun, Y (corresponding author), RMIT Univ, Sch Sci, Melbourne, Vic 3001, Australia.	yuan.sun@rmit.edu.au; xiaodong.li@rmit.edu.au; andreas.ernst@monash.edu	Ernst, Andreas T/B-4298-2008; Sun, Yuan/C-8772-2017; Li, Xiaodong/F-4334-2010	Ernst, Andreas T/0000-0002-1101-8359; Sun, Yuan/0000-0003-2911-0070; Li, Xiaodong/0000-0003-0346-1526	Australian Research Council [DP180101170]	Australian Research Council(Australian Research Council)	This work was supported by an ARC Discovery Grant (DP180101170) from the Australian Research Council.	Akiba T, 2016, THEOR COMPUT SCI, V609, P211, DOI 10.1016/j.tcs.2015.09.023; Amunts K, 2013, SCIENCE, V340, P1472, DOI 10.1126/science.1235381; Balcan MF, 2018, PR MACH LEARN RES, V80; Bateni M, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1138, DOI 10.1145/3219819.3220081; Bischl B, 2016, ARTIF INTELL, V237, P41, DOI 10.1016/j.artint.2016.04.003; Blum C, 2016, COMPUT OPER RES, V68, P75, DOI 10.1016/j.cor.2015.10.014; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Boyan JA, 2001, J MACH LEARN RES, V1, P77, DOI 10.1162/15324430152733124; Branke J, 2016, IEEE T EVOLUT COMPUT, V20, P110, DOI 10.1109/TEVC.2015.2429314; Brech CH, 2019, EUR J OPER RES, V274, P253, DOI 10.1016/j.ejor.2018.04.003; Brendel W, 2010, ADV NEURAL INFORM PR, P307; Burke EK, 2013, J OPER RES SOC, V64, P1695, DOI 10.1057/jors.2013.71; Burke EK, 2012, EVOL COMPUT, V20, P63, DOI 10.1162/EVCO_a_00044; Butenko S, 2006, EUR J OPER RES, V173, P1, DOI 10.1016/j.ejor.2005.05.026; Cai S., 2016, P IJCAI 2016, P568; Chang C.-C., 2011, ACM T INTEL SYST TEC, V2, P1, DOI [10.1145/1961189.1961199, DOI 10.1145/1961189.1961199]; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; DeBonet JS, 1997, ADV NEUR IN, V9, P424; Dorigo M., 1997, IEEE Transactions on Evolutionary Computation, V1, P53, DOI 10.1109/4235.585892; Fan RE, 2005, J MACH LEARN RES, V6, P1889; Fan Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P622; Fang ZW, 2016, J ARTIF INTELL RES, V55, P799, DOI 10.1613/jair.4953; Fischetti M, 2019, COMPUT OPER RES, V106, P289, DOI 10.1016/j.cor.2018.04.006; Fomin FV, 2009, J ACM, V56, DOI 10.1145/1552285.1552286; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Harik GR, 1999, IEEE T EVOLUT COMPUT, V3, P287, DOI 10.1109/4235.797971; He H., 2014, ADV NEURAL INFORM PR, P3293; Hebrard E, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1316; Jelvez E, 2016, EUR J OPER RES, V249, P1169, DOI 10.1016/j.ejor.2015.10.044; Jiang H, 2017, AAAI CONF ARTIF INTE, P830; Jiang Hua, 2018, P 32 AAAI C ART INT; Johnson DS., 1993, DIMACS SERIES DISCRE, V26, P11; Kenny A, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P316, DOI 10.1145/3205455.3205538; Kenny A, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P1137, DOI 10.1145/3071178.3071241; Khalil E., 2017, ADV NEURAL INFORM PR, P6348; Khalil EB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P659; Lamm S, 2017, J HEURISTICS, V23, P207, DOI 10.1007/s10732-017-9337-x; Li CM, 2018, EUR J OPER RES, V270, P66, DOI 10.1016/j.ejor.2018.03.020; Li ZW, 2018, ADV NEUR IN, V31; Lin CJ, 2008, J MACH LEARN RES, V9, P627; Lindgren EM, 2016, ADV NEUR IN, V29; Liu R.-S., 2014, PROC 51 ACMEDACIEEE, P1; Lombardi M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5472; Martins D, 2018, ANN OPER RES, V263, P141, DOI 10.1007/s10479-014-1693-4; Mascia F, 2010, AAAI CONF ARTIF INTE, P1274; Nazari M, 2018, ADV NEUR IN, V31; Nogueira B, 2018, COMPUT OPER RES, V90, P232, DOI 10.1016/j.cor.2017.09.023; Pullan W, 2008, J HEURISTICS, V14, P117, DOI 10.1007/s10732-007-9026-2; Rossi RA, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P365, DOI 10.1145/2567948.2577283; Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292; Ruiz-Torrubiano R, 2010, IEEE COMPUT INTELL M, V5, P92, DOI 10.1109/MCI.2010.936308; Sheskin D., 2011, HDB PARAMETRIC NONPA, Vfifth, DOI [DOI 10.1201/9781420036268, 10.2307/2685909]; Shylo, 2018, ARXIV180810813; Smith-Miles KA, 2008, ACM COMPUT SURV, V41, DOI 10.1145/1456650.1456656; Nguyen S, 2013, IEEE T EVOLUT COMPUT, V17, P621, DOI 10.1109/TEVC.2012.2227326; Tayali HA, 2018, EXPERT SYST APPL, V92, P161, DOI 10.1016/j.eswa.2017.09.009; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Wang YY, 2016, AAAI CONF ARTIF INTE, P805; Wu QH, 2015, EUR J OPER RES, V242, P693, DOI 10.1016/j.ejor.2014.09.064; Wu QH, 2012, ANN OPER RES, V196, P611, DOI 10.1007/s10479-012-1124-3; Zhang W., 2000, J ARTIF INTELL RES, V1, P1; Zhou Y, 2017, EUR J OPER RES, V257, P41, DOI 10.1016/j.ejor.2016.07.056	67	5	5	4	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1746	1760		10.1109/TPAMI.2019.2954827	http://dx.doi.org/10.1109/TPAMI.2019.2954827			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31751227				2022-12-18	WOS:000637533800019
J	Liu, DQ; Bober, M; Kittler, J				Liu, Daqi; Bober, Miroslaw; Kittler, Josef			Visual Semantic Information Pursuit: A Survey	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Semantics; Task analysis; Visual perception; Cognition; Object detection; Deep learning; Semantic scene understanding; visual perception; visual context reasoning; deep learning; variational free energy minimization; message passing	NEURAL-NETWORKS; IMAGE; ATTENTION; FEATURES; FIELD	Visual semantic information comprises two important parts: the meaning of each visual semantic unit and the coherent visual semantic relation conveyed by these visual semantic units. Essentially, the former one is a visual perception task while the latter corresponds to visual context reasoning. Remarkable advances in visual perception have been achieved due to the success of deep learning. In contrast, visual semantic information pursuit, a visual scene semantic interpretation task combining visual perception and visual context reasoning, is still in its early stage. It is the core task of many different computer vision applications, such as object detection, visual semantic segmentation, visual relationship detection, or scene graph generation. Since it helps to enhance the accuracy and the consistency of the resulting interpretation, visual context reasoning is often incorporated with visual perception in current deep end-to-end visual semantic information pursuit methods. Surprisingly, a comprehensive review for this exciting area is still lacking. In this survey, we present a unified theoretical paradigm for all these methods, followed by an overview of the major developments and the future trends in each potential direction. The common benchmark datasets, the evaluation metrics and the comparisons of the corresponding methods are also introduced.	[Liu, Daqi; Bober, Miroslaw; Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England	University of Surrey	Kittler, J (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.	daqi.liu@surrey.ac.uk; m.bober@surrey.ac.uk; j.kittler@surrey.ac.uk		Kittler, Josef/0000-0002-8110-9205	U.K. Defence Science and Technology Laboratory; Engineering and Physical Research Council [EP/R018456/1]; EPSRC [EP/R018456/1] Funding Source: UKRI	U.K. Defence Science and Technology Laboratory; Engineering and Physical Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported in part by the U.K. Defence Science and Technology Laboratory, and in part by the Engineering and Physical Research Council (collaboration between U.S. DOD, U.K. MOD, and U.K. EPSRC through the Multidisciplinary University Research Initiative) under Grant EP/R018456/1.	Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arnab A, 2018, IEEE SIGNAL PROC MAG, V35, P37, DOI 10.1109/MSP.2017.2762355; Barabasi AL, 1999, PHYSICA A, V272, P173, DOI 10.1016/S0378-4371(99)00291-5; Battaglia Peter W, 2016, ARXIV161200222; BATTITI R, 1992, NEURAL COMPUT, V4, P141, DOI 10.1162/neco.1992.4.2.141; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311; Bilen H, 2015, PROC CVPR IEEE, P1081, DOI 10.1109/CVPR.2015.7298711; Blake A, 2011, MARKOV RANDOM FIELDS FOR VISION AND IMAGE PROCESSING, P1; Byeon W, 2015, PROC CVPR IEEE, P3547, DOI 10.1109/CVPR.2015.7298977; Chen L.-C., 2015, COMPUT SCI; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440; Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231; Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352; Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Fazelnia G., 2018, P INT C MACH LEARN, P1476; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Georges A, 1996, REV MOD PHYS, V68, P13, DOI 10.1103/RevModPhys.68.13; Gilmer J, 2017, PR MACH LEARN RES, V70; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Graves A, 2014, NEURAL TURING MACHIN; Gupta A, 2009, PROC CVPR IEEE, P2012, DOI 10.1109/CVPRW.2009.5206492; Hakeem A, 2007, ARTIF INTELL, V171, P586, DOI 10.1016/j.artint.2007.04.002; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; He XD, 2017, IEEE SIGNAL PROC MAG, V34, P109, DOI 10.1109/MSP.2017.2741510; Herzig R., 2018, P ANN C NEUR INF PRO, P7211; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Hu Z., 2016, P C EMP METH NAT LAN, P1670, DOI DOI 10.18653/V1/D16-1173; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; Jahangiri E., 2017, ARXIV170102343; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuettel D, 2010, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2010.5539869; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Li Y., 2016, P INT C LEARN REPR; Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21; Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142; Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536; Liu Q, 2013, J MACH LEARN RES, V14, P3165; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Liu Y, 2018, PROC CVPR IEEE, P6985, DOI 10.1109/CVPR.2018.00730; Liu ZW, 2018, IEEE T PATTERN ANAL, V40, P1814, DOI 10.1109/TPAMI.2017.2737535; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Malinowski M, 2017, INT J COMPUT VISION, V125, P110, DOI 10.1007/s11263-017-1038-2; Meltzer T., 2009, P UAI, P393; Mikolov T., 2013, EFFICIENT ESTIMATION, P1, DOI DOI 10.48550/ARXIV.1301.3781; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Peyre J, 2017, IEEE I CONF COMP VIS, P5189, DOI 10.1109/ICCV.2017.554; Pinheiro PO, 2014, PR MACH LEARN RES, V32; Plesse F., 2018, INT C CONTENT BASED, P1; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Ren S., 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.169; Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152; Ross S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2737, DOI 10.1109/CVPR.2011.5995724; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santoro A, 2016, PR MACH LEARN RES, V48; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Sharma A, 2014, 2014 PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP 2014), P247; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shuai B, 2018, IEEE T PATTERN ANAL, V40, P1480, DOI 10.1109/TPAMI.2017.2712691; Shuai B, 2016, PROC CVPR IEEE, P3620, DOI 10.1109/CVPR.2016.394; Shuai B, 2016, IEEE T IMAGE PROCESS, V25, P2379, DOI 10.1109/TIP.2016.2533862; Socher Richard, 2013, NEURIPS; Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013; Teney D, 2017, IEEE SIGNAL PROC MAG, V34, P63, DOI 10.1109/MSP.2017.2739826; Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386; Vinyals Oriol, 2016, ARXIV160604080, P3630; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150; Weiss Y., 2007, PROC C UNCERTAINTY A, P416; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Woo S, 2018, ADV NEUR IN, V31; Xie SN, 2016, LECT NOTES COMPUT SC, V9908, P302, DOI 10.1007/978-3-319-46493-0_19; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yang JM, 2014, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2014.415; Yang Z., 2018, P INT C NEUR INF PRO, P8964; Yedidia JS, 2001, ADV NEUR IN, V13, P689; Yin GJ, 2018, LECT NOTES COMPUT SC, V11207, P330, DOI 10.1007/978-3-030-01219-9_20; Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611; Zeng XY, 2018, IEEE T PATTERN ANAL, V40, P2109, DOI 10.1109/TPAMI.2017.2745563; Zhang C, 2006, ADV NEURAL INFORM PR, P1417; Zhang C, 2019, IEEE T PATTERN ANAL, V41, P2008, DOI 10.1109/TPAMI.2018.2889774; Zhang HW, 2017, IEEE I CONF COMP VIS, P4243, DOI 10.1109/ICCV.2017.454; Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331; Zhang J, 2019, AAAI CONF ARTIF INTE, P9185; Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555; Zhang YM, 2013, IEEE T PATTERN ANAL, V35, P2468, DOI 10.1109/TPAMI.2013.33; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhuang BH, 2017, IEEE I CONF COMP VIS, P589, DOI 10.1109/ICCV.2017.71; Zoller T, 2007, IEEE T PATTERN ANAL, V29, P1147, DOI 10.1109/TPAMI.2007.1150	123	5	5	7	34	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1404	1422		10.1109/TPAMI.2019.2950025	http://dx.doi.org/10.1109/TPAMI.2019.2950025			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	31675316	Green Submitted			2022-12-18	WOS:000626525300022
J	Wei, ZJ; Wang, BY; Hoai, M; Zhang, JM; Shen, XH; Lin, Z; Mech, R; Samaras, D				Wei, Zijun; Wang, Boyu; Hoai, Minh; Zhang, Jianming; Shen, Xiaohui; Lin, Zhe; Mech, Radomir; Samaras, Dimitris			Sequence-to-Segments Networks for Detecting Segments in Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Proposals; Decoding; Time series analysis; Task analysis; Microsoft Windows; Computer architecture; Segment detection; video analysis; video summarization; video highlighting; video temporal action proposal	REPRESENTATIONS	Detecting segments of interest from videos is a common problem for many applications. And yet it is a challenging problem as it often requires not only knowledge of individual target segments, but also contextual understanding of the entire video and the relationships between the target segments. To address this problem, we propose the Sequence-to-Segments Network ((SN)-N-2), a novel and general end-to-end sequential encoder-decoder architecture. (SN)-N-2 first encodes the input video into a sequence of hidden states that capture information progressively, as it appears in the video. It then employs the Segment Detection Unit (SDU), a novel decoding architecture, that sequentially detects segments. At each decoding step, the SDU integrates the decoder state and encoder hidden states to detect a target segment. During training, we address the problem of finding the best assignment of predicted segments to ground truth using the Hungarian Matching Algorithm with Lexicographic Cost. Additionally we propose to use the squared Earth Mover's Distance to optimize the localization errors of the segments. We show the state-of-the-art performance of (SN)-N-2 across numerous tasks, including video highlighting, video summarization, and human action proposal generation.	[Wei, Zijun; Wang, Boyu; Hoai, Minh; Samaras, Dimitris] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Zhang, Jianming; Lin, Zhe; Mech, Radomir] Adobe Res, San Jose, CA 95110 USA; [Shen, Xiaohui] ByteDance AI Lab, Melon Pk, CA 94025 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Adobe Systems Inc.	Wang, BY (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	zijwei@cs.stonybrook.edu; boywang@cs.stonybrook.edu; minhhoai@cs.stonybrook.edu; jianmzha@adobe.com; xshen@adobe.com; zlin@adobe.com; rmech@adobe.com; samaras@cs.sunysb.edu	Zhang, Jianming/B-1665-2017	Zhang, Jianming/0000-0002-9954-6294; Mech, Radomir/0000-0002-5558-0327; Samaras, Dimitris/0000-0002-1373-0294	NSF [CNS-1718014, IIS-1763981, IIS-1566248]; Partner University Fund; SUNY2020 Infrastructure Transportation Security Center	NSF(National Science Foundation (NSF)); Partner University Fund; SUNY2020 Infrastructure Transportation Security Center	This project was partially supported by NSF-CNS-1718014, NSF-IIS-1763981, NSF-IIS-1566248, the Partner University Fund, the SUNY2020 Infrastructure Transportation Security Center, and a gift from Adobe. Zijun Wei and Boyu Wang contributed equally to this work.	Adi Y, 2017, INT CONF ACOUST SPEE, P2422, DOI 10.1109/ICASSP.2017.7952591; Alwassel H, 2018, LECT NOTES COMPUT SC, V11207, P264, DOI 10.1007/978-3-030-01219-9_16; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Buch S, 2017, P BRIT MACH VIS; Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cho K., 2014, P 2014 C EMP METH NA, P1724; Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981; Chung J., 2014, DEEP LEARN WORKSH C; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47; Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392; Gehring J, 2017, PR MACH LEARN RES, V70; Gong B., 2014, ADV NEURAL INFORM PR, P2069; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2014, NEURAL TURING MACHIN; Graves A., 2006, P INT C MACH LEARN I; Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928; Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33; Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211; Hoai M., ADV STRUCTURED PREDI; Hoai M, 2014, PATTERN RECOGN, V47, P1523, DOI 10.1016/j.patcog.2013.09.028; Hou L., 2016, ARXIV161105916; Ji Z., 2017, ARXIV170809545; Jiang Y.-G., 2014, ECCV WORKSH; Johnson J., 2016, P INT C LEARN REPR W; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kang H.-W., 2006, COMPUTER VISION PATT, P1331, DOI DOI 10.1109/CVPR.2006.284; Kelley DR, 2018, GENOME RES, V28, P739, DOI 10.1101/gr.227819.117; Kingma D.P., 2015, ICLR, P1; Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053; Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820; Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Luenberger D. G., 1973, INTRO LINEAR NONLINE; Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214; Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318; Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3; Minh Hoai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3265, DOI 10.1109/CVPR.2011.5995470; Nguyen MH, 2009, IEEE I CONF COMP VIS, P1925, DOI 10.1109/ICCV.2009.5459426; Namikawa J, 2008, NEURAL NETWORKS, V21, P1466, DOI 10.1016/j.neunet.2008.09.005; Nepal S., 2001, P 9 ACM INT C MULTIM, P261, DOI DOI 10.1145/500141.500181; Peng Nanyun, 2015, P 2015 C EMP METH NA, P548, DOI DOI 10.18653/V1/D15-1064; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Simon T, 2010, PROC CVPR IEEE, P2737, DOI 10.1109/CVPR.2010.5539998; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255; Sun M, 2016, P EUR C COMP VIS, P609; Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tang H, 2011, IEEE INT CON MULTI; Vaswani A, 2017, ADV NEUR IN, V30; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Vinyals Oriol, 2015, NIPS; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xiong Yuanjun, 2016, ARXIV160800797; Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526; Yao K., 2015, ARXIV150803790; Yeo B.-L., 1995, P AS C COMP VIS, P11; Yu G, 2015, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2015.7298735; Zhang J, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901336; Zhang K, 2018, LECT NOTES COMPUT SC, V11212, P391, DOI 10.1007/978-3-030-01237-3_24; Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47; Zhao B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P863, DOI 10.1145/3123266.3123328; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317; Zhou K, 2017, P AAAI C ART INT, P7582	74	5	5	2	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2021	43	3					1009	1021		10.1109/TPAMI.2019.2940225	http://dx.doi.org/10.1109/TPAMI.2019.2940225			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QE6IS	31514124				2022-12-18	WOS:000616309900018
J	Lin, ZH; Huang, SY; Wang, YCF				Lin, Zhi-Hao; Huang, Sheng Yu; Wang, Yu-Chiang Frank			Learning of 3D Graph Convolution Networks for Point Cloud Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Feature extraction; Convolution; Kernel; Shape; Two dimensional displays; Task analysis; 3D vision; point clouds; deformable kernels; graph convolution networks; 3D classification; 3D segmentation		Point clouds are among the popular geometry representations in 3D vision. However, unlike 2D images with pixel-wise layouts, such representations containing unordered data points which make the processing and understanding the associated semantic information quite challenging. Although a number of previous works attempt to analyze point clouds and achieve promising performances, their performances would degrade significantly when data variations like shift and scale changes are presented. In this paper, we propose 3D graph convolution networks (3D-GCN), which uniquely learns 3D kernels with graph max-pooling mechanisms for extracting geometric features from point cloud data across different scales. We show that, with the proposed 3D-GCN, satisfactory shift and scale invariance can be jointly achieved. We show that 3D-GCN can be applied to point cloud classification and segmentation tasks, with ablation studies and visualizations verifying the design of 3D-GCN.	[Lin, Zhi-Hao; Huang, Sheng Yu; Wang, Yu-Chiang Frank] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan	National Taiwan University	Lin, ZH (corresponding author), Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.	r08942062@ntu.edu.tw; r08942095@ntu.edu.tw; ycwang@ntu.edu.tw			Ministry of Science and Technology of Taiwan [MOST 109-2634-F-002-037]	Ministry of Science and Technology of Taiwan(Ministry of Science and Technology, Taiwan)	This work was supported in part by the Ministry of Science and Technology of Taiwan under Grant MOST 109-2634-F-002-037.	Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170; Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Belongie S, 2001, ADV NEUR IN, V13, P831; Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Defferrard M, 2016, ADV NEUR IN, V29; Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7; Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828; Hamilton WL, 2017, ADV NEUR IN, V30; Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278; Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526; Kipf Thomas N, 2016, 5 INT C LEARN REPR I; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li YY, 2018, ADV NEUR IN, V31; Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910; Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI 10.1109/CVPR.2015.7298965; Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ronneberger O., 2015, P INT C MED IMAG COM, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, DOI 10.48550/ARXIV.1505.04597]; Roynard X., 2018, ARXIV 180403583; Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478; Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679; van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x; Velickovic P., 2018, P INT C LEARN REPR; Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169	56	5	5	13	25	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 16	2021	44	8					4212	4224		10.1109/TPAMI.2021.3059758	http://dx.doi.org/10.1109/TPAMI.2021.3059758			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	2Q6HM	33591911				2022-12-18	WOS:000820521600002
J	Hofmeyr, DP				Hofmeyr, David P.			Fast Exact Evaluation of Univariate Kernel Sums	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Estimation; Probability distribution; Independent component analysis; Deconvolution; Image reconstruction; Noise measurement; Linear time; density estimation; density derivative; projection pursuit; independent component analysis; non-parametric regression; image deconvolution; image denoising; image reconstruction	BANDWIDTH SELECTION; DENSITY-ESTIMATION	This paper presents new methodology for computationally efficient evaluation of univariate kernel sums. It is shown that a rich class of kernels allows for exact evaluation of functions expressed as a sum of kernels using simple recursions. Given an ordered sample the computational complexity is linear in the sample size. Direct applications to the estimation of denisties and their derivatives shows that the proposed approach is competitive with the state-of-the-art. Extensions to multivariate problems including independent component analysis and spatial smoothing illustrate the versatility of univariate kernel estimators, and highlight the efficiency and accuracy of the proposed approach. Multiple applications in image processing, including image deconvolution; denoising; and reconstruction are considered, showing that the proposed approach offers very promising potential in these fields.	[Hofmeyr, David P.] Stellenbosch Univ, Dept Stat & Actuarial Sci, Stellenbosch, South Africa	Stellenbosch University	Hofmeyr, DP (corresponding author), Stellenbosch Univ, Dept Stat & Actuarial Sci, Stellenbosch, South Africa.	dhofmeyr@sun.ac.za	Hofmeyr, David/AAC-4042-2021	Hofmeyr, David/0000-0003-3068-8128				Baddeley A, 2016, CHAP HALL CRC INTERD, P1; Chernozhukov V., 2007, 14385 NONLINEAR ECON; Eddelbuettel D, 2011, J STAT SOFTW, V40, P1; FAN J, 1994, J COMPUTATIONAL GRAP, V3, P35; Farid H., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P262, DOI 10.1109/CVPR.1999.786949; FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; FRIEDMAN JH, 1984, J AM STAT ASSOC, V79, P599, DOI 10.2307/2288406; Hall P, 1996, J MULTIVARIATE ANAL, V56, P165, DOI 10.1006/jmva.1996.0009; Heidenreich NB, 2013, ASTA-ADV STAT ANAL, V97, P403, DOI 10.1007/s10182-013-0216-y; HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; IZENMAN AJ, 1991, J AM STAT ASSOC, V86, P205, DOI 10.2307/2289732; Kohler M, 2014, INT STAT REV, V82, P243, DOI 10.1111/insr.12039; MARRON JS, 1992, ANN STAT, V20, P712, DOI 10.1214/aos/1176348653; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Ole A., 2018, EBIMAGE IMAGE PROCES; Pavlidis NG, 2016, J MACH LEARN RES, V17; Raykar VC, 2010, J COMPUT GRAPH STAT, V19, P205, DOI 10.1198/jcgs.2010.09046; SCOTT DW, 1985, COMMUN STAT-THEOR M, V14, P1353, DOI 10.1080/03610928508828980; Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939; Silverman B.W., 1986, DENSITY ESTIMATION S, V26; SILVERMAN BW, 1982, APPL STAT-J ROY ST C, V31, P93, DOI 10.2307/2347084; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340; Yang CJ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P464	24	5	5	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2021	43	2					447	458		10.1109/TPAMI.2019.2930501	http://dx.doi.org/10.1109/TPAMI.2019.2930501			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PR6ZZ	31352334	Green Submitted			2022-12-18	WOS:000607383300006
J	Dixit, M; Li, YS; Vasconcelos, N				Dixit, Mandar; Li, Yunsheng; Vasconcelos, Nuno			Semantic Fisher Scores for Task Transfer: Using Objects to Classify Scenes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Neural networks; Training data; Object recognition; Neural networks; Computational modeling; Probability; Deep neural network; scene classification; fisher vector; MFA	IMAGE; CLASSIFICATION; MODELS	The transfer of a neural network (CNN) trained to recognize objects to the task of scene classification is considered. A Bag-of-Semantics (BoS) representation is first induced, by feeding scene image patches to the object CNN, and representing the scene image by the ensuing bag of posterior class probability vectors (semantic posteriors). The encoding of the BoS with a Fisher vector (FV) is then studied. A link is established between the FVof any probabilistic model and theQ-function of the expectation-maximization (EM) algorithm used to estimate its parameters by maximumlikelihood. This enables 1) immediate derivation of FVs for any model for which an EM algorithm exists, and 2) leveraging efficient implementations from theEM literature for the computation of FVs. It is then shown that standard FVs, such as those derived from Gaussian or even Dirichlet mixtures, are unsuccessful for the transfer of semantic posteriors, due to the highly non-linear nature of the probability simplex. The analysis of these FVs shows that significant benefits can ensue by 1) designing FVs in the natural parameter space of the multinomial distribution, and 2) adopting sophisticated probabilistic models of semantic feature covariance. The combination of these two insights leads to the encoding of the BoS in the natural parameter space of the multinomial, using a vector of Fisher scores derived from a mixture of factor analyzers (MFA). A network implementation of the MFA Fisher Score (MFA-FS), denoted as the MFAFSNet, is finally proposed to enable end-to-end training. Experiments with various object CNNs and datasets show that the approach has state-of-the-art transfer performance. Somewhat surprisingly, the scene classification results are superior to those of a CNN explicitly trained for scene classification, using a large scene dataset (Places). This suggests that holistic analysis is insufficient for scene classification. The modeling of local object semantics appears to be at least equally important. The two approaches are also shown to be strongly complementary, leading to very large scene classification gains when combined, and outperforming all previous scene classification approaches by a sizable margin.	[Dixit, Mandar] Microsoft Corp, Redmond, WA 98052 USA; [Li, Yunsheng; Vasconcelos, Nuno] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA	Microsoft; University of California System; University of California San Diego	Li, YS (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.	madixit@microsoft.com; yul554@ucsd.edu; nvasconcelos@ucsd.edu		Vasconcelos, Nuno/0000-0002-9024-4302	NSF [IIS-1208522, IIS-1637941]	NSF(National Science Foundation (NSF))	This work was supported by NSF awards IIS-1208522 and IIS-1637941 and GPU donations by Nvidia.	Adelson EH, 2001, PROC SPIE, V4299, P1, DOI 10.1117/12.429489; Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Bergamo A., 2011, NIPS, V24, P2088; Bergamo A, 2014, IEEE T PATTERN ANAL, V36, P1988, DOI 10.1109/TPAMI.2014.2313111; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3; Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007; Cinbis RG, 2012, PROC CVPR IEEE, P2184, DOI 10.1109/CVPR.2012.6247926; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Delhumeau Jonathan, 2013, ACM MM, P653, DOI DOI 10.1145/2502081.2502171; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dixit M, 2015, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2015.7298916; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Ghahramani Z, 1997, CRGTR961; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26; Guo S, 2017, IEEE T IMAGE PROCESS, V26, P808, DOI 10.1109/TIP.2016.2629443; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Khan SH, 2017, IEEE I CONF COMP VIS, P5639, DOI 10.1109/ICCV.2017.601; Kobayashi T, 2014, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR.2014.413; Kordumova S, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P143, DOI 10.1145/2911996.2912007; Krapac J, 2011, IEEE I CONF COMP VIS, P1487, DOI 10.1109/ICCV.2011.6126406; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Kwitt Roland, 2012, Computer Vision - ECCV 2012. Proceedings of the 12th European Conference on Computer Vision, P359, DOI 10.1007/978-3-642-33765-9_26; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x; Li WX, 2017, INT J COMPUT VISION, V122, P334, DOI 10.1007/s11263-016-0918-1; Li Y, 2015, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2015.7298699; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu J, 2011, 2011 AASRI CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INDUSTRY APPLICATION (AASRI-AIIA 2011), VOL 4, P333; Liu L., 2014, P ADV NEUR INF PROC, V27, P1143; Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2335, DOI 10.1109/TPAMI.2017.2651061; Minka T. P., 2000, TECH REP; Nikhil R., 2008, 2008 IEEE C COMP VIS, P1; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Perronnin F, 2007, PROC CVPR IEEE, P2272; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138; Rasiwasia N, 2012, IEEE T PATTERN ANAL, V34, P902, DOI 10.1109/TPAMI.2011.175; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Su H., 2010, ADV NEURAL PROCESSIN, V1, P1378; Su Y, 2012, INT J COMPUT VISION, V100, P59, DOI 10.1007/s11263-012-0529-4; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tanaka Masayuki, 2013, IPSJ T COMPUTER VISI, V5, P50, DOI 10.2197/ipsjtcva.5.50; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; van der Maaten L., 2011, P INT C MACH LEARN, P217; Vasconcelos N, 2000, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2000.855822; Verbeek J, 2006, IEEE T PATTERN ANAL, V28, P1236, DOI 10.1109/TPAMI.2006.166; Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1; Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757; Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	72	5	6	0	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2020	42	12					3102	3118		10.1109/TPAMI.2019.2921960	http://dx.doi.org/10.1109/TPAMI.2019.2921960			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	OP2KH	31180842	hybrid, Green Submitted			2022-12-18	WOS:000587912800010
J	Wei, X; Shen, H; Kleinsteuber, M				Wei, Xian; Shen, Hao; Kleinsteuber, Martin			Trace Quotient with Sparsity Priors for Learning Low Dimensional Image Representations	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Learning systems; Sparse matrices; Semantics; Machine learning; Machine learning algorithms; Image processing; Semisupervised learning; Representation learning; sparse representation; trace quotient; dictionary learning; geometric conjugate gradient algorithm; supervised learning; unsupervised learning; semi-supervised learning	FACE RECOGNITION; K-SVD; DICTIONARY; REDUCTION; ALGORITHM; REGULARIZATION; OPTIMIZATION; FRAMEWORK; MODELS	This work studies the problem of learning appropriate low dimensional image representations. We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion, to disentangle underlying factors of variation in high dimensional images. Specifically, we aim to learn simple representations of low dimensional, discriminant factors by applying the trace quotient criterion to well-engineered sparse representations. We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation. The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning. In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable. Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold. Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face/digit recognition, and object/scene categorization.	[Wei, Xian] Chinese Acad Sci, Fujian Inst Res Struct Matter, Beijing 100864, Peoples R China; [Wei, Xian; Shen, Hao; Kleinsteuber, Martin] Tech Univ Munich, D-80333 Munich, Germany; [Shen, Hao] Fortiss GmbH, D-80805 Munich, Germany; [Kleinsteuber, Martin] Mercateo AG, D-80331 Munich, Germany	Chinese Academy of Sciences; Fujian Institute of Research on the Structure of Matter, CAS; Technical University of Munich; fortiss	Shen, H (corresponding author), Fortiss GmbH, D-80805 Munich, Germany.	xian.wei@tum.de; shen@fortiss.de; kleinsteuber@tum.de	shen, hao/HGF-0551-2022		German Research Foundation (DFG) [KL 2189/9-1]; CAS Pioneer Hundred Talents Program (Type C) [2017-122]; National Science Found for Young Scholars [61806186]	German Research Foundation (DFG)(German Research Foundation (DFG)); CAS Pioneer Hundred Talents Program (Type C); National Science Found for Young Scholars	This work has been supported by the German Research Foundation (DFG) under Grant No. KL 2189/9-1, the CAS Pioneer Hundred Talents Program (Type C) under Grant No.2017-122, and the National Science Found for Young Scholars under Grant No. 61806186. The authors would like to acknowledge Prof. Junbin Gao (The University of Sydney Business School, Australia) for his valuable opinions and suggestions.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; [Anonymous], 2008, ANAL 2; [Anonymous], 2007, CALTECH 256 OBJECT C; Bagnell J. A., 2009, ADV NEURAL INF PROCE, V21, P113; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Billingsley P., 2009, PROBABILITY MEASURE; Cai D, 2007, IEEE I CONF COMP VIS, P222; Cai D, 2006, IEEE T IMAGE PROCESS, V15, P3608, DOI 10.1109/TIP.2006.881945; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Gao JB, 2012, PATTERN RECOGN LETT, V33, P1163, DOI 10.1016/j.patrec.2012.02.007; Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63; Gkioulekas I., 2011, ADV NEURAL INF PROCE, P271; Hawe S, 2013, IEEE T IMAGE PROCESS, V22, P2138, DOI 10.1109/TIP.2013.2246175; He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88; Kim H., 2018, 35 INT C MACH LEARN; Kleinsteuber M, 2007, INT CONF ACOUST SPEE, P1405; Kokiopoulou E, 2011, NUMER LINEAR ALGEBR, V18, P565, DOI 10.1002/nla.743; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; LeCun Y., 2015, NATURE, V521, P436, DOI DOI 10.1038/NATURE14539; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Lobel H, 2015, IEEE T PATTERN ANAL, V37, P2218, DOI 10.1109/TPAMI.2015.2408349; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Mairal J, 2010, J MACH LEARN RES, V11, P19; Ngo TT, 2010, SIAM J MATRIX ANAL A, V31, P2950, DOI 10.1137/090776603; Perronnin F, 2008, IEEE T PATTERN ANAL, V30, P1243, DOI 10.1109/TPAMI.2007.70755; Qiu Q, 2014, IEEE T PATTERN ANAL, V36, P2173, DOI 10.1109/TPAMI.2014.2316824; Ranzato M.A., 2006, ADV NEURAL INFORM PR, V19, P1137, DOI DOI 10.7551/MITPRESS/7503.003.0147; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Skretting K, 2010, IEEE T SIGNAL PROCES, V58, P2121, DOI 10.1109/TSP.2010.2040671; Srinivas U, 2015, IEEE T IMAGE PROCESS, V24, P1763, DOI 10.1109/TIP.2015.2409572; Teh YW, 2004, J MACH LEARN RES, V4, P1235; Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018; Wei X, 2016, PROC CVPR IEEE, P5268, DOI 10.1109/CVPR.2016.569; Wei X, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL ASIA CONFERENCE ON INDUSTRIAL ENGINEERING AND MANAGEMENT INNOVATION: CORE THEORY AND APPLICATIONS OF INDUSTRIAL ENGINEERING, VOL 1, P23, DOI 10.2991/978-94-6239-148-2_3; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757; Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8; Yu GX, 2012, PATTERN RECOGN, V45, P1119, DOI 10.1016/j.patcog.2011.08.024; Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhou P, 2017, IEEE T IMAGE PROCESS, V26, P1173, DOI 10.1109/TIP.2016.2623487; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	55	5	5	1	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2020	42	12					3119	3135		10.1109/TPAMI.2019.2921031	http://dx.doi.org/10.1109/TPAMI.2019.2921031			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	OP2KH	31180888	Green Submitted			2022-12-18	WOS:000587912800011
J	Gu, SH; Guo, S; Zuo, WM; Chen, YJ; Timofte, R; Van Gool, L; Zhang, L				Gu, Shuhang; Guo, Shi; Zuo, Wangmeng; Chen, Yunjin; Timofte, Radu; Van Gool, Luc; Zhang, Lei			Learned Dynamic Guidance for Depth Image Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Analytical models; Optimization; Image reconstruction; Training data; Data models; Network architecture	MINIMIZATION	The depth images acquired by consumer depth sensors (e.g., Kinect and ToF) usually are of low resolution and insufficient quality. One natural solution is to incorporate a high resolution RGB camera and exploit the statistical correlation of its data and depth. In recent years, both optimization-based and learning-based approaches have been proposed to deal with the guided depth reconstruction problems. In this paper, we introduce a weighted analysis sparse representation (WASR) model for guided depth image enhancement, which can be considered a generalized formulation of a wide range of previous optimization-based models. We unfold the optimization by the WASR model and conduct guided depth reconstruction with dynamically changed stage-wise operations. Such a guidance strategy enables us to dynamically adjust the stage-wise operations that update the depth image, thus improving the reconstruction quality and speed. To learn the stage-wise operations in a task-driven manner, we propose two parameterizations and their corresponding methods: dynamic guidance with Gaussian RBF nonlinearity parameterization (DG-RBF) and dynamic guidance with CNN nonlinearity parameterization (DG-CNN). The network structures of the proposed DG-RBF and DG-CNN methods are designed with the the objective function of our WASR model in mind and the optimal network parameters are learned from paired training data. Such optimization-inspired network architectures enable our models to leverage the previous expertise as well as take benefit from training data. The effectiveness is validated for guided depth image super-resolution and for realistic depth image reconstruction tasks using standard benchmarks. Our DG-RBF and DG-CNN methods achieve the best quantitative results (RMSE) and better visual quality than the state-of-the-art approaches at the time of writing. The code is available at https://github.com/ShuhangGu/GuidedDepthSR.	[Gu, Shuhang; Timofte, Radu; Van Gool, Luc] Swiss Fed Inst Technol, Comp Vis Lab, CH-8092 Zurich, Switzerland; [Guo, Shi; Zuo, Wangmeng] Harbin Inst Technol, Harbin 150001, Peoples R China; [Chen, Yunjin] ULSee Inc, San Francisco, CA 94105 USA; [Van Gool, Luc] Katholieke Univ Leuven, B-3000 Leuven, Belgium; [Zhang, Lei] Hong Kong Polytech Univ, Kowloon, Hong Kong, Peoples R China	Swiss Federal Institutes of Technology Domain; ETH Zurich; Harbin Institute of Technology; KU Leuven; Hong Kong Polytechnic University	Gu, SH (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, CH-8092 Zurich, Switzerland.	shuhanggu@gmail.com; guoshi28@outlook.com; cswmzuo@gmail.com; cheny@icg.tugraz.at; timofter@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch; cslzhang@comp.polyu.edu.hk	Timofte, Radu/AAK-6022-2021; Guo, Shi/AAD-9369-2022	Timofte, Radu/0000-0002-1478-0402; Zuo, Wangmeng/0000-0002-3330-783X; Zhang, Lei/0000-0002-2078-4215; Van Gool, Luc/0000-0002-3445-5711	ETH Zurich OK Fund; Huawei; Nvidia through a GPU Grant	ETH Zurich OK Fund; Huawei(Huawei Technologies); Nvidia through a GPU Grant	This work was supported in part by the ETH Zurich OK Fund, by Huawei and by Nvidia through a GPU Grant.	[Anonymous], 2010, P INT C MACH LEARN; [Anonymous], 2017, ANALYSIS-UK, DOI DOI 10.3390/EN10050651; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Beck A, 2009, INT CONF ACOUST SPEE, P693, DOI 10.1109/ICASSP.2009.4959678; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Chen YJ, 2015, PROC CVPR IEEE, P5261, DOI 10.1109/CVPR.2015.7299163; Diebel James, 2005, NEURAL INF PROCESS S, P291; Domke J, 2013, IEEE T PATTERN ANAL, V35, P2454, DOI 10.1109/TPAMI.2013.31; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592; Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270; Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189; Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hochreiter S., 2001, FIELD GUIDE DYNAMICA, DOI DOI 10.1109/9780470544037.CH14; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; King DB, 2015, ACS SYM SER, V1214, P1; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Kwon H, 2015, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2015.7298611; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10; Li YY, 2009, INVERSE PROBL IMAG, V3, P487, DOI 10.3934/ipi.2009.3.487; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Liu RS, 2020, IEEE T PATTERN ANAL, V42, P3027, DOI 10.1109/TPAMI.2019.2920591; Lu JJ, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5966, DOI 10.1145/3025453.3025478; Lu S, 2014, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2014.433; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Obermaier M, 2018, COMMUN RES, V45, P1031, DOI 10.1177/0093650215617505; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Riegler G., 2016, P BRIT MACH VIS C; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; Rubinstein R, 2013, IEEE T SIGNAL PROCES, V61, DOI 10.1109/TSP.2012.2226445; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Saier MH, 2007, WATER AIR SOIL POLL, V181, P1, DOI 10.1007/s11270-007-9372-6; Schmidt M., 2013, MINFUNC; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI 10.1117/12.408568; Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206; Wan CD, 2018, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2018.00540; Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211; Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25; Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776; Yang Y, 2016, ADV NEUR IN, V29; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu JJ, 2008, PROC CVPR IEEE, P3262; Zhu JJ, 2011, IEEE T PATTERN ANAL, V33, P1400, DOI 10.1109/TPAMI.2010.172; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zuo WM, 2015, PROC CVPR IEEE, P3232, DOI 10.1109/CVPR.2015.7298943	64	5	5	3	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2020	42	10					2437	2452		10.1109/TPAMI.2019.2961672	http://dx.doi.org/10.1109/TPAMI.2019.2961672			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NL5QY	31870979	Green Accepted			2022-12-18	WOS:000567471300009
J	Wang, SF; Zheng, ZQ; Yin, S; Yang, JJ; Ji, Q				Wang, Shangfei; Zheng, Zhuangqiang; Yin, Shi; Yang, Jiajia; Ji, Qiang			A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Hidden Markov models; Bayes methods; Muscles; Face recognition; Analytical models; Facial muscles; Interval temporal restricted Boltzmann machine; global spatial and temporal patterns; posed and spontaneous expressions distinction; expressions categories recognition	RECOGNITION; DELIBERATE	Facial expression analysis could be greatly improved by incorporating spatial and temporal patterns present in facial behavior, but the patterns have not yet been utilized to their full advantage. We remedy this via a novel dynamic model-an interval temporal restricted Boltzmann machine (IT-RBM) - that is able to capture both universal spatial patterns and complicated temporal patterns in facial behavior for facial expression analysis. We regard a facial expression as a multifarious activity composed of sequential or overlapping primitive facial events. Allen's interval algebra is implemented to portray these complicated temporal patterns via a two-layer Bayesian network. The nodes in the upper-most layer are representative of the primitive facial events, and the nodes in the lower layer depict the temporal relationships between those events. Our model also captures inherent universal spatial patterns via a multi-value restricted Boltzmann machine in which the visible nodes are facial events, and the connections between hidden and visible nodes model intrinsic spatial patterns. Efficient learning and inference algorithms are proposed. Experiments on posed and spontaneous expression distinction and expression recognition demonstrate that our proposed IT-RBM achieves superior performance compared to state-of-the art research due to its ability to incorporate these facial behavior patterns.	[Wang, Shangfei; Zheng, Zhuangqiang; Yin, Shi; Yang, Jiajia] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China; [Ji, Qiang] Rensselaer Polytech Inst, Dept Elect Comp Syst Engn, Troy, NY 12180 USA	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Rensselaer Polytechnic Institute	Yang, JJ (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.	sfwang@ustc.edu.cn; zheng001@mail.ustc.edu.cn; davidyin@mail.ustc.edu.cn; yang25@mail.ustc.edu.cn; qji@ecse.rpi.edu		wang, shangfei/0000-0003-1164-9895	National Key R&D Program of China [2018YFB1307102]; National Science Foundation of China [917418129]	National Key R&D Program of China; National Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work has been supported by the National Key R&D Program of China (2018YFB1307102) and the National Science Foundation of China (917418129).	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434; [Anonymous], 2017, FRONT INFORM TECH EL, DOI DOI 10.1631/FITEE.1600041; Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X; Cohn JF, 2004, INT J WAVELETS MULTI, V2, P121; Dibeklioglu H., 2010, P 18 ACM INT C MULT, P703; Dibeklioglu H, 2015, IEEE T MULTIMEDIA, V17, P279, DOI 10.1109/TMM.2015.2394777; Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38; Eckman P., 2003, EMOTIONS REVEALED; EKMAN P, 1981, PSYCHOPHYSIOLOGY, V18, P101, DOI 10.1111/j.1469-8986.1981.tb02919.x; Ekman P, 2003, ANN NY ACAD SCI, V1000, P205, DOI 10.1196/annals.1280.010; EKMAN P, 1982, J NONVERBAL BEHAV, V6, P238, DOI 10.1007/BF00987191; el Kaliouby R, 2005, REAL-TIME VISION FOR HUMAN-COMPUTER INTERACTION, P181; Elaiwat S, 2016, PATTERN RECOGN, V49, P152, DOI 10.1016/j.patcog.2015.07.006; Gan Q, 2017, AAAI CONF ARTIF INTE, P4039; Hinton G., 2010, MOMENTUM, V9, P1; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Joyce J. M., 2011, INT ENCY STAT SCI, P720; Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611; Littlewort GC, 2009, IMAGE VISION COMPUT, V27, P1797, DOI 10.1016/j.imavis.2008.12.010; Lucey P., 2010, P IEEE COMP SOC C CO, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Martinez Brais, 2019, IEEE Transactions on Affective Computing, V10, P325, DOI 10.1109/TAFFC.2017.2731763; Mavadati M, 2016, IEEE COMPUT SOC CONF, P1452, DOI 10.1109/CVPRW.2016.182; Namba S, 2017, CURR PSYCHOL, V36, P593, DOI 10.1007/s12144-016-9448-9; Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401; Rodriguez P, 2022, IEEE T CYBERNETICS, V52, P3314, DOI 10.1109/TCYB.2017.2662199; Ross ED, 2013, CORTEX, V49, P1280, DOI 10.1016/j.cortex.2012.05.002; Salakhutdinov R., 2008, P 25 INT C MACH LEAR, V25, P872; Sariyanidi E, 2017, IEEE T IMAGE PROCESS, V26, P1965, DOI 10.1109/TIP.2017.2662237; Schmidt KL, 2009, J NONVERBAL BEHAV, V33, P35, DOI 10.1007/s10919-008-0058-6; Seckington M., 2011, THESIS; Shang LF, 2009, PROC CVPR IEEE, P2090, DOI 10.1109/CVPRW.2009.5206509; Valstar M., 2010, P 3 INT WORKSH EMOTI; Valstar M. F., 2006, P 8 INT C MULT INT, P162; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710; Wang SF, 2016, COMPUT VIS IMAGE UND, V147, P69, DOI 10.1016/j.cviu.2015.08.007; Wang SF, 2015, MACH VISION APPL, V26, P219, DOI 10.1007/s00138-015-0657-2; Wang ZH, 2013, PROC CVPR IEEE, P3422, DOI 10.1109/CVPR.2013.439; Wu P., 2014, P IEEE INT C AC SPEE, P1240; Wu T., 2010, P IEEE COMP SOC C CO, P42; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yang JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P469, DOI 10.1145/3123266.3123350; Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110	43	5	6	2	32	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2020	42	9					2082	2095		10.1109/TPAMI.2019.2911937	http://dx.doi.org/10.1109/TPAMI.2019.2911937			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	MW9MI	30998459				2022-12-18	WOS:000557354900002
J	Zhou, MY; Ding, YQ; Ji, Y; Young, SS; Yu, JY; Ye, JW				Zhou, Mingyuan; Ding, Yuqi; Ji, Yu; Young, S. Susan; Yu, Jingyi; Ye, Jinwei			Shape and Reflectance Reconstruction Using Concentric Multi-Spectral Light Field	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Shape; Surface reconstruction; Lighting; Light sources; Image reconstruction; Computational modeling; Shape reconstruction; surface reflectance; multi-spectral; light field	PHOTOMETRIC STEREO; MULTIVIEW STEREO; COLOR	Recovering the shape and reflectance of non-Lambertian surfaces remains a challenging problem in computer vision since the view-dependent appearance invalidates traditional photo-consistency constraint. In this paper, we introduce a novel concentric multi-spectral light field (CMSLF) design that is able to recover the shape and reflectance of surfaces of various materials in one shot. Our CMSLF system consists of an array of cameras arranged on concentric circles where each ring captures a specific spectrum. Coupled with a multi-spectral ring light, we are able to sample viewpoint and lighting variations in a single shot via spectral multiplexing. We further show that our concentric camera and light source setting results in a unique single-peak pattern in specularity variations across viewpoints. This property enables robust depth estimation for specular points. To estimate depth and multi-spectral reflectance map, we formulate a physics-based reflectance model for the CMSLF under the surface camera (S-Cam) representation. Extensive synthetic and real experiments show that our method outperforms the state-of-the-art shape reconstruction methods, especially for non-Lambertian surfaces.	[Zhou, Mingyuan; Ji, Yu] DGene Digital Technol, Baton Rouge, LA 70820 USA; [Ding, Yuqi] Louisiana State Univ, Baton Rouge, LA 70803 USA; [Young, S. Susan] US Army Res Lab, Adelphi, MD 20783 USA; [Yu, Jingyi] Shanghai Tech Univ, Shanghai 201210, Peoples R China; [Ye, Jinwei] Louisiana State Univ, Div Comp Sci & Engn, Baton Rouge, LA 70803 USA	Louisiana State University System; Louisiana State University; United States Department of Defense; United States Army; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL); ShanghaiTech University; Louisiana State University System; Louisiana State University	Ye, JW (corresponding author), Louisiana State Univ, Div Comp Sci & Engn, Baton Rouge, LA 70803 USA.	mingyuan.zhou@dgene.com; yding18@lsu.edu; yu.ji@dgene.com; susanyoung00@yahoo.com; yujingyi@shanghaitech.edu.cn; jye@csc.lsu.edu		Ye, Jinwei/0000-0001-7780-7943	Louisiana Board of Regents [LEQSF(2018-21)-RD-A-10]	Louisiana Board of Regents	This work was supported in part by the Louisiana Board of Regents under Grant LEQSF(2018-21)-RD-A-10.	Alldrin N., 2008, IEEE C COMP VIS PATT, P1; Anderson R, 2011, IEEE I CONF COMP VIS, P2182, DOI 10.1109/ICCV.2011.6126495; [Anonymous], 2008, NEW STANFORD LIGHT F; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Burr J, 2007, HEALTH TECHNOL ASSES, V11, P1, DOI 10.3310/hta11410; Chandraker M, 2014, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2014.279; Chandraker M, 2014, LECT NOTES COMPUT SC, V8695, P202, DOI 10.1007/978-3-319-10584-0_14; Fyffe G, 2016, COMPUT GRAPH FORUM, V35, P353, DOI 10.1111/cgf.12837; Fyffe G., 2011, P IEEE INT C COMP PH, P16, DOI [10.1109/ICCPHOT.2011.5753116, DOI 10.1109/ICCPH0T.2011.5753116]; Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163; Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933; Goldman DB, 2010, IEEE T PATTERN ANAL, V32, P1060, DOI 10.1109/TPAMI.2009.102; Gotardo PFU, 2015, IEEE I CONF COMP VIS, P846, DOI 10.1109/ICCV.2015.103; Guo W, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P1331, DOI 10.1109/ICMA.2017.8016010; Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691; Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762; Jin H., 2004, P IEEE COMP SOC C VI, pI; Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270; Li QY, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P557; Lin S, 2002, LECT NOTES COMPUT SC, V2352, P210; Mallick SP, 2005, PROC CVPR IEEE, P619; MALONEY LT, 1986, J OPT SOC AM A, V3, P1673, DOI 10.1364/JOSAA.3.001673; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; Mecca Roberto, 2016, P IEEE WINT C APPL C, P1; Ng, 2005, LIGHT FIELD PHOTOGRA; Nishino K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P599, DOI 10.1109/ICCV.2001.937573; Oxholm G, 2014, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2014.277; Oxholm G, 2012, LECT NOTES COMPUT SC, V7572, P528, DOI 10.1007/978-3-642-33718-5_38; Park J, 2017, IEEE T PATTERN ANAL, V39, P1591, DOI 10.1109/TPAMI.2016.2608944; PARKKINEN JPS, 1989, J OPT SOC AM A, V6, P318, DOI 10.1364/JOSAA.6.000318; SATO Y, 1994, J OPT SOC AM A, V11, P2990, DOI 10.1364/JOSAA.11.002990; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; Tao MW, 2016, IEEE T PATTERN ANAL, V38, P1155, DOI 10.1109/TPAMI.2015.2477811; Tao MW, 2015, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2015.7298804; Tao MW, 2015, LECT NOTES COMPUT SC, V8926, P533, DOI 10.1007/978-3-319-16181-5_41; Tozza S, 2016, J MATH IMAGING VIS, V56, P57, DOI 10.1007/s10851-016-0633-0; Vogiatzis G, 2005, IEEE I CONF COMP VIS, P228; Wang TC, 2016, PROC CVPR IEEE, P5451, DOI 10.1109/CVPR.2016.588; Wanner S, 2012, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2012.6247656; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu CL, 2011, IEEE T VIS COMPUT GR, V17, P1082, DOI [10.1109/TVCG.2010.224, 10.1109/TPDS.2010.224]; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Yu JY, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P137, DOI 10.1109/PCCGA.2002.1167847; Yu Z, 2013, IEEE I CONF COMP VIS, P2792, DOI 10.1109/ICCV.2013.347; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	47	5	5	1	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2020	42	7					1594	1605		10.1109/TPAMI.2020.2986764	http://dx.doi.org/10.1109/TPAMI.2020.2986764			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MC0DH	32305895				2022-12-18	WOS:000542967200006
J	Wei, YH; Tang, Y; McNicholas, PD				Wei, Yuhong; Tang, Yang; McNicholas, Paul D.			Flexible High-Dimensional Unsupervised Learning with Missing Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Analytical models; Computational modeling; Data models; Unsupervised learning; Covariance matrices; Clustering algorithms; Mixture models; Clustering; factor analysis; generalized hyperbolic; missing data; mixture of factor analyzers; mixture model; model-based clustering; unsupervised classification	STOCHASTIC OZONE DAYS; T-FACTOR ANALYZERS; MIXTURE-MODELS; DISCRIMINANT-ANALYSIS; MAXIMUM-LIKELIHOOD; BAYESIAN-ANALYSIS; ALGORITHM	The mixture of factor analyzers (MFA) model is a famous mixture model-based approach for unsupervised learning with high-dimensional data. It can be useful, inter alia, in situations where the data dimensionality far exceeds the number of observations. In recent years, the MFA model has been extended to non-Gaussian mixtures to account for clusters with heavier tail weight and/or asymmetry. The generalized hyperbolic factor analyzers (MGHFA) model is one such extension, which leads to a flexible modelling paradigm that accounts for both heavier tail weight and cluster asymmetry. In many practical applications, the occurrence of missing values often complicates data analyses. A generalization of the MGHFA is presented to accommodate missing values. Under a missing-at-random mechanism, we develop a computationally efficient alternating expectation conditional maximization algorithm for parameter estimation of the MGHFA model with different patterns of missing values. The imputation of missing values under an incomplete-data structure of MGHFA is also investigated. The performance of our proposed methodology is illustrated through the analysis of simulated and real data.	[Wei, Yuhong; Tang, Yang; McNicholas, Paul D.] McMaster Univ, Dept Math & Stat, Hamilton, ON L8S 4L8, Canada	McMaster University	McNicholas, PD (corresponding author), McMaster Univ, Dept Math & Stat, Hamilton, ON L8S 4L8, Canada.	weiy38@math.mcmaster.ca; yang.tang@math.mcmaster.ca; mcnicholas@math.mcmaster.ca		McNicholas, Paul/0000-0002-2482-523X	Ontario Graduate Scholarship, Faculty of Science; Canada Research Chairs programs	Ontario Graduate Scholarship, Faculty of Science(Ontario Graduate Scholarship); Canada Research Chairs programs(Canada Research Chairs)	The authors gratefully acknowledge financial support from the Ontario Graduate Scholarship, Faculty of Science Research Fellows, and Canada Research Chairs programs.	Aitken A.C., 1926, P ROYAL SOC EDINBURG, V45, P14, DOI [DOI 10.1017/S0370164600024871, 10.1017/S0370164600024871]; Andrews JL, 2012, STAT COMPUT, V22, P1021, DOI 10.1007/s11222-011-9272-x; Andrews JL, 2011, STAT COMPUT, V21, P361, DOI 10.1007/s11222-010-9175-2; [Anonymous], 2015, MIXGHD MODEL BASED C; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], 1981, STAT DISTRIBUTIONS S; Baek J, 2011, BIOINFORMATICS, V27, P1269, DOI 10.1093/bioinformatics/btr112; Baek J, 2010, IEEE T PATTERN ANAL, V32, P1298, DOI 10.1109/TPAMI.2009.149; BANFIELD JD, 1993, BIOMETRICS, V49, P803, DOI 10.2307/2532201; BARNDORFFNIELSEN O, 1977, Z WAHRSCHEINLICHKEIT, V38, P309, DOI 10.1007/BF00533162; Bhattacharya S, 2014, ADV DATA ANAL CLASSI, V8, P45, DOI 10.1007/s11634-013-0155-1; BOHNING D, 1994, ANN I STAT MATH, V46, P373, DOI 10.1007/BF01720593; Bouveyron C, 2014, COMPUT STAT DATA AN, V71, P52, DOI 10.1016/j.csda.2012.12.008; Browne RP, 2015, CAN J STAT, V43, P176, DOI 10.1002/cjs.11246; CELEUX G, 1995, PATTERN RECOGN, V28, P781, DOI 10.1016/0031-3203(94)00125-6; Chi JT, 2016, AM STAT, V70, P91, DOI 10.1080/00031305.2015.1086685; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FORINA M, 1986, VITIS, V25, P189; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; Fraley C., 2012, MCLUST VERSION 4 R N; Ghahramani Z, 1997, CRGTR961; HALGREEN C, 1979, Z WAHRSCHEINLICHKEIT, V47, P13, DOI 10.1007/BF00533246; Honaker J, 2011, J STAT SOFTW, V45, P1; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Hunter DR, 2000, J COMPUT GRAPH STAT, V9, P52, DOI 10.2307/1390612; Hurley CB, 2004, J COMPUT GRAPH STAT, V13, P788, DOI 10.1198/106186004X12425; Jorgensen B., 1982, STAT PROPERTIES GEN; Lawley D.N., 1962, J R STAT SOC, V12, P209, DOI [https://doi.org/10.2307/2986915, DOI 10.2307/2986915]; Lichman M., 2013, UCI MACHINE LEARNING; Lin TI, 2006, PATTERN RECOGN, V39, P1177, DOI 10.1016/j.patcog.2005.12.014; Lin TI, 2004, STAT COMPUT, V14, P119, DOI 10.1023/B:STCO.0000021410.33077.10; Lin TI, 2016, J MULTIVARIATE ANAL, V143, P398, DOI 10.1016/j.jmva.2015.09.025; Lin TI, 2009, COMPUTATION STAT, V24, P375, DOI 10.1007/s00180-008-0129-5; Lindsay BG., 1995, MIXTURE MODELS THEOR, DOI [10.1214/cbms/1462106013, DOI 10.1214/CBMS/1462106013]; Little RJA, 1987, STAT ANAL MISSING DA; Mclachlan G., 2000, WILEY SER PROB STAT; McLachlan GJ, 2007, COMPUT STAT DATA AN, V51, P5327, DOI 10.1016/j.csda.2006.09.015; McNicholas PD, 2010, COMPUT STAT DATA AN, V54, P711, DOI 10.1016/j.csda.2009.02.011; McNicholas P.D., 2016, MIXTURE MODEL BASED; McNicholas PD, 2016, J CLASSIF, V33, P331, DOI 10.1007/s00357-016-9211-9; McNicholas PD, 2010, BIOINFORMATICS, V26, P2705, DOI 10.1093/bioinformatics/btq498; McNicholas PD, 2008, STAT COMPUT, V18, P285, DOI 10.1007/s11222-008-9056-0; McNicholas SM, 2017, CONTRIB STAT, P369, DOI 10.1007/978-3-319-41573-4_18; Meng XL, 1997, J ROY STAT SOC B MET, V59, P511, DOI 10.1111/1467-9868.00082; Murray PM, 2014, COMPUT STAT DATA AN, V77, P326, DOI 10.1016/j.csda.2014.03.012; R Development Core Team, 2021, LANG ENV STAT COMP; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Su YS, 2011, J STAT SOFTW, V45, P1; Tortora C, 2016, ADV DATA ANAL CLASSI, V10, P423, DOI 10.1007/s11634-015-0204-z; Utsugi A, 2001, NEURAL COMPUT, V13, P993, DOI 10.1162/08997660151134299; van Buuren S, 2011, J STAT SOFTW, V45, P1; Vrbik I, 2014, COMPUT STAT DATA AN, V71, P196, DOI 10.1016/j.csda.2013.07.008; Wagstaff KL, 2005, ASTR SOC P, V347, P172; Wang WL, 2015, COMPUT STAT DATA AN, V83, P223, DOI 10.1016/j.csda.2014.10.007; Wang WL, 2013, J MULTIVARIATE ANAL, V117, P120, DOI 10.1016/j.jmva.2013.02.003; Wei YH, 2019, COMPUT STAT DATA AN, V130, P18, DOI 10.1016/j.csda.2018.08.016; Zhang K, 2008, KNOWL INF SYST, V14, P299, DOI 10.1007/s10115-007-0095-1; Zhang K, 2006, IEEE DATA MINING, P753	63	5	5	3	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2020	42	3					610	621		10.1109/TPAMI.2018.2885760	http://dx.doi.org/10.1109/TPAMI.2018.2885760			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LC5KN	30530313	Green Submitted			2022-12-18	WOS:000525365300007
J	Tang, DH; Ye, Q; Yuan, SX; Taylor, J; Kohli, P; Keskin, C; Kim, TK; Shotton, J				Tang, Danhang; Ye, Qi; Yuan, Shanxin; Taylor, Jonathan; Kohli, Pushmeet; Keskin, Cem; Kim, Tae-Kyun; Shotton, Jamie			Opening the Black Box: Hierarchical Sampling Optimization for Hand Pose Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Hand pose estimation; kinematic hierarchy; sampling; random forest; convolutional neural network; particle swarm optimization	TRACKING	Hand pose estimation, formulated as an inverse problem, is typically optimized by an energy function over pose parameters using a 'black box' image generation procedure, knowing little about either the relationships between the parameters or the form of the energy function. In this paper, we show significant improvement upon such black box optimization by exploiting high-level knowledge of the parameter structure and using a local surrogate energy function. Our new framework, hierarchical sampling optimization (HSO), consists of a sequence of discriminative predictors organized into a kinematic hierarchy. Each predictor is conditioned on its ancestors, and generates a set of samples over a subset of the pose parameters, with only one selected by the highly-efficient surrogate energy. The selected partial poses are concatenated to generate a full-pose hypothesis. Repeating the same process, several hypotheses are generated and the full energy function selects the best result. Under the same kinematic hierarchy, two methods based on decision forest and convolutional neural network are proposed to generate the samples and two optimization methods are studied when optimizing these samples. Experimental evaluations on three publicly available datasets show that our method is particularly impressive in low-compute scenarios where it significantly outperforms all other state-of-the-art methods.	[Tang, Danhang; Taylor, Jonathan; Keskin, Cem] PerceptiveIO, San Francisco, CA 94103 USA; [Ye, Qi; Yuan, Shanxin; Kim, Tae-Kyun] Imperial Coll London, London SW7 2AZ, England; [Kohli, Pushmeet] Google DeepMind, London N1C 4AG, England; [Shotton, Jamie] Microsoft, Redmond, WA 98052 USA	Imperial College London; Google Incorporated; Microsoft	Ye, Q (corresponding author), Imperial Coll London, London SW7 2AZ, England.	danhang.tang@gmail.com; q.ye14@imperial.ac.uk; s.yuan14@imperial.ac.uk; jtaylor@pereeptiveio.com; pushmeet@outlook.com; cem.kskn@gmail.com; tk.kim@imperial.ac.uk; jamiesho@microsoft.com		Tang, Danhang/0000-0001-6164-8263	EPSRC [EP/J012106/1] Funding Source: UKRI	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		[Anonymous], 2013, GRAPHICS INTERFACE 2; Athitsos V, 2003, PROC CVPR IEEE, P432; Baak A, 2011, IEEE I CONF COMP VIS, P1092, DOI 10.1109/ICCV.2011.6126356; Ballan L, 2012, MULTIMEDIA COMPUT CO, P121; Bishop C. M., 1994, MIXTURE DENSITY NETW; Bishop C.M, 2006, PATTERN RECOGN; Chang HJ, 2016, COMPUT VIS IMAGE UND, V148, P87, DOI 10.1016/j.cviu.2016.01.010; Choi C, 2015, IEEE I CONF COMP VIS, P2336, DOI 10.1109/ICCV.2015.269; Criminisi A., 2013, DECISION FORESTS COM; de La Gorce M, 2011, IEEE T PATTERN ANAL, V33, P1793, DOI 10.1109/TPAMI.2011.33; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270; Guzman-Rivera A, 2014, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2014.146; Hirsch M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618505; Ionescu C, 2014, PROC CVPR IEEE, P1661, DOI 10.1109/CVPR.2014.215; Jacob Y., 2015, P IEEE COMP SOC WORK; Jang Y, 2015, IEEE T VIS COMPUT GR, V21, P501, DOI 10.1109/TVCG.2015.2391860; Jung HY, 2015, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2015.7298861; Keskin C, 2012, LECT NOTES COMPUT SC, V7577, P852, DOI 10.1007/978-3-642-33783-3_61; Khamis S, 2015, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2015.7298869; Kingma DP, 2014, ADAM AMETHOD STOCHAS; Krejov Philip, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163141; Kulesza A., 2010, P NEURIPS, V1411, P1412; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Neverova N, 2015, LECT NOTES COMPUT SC, V9005, P687, DOI 10.1007/978-3-319-16811-1_45; Oberweger M, 2015, IEEE I CONF COMP VIS, P3316, DOI 10.1109/ICCV.2015.379; Oberweger Markus, 2015, ARXIV150206807; Oikonomidis I, 2012, PROC CVPR IEEE, P1862, DOI 10.1109/CVPR.2012.6247885; Oikonomidis I, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.101; Oikonomidis I, 2011, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2011.6126483; Panteleris P., 2015, P BRIT MACH VIS, P123; Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552; Poier G., 2015, P BRIT MACHINE VISIO, DOI [10.5244/C.29.182, DOI 10.5244/C.29.182]; Qian C, 2014, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2014.145; Rehg J. M., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P35, DOI 10.1007/BFb0028333; Salzmann M, 2010, PROC CVPR IEEE, P647, DOI 10.1109/CVPR.2010.5540155; Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Sridhar S, 2016, LECT NOTES COMPUT SC, V9906, P294, DOI 10.1007/978-3-319-46475-6_19; Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941; Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305; Stenger B, 2001, PROC CVPR IEEE, P310; Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189; Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683; Supancic JS, 2015, IEEE I CONF COMP VIS, P1868, DOI 10.1109/ICCV.2015.217; Tan DJ, 2016, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR.2016.605; Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490; Tang DH, 2013, IEEE I CONF COMP VIS, P3224, DOI 10.1109/ICCV.2013.400; TANG DH, 2017, IEEE T PATTERN ANAL, V39, P1374, DOI DOI 10.1109/TPAMI.2016.2599170; TAYLOR J, 2012, PROC CVPR IEEE, P103; Taylor Jonathan, 2016, ACM T GRAPHIC, V35, P4; Team T. T. D., 2016, 160502688 ARXIV; Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500; Tzionas D, 2016, INT J COMPUT VISION, V118, P172, DOI 10.1007/s11263-016-0895-4; Wan CD, 2016, LECT NOTES COMPUT SC, V9907, P554, DOI 10.1007/978-3-319-46487-9_34; Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83; Xu C, 2013, IEEE I CONF COMP VIS, P3456, DOI 10.1109/ICCV.2013.429; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Ye M, 2011, IEEE I CONF COMP VIS, P731, DOI 10.1109/ICCV.2011.6126310; Ye Q, 2016, LECT NOTES COMPUT SC, V9912, P346, DOI 10.1007/978-3-319-46484-8_21; Yin F, 2016, LECT NOTES COMPUT SC, V9911, P434, DOI 10.1007/978-3-319-46478-7_27; Youding Zhu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563163; Zafrulla Z., 2011, P 13 INT C MULT INT, P279, DOI DOI 10.1145/2070481.2070532	63	5	5	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2019	41	9					2161	2175		10.1109/TPAMI.2018.2847688	http://dx.doi.org/10.1109/TPAMI.2018.2847688			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IP9BY	29994653				2022-12-18	WOS:000480343900009
J	Xue, TF; Wu, JJ; Bouman, KL; Freeman, WT				Xue, Tianfan; Wu, Jiajun; Bouman, Katherine L.; Freeman, William T.			Visual Dynamics: Stochastic Future Generation via Layered Cross Convolutional Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						future prediction; frame synthesis; probabilistic modeling; convolutional networks; cross convolution	STATISTICS	We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods that have tackled this problem in a deterministic or non-parametric way, we propose to model future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, and on real-world video frames. We present analyses of the learned network representations, showing it is implicitly learning a compact encoding of object appearance and motion. We also demonstrate a few of its applications, including visual analogy-making and video extrapolation.	[Xue, Tianfan; Freeman, William T.] Google Inc, Mountain View, CA 94043 USA; [Wu, Jiajun; Freeman, William T.] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA; [Bouman, Katherine L.] Harvard Univ, Cambridge, MA 02138 USA	Google Incorporated; Massachusetts Institute of Technology (MIT); Harvard University	Wu, JJ (corresponding author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.	tianfan.xue@gmail.com; jiajunwu@mit.edu; klbouman@gmail.com; billf@mit.edu	Xue, Tianfan/AAG-5546-2019; Wu, JiaJun/GQH-7885-2022		NSF Robust Intelligence [1212849]; NSF Big Data [1447476]; ONR MURI [6923196]; Adobe; Shell Research	NSF Robust Intelligence; NSF Big Data; ONR MURI(MURIOffice of Naval Research); Adobe; Shell Research	We thank Zhijian Liu and Yining Wang for helpful discussions and anonymous reviewers for constructive comments. This work was supported by NSF Robust Intelligence 1212849, NSF Big Data 1447476, ONR MURI 6923196, Adobe, Shell Research, and a hardware donation from Nvidia. T. Xue and J. Wucontributed equally to thiswork.	Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268; [Anonymous], 1993, P 6 ANN C COMPUTATIO, DOI DOI 10.1145/168304.168306; [Anonymous], 2016, ARXIV PREPRINT ARXIV; Brabandere B.D., 2016, ADV NEURAL INFORM PR, P667; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Denton Emily L, 2015, NEURIPS, V2, P4; Doersch Carl, 2016, ARXIV160605908V2; Fleet DJ, 2000, INT J COMPUT VISION, V36, P171, DOI 10.1023/A:1008156202475; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Huang SY, 2017, PROC CVPR IEEE, P4664, DOI 10.1109/CVPR.2017.496; Joshi N, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P251; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; LeCun Y, 2016, P INT C LEARN REPR; Liao Z., 2013, TOG SIGGRAPH, V32, P4; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu Ce, 2009, THESIS, P2; Lu H., 2006, ADV NEURAL INFORM PR, V18, P827; Oh J., 2015, P ADV NEUR INF PROC, P2863; Pintea SL, 2014, LECT NOTES COMPUT SC, V8691, P172, DOI 10.1007/978-3-319-10578-9_12; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Radford A., 2016, P INT C LEARN REPR; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Roth S, 2005, IEEE I CONF COMP VIS, P42; Schodl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012; Sohn Kihyuk, 2015, NEURAL INFORM PROCES; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; THIES J, 2016, PROC CVPR IEEE, P2387, DOI DOI 10.1109/CVPR.2016.262; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51; Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281; Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416; Wang J. Y. A., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P361, DOI 10.1109/CVPR.1993.341105; Weiss Y., 1998, MEMO NO1624, P1; Welling M, 2014, AUTOENCODING VARIATI; Wexler Y, 2004, PROC CVPR IEEE, P120; Wu J., 2016, P BRIT MACH VIS C; Wu J., 2015, ADV NEURAL INFORM PR; Wu Jiajun, 2017, ADV NEURAL INFORM PR, P152; Wu Y. N., 2017, P IEEE C COMP VIS PA, P7093; Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51; Xue TF, 2014, LECT NOTES COMPUT SC, V8691, P767, DOI 10.1007/978-3-319-10578-9_50; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Zhang L, 2017, ORG CHEM FRONT, V4, P119, DOI 10.1039/c6qo00544f; Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280; Zheng D., 2018, P C UNC ART INT; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18	49	5	6	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2019	41	9					2236	2250		10.1109/TPAMI.2018.2854726	http://dx.doi.org/10.1109/TPAMI.2018.2854726			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IP9BY	30004870	Green Submitted			2022-12-18	WOS:000480343900014
J	Sheng, L; Cai, JF; Cham, TJ; Pavlovic, V; Ngan, KN				Sheng, Lu; Cai, Jianfei; Cham, Tat-Jen; Pavlovic, Vladimir; Ngan, King Ngi			Visibility Constrained Generative Model for Depth-Based 3D Facial Pose Tracking	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						3D facial pose tracking; generative model; depth; online Bayesian model; mixture of Gaussian models		In this paper, we propose a generative framework that unifies depth-based 3D facial pose tracking and face model adaptation on-the-fly, in the unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Specifically, we introduce a statistical 3D morphable model that flexibly describes the distribution of points on the surface of the face model, with an efficient switchable online adaptation that gradually captures the identity of the tracked subject and rapidly constructs a suitable face model when the subject changes. Moreover, unlike prior art that employed ICP-based facial pose estimation, to improve robustness to occlusions, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility with respect to the input point cloud. Ablation studies and experimental results on Biwi and ICT-3DHP datasets demonstrate that the proposed framework is effective and outperforms completing state-of-the-art depth-based methods.	[Sheng, Lu; Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China; [Cai, Jianfei; Cham, Tat-Jen] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore; [Pavlovic, Vladimir] Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA	Chinese University of Hong Kong; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Rutgers State University New Brunswick	Sheng, L (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.	lsheng@ee.cuhk.edu.hk; asjfcai@ntu.edu.sg; astjcham@ntu.edu.sg; vladimir@cs.rutgers.edu; knngan@ee.cuhk.edu.hk	Sheng, Lu/V-2526-2019; Ngan, N/E-8240-2014; Cai, Jianfei/A-3691-2011	Sheng, Lu/0000-0002-8525-9163; Ngan, N/0000-0003-1946-3235; Cai, Jianfei/0000-0002-9444-3763; Cham, Tat-Jen/0000-0001-5264-2572	BeingTogether Centre; Nanyang Technological University (NTU) Singapore; University of North Carolina (UNC) at Chapel Hill; National Research Foundation, Prime Ministers Office, Singapore, under its International Research Centres in Singapore Funding Initiative; MoE Tier-2 Grant [2016-T2-2-065]	BeingTogether Centre; Nanyang Technological University (NTU) Singapore(Nanyang Technological University); University of North Carolina (UNC) at Chapel Hill; National Research Foundation, Prime Ministers Office, Singapore, under its International Research Centres in Singapore Funding Initiative; MoE Tier-2 Grant	This research is supported by the BeingTogether Centre, a collaboration between Nanyang Technological University (NTU) Singapore and the University of North Carolina (UNC) at Chapel Hill. The BeingTogether Centre is supported by the National Research Foundation, Prime Ministers Office, Singapore, under its International Research Centres in Singapore Funding Initiative. This work is also in part supported MoE Tier-2 Grant (2016-T2-2-065) of Singapore.	Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206; [Anonymous], P BRIT MACH VIS C; Baltrusaitis T, 2012, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2012.6247980; Bishop C.M, 2006, PATTERN RECOGN; BLACK MJ, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P374, DOI 10.1109/ICCV.1995.466915; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Bolkart T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P103, DOI 10.1109/3DV.2013.22; Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580; Bouaziz Sofien, 2013, ACM T GRAPHIC, V32, P4; Breitenstein M. D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.458; Brunton A, 2014, COMPUT VIS IMAGE UND, V128, P1, DOI 10.1016/j.cviu.2014.05.005; Cao C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766943; Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Cao Chen, 2013, ACM T GRAPHIC, V32, P4; Chen C., 2014, P AS C COMP VIS SPR, P336; DeCarlo D, 2000, INT J COMPUT VISION, V38, P99, DOI 10.1023/A:1008122917811; Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164; Egger B, 2018, INT J COMPUT VISION, V126, P1269, DOI 10.1007/s11263-018-1064-8; Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11; Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458; Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742; Hsieh PL, 2015, PROC CVPR IEEE, P1675, DOI 10.1109/CVPR.2015.7298776; Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117; Kazemi Vahid, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P369, DOI 10.1109/3DV.2014.93; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; LI HB, 1993, IEEE T PATTERN ANAL, V15, P545, DOI 10.1109/34.216724; Li SN, 2016, IEEE T PATTERN ANAL, V38, P1922, DOI 10.1109/TPAMI.2015.2500221; Luo LJ, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462026; Luthi M, 2018, IEEE T PATTERN ANAL, V40, P1860, DOI 10.1109/TPAMI.2017.2739743; Martin Manuel, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P641, DOI 10.1109/3DV.2014.54; Meyer GP, 2015, IEEE I CONF COMP VIS, P3649, DOI 10.1109/ICCV.2015.416; Padeleris P., 2012, 2012 IEEE COMP SOC C, P42, DOI DOI 10.1109/CVPRW.2012.6239236; Papazov C, 2015, PROC CVPR IEEE, P4722, DOI 10.1109/CVPR.2015.7299104; Pham HX, 2016, INT CONF 3D VISION, P441, DOI 10.1109/3DV.2016.54; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15; Sheng L, 2017, PROC CVPR IEEE, P4598, DOI 10.1109/CVPR.2017.489; Storer Markus, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P192, DOI 10.1109/ICCVW.2009.5457701; Sun YJ, 2008, SENSYS'08: PROCEEDINGS OF THE 6TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P1, DOI 10.1145/1460412.1460414; Tan FW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P64, DOI 10.1145/3123266.3123281; Thomas Diego, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3299, DOI 10.1109/CVPR.2016.359; Tulyakov S, 2014, INT C PATT RECOG, P2263, DOI 10.1109/ICPR.2014.393; Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209; Wang RZ, 2016, LECT NOTES COMPUT SC, V9911, P271, DOI 10.1007/978-3-319-46478-7_17; Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171; Weise T., 2011, ACM T GRAPHIC, V30, P77; Yu Y, 2018, IEEE T PATTERN ANAL, V40, P2653, DOI 10.1109/TPAMI.2018.2841403	49	5	5	2	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2019	41	8					1994	2007		10.1109/TPAMI.2018.2877675	http://dx.doi.org/10.1109/TPAMI.2018.2877675			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IG2BD	30369437	Green Submitted			2022-12-18	WOS:000473598800015
J	Terenin, A; Magnusson, M; Jonsson, L; Draper, D				Terenin, Alexander; Magnusson, Mans; Jonsson, Leif; Draper, David			Polya Urn Latent Dirichlet Allocation: A Doubly Sparse Massively Parallel Sampler	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bayesian inference; big data; computational complexity; gibbs sampling; latent dirichlet allocation; markov chain monte carlo; natural language processing; parallel and distributed systems; topic models		Latent Dirichlet Allocation (LDA) is a topic model widely used in natural language processing and machine learning. Most approaches to training the model rely on iterative algorithms, which makes it difficult to run LDA on big corpora that are best analyzed in parallel and distributed computational environments. Indeed, current approaches to parallel inference either don't converge to the correct posterior or require storage of large dense matrices in memory. We present a novel sampler that overcomes both problems, and we show that this sampler is faster, both empirically and theoretically, than previous Gibbs samplers for LDA. We do so by employing a novel Polya-urn-based approximation in the sparse partially collapsed sampler for LDA. We prove that the approximation error vanishes with data size, making our algorithm asymptotically exact, a property of importance for large-scale topic models. In addition, we show, via an explicit example, that-contrary to popular belief in the topic modeling literature-partially collapsed samplers can be more efficient than fully collapsed samplers. We conclude by comparing the performance of our algorithm with that of other approaches on well-known corpora.	[Terenin, Alexander] Imperial Coll London, Dept Math, Stat Sect, London SW7 2AZ, England; [Magnusson, Mans; Jonsson, Leif] Linkoping Univ, S-58183 Linkoping, Sweden; [Draper, David] Univ Calif Santa Cruz, Dept Appl Math & Stat, Stat, Santa Cruz, CA 95064 USA	Imperial College London; Linkoping University; University of California System; University of California Santa Cruz	Terenin, A (corresponding author), Imperial Coll London, Dept Math, Stat Sect, London SW7 2AZ, England.	a.terenin17@imperial.ac.uk; mons.magnusson@gmail.com; leif.jonsson@ericsson.se; draper@ucsc.edu		Jonsson, Leif/0000-0002-8989-0251; Terenin, Alexander/0000-0001-5292-3104; Draper, David/0000-0002-6367-3101	Swedish Foundation for Strategic Research [RIT 15-0097]	Swedish Foundation for Strategic Research(Swedish Foundation for Strategic Research)	We are thankful to Graham Neubig, Tamara Broderick, Eric P. Xing, Qirong Ho, Wei Dai, Willie Neiswanger, Murat Demirbas, Shawfeng Dong, Thanasis Kottas, Kunal Sarkhel, and Mattias Villani for their thoughts, and to several referees whose comments substantially improved the paper. Membership on this list does not imply agreement with the ideas expressed here, nor responsibility for any errors that may be present. Mans Magnusson was partially financially supported by the Swedish Foundation for Strategic Research (Smart Systems: RIT 15-0097).	Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Anandkumar Anima, 2012, NIPS; [Anonymous], 2007, P 45 ANN M ASS COMP; Araujo M., 1997, P 4 S AM WORKSH STRI, P2; BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chamandy N, 2015, AM STAT, V69, P283, DOI 10.1080/00031305.2015.1089790; Chen JF, 2016, PROC VLDB ENDOW, V9, P744, DOI 10.14778/2977797.2977801; Draper D., OPTIMAL BAYESIAN ANA; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Heaps Harold S., 1978, INFORM RETRIEVAL COM; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Ihler A, 2012, IEEE T KNOWL DATA EN, V24, P952, DOI 10.1109/TKDE.2011.29; Johndrow James E, 2018, J AM STAT ASS; JOHNDROW JE, 2015, ARXIV150803387; JOHNDROW JE, 2017, ARXIV170500841; Kotz S., 2000, WILEY SER PROB STAT, V2nd ed, DOI 10.1002/0471722065; Li AQ, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P891, DOI 10.1145/2623330.2623756; Li Q, 2017, IEEE PHOTON CONF, P49, DOI 10.1109/IPCon.2017.8116001; LIU JS, 1995, J ROY STAT SOC B MET, V57, P157; Magnusson M, 2018, J COMPUT GRAPH STAT, V27, P449, DOI 10.1080/10618600.2017.1366913; Marsaglia G, 2000, ACM T MATH SOFTWARE, V26, P363, DOI 10.1145/358407.358414; McCallum Andrew Kachites, 2002, MALLET MACHINE LEARN; Mimno D, 2011, P C EMP METH NAT LAN, P262, DOI DOI 10.5555/2145432.2145462; Negrea J., 2017, ARXIV170207441; Newman D, 2009, J MACH LEARN RES, V10, P1801; Oza NC, 2005, IEEE SYS MAN CYBERN, P2340; Teh Yee Whye, 2010, ENCY MACHINE LEARNIN, DOI DOI 10.1007/978-0-387-30164-8_219; Terenin A, 2018, STAT COMPUT, P1; Terenin A., 2016, ARXIV150908999; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Walker A. J., 1977, ACM Transactions on Mathematical Software, V3, P253, DOI 10.1145/355744.355749; Wallach Hanna M., 2009, P 26 INT C MACH LEAR, P1105, DOI DOI 10.1145/1553374.1553515; Yao LM, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P937; Yuan JH, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1351, DOI 10.1145/2736277.2741115; Yurochkin M., 2017, ADV NEURAL INFORM PR, P3881; Yurochkin M., 2016, ADV NEURAL INFORM PR, P2505; Zhang J., 2005, ADV NEURAL INFORM PR, P1617; Zipf G. K., 1968, PSYCHOBIOLOGY LANGUA	39	5	5	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2019	41	7					1709	1719		10.1109/TPAMI.2018.2832641	http://dx.doi.org/10.1109/TPAMI.2018.2832641			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IC4XW	29994329	Green Submitted			2022-12-18	WOS:000470972300014
J	Teng, D; Chu, DL				Teng, Dan; Chu, Delin			A Fast Frequent Directions Algorithm for Low Rank Approximation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Low rank approximation; randomized algorithms; frequent directions; sparse subspace embedding	RANDOMIZED ALGORITHM; MATRIX ALGORITHMS; JOHNSON	Recently a deterministic method, frequent directions (FD) is proposed to solve the high dimensional low rank approximation problem. It works well in practice, but experiences high computational cost. In this paper, we establish a fast frequent directions algorithm for the low rank approximation problem, which implants a randomized algorithm, sparse subspace embedding (SpEmb) in FD. This new algorithm makes use of FD's natural block structure and sends more information through SpEmb to each block in FD. We prove that our new algorithm produces a good low rank approximation with a sketch of size linear on the rank approximated. Its effectiveness and efficiency are demonstrated by the experimental results on both synthetic and real world datasets, as well as applications in network analysis.	[Teng, Dan; Chu, Delin] Natl Univ Singapore, Dept Math, Block S17,10 Lower Kent Ridge Rd, Singapore 119076, Singapore	National University of Singapore	Teng, D (corresponding author), Natl Univ Singapore, Dept Math, Block S17,10 Lower Kent Ridge Rd, Singapore 119076, Singapore.	tengdan@u.nus.edu; matchudl@nus.edu.sg		Teng, Dan/0000-0002-9727-5347	NUS research grant [R-146-000-236-114]	NUS research grant	The authors would like to thank the associate editor and anonymous reviewers for their valuable comments and suggestions on earlier versions of this paper. This work was supported in part by NUS research grant R-146-000-236-114.	Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; [Anonymous], 2011, TECH REP CNS T 2011; Avron H., 2011, NUMER MATH, V117, P219; Benzi M, 2013, LINEAR ALGEBRA APPL, V438, P2447, DOI 10.1016/j.laa.2012.10.022; Blondel M, 2013, MACH LEARN, V93, P31, DOI 10.1007/s10994-013-5367-2; Boutsidis C, 2013, SIAM J MATRIX ANAL A, V34, P1301, DOI 10.1137/120874540; Boutsidis C, 2011, ANN IEEE SYMP FOUND, P305, DOI 10.1109/FOCS.2011.21; Brandes U, 2005, NETWORK ANAL METHODO, V3418; CHANDRASEKARAN S, 1994, SIAM J MATRIX ANAL A, V15, P592, DOI 10.1137/S0895479891223781; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Desai A, 2016, IEEE T KNOWL DATA EN, V28, P1678, DOI 10.1109/TKDE.2016.2539943; DESHPANDE A, 2006, THEORY COMPUT, V2, P225; Dhanjal C, 2014, NEUROCOMPUTING, V131, P440, DOI 10.1016/j.neucom.2013.11.015; Dredze M., 2008, P 25 INT C MACHINE L, V307, P264, DOI DOI 10.1145/1390156.1390190; Drineas P, 2003, SIAM PROC S, P223; Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684; Drineas P, 2006, LECT NOTES COMPUT SC, V4110, P316; Estrada E, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.056103; Estrada E, 2010, SIAM REV, V52, P696, DOI 10.1137/090761070; Fierro RD, 1997, NUMER ALGORITHMS, V15, P37, DOI 10.1023/A:1019254318361; Ghashami M, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P845, DOI 10.1145/2939672.2939800; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Golub G., 2013, MATRIX COMPUTATIONS; Gu M, 2015, SIAM J SCI COMPUT, V37, pA1139, DOI 10.1137/130938700; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Holodnak JT, 2015, SIAM J MATRIX ANAL A, V36, P110, DOI 10.1137/130940116; Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liberty E, 2007, P NATL ACAD SCI USA, V104, P20167, DOI 10.1073/pnas.0709640104; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Musco C, 2015, ADV NEUR IN, V28; Nelson Jelani, 2013, ANN IEEE SYMP FOUND, P117, DOI DOI 10.1109/FOCS.2013.21; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Rokhlin V, 2009, SIAM J MATRIX ANAL A, V31, P1100, DOI 10.1137/080736417; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787; Urano Y., 2013, THESIS; Vershynin R., 2009, PROBABILITY THEORY R, V150, P471; Wang J.-Y., 2002, APPL SUPPORT VECTOR; Wang S., 2013, J MACHINE LEARNING R, V14, P2549; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Woolfe F, 2008, APPL COMPUT HARMON A, V25, P335, DOI 10.1016/j.acha.2007.12.002; Yang JY, 2016, P IEEE, V104, P58, DOI 10.1109/JPROC.2015.2494219; 2011, FDN TRENDS MACH LEAR, V3, P123, DOI DOI 10.1561/2200000035	48	5	5	1	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2019	41	6					1279	1293		10.1109/TPAMI.2018.2839198	http://dx.doi.org/10.1109/TPAMI.2018.2839198			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HW9UU	29993709	Green Submitted			2022-12-18	WOS:000467037000001
J	Zhao, TY; Chen, QY; Kuang, ZZ; Yu, J; Zhang, W; Fan, JP				Zhao, Tianyi; Chen, Qiuyu; Kuang, Zhenzhong; Yu, Jun; Zhang, Wei; Fan, Jianping			Deep Mixture of Diverse Experts for Large-Scale Visual Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deep mixture of diverse experts; base deep CNNs; mixture network; deep multi-task learning; large-scale visual recognition	CLASSIFICATION	In this paper, a deep mixture of diverse experts algorithm is developed to achieve more efficient learning of a huge (mixture) network for large-scale visual recognition application. First, a two-layer ontology is constructed to assign large numbers of atomic object classes into a set of task groups according to the similarities of their learning complexities, where certain degrees of inter-group task overlapping are allowed to enable sufficient inter-group message passing. Second, one particular base deep CNNs with M + 1 outputs is learned for each task group to recognize its M atomic object classes and identify one special class of "not-in-group", where the network structure (numbers of layers and units in each layer) of the well-designed deep CNNs (such as AlexNet, VGG, GoogleNet, ResNet) is directly used to configure such base deep CNNs. For enhancing the separability of the atomic object classes in the same task group, two approaches are developed to learn more discriminative base deep CNNs: (a) our deep multi-task learning algorithm that can effectively exploit the inter-class visual similarities; (b) our two-layer network cascade approach that can improve the accuracy rates for the hard object classes at certain degrees while effectively maintaining the high accuracy rates for the easy ones. Finally, all these complementary base deep CNNs with diverse but overlapped outputs are seamlessly combined to generate a mixture network with larger outputs for recognizing tens of thousands of atomic object classes. Our experimental results have demonstrated that our deep mixture of diverse experts algorithm can achieve very competitive results on large-scale visual recognition.	[Kuang, Zhenzhong; Yu, Jun] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Zhejiang, Peoples R China; [Kuang, Zhenzhong; Yu, Jun; Zhang, Wei] UNC Charlotte, Charlotte, NC 28223 USA; [Zhao, Tianyi; Chen, Qiuyu; Fan, Jianping] Univ North Carolina Charlotte, Dept Comp Sci, Charlotte, NC 28223 USA; [Zhang, Wei] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China	Hangzhou Dianzi University; University of North Carolina; University of North Carolina Charlotte; University of North Carolina; University of North Carolina Charlotte; Fudan University	Fan, JP (corresponding author), Univ North Carolina Charlotte, Dept Comp Sci, Charlotte, NC 28223 USA.	tzhao4@uncc.edu; qchen12@uncc.edu; zzkuang@hdu.edu.cn; yujun@hdu.edu.cn; weizh@fudan.edu.cn; jfan@uncc.edu			Program of Shaanxi Province Innovative Research Team [2014KCT-17]; Program for Changjiang Scholars and Innovative Research Team in University [IRT13090]; US National Science Foundation [1651166-CNS, NSFC-61772161, NSFC-61622205, NSFC-61472110]	Program of Shaanxi Province Innovative Research Team; Program for Changjiang Scholars and Innovative Research Team in University(Program for Changjiang Scholars & Innovative Research Team in University (PCSIRT)); US National Science Foundation(National Science Foundation (NSF))	The authors want to appreciate all the reviewers for their insightful comments and suggestions which helped to make this paper more readable. Ji Zhang has provided us his identification results of 7,756 atomic object classes in Image-Net10K for this research. The useful discussions with Ji Zhang and Haixi Zhang are also appreciated when they were visiting scholars at UNC-Charlotte. This research is partly supported by Program of Shaanxi Province Innovative Research Team (No. 2014KCT-17) and Program for Changjiang Scholars and Innovative Research Team in University (No. IRT13090). This research is supported by US National Science Foundation under 1651166-CNS and NSFC-61772161; Jun Yu is also supported by NSFC-61622205 and NSFC-61472110. Tianyi Zhao, Qiuyu Chen, and Zhenzhong Kuang have equal contributions on this work. The first three authors have equal contribution to this paper.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Barthe E, 2009, POLICE PRACT RES, V10, P255, DOI 10.1080/15614260802381067; Bengio Samy, 2010, ADV NEURAL INFORM PR, V1, P163, DOI [10.5555/2997189.2997208, DOI 10.5555/2997189.2997208]; Bengio Y., 2012, JMLR WORKSHOP C P, V27, P17; Bulo SR, 2014, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2014.18; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Dean T, 2013, PROC CVPR IEEE, P1814, DOI 10.1109/CVPR.2013.237; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6; Deng Jia, 2011, ADV NEURAL INFORM PR, V1, P567, DOI [10.5555/2986459.2986523, DOI 10.5555/2986459.2986523]; Deng L, 2012, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2012.6288333; Donahue J, 2014, PR MACH LEARN RES, V32; Eigen  D., 2013, ICLR 2014 WORKSH; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan JP, 2008, IEEE T IMAGE PROCESS, V17, P407, DOI 10.1109/TIP.2008.916999; Fan JP, 2017, IEEE T IMAGE PROCESS, V26, P1923, DOI 10.1109/TIP.2017.2667405; Fan JP, 2015, IEEE T IMAGE PROCESS, V24, P4172, DOI 10.1109/TIP.2015.2457337; Farrell R, 2011, IEEE I CONF COMP VIS, P161, DOI 10.1109/ICCV.2011.6126238; Frazao X, 2014, LECT NOTES COMPUT SC, V8827, P674, DOI 10.1007/978-3-319-12568-8_82; Gama J, 2000, MACH LEARN, V41, P315, DOI 10.1023/A:1007652114878; Ge Z.Y., 2016, APPL COMPUTER VISION, P1, DOI [10.1109/WACV.2016.7477700, DOI 10.1109/WACV.2016.7477700]; Griffin G, 2008, PROC CVPR IEEE, P533; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hu HX, 2016, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2016.323; Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Huh  M., 2016, P NIPS WORKSH LSCVS; Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81; Jia D, 2012, PROC CVPR IEEE, P3450, DOI 10.1109/CVPR.2012.6248086; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang J, 1997, INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, 1997 DIGEST OF TECHNICAL PAPERS, P94; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar  A., 2015, COMPUT SCI; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee CY, 2016, JMLR WORKSH CONF PRO, V51, P464; Li J, 2015, AAAI CONF ARTIF INTE, P3804; Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x; Li SJ, 2015, INT J COMPUT VISION, V113, P19, DOI 10.1007/s11263-014-0767-8; Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37; Lin D., 1998, P INT C MACH LEARN; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477; Liu BY, 2013, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2013.114; Long  M., 2016, P MACH LEARN RES; Marszalek M, 2007, PROC CVPR IEEE, P2319; Martinez-Munoz G, 2009, PROC CVPR IEEE, P549, DOI 10.1109/CVPRW.2009.5206574; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Moghimi  M., 2015, P BRIT MACH VIS C BM; Mohammadi  M., 2016, P INT C LEARN REPR; Murthy VN, 2016, PROC CVPR IEEE, P2240, DOI 10.1109/CVPR.2016.246; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Perronnin F, 2012, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2012.6248090; Peter  K., 2015, P IEEE INT C COMP VI, P1467; Resnik P, 1995, INT JOINT CONF ARTIF, P448; Sfar AR, 2013, PROC CVPR IEEE, P835, DOI 10.1109/CVPR.2013.113; Shu  X., 2015, P ACM INT C MULT; Srivastava  N., 2013, P ADV NEUR INF PROC, P1, DOI DOI 10.1017/S0272263100006331; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tang  Y., 2012, P INT C MACH LEARN; Tommasi T, 2010, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2010.5540064; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; Tu ZW, 2005, IEEE I CONF COMP VIS, P1589; Yang  Z., 2015, P IEEE CVPR, P2740; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhao B., 2011, P 24 INT C NEUR INF, P1251; Zhao HM, 2004, IEEE T KNOWL DATA EN, V16, P727, DOI 10.1109/TKDE.2004.3	75	5	5	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2019	41	5					1072	1087		10.1109/TPAMI.2018.2828821	http://dx.doi.org/10.1109/TPAMI.2018.2828821			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HS1FL	29993909	hybrid, Green Submitted			2022-12-18	WOS:000463607400004
J	Aguinaga, S; Chiang, D; Weninger, T				Aguinaga, Salvador; Chiang, David; Weninger, Tim			Learning Hyperedge Replacement Grammars for Graph Generation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						graphs; hyperedge replacement grammar; graph generation	TREE	The discovery and analysis of network patterns are central to the scientific enterprise. In the present work, we developed and evaluated a new approach that learns the building blocks of graphs that can be used to understand and generate new realistic graphs. Our key insight is that a graph's clique tree encodes robust and precise information. We show that a Hyperedge Replacement Grammar (HRG) can be extracted from the clique tree, and we develop a fixed-size graph generation algorithm that can be used to produce new graphs of a specified size. In experiments on large real-world graphs, we show that graphs generated from the HRG approach exhibit a diverse range of properties that are similar to those found in the original networks. In addition to graph properties like degree or eigenvector centrality, what a graph "looks like" ultimately depends on small details in local graph substructures that are difficult to define at a global level. We show that the HRG model can also preserve these local substructures when generating new graphs.	[Aguinaga, Salvador; Chiang, David; Weninger, Tim] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA	University of Notre Dame	Weninger, T (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.	saguinag@nd.edu; dchiang@nd.edu; tweninge@nd.edu		Weninger, Tim/0000-0003-3164-2615	Templeton Foundation [FP053369-M/O]; NSF IIS [1652492]	Templeton Foundation; NSF IIS(National Science Foundation (NSF))	This work is funded by a grant from the Templeton Foundation (FP053369-M/O) and NSF IIS (#1652492).	Aguinaga S., 2016, P 2016 IEEE EC TECHN, P1; Aguinaga S, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P469, DOI 10.1145/2983323.2983826; Ahmed NK, 2015, IEEE DATA MINING, P1, DOI 10.1109/ICDM.2015.141; Albert R, 2000, NATURE, V406, P378, DOI 10.1038/35019019; ARNBORG S, 1987, SIAM J ALGEBRA DISCR, V8, P277, DOI 10.1137/0608024; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Chung F., 2002, ANN COMB, V6, P125, DOI [DOI 10.1007/PL00012580.PDF, 10.1007/PL00012580]; CLARKE BL, 1975, J CHEM PHYS, V62, P773, DOI 10.1063/1.430524; Craciun G, 2008, J MATH CHEM, V44, P244, DOI 10.1007/s10910-007-9307-x; David Chiang Jacob, 2013, LONG PAPERS, P924; Doolittle WF, 2007, P NATL ACAD SCI USA, V104, P2043, DOI 10.1073/pnas.0610699104; Dorogovtsev SN, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.040601; Drewes F, 2002, J COMPUT SYST SCI, V64, P249, DOI 10.1006/jcss.2001.1790; Easley D., 2010, NETWORKS CROWDS MARK; Geman S, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P279; GRANOVETTER MS, 1973, AM J SOCIOL, V78, P1360, DOI 10.1086/225469; Heaps Harold S., 1978, INFORM RETRIEVAL COM; Holder L. B., 1994, KDD WORKSH, P169; Hunter DR, 2008, J STAT SOFTW, V24, DOI DOI 10.18637/JSS.V024.I03; Jonyer I., 2006, MINING GRAPH DATA, P183; Kauffman S.A., 1993, ORIGINS ORDER SELF O; Kemp C, 2008, P NATL ACAD SCI USA, V105, P10687, DOI 10.1073/pnas.0802631105; Koller D., 2009, PROBABILISTIC GRAPHI; Kuhn T.S., 2012, STRUCTURE SCI REVOLU; Kukluk Jacek P., 2007, 2007 International Conference on Bioinformatics & Computational Biology (BIOCOMP'07), P44; Kukluk JP, 2008, INT J AP MAT COM-POL, V18, P241, DOI 10.2478/v10006-008-0022-y; LAUTEMANN C, 1988, LECT NOTES COMPUT SC, V299, P28; Leskovec J., 2006, P 12 ACM SIGKDD INT, P631; Leskovec J, 2010, J MACH LEARN RES, V11, P985; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Luerssen M. H., 2005, P 28 AUSTR COMP SCI, P229; Nederhof M.-J., 2003, P 8 INT WORKSH PARS, P137; Przulj N, 2007, BIOINFORMATICS, V23, pE177, DOI 10.1093/bioinformatics/btl301; ROBERTSON N, 1986, J ALGORITHM, V7, P309, DOI 10.1016/0196-6774(86)90023-4; Robins G, 2007, SOC NETWORKS, V29, P173, DOI 10.1016/j.socnet.2006.08.002; Sedgewick R., 2013, INTRO ANAL ALGORITHM; Seshadhri C, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.056109; STOLCKE A, 1995, COMPUT LINGUIST, V21, P165; Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725; TARJAN RE, 1985, SIAM J COMPUT, V14, P254, DOI 10.1137/0214020; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Yaveroglu ON, 2015, BIOINFORMATICS, V31, P2697, DOI 10.1093/bioinformatics/btv170	43	5	5	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2019	41	3					625	638		10.1109/TPAMI.2018.2810877	http://dx.doi.org/10.1109/TPAMI.2018.2810877			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	HK7LA	29994579	hybrid, Green Submitted			2022-12-18	WOS:000458168800008
J	Chang, HJ; Fischer, T; Petit, M; Zambelli, M; Demiris, Y				Chang, Hyung Jin; Fischer, Tobias; Petit, Maxime; Zambelli, Martina; Demiris, Yiannis			Learning Kinematic Structure Correspondences Using Multi-Order Similarities	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Articulated kinematic structure correspondences; hypergraph matching; subgraph isomorphism aggregation; kinematic correlation; combinatorial local motion similarity; humanoid robotics	NETWORK ALIGNMENT; ARTICULATED STRUCTURE; GLOBAL ALIGNMENT; GRAPH; MODELS; MOTION; SHAPE	In this paper, we present a novel framework for finding the kinematic structure correspondences between two articulated objects in videos via hypergraph matching. In contrast to appearance and graph alignment based matching methods, which have been applied among two similar static images, the proposed method finds correspondences between two dynamic kinematic structures of heterogeneous objects in videos. Thus our method allows matching the structure of objects which have similar topologies or motions, or a combination of the two. Our main contributions can be summarised as follows: (i) casting the kinematic structure correspondence problem into a hypergraph matching problem by incorporating multi-order similarities with normalising weights, (ii) introducing a structural topology similarity measure by aggregating topology constrained subgraph isomorphisms, (iii) measuring kinematic correlations between pairwise nodes, and (iv) proposing a combinatorial local motion similarity measure using geodesic distance on the Riemannian manifold. We demonstrate the robustness and accuracy of our method through a number of experiments on synthetic and real data, outperforming various other state of the art methods. Our method is not limited to a specific application nor sensor, and can be used as building block in applications such as action recognition, human motion retargeting to robots, and articulated object manipulation amongst others.	[Chang, Hyung Jin; Fischer, Tobias; Petit, Maxime; Zambelli, Martina; Demiris, Yiannis] Imperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, England	Imperial College London	Chang, HJ (corresponding author), Imperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, England.	hj.chang@imperial.ac.uk; t.fischer@imperial.ac.uk; m.petit@imperial.ac.uk; m.zambelli13@imperial.ac.uk; y.demiris@imperial.ac.uk	Demiris, Yiannis/AAF-3917-2019; Fischer, Tobias/D-4405-2018	Demiris, Yiannis/0000-0003-4917-3343; Fischer, Tobias/0000-0003-2183-017X; Zambelli, Martina/0000-0001-6061-191X; Chang, Hyung Jin/0000-0001-7495-9677; Petit, Maxime/0000-0001-5785-2915	EU FP7 project WYSI-WYD [612139]	EU FP7 project WYSI-WYD	This work was supported in part by EU FP7 project WYSI-WYD under Grant 612139. We thank Minsu Cho for fruitful discussions. The authors gratefully acknowledge the support from the members of the Personal Robotics Lab. Research presented in this paper is a continuation of Chang et al. [1] and includes results from [1].	Chaaraoui AA, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P91, DOI 10.1109/ICCVW.2013.19; Bai X, 2008, IEEE T PATTERN ANAL, V30, P1282, DOI 10.1109/TPAMI.2007.70769; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Bulo SR, 2013, IEEE T PATTERN ANAL, V35, P1312, DOI 10.1109/TPAMI.2012.226; Chan HL, 2018, INT J PROD RES, V56, P3397, DOI 10.1080/00207543.2016.1278283; Chang HJ, 2016, PROC CVPR IEEE, P4216, DOI 10.1109/CVPR.2016.457; Chang HJ, 2015, PROC CVPR IEEE, P3138, DOI 10.1109/CVPR.2015.7298933; Cho M., 2014, P IEEE C COMPUTER VI, P2083; Cho M, 2012, PROC CVPR IEEE, P398, DOI 10.1109/CVPR.2012.6247701; Cho M, 2009, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2009.5459322; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228; Cordella LP, 2004, IEEE T PATTERN ANAL, V26, P1367, DOI 10.1109/TPAMI.2004.75; Cour T., 2007, P ADV NEURAL INFORM, P313; Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422; Del Pero L, 2017, INT J COMPUT VISION, V121, P303, DOI 10.1007/s11263-016-0939-9; Del Pero L, 2016, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2016.84; Del Pero L, 2015, PROC CVPR IEEE, P2151, DOI 10.1109/CVPR.2015.7298827; Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110; Ehrlich HC, 2012, J CHEMINFORMATICS, V4, DOI 10.1186/1758-2946-4-13; Fayad J, 2011, IEEE I CONF COMP VIS, P431, DOI 10.1109/ICCV.2011.6126272; Gao YX, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4398, DOI 10.1109/IROS.2016.7759647; Hadfield S, 2014, LECT NOTES COMPUT SC, V8690, P758, DOI 10.1007/978-3-319-10605-2_49; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Hein M., 2013, P ADV NEUR INF PROC, V26; Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282; Huang XX, 2014, ROBOT AUTON SYST, V62, P497, DOI 10.1016/j.robot.2013.12.006; Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2; Jacquet B, 2013, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2013.198; Jain A. K., 1989, FUNDAMENTALS DIGITAL; Jia C, 2013, INT CONF ACOUST SPEE, P2493, DOI 10.1109/ICASSP.2013.6638104; Jungmin Lee, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2816, DOI 10.1109/ICPR.2010.690; Katz D, 2013, IEEE INT CONF ROBOT, P5003, DOI 10.1109/ICRA.2013.6631292; Kofidis E, 2002, SIAM J MATRIX ANAL A, V23, P863, DOI 10.1137/S0895479801387413; Koyuturk M, 2006, J COMPUT BIOL, V13, P182, DOI 10.1089/cmb.2006.13.182; Kuchaiev O, 2011, BIOINFORMATICS, V27, P1390, DOI 10.1093/bioinformatics/btr127; Kuchaiev O, 2010, J R SOC INTERFACE, V7, P1341, DOI 10.1098/rsif.2010.0063; Lee J, 2011, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2011.5995387; Leordeanu M., 2012, P 15 INT C ART INT S, P676; Leordeanu M, 2011, IEEE I CONF COMP VIS, P2274, DOI 10.1109/ICCV.2011.6126507; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384; Meng L, 2016, BIOINFORMATICS, V32, P3155, DOI 10.1093/bioinformatics/btw348; Moakher M, 2002, SIAM J MATRIX ANAL A, V24, P1, DOI 10.1137/S0895479801383877; Neyshabur B, 2013, BIOINFORMATICS, V29, P1654, DOI 10.1093/bioinformatics/btt202; Nicolas G, 2007, J BIOMECH, V40, P1048, DOI 10.1016/j.jbiomech.2006.04.010; Ochs P, 2012, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2012.6247728; Ngoc QN, 2015, PROC CVPR IEEE, P5270, DOI 10.1109/CVPR.2015.7299164; Rosman B, 2011, INT J ROBOT RES, V30, P1328, DOI 10.1177/0278364911408155; Ross DA, 2010, INT J COMPUT VISION, V88, P214, DOI 10.1007/s11263-010-0325-y; Schulz J, 2015, J COMPUT GRAPH STAT, V24, P539, DOI 10.1080/10618600.2014.914947; Shashua A, 2006, LECT NOTES COMPUT SC, V3954, P595; Shen W, 2013, PATTERN RECOGN, V46, P539, DOI 10.1016/j.patcog.2012.07.023; Singh R, 2008, P NATL ACAD SCI USA, V105, P12763, DOI 10.1073/pnas.0806627105; Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305; Strzodka R, 2004, P EG IEEE TCVG S VIS, P221, DOI DOI 10.2312/VISSYM/VISSYM04/221-230; Sturm J, 2011, J ARTIF INTELL RES, V41, P477, DOI 10.1613/jair.3229; Sturm J, 2009, J PHYSIOL-PARIS, V103, P220, DOI 10.1016/j.jphysparis.2009.08.005; Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460; Torresani L, 2008, LECT NOTES COMPUT SC, V5303, P596, DOI 10.1007/978-3-540-88688-4_44; Tosun T., 2014, P ASME INT MECH ENG, P1; Tresadern P, 2005, PROC CVPR IEEE, P1110; Tung T, 2014, IEEE T PATTERN ANAL, V36, P901, DOI 10.1109/TPAMI.2013.179; VALIENTE G, 2002, ALGORITHMS TREES GRA; Vijayakumar V, 2018, INT J CONTROL, V91, P204, DOI 10.1080/00207179.2016.1276633; Vijayan V, 2015, BIOINFORMATICS, V31, P2409, DOI 10.1093/bioinformatics/btv161; Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171; Wu Y, 2010, IEEE INT CONF ROBOT, P2889, DOI 10.1109/ROBOT.2010.5509429; Xu WW, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531341; Yan J., 2006, P CVPR, P712; Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739; Yan JC, 2015, PROC CVPR IEEE, P1520, DOI 10.1109/CVPR.2015.7298759; Yan Wu, 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2664, DOI 10.1109/ROBIO.2011.6181707; Zambelli M., 2016, P IEEE INT C INT ROB; Zass R., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587500; Zeng Y, 2010, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2010.5540189; Zhou F, 2013, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2013.376; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667; Zhou YX, 2016, PROC CVPR IEEE, P5791, DOI 10.1109/CVPR.2016.624	80	5	5	0	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2018	40	12					2920	2934		10.1109/TPAMI.2017.2777486	http://dx.doi.org/10.1109/TPAMI.2017.2777486			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GZ4HL	29989982	Green Submitted			2022-12-18	WOS:000449355500010
J	Sagonas, C; Ververas, E; Panagakis, Y; Zafeiriou, S				Sagonas, Christos; Ververas, Evangelos; Panagakis, Yannis; Zafeiriou, Stefanos			Recovering Joint and Individual Components in Facial Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Low-rank; sparsity; facial expression synthesis; face age progression; joint and individual components	BEHAVIOR	A set of images depicting faces with different expressions or in various ages consists of components that are shared across all images (i.e., joint components) imparting to the depicted object the properties of human faces as well as individual components that are related to different expressions or age groups. Discovering the common (joint) and individual components in facial images is crucial for applications such as facial expression transfer and age progression. The problem is rather challenging when dealing with images captured in unconstrained conditions in the presence of sparse non-Gaussian errors of large magnitude (i.e., sparse gross errors or outliers) and contain missing data. In this paper, we investigate the use of a method recently introduced in statistics, the so-called Joint and Individual Variance Explained (JIVE) method, for the robust recovery of joint and individual components in visual facial data consisting of an arbitrary number of views. Since the JIVE is not robust to sparse gross errors, we propose alternatives, which are (1) robust to sparse gross, non-Gaussian noise, (2) able to automatically find the individual components rank, and (3) can handle missing data. We demonstrate the effectiveness of the proposed methods to several computer vision applications, namely facial expression synthesis and 2D and 3D face age progression 'in-the-wild.	[Sagonas, Christos] Onfido, London WC2E 9LG, England; [Ververas, Evangelos; Panagakis, Yannis; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London SW7 2RH, England; [Panagakis, Yannis] Middlesex Univ London, Dept Comp Sci, London NW4 4BT, England	Imperial College London; Middlesex University	Sagonas, C (corresponding author), Onfido, London WC2E 9LG, England.	ch.sagonas@gmail.com; e.ververas16@imperial.ac.uk; i.panagakis@imperial.ac.uk; s.zafeiriou@imperial.ac.uk	Panagakis, Yannis/AAZ-8090-2020	Panagakis, Ioannis/0000-0003-0153-5210; Ververas, Evangelos/0000-0003-4345-1744	EPSRC [EP/N007743/1]; EPSRC [EP/J017787/1, EP/H016988/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/J017787/1, EP/H016988/1, EP/N007743/1] Funding Source: researchfish	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was partially funded by EPSRC Project EP/N007743/1 (FACER2VM). Finally, we would like to thank Alina Leidinger for some insights on an earlier version of this manuscript. This work was completed while C. Sagonas was at Imperial College London, UK.	Alabort-i-Medina J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P679, DOI 10.1145/2647868.2654890; Bertsekas D. P., 2014, CONSTRAINED OPTIMIZA; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7; Booth James, 2017, P IEEE C COMP VIS PA, P7; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49; Cheng-Ta Shen, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P123, DOI 10.1109/ISM.2011.28; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Duong CN, 2016, PROC CVPR IEEE, P5772, DOI 10.1109/CVPR.2016.622; Fazel M., 2002, MATRIX RANK MINIMIZA; Georgakis C, 2018, INT J COMPUT VISION, V126, P333, DOI 10.1007/s11263-016-0985-3; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Huang D, 2010, LECT NOTES COMPUT SC, V6312, P364, DOI 10.1007/978-3-642-15552-9_27; Huang Gary B, 2014, 14003 U MASS AMH DEP, V14, P1; Huber P. J., 2011, ROBUST STAT; KEMELMACHERSHLIZER, 2014, PROC CVPR IEEE, P3334, DOI DOI 10.1109/CVPR.2014.426; Klami A, 2008, NEUROCOMPUTING, V72, P39, DOI 10.1016/j.neucom.2007.12.044; Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553; Lock EF, 2013, ANN APPL STAT, V7, P523, DOI 10.1214/12-AOAS597; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Matthews I, 2010, P IEEE C COMP VIS PA, P94, DOI DOI 10.1109/CVPRW.2010.5543262; Mollahosseini A, 2016, IEEE COMPUT SOC CONF, P1509, DOI 10.1109/CVPRW.2016.188; Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Nicolaou MA, 2014, IEEE T PATTERN ANAL, V36, P1299, DOI 10.1109/TPAMI.2014.16; Panagakis Y, 2016, IEEE T PATTERN ANAL, V38, P1665, DOI 10.1109/TPAMI.2015.2497700; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Ramanathan N., 2006, P IEEE COMP SOC C CO, P387, DOI DOI 10.1109/CVPR.2006.187; Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41; Sagonas C., 2017, P IEEE C COMP VIS PA, P739; Sagonas C, 2016, INT C PATT RECOG, P4226, DOI 10.1109/ICPR.2016.7900297; Sagonas C, 2017, INT J COMPUT VISION, V122, P270, DOI 10.1007/s11263-016-0920-7; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2014, PROC CVPR IEEE, P1789, DOI 10.1109/CVPR.2014.231; Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452; TUCKER LR, 1958, PSYCHOMETRIKA, V23, P111, DOI 10.1007/BF02289009; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Wang W, 2016, PROC CVPR IEEE, P2378, DOI 10.1109/CVPR.2016.261; Wright J, 2010, IEEE T INFORM THEORY, V56, P3540, DOI 10.1109/TIT.2010.2048473; Zhou GX, 2016, IEEE T NEUR NET LEAR, V27, P2426, DOI 10.1109/TNNLS.2015.2487364	46	5	5	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2018	40	11					2668	2681		10.1109/TPAMI.2017.2784421	http://dx.doi.org/10.1109/TPAMI.2017.2784421			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GW2AF	29990036	Green Accepted, Green Submitted			2022-12-18	WOS:000446683700011
J	Alayrac, JB; Bojanowski, P; Agrawal, N; Sivic, J; Laptev, I; Lacoste-Julien, S				Alayrac, Jean-Baptiste; Bojanowski, Piotr; Agrawal, Nishant; Sivic, Josef; Laptev, Ivan; Lacoste-Julien, Simon			Learning from Narrated Instruction Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Step discovery; narrated instruction videos; unsupervised learning	MULTIPLE SEQUENCE ALIGNMENT	Automatic assistants could guide a person or a robot in performing new tasks, such as changing a car tire or repotting a plant. Creating such assistants, however, is non-trivial and requires understanding of visual and verbal content of a video. Towards this goal, we here address the problem of automatically learning the main steps of a task from a set of narrated instruction videos. We develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method sequentially clusters textual and visual representations of a task, where the two clustering problems are linked by joint constraints to obtain a single coherent sequence of steps in both modalities. To evaluate our method, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains videos for five different tasks with complex interactions between people and objects, captured in a variety of indoor and outdoor settings. We experimentally demonstrate that the proposed method can automatically discover, learn and localize the main steps of a task in input videos.	[Alayrac, Jean-Baptiste; Bojanowski, Piotr; Agrawal, Nishant; Sivic, Josef; Laptev, Ivan] PSL Res Univ, CNRS, INRIA, Dept Informat,ENS, F-75005 Paris, France; [Sivic, Josef] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Prague, Czech Republic; [Lacoste-Julien, Simon] Univ Montreal, CS & OR Dept, Montreal, PQ H3C 3J7, Canada	Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Czech Technical University Prague; Universite de Montreal	Alayrac, JB (corresponding author), PSL Res Univ, CNRS, INRIA, Dept Informat,ENS, F-75005 Paris, France.	jean-baptiste.alayrac@inria.fr; piotr.bojanowski@inria.fr; nishant.agrawal@inria.fr; Josef.Sivic@ens.fr; ivan.laptev@inria.fr; slacoste@iro.umontreal.ca			ERC grant ACTIVIA [307574]; ERC grant LEAP [336845]; CIFAR Learning in Machines Brains program; ESIF; OP Research, development and education Project IMPACT [CZ.02.1.01/0.0/0.0/15_003/0000468]; Google Research Award	ERC grant ACTIVIA; ERC grant LEAP; CIFAR Learning in Machines Brains program; ESIF; OP Research, development and education Project IMPACT; Google Research Award(Google Incorporated)	This work has been supported in part by ERC grants ACTIVIA (no. 307574) and LEAP (no. 336845), CIFAR Learning in Machines & Brains program, ESIF, OP Research, development and education Project IMPACT No. CZ.02.1.01/0.0/0.0/15_003/0000468 and a Google Research Award.	[Anonymous], 2006, P LREC; Bach F., 2007, P NIPS, P49; Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507; Bojanowski P, 2013, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2013.283; Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41; Chambers Nathanael, 2008, P 46 ANN M ASS COMP, P789; Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007; Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; Frermann L., 2014, P 14 C EUR CHAPT ASS, P49; HIGGINS DG, 1988, GENE, V73, P237, DOI 10.1016/0378-1119(88)90330-7; Hsu D, 2014, FOUND COMPUT MATH, V14, P569, DOI 10.1007/s10208-014-9192-1; Jaggi M., 2013, P 29 INT C MACH LEAR; Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Lacoste-Julien S., 2013, P INT C MACH LEARN; Lacoste-Julien S., 2016, ARXIV PREPRINT ARXIV; Laptev I, 2008, CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587756; Lee C, 2002, BIOINFORMATICS, V18, P452, DOI 10.1093/bioinformatics/18.3.452; Liao T., 2014, PATTERN RECOGN, V38, P1857; Malmaud J., 2015, P C N AM CHAPT ASS C, P143; Miech A., 2017, P IEEE INT C COMP VI; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Naim Iftekhar, 2015, P C N AM CHAPT ASS C, P164; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Osokin A, 2016, PR MACH LEARN RES, V48; Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35; Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342; Regneri M, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P979; Sener O, 2015, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2015.509; Simonyan K., 2015, P ICLR; Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang L, 1994, J Comput Biol, V1, P337, DOI 10.1089/cmb.1994.1.337	37	5	5	1	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2018	40	9					2194	2208		10.1109/TPAMI.2017.2749223	http://dx.doi.org/10.1109/TPAMI.2017.2749223			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GP4UX	28885149	Green Submitted			2022-12-18	WOS:000440868400012
J	Chang, HJ; Demiris, Y				Chang, Hyung Jin; Demiris, Yiannis			Highly Articulated Kinematic Structure Estimation Combining Motion and Skeleton Information	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Highly articulated kinematic structure estimation; adaptive motion segmentation; density weighted silhouette generation from sparse points; adaptive kernel selection	FACTORIZATION-BASED APPROACH; SUPPORT; SHAPE; SEGMENTATION; PERFORMANCE; OBJECTS	In this paper, we present a novel framework for unsupervised kinematic structure learning of complex articulated objects from a single-view 2D image sequence. In contrast to prior motion-based methods, which estimate relatively simple articulations, our method can generate arbitrarily complex kinematic structures with skeletal topology via a successive iterative merging strategy. The iterative merge process is guided by a density weighted skeleton map which is generated from a novel object boundary generation method from sparse 2D feature points. Our main contributions can be summarised as follows: (i) An unsupervised complex articulated kinematic structure estimation method that combines motion segments with skeleton information. (ii) An iterative fine-to-coarse merging strategy for adaptive motion segmentation and structural topology embedding. (iii) A skeleton estimation method based on a novel silhouette boundary generation from sparse feature points using an adaptive model selection method. (iv) A new highly articulated object dataset with ground truth annotation. We have verified the effectiveness of our proposed method in terms of computational time and estimation accuracy through rigorous experiments with multiple datasets. Our experiments show that the proposed method outperforms state-of-the-art methods both quantitatively and qualitatively.	[Chang, Hyung Jin; Demiris, Yiannis] Imperial Coll London, Personal Robot Lab, Dept Elect & Elect Engn, London SW7 2AZ, England	Imperial College London	Chang, HJ (corresponding author), Imperial Coll London, Personal Robot Lab, Dept Elect & Elect Engn, London SW7 2AZ, England.	hj.chang@imperial.ac.uk; y.demiris@imperial.ac.uk	Demiris, Yiannis/AAF-3917-2019	Demiris, Yiannis/0000-0003-4917-3343; Chang, Hyung Jin/0000-0001-7495-9677	EU FP7 project WYSIWYD [612139]	EU FP7 project WYSIWYD	This work was supported in part by the EU FP7 project WYSIWYD under Grant 612139. The authors wish to thank Chris Russell for helpful comments on the energy based method implementation and providing the dataset. The authors gratefully acknowledge the support from the members of the Personal Robotics Laboratory at Imperial College London. This research presented in this paper is a continuation of Chang and Demiris [1] and includes results from [1].	Chaaraoui AA, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P91, DOI 10.1109/ICCVW.2013.19; [Anonymous], 1998, NEURAL NETWORKS COMP; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Ben-Artzi G., 2016, CORR; Black M. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P551, DOI 10.1109/ICCV.1999.791271; BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; Botev ZI, 2010, ANN STAT, V38, P2916, DOI 10.1214/10-AOS799; Boult T. E., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P179, DOI 10.1109/WVM.1991.212809; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Cauwenberghs G, 2001, ADV NEUR IN, V13, P409; Chang HJ, 2016, PROC CVPR IEEE, P4216, DOI 10.1109/CVPR.2016.457; Chang HJ, 2015, PROC CVPR IEEE, P3138, DOI 10.1109/CVPR.2015.7298933; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fayad J, 2011, IEEE I CONF COMP VIS, P431, DOI 10.1109/ICCV.2011.6126272; Fragkiadaki K, 2012, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2012.6247883; Gear CW, 1998, INT J COMPUT VISION, V29, P133, DOI 10.1023/A:1008026310903; Gunn S. R., 1998, ISIS TECH REP, V14; Guo Q., INT J GEOGRAPHICAL I, V25, P1697; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Huang XX, 2014, ROBOT AUTON SYST, V62, P497, DOI 10.1016/j.robot.2013.12.006; Itseez, 2015, OP SOURC COMP VIS LI; Jacquet B, 2013, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2013.198; Jain A. K., 1989, FUNDAMENTALS DIGITAL; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Jianhao Ding, 2010, 2010 2nd International Workshop on Education Technology and Computer Science (ETCS), P71, DOI 10.1109/ETCS.2010.241; Jung H, 2014, PROC CVPR IEEE, P1210, DOI 10.1109/CVPR.2014.158; Katz D, 2013, IEEE INT CONF ROBOT, P5003, DOI 10.1109/ICRA.2013.6631292; Kim P, 2008, THESIS; Kim P.J., 2008, P 19 INT C PATT REC, P1; Landgrebe TCW, 2006, PATTERN RECOGN LETT, V27, P908, DOI 10.1016/j.patrec.2005.10.015; Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242; Ochs P, 2012, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2012.6247728; Ochs P, 2011, IEEE I CONF COMP VIS, P1583, DOI 10.1109/ICCV.2011.6126418; Pillai S., 2014, P ROB SCI SYST C BER, DOI [10.15607/RSS.2014.X.050, DOI 10.15607/RSS.2014.X.050]; Ross D. J. K, 2008, THESIS; Ross DA, 2010, INT J COMPUT VISION, V88, P214, DOI 10.1007/s11263-010-0325-y; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Spoerri A., 1991, THESIS; Straka M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.69; Strzodka R, 2004, P EG IEEE TCVG S VIS, P221, DOI DOI 10.2312/VISSYM/VISSYM04/221-230; Sturm J, 2008, IEEE INT CONF ROBOT, P3328, DOI 10.1109/ROBOT.2008.4543718; Sturm J, 2011, J ARTIF INTELL RES, V41, P477, DOI 10.1613/jair.3229; Sturm J, 2009, J PHYSIOL-PARIS, V103, P220, DOI 10.1016/j.jphysparis.2009.08.005; Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Tax DMJ, 2002, J MACH LEARN RES, V2, P155, DOI 10.1162/15324430260185583; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Tran QA, 2005, PATTERN RECOGN LETT, V26, P1174, DOI 10.1016/j.patrec.2004.11.001; Tresadern P, 2005, PROC CVPR IEEE, P1110; Tron R., 2008, P IEEE C COMP VIS PA, P1; Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974; Unger M, 2012, PROC CVPR IEEE, P1878, DOI 10.1109/CVPR.2012.6247887; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Weinzaepfel P, 2015, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2015.7298873; Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739; Yan JY, 2007, LECT NOTES COMPUT SC, V4358, P75; Yan JY, 2005, PROC CVPR IEEE, P815; Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301	63	5	5	1	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2018	40	9					2165	2179		10.1109/TPAMI.2017.2748579	http://dx.doi.org/10.1109/TPAMI.2017.2748579			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GP4UX	28880158	Green Submitted, Green Published			2022-12-18	WOS:000440868400010
J	Knoll, C; Mehta, D; Chen, TR; Pernkopf, F				Knoll, Christian; Mehta, Dhagash; Chen, Tianran; Pernkopf, Franz			Fixed Points of Belief Propagation-An Analysis via Polynomial Homotopy Continuation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Graphical models; belief propagation; probabilistic inference; sum-product algorithm; Bethe free energy; phase transitions; inference algorithms; dynamical equations	PROBABILITY PROPAGATION; APPROXIMATE INFERENCE; CONVERGENCE	Belief propagation (BP) is an iterative method to perform approximate inference on arbitrary graphical models. Whether BP converges and if the solution is a unique fixed point depends on both the structure and the parametrization of the model. To understand this dependence it is interesting to find all fixed points. In this work, we formulate a set of polynomial equations, the solutions of which correspond to BP fixed points. To solve such a nonlinear system we present the numerical polynomial-homotopy-continuation (NPHC) method. Experiments on binary Ising models and on error-correcting codes show how our method is capable of obtaining all BP fixed points. On Ising models with fixed parameters we show how the structure influences both the number of fixed points and the convergence properties. We further asses the accuracy of the marginals and weighted combinations thereof. Weighting marginals with their respective partition function increases the accuracy in all experiments. Contrary to the conjecture that uniqueness of BP fixed points implies convergence, we find graphs for which BP fails to converge, even though a unique fixed point exists. Moreover, we show that this fixed point gives a good approximation, and the NPHC method is able to obtain this fixed point.	[Knoll, Christian; Pernkopf, Franz] Graz Univ Technol, Signal Proc & Speech Commun Lab, A-8010 Graz, Austria; [Mehta, Dhagash] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA; [Chen, Tianran] Auburn Univ, Dept Math & Comp Sci, Montgomery, AL 36117 USA	Graz University of Technology; University of Notre Dame; Auburn University System; Auburn University	Knoll, C (corresponding author), Graz Univ Technol, Signal Proc & Speech Commun Lab, A-8010 Graz, Austria.	christian.knoll.c@ieee.org; Dhagash.B.Mehta.11@nd.edu; ti@nranchen.org; pernkopf@tugraz.at	Knoll, Christian/AAM-3314-2021	Knoll, Christian/0000-0003-3920-1419; Pernkopf, Franz/0000-0002-6356-3367	NSF [DMS 11-15587]; Austrian Science Fund (FWF) [P27803-N15]	NSF(National Science Foundation (NSF)); Austrian Science Fund (FWF)(Austrian Science Fund (FWF))	Research supported in part by NSF under Grant DMS 11-15587. This work was supported by the Austrian Science Fund (FWF) under the project number P27803-N15.	Aji SM, 2000, IEEE T INFORM THEORY, V46, P325, DOI 10.1109/18.825794; Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1; Braunstein A, 2005, RANDOM STRUCT ALGOR, V27, P201, DOI 10.1002/rsa.20057; Chen T., 2017, IEEE T POWER SYSTEMS; Chen T., 2016, ARXIV160305905; Chen TR, 2015, COMMUN INF SYST, V15, P119, DOI 10.4310/CIS.2015.v15.n2.a1; Cox D., 1992, IDEALS VARIETIES ALG, V3; Cox D.A., 2005, GRADUATE TEXTS MATH; Elidan G., 2006, UAI, P165, DOI DOI 10.5555/3020419.3020440; Gallager R. G., 1968, INFORM THEORY RELIAB, V2; Georgii H.-O., 2011, GIBBS MEASURES PHASE, V9; Heskes T, 2004, NEURAL COMPUT, V16, P2379, DOI 10.1162/0899766041941943; Heskes T., 2003, ADV NEURAL INFORM PR, V15, P359; HUBER B, 1995, MATH COMPUT, V64, P1541, DOI 10.2307/2153370; Ihler A., 2007, P 23 C UNC ART INT U, P183; Ihler AT, 2005, J MACH LEARN RES, V6, P905; Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026; Khovanskii A.G., 1978, FUNCT ANAL APPL+, V12, P38, DOI [10.1007/BF01077562, DOI 10.1007/BF01077562]; Knoll C, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Knoll C, 2015, LECT NOTES ARTIF INT, V9285, P295, DOI 10.1007/978-3-319-23525-7_18; Koller D., 2009, PROBABILISTIC GRAPHI; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; Kushnirenko A., 1976, FUNKTSIONAL ANAL PRI, V10, P82; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Li T. Y., 1997, Acta Numerica, V6, P399, DOI 10.1017/S0962492900002749; Li T.Y., 2003, HDB NUMERICAL ANAL, VVolume 11, P209, DOI [10.1016/S1570-8659(02)11004-0, DOI 10.1016/S1570-8659(02)11004-0]; Loeliger HA, 2004, IEEE SIGNAL PROC MAG, V21, P28, DOI 10.1109/MSP.2004.1267047; MacKay D., 2001, TR200118 MERL CAMBR; MacKay DJ, 2003, INFORM THEORY INFERE; MAYR EW, 1982, ADV MATH, V46, P305, DOI 10.1016/0001-8708(82)90048-2; Meshi O., 2009, UAI, P402; Mezard M., 2009, INFORM PHYS COMPUTAT; MOLLER HM, 1984, LECT NOTES COMPUT SC, V174, P172; Mooij JM, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/11/P11012; Mooij JM, 2007, IEEE T INFORM THEORY, V53, P4422, DOI 10.1109/TIT.2007.909166; Mooij JM, 2010, J MACH LEARN RES, V11, P2169; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Pernkopf F., 2014, INTRO PROBABILISTIC; Shin J., 2012, P AISTATS, P1037; Sommese AJ, 2005, NUMERICAL SOLUTION OF SYSTEMS OF POLYNOMIALS: ARISING IN ENGINEERING AND SCIENCE, P1, DOI 10.1142/9789812567727; Srinivasa C, 2016, JMLR WORKSH CONF PRO, V51, P286; Sutton C., 2007, 23 C UNC ART INT UAI, P376; Taga N, 2006, LECT NOTES ARTIF INT, V4293, P197; Tatikonda S. C., 2002, P 18 C UNC ART INT, P493; Teschl G., 2003, ORDINARY DIFFERENTIA; Tianran Chen, 2014, Mathematical Software - ICMS 2014. 4th International Congress. Proceedings. LNCS: 8592, P183, DOI 10.1007/978-3-662-44199-2_30; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; Weller A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P858; Welling M, 2003, ARTIF INTELL, V143, P19, DOI 10.1016/S0004-3702(02)00361-2; Welling M., 2001, P 17 C UNC ART INT, P554; Wymeersch H., 2007, ITERATIVE RECEIVER D, V234; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958	57	5	5	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2018	40	9					2124	2136		10.1109/TPAMI.2017.2749575	http://dx.doi.org/10.1109/TPAMI.2017.2749575			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GP4UX	28885150	Green Submitted			2022-12-18	WOS:000440868400007
J	Roubtsova, N; Guillemaut, JY				Roubtsova, Nadejda; Guillemaut, Jean-Yves			Bayesian Helmholtz Stereopsis with Integrability Prior	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Helmholtz Stereopsis; 3D; complex reflectance; MAP	MULTIVIEW PHOTOMETRIC STEREO; RECONSTRUCTION; SHAPE; GRADIENT; CAPTURE; IMAGE	Helmholtz Stereopsis is a 3D reconstruction method uniquely independent of surface reflectance. Yet, its sub-optimal maximum likelihood formulation with drift-prone normal integration limits performance. Via three contributions this paper presents a complete novel pipeline for Helmholtz Stereopsis. First, we propose a Bayesian formulation replacing the maximum likelihood problem by a maximum a posteriori one. Second, a tailored prior enforcing consistency between depth and normal estimates via a novel metric related to optimal surface integrability is proposed. Third, explicit surface integration is eliminated by taking advantage of the accuracy of prior and high resolution of the coarse-to-fine approach. The pipeline is validated quantitatively and qualitatively against alternative formulations, reaching sub-millimetre accuracy and coping with complex geometry and reflectance.	[Roubtsova, Nadejda; Guillemaut, Jean-Yves] Univ Surrey, CVSSP, Guildford GU2 7XH, Surrey, England; [Roubtsova, Nadejda] Univ Bath, CAMERA, Bath BA2 7AY, Avon, England	University of Surrey; University of Bath	Roubtsova, N (corresponding author), Univ Surrey, CVSSP, Guildford GU2 7XH, Surrey, England.; Roubtsova, N (corresponding author), Univ Bath, CAMERA, Bath BA2 7AY, Avon, England.	n.s.roubtsova@bath.ac.uk; j.guillemaut@surrey.ac.uk	Guillemaut, Jean-Yves/N-7739-2014	Guillemaut, Jean-Yves/0000-0001-8223-5505	EPSRC [EP/M021793/1]; EPSRC [EP/M023281/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/M021793/1] Funding Source: researchfish	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank our industrial partners The Foundry and Double Negative for useful discussions. This work was supported by the EPSRC (grant EP/M021793/1). Our synthetic data is available at https://doi.org/10.15126/surreydata.00841369. Source of 3D models used (pear/bunny): Suggestive Contour model database, http://gfx.cs.princeton.edu/proj/sugcon/models/ and Stanford 3D scanning repository, https://graphics.stanford.edu/data/3Dscanrep/respectively.	Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; AGRAWAL A, 2006, P EUR C COMP VIS, P578; Alldrin NG, 2007, IEEE I CONF COMP VIS, P417; Baumgart B. G., 1974, Geometric modeling for computer vision; Brostow GJ, 2011, IEEE T PATTERN ANAL, V33, P2104, DOI 10.1109/TPAMI.2011.37; Chandraker M, 2013, IEEE T PATTERN ANAL, V35, P2941, DOI 10.1109/TPAMI.2012.217; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163; Guillemaut JY, 2011, INT J COMPUT VISION, V93, P73, DOI 10.1007/s11263-010-0413-z; Guillemaut JY, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P10, DOI 10.1109/TDPVT.2004.1335135; Han Y, 2013, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2013.204; Haque SM, 2014, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2014.292; Helmholtz H., 1925, TREATISE PHYSL OPTIC; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Hernandez C, 2011, IEEE T PATTERN ANAL, V33, P419, DOI 10.1109/TPAMI.2010.181; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; HORN BKP, 1990, INT J COMPUT VISION, V5, P37, DOI 10.1007/BF00056771; Izadi Shahram, 2011, UIST, DOI [10.1145/2047196.2047270, DOI 10.1145/2047196.2047270]; Janko Zsolt, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P55, DOI 10.1007/978-3-642-19309-5_5; Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; Liang C, 2010, IMAGE VISION COMPUT, V28, P579, DOI 10.1016/j.imavis.2009.09.012; Liu J., 2014, VISUAL COMPUT, V31, P1, DOI DOI 10.1002/DMRR.2580; Ma W.-C., 2007, P 18 EUR C REND TECH, V2007, P10; Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226; Okabe T, 2009, IEEE I CONF COMP VIS, P1693, DOI 10.1109/ICCV.2009.5459381; Oxholm G, 2014, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2014.277; Park J, 2013, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2013.148; Roubtsova N, 2017, INT J COMPUT VISION, V124, P18, DOI 10.1007/s11263-016-0951-0; Roubtsova N, 2015, COMM COM INF SC, V550, P223, DOI 10.1007/978-3-319-25117-2_14; Roubtsova N, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P335; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Shi BX, 2016, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2016.403; Shi BX, 2012, LECT NOTES COMPUT SC, V7574, P455, DOI 10.1007/978-3-642-33712-3_33; Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68; Tan P, 2011, IEEE T PATTERN ANAL, V33, P2506, DOI 10.1109/TPAMI.2011.35; Vlasic D., 2009, P SIGGRAPH AS; Vogiatzis G., 2006, P 2006 IEEE COMP SOC, V2, P1847, DOI [10.1109/CVPR.2006.245, DOI 10.1109/CVPR.2006.245]; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Woodham R. J., 1989, SHAPE SHADING, P513; Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186; Zhang Q, 2012, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2012.6247962; Zhou ZL, 2013, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2013.195; Zickler TE, 2003, PROC CVPR IEEE, P548; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513	49	5	5	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2018	40	9					2265	2272		10.1109/TPAMI.2017.2749373	http://dx.doi.org/10.1109/TPAMI.2017.2749373			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GP4UX	28952934	Bronze, Green Published			2022-12-18	WOS:000440868400017
J	Sikka, K; Sharma, G				Sikka, Karan; Sharma, Gaurav			Discriminatively Trained Latent Ordinal Model for Video Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Weakly supervised leaning; facial analysis; human actions; latent variable model; video classification	CONDITIONAL RANDOM-FIELDS; RECOGNITION; REPRESENTATION; LOCALIZATION; HUMANS	We address the problem of video classification for facial analysis and human action recognition. We propose a novel weakly supervised learning method that models the video as a sequence of automatically mined, discriminative sub-events (e.g., onset and offset phase for "smile", running and jumping for "highjump"). The proposed model is inspired by the recent works on Multiple Instance Learning and latent SVM/HCRF - it extends such frameworks to model the ordinal aspect in the videos, approximately. We obtain consistent improvements over relevant competitive baselines on four challenging and publicly available video based facial analysis datasets for prediction of expression, clinical pain and intent in dyadic conversations, and on three challenging human action datasets. We also validate the method with qualitative results and show that they largely support the intuitions behind the method.	[Sikka, Karan] Univ Calif San Diego, San Diego, CA 92103 USA; [Sikka, Karan] SRI Int, Princeton, NJ 08540 USA; [Sharma, Gaurav] Max Planck Inst Informat, Saarbrucken, Germany; [Sharma, Gaurav] Indian Inst Technol Kanpur, Kanpur 208016, Uttar Pradesh, India	University of California System; University of California San Diego; SRI International; Max Planck Society; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kanpur	Sikka, K (corresponding author), Univ Calif San Diego, San Diego, CA 92103 USA.; Sikka, K (corresponding author), SRI Int, Princeton, NJ 08540 USA.	karan.sikka1@gmail.com; grv@cse.iitk.ac.in	Cataldi, Antonio/AAM-7411-2021	IIT Hyderabad, EE Department/0000-0002-5880-4023	Early Career Research Award from SERB India [ECR/2016/001158]; Research-I foundation at IIT Kanpur; NIH [R01 NR013500]; NATIONAL INSTITUTE OF NURSING RESEARCH [R01NR013500] Funding Source: NIH RePORTER	Early Career Research Award from SERB India; Research-I foundation at IIT Kanpur; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NATIONAL INSTITUTE OF NURSING RESEARCH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Nursing Research (NINR))	The authors thank Sanjoy Dasgupta and Harpreet Sawhney for discussions. Gaurav Sharma was supported by the Early Career Research Award from SERB India (ECR/2016/001158) and the Research-I foundation at IIT Kanpur. Karan Sikka was supported by NIH grant R01 NR013500. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Institute of Health.	Alnajar F., 2014, P BRIT MACH VIS C; [Anonymous], 2002, P 15 INT C NEUR INF; [Anonymous], 2009, BMVC, DOI DOI 10.5244/C.23.124; Ballas N., 2015, INT C LEARN REPR; Bhattarai B, 2016, INT CONF ACOUST SPEE, P1901, DOI 10.1109/ICASSP.2016.7472007; Bilen Hakan, 2012, Pattern Recognition. Proceedings Joint 34th DAGM and 36th OAGM Symposium, P134, DOI 10.1007/978-3-642-32717-9_14; Bilen H., 2011, BMVC, P1; Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331; Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316; Carreira J., 2017, P IEEE C COMP VIS PA, P6299; Chang KY, 2009, PROC CVPR IEEE, P533, DOI 10.1109/CVPRW.2009.5206612; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3; Feichtenhofer C., 2016, ARXIV160406573; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Fernando Basura, 2017, P IEEE C COMP VIS PA; Gaidon A, 2014, INT J COMPUT VISION, V107, P219, DOI 10.1007/s11263-013-0677-1; Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65; Gkioxari G, 2015, IEEE I CONF COMP VIS, P2470, DOI 10.1109/ICCV.2015.284; Guo YM, 2012, LECT NOTES COMPUT SC, V7573, P631, DOI 10.1007/978-3-642-33709-3_45; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoai M., 2014, P AS C COMP VIS, P3, DOI DOI 10.1007/978-3-319-16814-21; Hoai M, 2014, PROC CVPR IEEE, P875, DOI 10.1109/CVPR.2014.117; Huang G. B., 2007, UMASS AMHERST AMHERS; Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332; Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31; Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353; Kaltwang S, 2012, LECT NOTES COMPUT SC, V7432, P368, DOI 10.1007/978-3-642-33191-6_36; Kar A., 2017, P IEEE C COMP VIS PA, P3376; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Ke Y, 2004, PROC CVPR IEEE, P506; Klaser A., 2008, SPATIO TEMPORAL DESC, DOI 10.5244/C.22.99; Krizhevsky A., 2012, P INT C NEUR INF PRO; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lee Hsin-Ying, 2017, P IEEE INT C COMP VI; Li WX, 2015, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2015.7299056; Li WX, 2013, IEEE I CONF COMP VIS, P2728, DOI 10.1109/ICCV.2013.339; Li Z., 2016, ARXIV160701794; Lien JJ, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P390, DOI 10.1109/AFGR.1998.670980; Littlewort G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P298, DOI 10.1109/FG.2011.5771414; Littlewort G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P897, DOI 10.1109/FG.2011.5771370; Liu CW, 2016, INT J COMPUT VISION, V118, P240, DOI 10.1007/s11263-016-0897-2; Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226; Lorincz A, 2013, IEEE COMPUT SOC CONF, P889, DOI 10.1109/CVPRW.2013.131; Lovegrove S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.93; Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134; Lucey P., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Lucey P., 2010, P IEEE COMP SOC C CO, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Lucey Patrick, 2008, Int Conf Audit Vis Speech Process, V2008, P167; Ma SG, 2015, PROC CVPR IEEE, P5024, DOI 10.1109/CVPR.2015.7299137; Maji S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3177, DOI 10.1109/CVPR.2011.5995631; Messing R, 2009, IEEE I CONF COMP VIS, P104, DOI 10.1109/ICCV.2009.5459154; Nguyen MH, 2009, IEEE I CONF COMP VIS, P1925, DOI 10.1109/ICCV.2009.5459426; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124; Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Rudovic O, 2012, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2012.6247983; Ruiz A., 2014, P BRIT MACH VIS C; Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39; Scovanner P., 2007, ACM MM, P357; Sharma Gaurav, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P65, DOI 10.1109/CVPRW.2015.7301321; Sharma G, 2017, IEEE T PATTERN ANAL, V39, P87, DOI 10.1109/TPAMI.2016.2537325; Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093; Sharma S., 2015, NEURAL INFORM PROCES; Sharpe GJ, 2011, COMBUST THEOR MODEL, V15, P691, DOI 10.1080/13647830.2011.558594; Sheerman-Chase Tim, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1985, DOI 10.1109/ICCVW.2009.5457525; Sikka Karan, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P18, DOI 10.1109/CVPRW.2015.7301350; Sikka K, 2016, PROC CVPR IEEE, P5580, DOI 10.1109/CVPR.2016.602; Sikka K, 2014, IMAGE VISION COMPUT, V32, P659, DOI 10.1016/j.imavis.2014.02.008; Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25; Simon T, 2010, PROC CVPR IEEE, P2737, DOI 10.1109/CVPR.2010.5539998; Simonyan K, 2015, 3 INT C LEARN REPR I; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Sun J, 2010, IEEE INT CON MULTI, P322, DOI 10.1109/ICME.2010.5583046; Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808; Tian YL, 2005, HANDBOOK OF FACE RECOGNITION, P247, DOI 10.1007/0-387-27257-7_12; Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang LM, 2013, IEEE I CONF COMP VIS, P2680, DOI 10.1109/ICCV.2013.333; Wang Xingxing, 2012, ASIAN C COMPUTER VIS, P572; Wang Y, 2009, PROC CVPR IEEE, P872, DOI 10.1109/CVPRW.2009.5206709; Wu CC, 2015, IEEE GLOB COMM CONF, DOI [10.1109/GLOCOM.2015.7417352, 10.1109/FG.2015.7163116]; Wu JX, 2014, PROC CVPR IEEE, P2577, DOI 10.1109/CVPR.2014.330; Wu T., 2010, 2010 IEEE COMP SOC C, P42, DOI [DOI 10.1109/CVPRW.2010.5543267, 10.1109/CVPRW.2010.5543267]; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Zhang C, 2006, ADV NEURAL INFORM PR, P1417; Zhang YM, 2005, IEEE T PATTERN ANAL, V27, P699, DOI 10.1109/TPAMI.2005.93; Zhang ZY, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P454, DOI 10.1109/AFGR.1998.670990; Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110; Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002; Zhu J, 2013, IEEE I CONF COMP VIS, P3559, DOI 10.1109/ICCV.2013.442; Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219	114	5	5	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2018	40	8					1829	1844		10.1109/TPAMI.2017.2741482	http://dx.doi.org/10.1109/TPAMI.2017.2741482			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GL6DT	28841549	Green Submitted			2022-12-18	WOS:000437271100004
J	Glowacki, P; Pinheiro, MA; Mosinska, A; Turetken, E; Lebrecht, D; Sznitman, R; Holtmaat, A; Kybic, J; Fua, P				Glowacki, Przemyslaw; Pinheiro, Miguel Amavel; Mosinska, Agata; Tueretken, Engin; Lebrecht, Daniel; Sznitman, Raphael; Holtmaat, Anthony; Kybic, Jan; Fua, Pascal			Reconstructing Evolving Tree Structures in Time Lapse Sequences by Enforcing Time-Consistency	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Curvilinear networks; tubular structures; curvilinear structures; automated reconstruction; temporal consistency; integer programming		We propose a novel approach to reconstructing curvilinear tree structures evolving over time, such as road networks in 20 aerial images or neural structures in 3D microscopy stacks acquired in vivo. To enforce temporal consistency, we simultaneously process all images in a sequence, as opposed to reconstructing structures of interest in each image independently. We formulate the problem as a Quadratic Mixed Integer Program and demonstrate the additional robustness that comes from using all available visual clues at once, instead of working frame by frame. Furthermore, when the linear structures undergo local changes over time, our approach automatically detects them.	[Glowacki, Przemyslaw; Mosinska, Agata; Fua, Pascal] Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland; [Pinheiro, Miguel Amavel; Kybic, Jan] Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Prague 16636, Czech Republic; [Tueretken, Engin] CSEM, CH-2002 Neuchatel, Switzerland; [Lebrecht, Daniel; Holtmaat, Anthony] Univ Geneva, Dept Basic Neurosci, CH-1205 Geneva, Switzerland; [Sznitman, Raphael] Univ Bern, ARTORG Ctr Biomed Engn Res, CH-3012 Bern, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Czech Technical University Prague; Swiss Center for Electronics & Microtechnology (CSEM); University of Geneva; University of Bern	Glowacki, P (corresponding author), Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland.	przemyslaw.glowacki@epfl.ch; amavemig@cmp.felk.cvut.cz; agata.mosinska@epfl.ch; engin.tueretken@alumni.epfl.ch; daniel.lebrecht@unige.ch; raphael.sznitman@artorg.unibe.ch; anthony.holtmaat@unige.ch; kybic@fel.cvut.cz; pascal.fua@epfl.ch	Jan, Kybic/K-8071-2017	Holtmaat, Anthony/0000-0002-7577-0769	Fundacao para a Ciencia e Tecnologia [SFRH/BD/77134/2011]; Czech Science Foundation [P202/11/0111]; Grant Agency of the Czech Technical University in Prague [SGS12/190/OHK3/3T/13]; Swiss National Science Foundation	Fundacao para a Ciencia e Tecnologia(Portuguese Foundation for Science and TechnologyEuropean Commission); Czech Science Foundation(Grant Agency of the Czech Republic); Grant Agency of the Czech Technical University in Prague; Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This work was supported in part by the Swiss National Science Foundation, by the Grant Agency of the Czech Technical University in Prague, grant No. SGS12/190/OHK3/3T/13, the Czech Science Foundation project P202/11/0111 and the Fundacao para a Ciencia e Tecnologia grant SFRH/BD/77134/2011.	Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156; Ascoli G., 2010, DIGITAL RECONSTRUCTI; Duhamel C, 2008, NETWORKS, V51, P34, DOI 10.1002/net.20194; Fischler M. A., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P351; Glowacki P, 2014, PROC CVPR IEEE, P3035, DOI 10.1109/CVPR.2014.388; Holtmaat A, 2013, EUR J PHARMACOL, V719, P128, DOI 10.1016/j.ejphar.2013.07.020; Law MWK, 2008, LECT NOTES COMPUT SC, V5305, P368, DOI 10.1007/978-3-540-88693-8_27; Lepetit V., 2005, MONOCULAR MODEL BASE; Li Q, 2011, IEEE T MED IMAGING, V30, P632, DOI 10.1109/TMI.2010.2090354; Li ZW, 2011, INVEST OPHTH VIS SCI, V52, P7205, DOI 10.1167/iovs.10-6868; NEVATIA R, 1980, COMPUT VISION GRAPH, V13, P257, DOI 10.1016/0146-664X(80)90049-0; Ramanan D, 2005, PROC CVPR IEEE, P271; Rasmussen CE., 2006, GAUSSIAN PROCESS MAC; Serradell E, 2012, PROC CVPR IEEE, P996, DOI 10.1109/CVPR.2012.6247776; Sironi A, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2462363; Turetken E, 2016, IEEE T PATTERN ANAL, V38, P2515, DOI 10.1109/TPAMI.2016.2519025; Yu X, 2007, LECT NOTES ENG COMP, P265	17	5	5	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2018	40	3					755	761		10.1109/TPAMI.2017.2680444	http://dx.doi.org/10.1109/TPAMI.2017.2680444			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FV3KC	28333621	Green Submitted			2022-12-18	WOS:000424465900018
J	Ren, Y; Wang, YN; Zhu, J				Ren, Yong; Wang, Yining; Zhu, Jun			Spectral Learning for Supervised Topic Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Spectral methods; supervised topic models; methods of moments	DECOMPOSITIONS	Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on variational approximation or Monte Carlo sampling, which often suffers from the local minimum defect. Spectral methods have been applied to learn unsupervised topic models, such as latent Dirichlet allocation (LDA), with provable guarantees. This paper investigates the possibility of applying spectral methods to recover the parameters of supervised LDA (sLDA). We first present a two-stage spectral method, which recovers the parameters of LDA followed by a power update method to recover the regression model parameters. Then, we further present a single-phase spectral algorithm to jointly recover the topic distribution matrix as well as the regression weights. Our spectral algorithms are provably correct and computationally efficient. We prove a sample complexity bound for each algorithm and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the spectral algorithms. In fact, our results on a large-scale review rating dataset demonstrate that our single-phase spectral algorithm alone gets comparable or even better performance than state-of-the-art methods, while previous work on spectral methods has rarely reported such promising performance.	[Ren, Yong; Zhu, Jun] Tsinghua Univ, CBICR Ctr, State Key Lab Intelligent Technol & Syst, Dept Comp Sci & Technol,TNList Lab, Beijing 100084, Peoples R China; [Wang, Yining] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Tsinghua University; Carnegie Mellon University	Ren, Y (corresponding author), Tsinghua Univ, CBICR Ctr, State Key Lab Intelligent Technol & Syst, Dept Comp Sci & Technol,TNList Lab, Beijing 100084, Peoples R China.	reny11@mails.tsinghua.edu.cn; yiningwa@andrew.cmu.edu; dcszj@tsinghua.edu.cn		Wang, Yining/0000-0001-9410-0392	Youth Top-notch Talent Support Program; German Research Foundation (DFG) [NSFC 61621136008 / DFC TRR-169]; NSFC [NSFC 61621136008 / DFC TRR-169]; National NSF of China [61620106010, 61322308, 61332007]; National 973 Basic Research Program of China [2013CB329403]	Youth Top-notch Talent Support Program; German Research Foundation (DFG)(German Research Foundation (DFG)); NSFC(National Natural Science Foundation of China (NSFC)); National NSF of China(National Natural Science Foundation of China (NSFC)); National 973 Basic Research Program of China(National Basic Research Program of China)	This work is supported by the National 973 Basic Research Program of China (No. 2013CB329403), National NSF of China (Nos. 61620106010, 61322308, 61332007), the NSFC and German Research Foundation (DFG) in Project Crsoss-modal Learning (No. NSFC 61621136008 / DFC TRR-169), and the Youth Top-notch Talent Support Program. Yong Ren and Yining Wang contributed equally to this work. Jun Zhu is the corresponding author.	Anandkumar A., 2014, ARXIV14111488; Anandkumar A., 2012, P C LEARN THEOR; Anandkumar Anima, 2012, NIPS; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Blei D.M., 2007, P 20 INT C NEURAL IN, P121; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Collins M., 2012, ADV NEURAL INFORM PR, P2519; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Gittens A., 2013, INT C MACHINE LEARNI, P567; Griffiths T., 2007, HDB LATENT SEMANT AN; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Huang Furong, 2013, ARXIV13090787; KRUSKAL JB, 1977, LINEAR ALGEBRA APPL, V18, P95, DOI 10.1016/0024-3795(77)90069-6; Lacoste-Julien S., 2008, P 21 INT C NEURAL IN, P897; LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071; McAuley Julian John, 2013, P 22 INT C WORLD WID, P897, DOI DOI 10.1145/2488388.2488466; Moitra A., 2014, LECT NOTES; Nguyen T., 2015, IS YOUR ANCHOR GOING; Porteous I., 2008, P 14 ACM SIGKDD INT, P569; Recht B., 2012, ADV NEURAL INFORM PR, V25, P1214; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931; Wang C., 2009, PROCEEDINGS OF THE I, P823; Wang Y, 2014, P 27 INT C NEUR INF, P1511; Weyl H, 1912, MATH ANN, V71, P441, DOI 10.1007/BF01456804; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260; Zhu J., 2011, P 27 C ANN C UNC ART, P831; Zhu J, 2014, J MACH LEARN RES, V15, P1799; Zhu J, 2014, J MACH LEARN RES, V15, P1073; Zhu J, 2012, J MACH LEARN RES, V13, P2237	34	5	7	0	21	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2018	40	3					726	739		10.1109/TPAMI.2017.2682085	http://dx.doi.org/10.1109/TPAMI.2017.2682085			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FV3KC	28320652	Green Submitted			2022-12-18	WOS:000424465900016
J	Prusa, D; Werner, T				Prusa, Daniel; Werner, Tomas			LP Relaxation of the Potts Labeling Problem Is as Hard as Any Linear Program	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Markov random field; graphical model; MAP inference; discrete energy minimization; valued constraint satisfaction; linear programming relaxation; uniform metric labeling problem; Potts model	APPROXIMATION ALGORITHMS; ENERGY MINIMIZATION	In our recent work, we showed that solving the LP relaxation of the pairwise min-sum labeling problem (also known as MAP inference in graphical models or discrete energy minimization) is not much easier than solving any linear program. Precisely, the general linear program reduces in linear time (assuming the Turing model of computation) to the LP relaxation of the min-sum labeling problem. The reduction is possible, though in quadratic time, even to the min-sum labeling problem with planar structure. Here we prove similar results for the pairwise min-sum labeling problem with attractive Potts interactions (also known as the uniform metric labeling problem).	[Prusa, Daniel; Werner, Tomas] Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Karlovo Namesti 13, Prague 12135, Czech Republic	Czech Technical University Prague	Prusa, D (corresponding author), Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Karlovo Namesti 13, Prague 12135, Czech Republic.	prusapa1@fel.cvut.cz; werner@fel.cvut.cz	Prusa, Daniel/G-8551-2014; Prusa, Daniel/G-4916-2018	Prusa, Daniel/0000-0003-4866-5709	Czech ministry of education, youth and sports under the ERC-CZ grant [LL1303]	Czech ministry of education, youth and sports under the ERC-CZ grant	This work has been supported by the Czech ministry of education, youth and sports under the ERC-CZ grant LL1303.	Billera LJ, 1996, COMBINATORICA, V16, P175, DOI 10.1007/BF01844844; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Calinescu G., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P48, DOI 10.1145/276698.276711; Chekuri C, 2005, SIAM J DISCRETE MATH, V18, P608, DOI 10.1137/S0895480101396937; Chuzhoy J, 2007, SIAM J COMPUT, V36, P1376, DOI 10.1137/06065430X; De Loera JA, 2006, SIAM J OPTIMIZ, V17, P806, DOI 10.1137/040610623; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268; Kolmogorov V, 2015, SIAM J COMPUT, V44, P1, DOI 10.1137/130945648; Koster AMCA, 1998, OPER RES LETT, V23, P89, DOI 10.1016/S0167-6377(98)00043-1; Kumar P., 2014, P ANN C NEUR INF PRO, P109; Prusa D, 2015, IEEE T PATTERN ANAL, V37, P898, DOI 10.1109/TPAMI.2014.2353626; Rother C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383203; Shlezinger M. I., 1976, Cybernetics, V12, P612, DOI 10.1007/BF01070399; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Zivny S., 2012, COMPLEXITY VALUED CO	19	5	5	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2017	39	7					1469	1475		10.1109/TPAMI.2016.2582165	http://dx.doi.org/10.1109/TPAMI.2016.2582165			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EW8BZ	27333598				2022-12-18	WOS:000402744400016
J	Takigawa, I; Mamitsuka, H				Takigawa, Ichigaku; Mamitsuka, Hiroshi			Generalized Sparse Learning of Linear Models Over the Complete Subgraph Feature Set	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Supervised learning for graphs; graph mining; sparsity-inducing regularization; block coordinate gradient descent; simultaneous feature learning	TREE PATTERNS; GRAPH; CLASSIFICATION; RETRIEVAL; KERNELS	Supervised learning over graphs is an intrinsically difficult problem: simultaneous learning of relevant features from the complete subgraph feature set, in which enumerating all subgraph features occurring in given graphs is practically intractable due to combinatorial explosion. We show that 1) existing graph supervised learning studies, such as Adaboost, LPBoost, and LARS/LASSO, can be viewed as variations of a branch-and-bound algorithm with simple bounds, which we call Morishita-Kudo bounds; 2) We present a direct sparse optimization algorithm for generalized problems with arbitrary twice-differentiable loss functions, to which Morishita-Kudo bounds cannot be directly applied; 3) We experimentally showed that i) our direct optimization method improves the convergence rate and stability, and ii) L1-penalized logistic regression (L1-LogReg) by our method identifies a smaller subgraph set, keeping the competitive performance, iii) the learned subgraphs by L1-LogReg are more size-balanced than competing methods, which are biased to small-sized subgraphs.	[Takigawa, Ichigaku] Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido 0600808, Japan; [Takigawa, Ichigaku] Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan; [Mamitsuka, Hiroshi] Kyoto Univ, Inst Chem Res, Kyoto, Japan; [Mamitsuka, Hiroshi] Aalto Univ, Dept Comp Sci, Espoo, Finland	Hokkaido University; Japan Science & Technology Agency (JST); Kyoto University; Aalto University	Takigawa, I (corresponding author), Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido 0600808, Japan.; Takigawa, I (corresponding author), Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan.	takigawa@ist.hokudai.ac.jp; mami@kuicr.kyoto-u.ac.jp	Takigawa, Ichigaku/B-3823-2012; Mamitsuka, Hiroshi/R-1110-2016	Takigawa, Ichigaku/0000-0001-5633-995X; Mamitsuka, Hiroshi/0000-0002-6607-5617	JSPS/MEXT KAKENHI [26120503, 26330242, 24300054, 16H02868]; Collaborative Research Program of Institute for Chemical Research, Kyoto University [2014-27, 2015-33]; JST PRESTO; FiDi-Pro, Tekes; Grants-in-Aid for Scientific Research [26330242] Funding Source: KAKEN	JSPS/MEXT KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Collaborative Research Program of Institute for Chemical Research, Kyoto University; JST PRESTO(Japan Science & Technology Agency (JST)); FiDi-Pro, Tekes; Grants-in-Aid for Scientific Research(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported in part by JSPS/MEXT KAKENHI Grant Number 26120503, 26330242, 24300054, 16H02868; the Collaborative Research Program of Institute for Chemical Research, Kyoto University (grant #2014-27 and #2015-33); JST PRESTO; and FiDi-Pro, Tekes.	Agrawal R., 1994, P VLDB, P487; Avis D, 1996, DISCRETE APPL MATH, V65, P21, DOI 10.1016/0166-218X(95)00026-N; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Barra V, 2013, PATTERN RECOGN, V46, P2985, DOI 10.1016/j.patcog.2013.03.019; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Deshpande M, 2005, IEEE T KNOWL DATA EN, V17, P1036, DOI 10.1109/TKDE.2005.127; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11; Harchaoui Z., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383049; Hashimoto K, 2008, BIOINFORMATICS, V24, pI167, DOI 10.1093/bioinformatics/btn293; Helma C, 2004, J CHEM INF COMP SCI, V44, P1402, DOI 10.1021/ci034254q; Inokuchi A., 2003, INT C MACHINE LEARNI, P321; Karklin Y, 2005, PACIFIC SYMPOSIUM ON BIOCOMPUTING 2005, P4; Kondor R., 2008, P 25 INT C MACH LEAR, P496, DOI DOI 10.1145/1390156.1390219; Kong XN, 2012, KNOWL INF SYST, V31, P281, DOI 10.1007/s10115-011-0407-3; Kudo T., 2005, ADV NEURAL INFORM PR, P729; Lu Bai, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P99, DOI 10.1007/978-3-662-44848-9_7; Mahe P, 2009, MACH LEARN, V75, P3, DOI 10.1007/s10994-008-5086-2; Morishita S., 2002, Progress discovery science. Final report of the Japanese discovery science project (Lecture Notes in Artificial Intelligence Vol.2281), P471; Nijssen S., 2004, P 10 ACM SIGKDD INT, P647, DOI [DOI 10.1145/1014052.1014134, 10.1145/1014052.1014134]; Nowozin S., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383171; Nowozin S., 2009, THESIS; Rogers D, 2010, J CHEM INF MODEL, V50, P742, DOI 10.1021/ci100050t; Saigo H, 2008, P 14 ACM SIGKDD INT, P578, DOI DOI 10.1145/1401890.1401961; Saigo H, 2008, IEEE DATA MINING, P1007, DOI 10.1109/ICDM.2008.62; Saigo H, 2009, MACH LEARN, V75, P69, DOI 10.1007/s10994-008-5089-z; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Takigawa I, 2013, DRUG DISCOV TODAY, V18, P50, DOI 10.1016/j.drudis.2012.07.016; Takigawa I, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0016999; Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0; Tsuda K., 2006, P 23 INT C MACH LEAR, P953; Tsuda K., 2007, P 24 INT C MACH LEAR, V2007, P919; Tsuda K., 2008, SIAM C DAT MIN, P432; Vert JP, 2006, LECT NOTES ARTIF INT, V4201, P7; Vert JP, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-S10-S8; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Wale N, 2008, KNOWL INF SYST, V14, P347, DOI 10.1007/s10115-007-0103-5; Warmuth M. K., 2008, P ADV NEUR INF PROC, V20, P1585; Warmuth MK, 2008, LECT NOTES ARTIF INT, V5254, P256, DOI 10.1007/978-3-540-87987-9_23; Yamanishi Y, 2007, BIOINFORMATICS, V23, P1211, DOI 10.1093/bioinformatics/btm090; Yan XF, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P721, DOI 10.1109/ICDM.2002.1184038; Yun S, 2011, COMPUT OPTIM APPL, V48, P273, DOI 10.1007/s10589-009-9251-8	42	5	5	1	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2017	39	3					617	624		10.1109/TPAMI.2016.2567399	http://dx.doi.org/10.1109/TPAMI.2016.2567399			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EM8IP	27187949	Green Published			2022-12-18	WOS:000395555100015
J	Demi, M				Demi, Marcello			Contour Tracking with a Spatio-Temporal Intensity Moment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Contour tracking; edge detection; intensity moments; optical flow	OPTICAL-FLOW ESTIMATION; REAL-TIME MEASUREMENT; NONRIGID REGISTRATION; GRADIENT; ULTRASOUND; SEGMENTATION; SYSTEM; REGULARIZATION; SEQUENCES; FILTERS	Standard edge detection operators such as the Laplacian of Gaussian and the gradient of Gaussian can be used to track contours in image sequences. When using edge operators, a contour, which is determined on a frame of the sequence, is simply used as a starting contour to locate the nearest contour on the subsequent frame. However, the strategy used to look for the nearest edge points may not work when tracking contours of non isolated gray level discontinuities. In these cases, strategies derived from the optical flow equation, which look for similar gray level distributions, appear to be more appropriate since these can work with a lower frame rate than that needed for strategies based on pure edge detection operators. However, an optical flow strategy tends to propagate the localization errors through the sequence and an additional edge detection procedure is essential to compensate for such a drawback. In this paper a spatio-temporal intensity moment is proposed which integrates the two basic functions of edge detection and tracking.	[Demi, Marcello] Fdn Toscana Gabriele Monasterio, Pisa, Italy		Demi, M (corresponding author), Fdn Toscana Gabriele Monasterio, Pisa, Italy.	demi@ifc.cnr.it						AMINI AA, 1991, LECT NOTES COMPUT SC, V511, P343, DOI 10.1007/BFb0033764; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; Bosch JG, 2002, IEEE T MED IMAGING, V21, P1374, DOI 10.1109/TMI.2002.806427; Boukerroui D, 2003, LECT NOTES COMPUT SC, V2732, P586; Brandt JW, 1997, INT J COMPUT VISION, V25, P5, DOI 10.1023/A:1007987001439; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chalana V, 1996, IEEE T MED IMAGING, V15, P290, DOI 10.1109/42.500138; Chao Lu, 2012, 2012 IEEE Workshop on Mathematical Methods in Biomedical Image Analysis (MMBIA), P129, DOI 10.1109/MMBIA.2012.6164742; Christmas WJ, 2000, IEEE T IMAGE PROCESS, V9, P1817, DOI 10.1109/83.869192; Ciompi F, 2009, LECT NOTES COMPUT SC, V5762, P869, DOI 10.1007/978-3-642-04271-3_105; Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416; DeCarlo D, 2000, INT J COMPUT VISION, V38, P99, DOI 10.1023/A:1008122917811; DEMI M, 1994, COMPUT BIOMED RES, V27, P157, DOI 10.1006/cbmr.1994.1015; Demi M, 1996, COMPUT VIS IMAGE UND, V63, P118, DOI 10.1006/cviu.1996.0008; Demi M, 2000, COMPUT VIS IMAGE UND, V80, P57, DOI 10.1006/cviu.2000.0861; Demi M, 2005, COMPUT VIS IMAGE UND, V97, P180, DOI 10.1016/j.cviu.2004.07.003; Demi M, 2008, LECT NOTES COMPUT SC, V5197, P585, DOI 10.1007/978-3-540-85920-8_71; Elad M, 2005, J MATH IMAGING VIS, V23, P345, DOI 10.1007/s10851-005-2027-6; Faita F, 2008, J ULTRAS MED, V27, P1353, DOI 10.7863/jum.2008.27.9.1353; Farid H, 2004, IEEE T IMAGE PROCESS, V13, P496, DOI 10.1109/TIP.2004.823819; FLEET DJ, 1995, IEEE T PATTERN ANAL, V17, P61, DOI 10.1109/34.368151; Fleet DJ., 2005, HDB MATH MODELS COMP; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Gemignani V, 2004, REAL-TIME IMAGING, V10, P103, DOI 10.1016/j.rti.2004.02.005; Gemignani V, 2007, IEEE T MED IMAGING, V26, P393, DOI 10.1109/TMI.2006.891477; Hildreth E., 1984, 761 AI MIT; Hoch M, 1996, VISUAL COMPUT, V12, P75; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Huber P., 1981, ROBUST STAT; Jacob G, 2002, IEEE T MED IMAGING, V21, P226, DOI 10.1109/42.996341; Jahne A., 1997, DIGITAL IMAGE PROCES; Jain A. K., 1989, FUNDAMENTALS DIGITAL; Jodoin PM, 2009, COMPUT VIS IMAGE UND, V113, P511, DOI 10.1016/j.cviu.2008.12.005; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Lai SH, 1998, INT J COMPUT VISION, V29, P87, DOI 10.1023/A:1008005509994; Ledesma-Carbayo MJ, 2005, IEEE T MED IMAGING, V24, P1113, DOI 10.1109/TMI.2005.852050; LEYMARIE F, 1993, IEEE T PATTERN ANAL, V15, P617, DOI 10.1109/34.216733; Lim S, 2005, IEEE T IMAGE PROCESS, V14, P1074, DOI 10.1109/TIP.2005.851688; Liu C., 2008, 2008 IEEE C COMP VIS, P1; Liu C., 2009, THESIS MIT CAMBRIDGE; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; Mikic I, 1998, IEEE T MED IMAGING, V17, P274, DOI 10.1109/42.700739; Noble JA, 2006, IEEE T MED IMAGING, V25, P987, DOI 10.1109/TMI.2006.877092; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; POGGIO T, 1985, 833 AI MIT; Pohl KM, 2005, LECT NOTES COMPUT SC, V3749, P310; Pratt W, 1991, DIGITAL IMAGE PROCES; PRESS WH, 1991, NUMERICAL RECIPES; Rossi AC, 2010, ULTRASOUND MED BIOL, V36, P467, DOI 10.1016/j.ultrasmedbio.2009.12.007; Sand P., 2006, CVPR, P2195; Schoenemann T, 2008, P IEEE C COMP VIS PA, P1; Steinbrucker F, 2009, IEEE I CONF COMP VIS, P1609, DOI 10.1109/ICCV.2009.5459364; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P413, DOI 10.1109/TPAMI.1986.4767807; Thompson WB, 1998, INT J COMPUT VISION, V30, P163, DOI 10.1023/A:1008026031844; TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769; Unal G, 2005, PROC CVPR IEEE, P168; Xu Chenyang, 2000, HDB MED IMAGING, V2, P129; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Yezzi A, 2003, MED IMAGE ANAL, V7, P171, DOI 10.1016/S1361-8415(03)00004-5; YOO TS, 1996, THESIS U N CAROLINA	70	5	5	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2016	38	6					1141	1154		10.1109/TPAMI.2015.2478438	http://dx.doi.org/10.1109/TPAMI.2015.2478438			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DL4LU	26390447				2022-12-18	WOS:000375609000008
J	Goulermas, JY; Kostopoulos, A; Mu, TT				Goulermas, John Yannis; Kostopoulos, Alexandros; Mu, Tingting			A New Measure for Analyzing and Fusing Sequences of Objects	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Seriation; sequencing; consensus/ensemble seriation; combinatorial data analysis; positional proximity coefficient; quadratic assignment problem	GENE-EXPRESSION PROFILES; QUADRATIC ASSIGNMENT; SPECTRAL ALGORITHM; ENVELOPE REDUCTION; CONSENSUS; DECOMPOSITION; VISUALIZATION; MODELS	This work is related to the combinatorial data analysis problem of seriation used for data visualization and exploratory analysis. Seriation re-sequences the data, so that more similar samples or objects appear closer together, whereas dissimilar ones are further apart. Despite the large number of current algorithms to realize such re-sequencing, there has not been a systematic way for analyzing the resulting sequences, comparing them, or fusing them to obtain a single unifying one. We propose a new positional proximity measure that evaluates the similarity of two arbitrary sequences based on their agreement on pairwise positional information of the sequenced objects. Furthermore, we present various statistical properties of this measure as well as its normalized version modeled as an instance of the generalized correlation coefficient. Based on this measure, we define a new procedure for consensus seriation that fuses multiple arbitrary sequences based on a quadratic assignment problem formulation and an efficient way of approximating its solution. We also derive theoretical links with other permutation distance functions and present their associated combinatorial optimization forms for consensus tasks. The utility of the proposed contributions is demonstrated through the comparison and fusion of multiple seriation algorithms we have implemented, using many real-world datasets from different application domains.	[Goulermas, John Yannis] Univ Liverpool, Dept Comp Sci, Ashton Bldg, Liverpool L69 3BX, Merseyside, England; [Kostopoulos, Alexandros; Mu, Tingting] Univ Liverpool, Dept Elect Engn & Elect, Brownlow Hill, Liverpool L69 3GJ, Merseyside, England	University of Liverpool; University of Liverpool	Goulermas, JY (corresponding author), Univ Liverpool, Dept Comp Sci, Ashton Bldg, Liverpool L69 3BX, Merseyside, England.; Kostopoulos, A; Mu, TT (corresponding author), Univ Liverpool, Dept Elect Engn & Elect, Brownlow Hill, Liverpool L69 3GJ, Merseyside, England.	j.y.goulermas@liverpool.ac.uk; a.kost@liverpool.ac.uk; t.mu@liverpool.ac.uk	Mu, Tingting/AAV-4795-2020	Mu, Tingting/0000-0001-6315-3432				ABBOTT A, 1995, ANNU REV SOCIOL, V21, P93, DOI 10.1146/annurev.so.21.080195.000521; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Anstreicher KM, 2003, MATH PROGRAM, V97, P27, DOI 10.1007/s10107-003-0437-z; Atkins JE, 1998, SIAM J COMPUT, V28, P297, DOI 10.1137/S0097539795285771; Bar-Joseph Z, 2001, BIOINFORMATICS S1, V17, pS22, DOI [10.1093/bioinformatics/17.suppl_1.S22, DOI 10.1093/BIOINFORMATICS/17.SUPPL_1.S22]; Barnard S. T., 1993, Proceedings SUPERCOMPUTING '93, P493, DOI 10.1145/169627.169790; BARNARD ST, 1995, NUMER LINEAR ALGEBR, V2, P317, DOI 10.1002/nla.1680020402; Brusco M.J., 2005, BRANCH BOUND APPL CO, V2; Burkard R., 2009, ASSIGNMENT PROBLEMS; Burkard Rainer E, 1999, HDB COMBINATORIAL OP, P75, DOI DOI 10.1007/978-1-4757-3023-4_2.; Caraux G, 2005, BIOINFORMATICS, V21, P1280, DOI 10.1093/bioinformatics/bti141; Cela E., 1997, COMBINATORIAL OPTIMI; Chen CH, 2002, STAT SINICA, V12, P7; Climer S, 2006, J MACH LEARN RES, V7, P919; Cook WD, 2006, EUR J OPER RES, V172, P369, DOI 10.1016/j.ejor.2005.03.048; Cox T., 2000, MULTIDIMENSIONAL SCA; Critchlow Douglas E., 1985, LECT NOTES STAT, V34; Daniels HE, 1944, BIOMETRIKA, V33, P129, DOI 10.2307/2334112; Ding C, 2004, P 21 INT C MACHINE L, P30, DOI [10.1145/1015330.1015407, DOI 10.1145/1015330.1015407]; Enright A. J., 2012, CORR; Friendly M, 2002, AM STAT, V56, P316, DOI 10.1198/000313002533; George A, 1997, SIAM J MATRIX ANAL A, V18, P706, DOI 10.1137/S089547989427470X; Hahsler M, 2008, J STAT SOFTW, V25, P1; Havens TC, 2012, IEEE T KNOWL DATA EN, V24, P813, DOI 10.1109/TKDE.2011.33; Horn R.A., 2013, MATRIX ANAL, P321; Hornik K, 2007, STUD CLASS DATA ANAL, P163, DOI 10.1007/978-3-540-70981-7_19; HUBERT L, 1976, BRIT J MATH STAT PSY, V29, P190, DOI 10.1111/j.2044-8317.1976.tb00714.x; HUBERT L. J., 2001, COMBINATORIAL DATA A; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Liiv Innar, 2010, Statistical Analysis and Data Mining, V3, P70, DOI 10.1002/sam.10071; Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032; MARCOTORCHINO F, 1982, REV STAT APPL, V30, P21; Marcotorchino F., 1991, APPL STOCH MODEL BUS, V7, P139, DOI DOI 10.1002/ASM.3150070204; Mavroeidis D, 2010, KNOWL INF SYST, V23, P243, DOI 10.1007/s10115-009-0215-1; MCCORMICK WT, 1972, OPER RES, V20, P993, DOI 10.1287/opre.20.5.993; Meila M., 2007, P 23 C UNC ART INT U, P285; Mu TT, 2012, IEEE T PATTERN ANAL, V34, P2216, DOI 10.1109/TPAMI.2012.20; Notterman DA, 2001, CANCER RES, V61, P3124; Petrie WMF, 1899, J ANTHR I GREAT BRIT, ppp295, DOI DOI 10.2307/2843012; Plya G, 1952, CAMBRIDGE MATH LIB; Slavov N, 2011, MOL BIOL CELL, V22, P1997, DOI 10.1091/mbc.E11-02-0132; Stewart G.W., 2001, J SOC IND APPL MATH; Tien YJ, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-155; Topchy A, 2005, IEEE T PATTERN ANAL, V27, P1866, DOI 10.1109/TPAMI.2005.237; Tsafrir D, 2005, BIOINFORMATICS, V21, P2301, DOI 10.1093/bioinformatics/bti329; Vega-Pons S, 2011, INT J PATTERN RECOGN, V25, P337, DOI 10.1142/S0218001411008683; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wakabayashi Y, 1998, RESENHAS IME USP, V3, P323; Webb A.R., 2003, STAT PATTERN RECOGNI; Wu HM, 2010, COMPUT STAT DATA AN, V54, P767, DOI 10.1016/j.csda.2008.09.029	52	5	5	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2016	38	5					833	848		10.1109/TPAMI.2015.2470671	http://dx.doi.org/10.1109/TPAMI.2015.2470671			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DJ4GZ	26353365	Green Submitted			2022-12-18	WOS:000374164700001
J	Saremi, S; Sejnowski, TJ				Saremi, Saeed; Sejnowski, Terrence J.			Correlated Percolation, Fractal Structures, and Scale-Invariant Distribution of Clusters in Natural Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Natural image statistics; scale invariance; percolation theory; fractal structures; image segmentation	STATISTICS	Natural images are scale invariant with structures at all length scales. We formulated a geometric view of scale invariance in natural images using percolation theory, which describes the behavior of connected clusters on graphs. We map images to the percolation model by defining clusters on a binary representation for images. We show that critical percolating structures emerge in natural images and study their scaling properties by identifying fractal dimensions and exponents for the scale-invariant distributions of clusters. This formulation leads to a method for identifying clusters in images from underlying structures as a starting point for image segmentation.	[Saremi, Saeed; Sejnowski, Terrence J.] Salk Inst Biol Studies, Howard Hughes Med Inst, 10010 North Torrey Pines Rd, La Jolla, CA 92037 USA	Howard Hughes Medical Institute; Salk Institute	Saremi, S; Sejnowski, TJ (corresponding author), Salk Inst Biol Studies, Howard Hughes Med Inst, 10010 North Torrey Pines Rd, La Jolla, CA 92037 USA.	saeed@salk.edu; terry@salk.edu	Sejnowski, Terrence/AAV-5558-2021		Howard Hughes Medical Institute Funding Source: Medline	Howard Hughes Medical Institute(Howard Hughes Medical Institute)		Mandelbrot BB, 1983, FRACTAL GEOMETRY NAT; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mehta P., 2014, ARXIV14103831; Newman MEJ, 2000, PHYS REV LETT, V85, P4104, DOI 10.1103/PhysRevLett.85.4104; Olshausen BA, 2005, NEURAL COMPUT, V17, P1665, DOI 10.1162/0899766054026639; RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814; Ruderman DL, 1997, VISION RES, V37, P3385, DOI 10.1016/S0042-6989(97)00008-4; Saremi S, 2014, NEURAL COMPUT, V26, P1329, DOI 10.1162/NECO_a_00607; Saremi S, 2013, P NATL ACAD SCI USA, V110, P3071, DOI 10.1073/pnas.1222618110; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Stauffer D., 2018, INTRO PERCOLATION TH; Stephens GJ, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.018701; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; WILSON KG, 1979, SCI AM, V241, P158, DOI 10.1038/scientificamerican0879-158	15	5	5	0	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2016	38	5					1016	1020		10.1109/TPAMI.2015.2481402	http://dx.doi.org/10.1109/TPAMI.2015.2481402			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DJ4GZ	26415153	Green Accepted			2022-12-18	WOS:000374164700014
J	Chen, L; Shen, CC; Vogelstein, JT; Priebe, CE				Chen, Li; Shen, Cencheng; Vogelstein, Joshua T.; Priebe, Carey E.			Robust Vertex Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Sparse representation; vertex classification; robustness; adjacency spectral embedding; stochastic blockmodel; latent position model; model dimension; classification consistency	SPARSE REPRESENTATION; UNDERDETERMINED SYSTEMS; NERVOUS-SYSTEM; LEAST-SQUARES; RECOVERY; PATTERN; GRAPHS; MODEL	For random graphs distributed according to stochastic blockmodels, a special case of latent position graphs, adjacency spectral embedding followed by appropriate vertex classification is asymptotically Bayes optimal; but this approach requires knowledge of and critically depends on the model dimension. In this paper, we propose a sparse representation vertex classifier which does not require information about the model dimension. This classifier represents a test vertex as a sparse combination of the vertices in the training set and uses the recovered coefficients to classify the test vertex. We prove consistency of our proposed classifier for stochastic blockmodels, and demonstrate that the sparse representation classifier can predict vertex labels with higher accuracy than adjacency spectral embedding approaches via both simulation studies and real data experiments. Our results demonstrate the robustness and effectiveness of our proposed vertex classifier when the model dimension is unknown.	[Chen, Li] Intel Corp, Hillsboro, OR 97124 USA; [Chen, Li; Shen, Cencheng; Priebe, Carey E.] Johns Hopkins Univ, Dept Appl Math & Stat, Baltimore, MD 21218 USA; [Vogelstein, Joshua T.] JHU Child Mind Inst, JHU & Inst Computat Med, Dept Biomed Engn, New York, NY 10022 USA	Intel Corporation; Johns Hopkins University	Chen, L (corresponding author), Intel Corp, Hillsboro, OR 97124 USA.; Chen, L; Shen, CC; Priebe, CE (corresponding author), Johns Hopkins Univ, Dept Appl Math & Stat, Baltimore, MD 21218 USA.; Vogelstein, JT (corresponding author), JHU Child Mind Inst, JHU & Inst Computat Med, Dept Biomed Engn, New York, NY 10022 USA.	li.chen@intel.com; cshen6@jhu.edu; jovo@jhu.edu; cep@jhu.edu	Vogelstein, Joshua/AAG-5489-2019; Shen, Cencheng/AAB-8052-2020	Vogelstein, Joshua/0000-0003-2487-6237; Shen, Cencheng/0000-0003-1030-1432	National Security Science and Engineering Faculty Fellowship (NSSEFF); Johns Hopkins University Human Language Technology Center of Excellence (JHU HLT COE); XDATA program of the Defense Advanced Research Projects Agency (DARPA) [FA8750-12-2-0303]	National Security Science and Engineering Faculty Fellowship (NSSEFF); Johns Hopkins University Human Language Technology Center of Excellence (JHU HLT COE); XDATA program of the Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was partially supported by a National Security Science and Engineering Faculty Fellowship (NSSEFF), Johns Hopkins University Human Language Technology Center of Excellence (JHU HLT COE), and the XDATA program of the Defense Advanced Research Projects Agency (DARPA) administered through Air Force Research Laboratory contract FA8750-12-2-0303. We would also like to thank Minh Tang for his thoughtful discussions.	Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Athreya A., 2014, SANKHYA IN PRESS; Balakrishnan S., 2011, ADV NEURAL INFORM PR, P954; Bollobas B., 2001, RANDOM GRAPHS, V73; Bruckstein AM, 2008, IEEE T INFORM THEORY, V54, P4813, DOI 10.1109/TIT.2008.929920; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Chaudhuri Kamalika, 2012, J MACHINE LEARNING R, P35; Chen L., 2015, JOINT GRAPH INFERENC; Chen L., 2015, THESIS J HOPKINS U B; Chen L, 2015, AAAI CONF ARTIF INTE, P4146; Chen Y., 2012, ADV NEURAL INFORM PR, P2204; Devroye Luc P., 1996, PROBABILISTIC THEORY, V31; Donoho DL, 2008, IEEE T INFORM THEORY, V54, P4789, DOI 10.1109/TIT.2008.929958; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fishkind D. E., 2015, ANN APPL ST IN PRESS; Fishkind DE, 2013, SIAM J MATRIX ANAL A, V34, P23, DOI 10.1137/120875600; Goldschmidt R, 1908, Z WISS ZOOL ABT A, V90, P73; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; HALL DH, 1991, J NEUROSCI, V11, P1; Handcock MS, 2007, J ROY STAT SOC A STA, V170, P301, DOI 10.1111/j.1467-985X.2007.00471.x; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jackson J.E., 2005, USERS GUIDE PRINCIPA, V587; Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274; Lyzinski V, 2014, ELECTRON J STAT, V8, P2905, DOI 10.1214/14-EJS978; Marchette D., 2011, P 57 ISI WORLD STAT, V1121, P1126; Meinshausen N, 2013, ELECTRON J STAT, V7, P1607, DOI 10.1214/13-EJS818; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Olhede SC, 2014, P NATL ACAD SCI USA, V111, P14722, DOI 10.1073/pnas.1400374111; Priebe C. E., 2014, J COMPUT GR IN PRESS; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551; Scheinerman ER, 2010, COMPUTATION STAT, V25, P1, DOI [10.1007/s00180-009-0158-8, 10.1007/S00180-009-0158-8]; Shen C., 2015, SPARSE REPRESENTATIO; Slawski M, 2013, ELECTRON J STAT, V7, P3004, DOI 10.1214/13-EJS868; Sussman DL, 2014, IEEE T PATTERN ANAL, V36, P48, DOI 10.1109/TPAMI.2013.135; Sussman DL, 2012, J AM STAT ASSOC, V107, P1119, DOI 10.1080/01621459.2012.699795; Tang M, 2013, ANN STAT, V41, P1406, DOI 10.1214/13-AOS1112; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Varshney LR, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001066; West D. B., 2001, INTRO GRAPH THEORY, V2; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yang Z, 2011, IEEE T SIGNAL PROCES, V59, P6285, DOI 10.1109/TSP.2011.2168216; Young SJ, 2007, LECT NOTES COMPUT SC, V4863, P138; Zhu M, 2006, COMPUT STAT DATA AN, V51, P918, DOI 10.1016/j.csda.2005.09.010	51	5	5	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2016	38	3					578	590		10.1109/TPAMI.2015.2456913	http://dx.doi.org/10.1109/TPAMI.2015.2456913			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DE6JD	26340770	Green Submitted			2022-12-18	WOS:000370738900012
J	Zhang, QS; Song, X; Shao, XW; Zhao, HJ; Shibasaki, R				Zhang, Quanshi; Song, Xuan; Shao, Xiaowei; Zhao, Huijing; Shibasaki, Ryosuke			Object Discovery: Soft Attributed Graph Mining	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Graph mining; graph matching; big visual data; attributed relational graphs; ubiquitous learning		We categorize this research in terms of its contribution to both graph theory and computer vision. From the theoretical perspective, this study can be considered as the first attempt to formulate the idea of mining maximal frequent subgraphs in the challenging domain of messy visual data, and as a conceptual extension to the unsupervised learning of graph matching. We define a soft attributed pattern (SAP) to represent the common subgraph pattern among a set of attributed relational graphs (ARGs), considering both their structure and attributes. Regarding the differences between ARGs with fuzzy attributes and conventional labeled graphs, we propose a new mining strategy that directly extracts the SAP with the maximal graph size without applying node enumeration. Given an initial graph template and a number of ARGs, we develop an unsupervised method to modify the graph template into the maximal-size SAP. From a practical perspective, this research develops a general platform for learning the category model (i.e., the SAP) from cluttered visual data (i.e., the ARGs) without labeling "what is where," thereby opening the possibility for a series of applications in the era of big visual data. Experiments demonstrate the superior performance of the proposed method on RGB/RGB-D images and videos.	[Zhang, Quanshi; Song, Xuan; Shao, Xiaowei; Shibasaki, Ryosuke] Univ Tokyo, Ctr Spatial Informat Sci, Tokyo, Japan; [Zhao, Huijing] Peking Univ, Key Lab Machine Percept MoE, Beijing 100871, Peoples R China	University of Tokyo; Peking University	Zhang, QS; Song, X; Shao, XW; Shibasaki, R (corresponding author), Univ Tokyo, Ctr Spatial Informat Sci, Tokyo, Japan.; Zhao, HJ (corresponding author), Peking Univ, Key Lab Machine Percept MoE, Beijing 100871, Peoples R China.	zqs1022@csis.u-tokyo.ac.jp; songxuan@csis.u-tokyo.ac.jp; shaoxw@csis.u-tokyo.ac.jp; zhaohj@cis.pku.edu.cn; shiba@csis.u-tokyo.ac.jp	Song, Xuan/L-8086-2018	Song, Xuan/0000-0003-4042-7888				Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316; Brunato M, 2008, LECT NOTES COMPUT SC, V5313, P41, DOI 10.1007/978-3-540-92695-5_4; Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28; Cho MS, 2013, IEEE I CONF COMP VIS, P25, DOI 10.1109/ICCV.2013.11; Cho M, 2012, PROC CVPR IEEE, P398, DOI 10.1109/CVPR.2012.6247701; Cho M, 2010, PROC CVPR IEEE, P1617, DOI 10.1109/CVPR.2010.5539777; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Hong PY, 2004, DISCRETE APPL MATH, V139, P113, DOI 10.1016/j.dam.2002.11.007; Huan J., 2004, PROC 10 ACM SIGKDD I, P581, DOI [DOI 10.1145/1014052.1014123, 10.1145/1014052.1014123]; Jiang C, 2012, KNOWL ENG REV, P1; Jiang H., 2003, P 9 INT C VIS INF SY, P446; Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719; Kim G., 2008, CVPR; Kim G, 2012, PROC CVPR IEEE, P837, DOI 10.1109/CVPR.2012.6247756; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Le Q., 2012, INT C MACH LEARN, DOI DOI 10.1109/MSP.2011.940881; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu M., 2009, P IEEE C COMP VIS PA, P1; Leordeanu M., 2008, IEEE C COMP VIS PATT, P1; Leordeanu M., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383091; Liu HR, 2010, PROC CVPR IEEE, P1609, DOI 10.1109/CVPR.2010.5539780; Mukherjee L, 2012, LECT NOTES COMPUT SC, V7575, P128, DOI 10.1007/978-3-642-33765-9_10; Parikh Devi, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2743, DOI 10.1109/CVPRW.2009.5206549; Quadrianto N., 2012, P 29 INT C MACH LEAR, P583; Szeliski R, 2006, LECT NOTES COMPUT SC, V3952, P16; Tan HK, 2009, IMAGE VISION COMPUT, V27, P1470, DOI 10.1016/j.imavis.2009.01.002; Thomas L., 2010, P 6 IEEE INT C DAT M, V4, P1097; Torresani L, 2008, LECT NOTES COMPUT SC, V5303, P596, DOI 10.1007/978-3-540-88688-4_44; Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8; Wang J, 2006, ICDE, P73, DOI DOI 10.1109/ICDE.2006.34; Xie H., 2012, ACM MULTIMEDIA, P1385; Yuan J., 2010, P ACM MULTIMEDIA, P975; Yuan JS, 2012, IEEE T IMAGE PROCESS, V21, P2207, DOI 10.1109/TIP.2011.2181952; Zeng Z., 2006, P 12 ACM SIGKDD INT, P797, DOI DOI 10.1145/1150402.1150506; Zhang Q., 2013, P IEEE INT C COMP VI, P1329; Zhang Q., 2014, P IEEE C COMP VIS PA, P23; Zhang QS, 2014, IEEE INT CONF ROBOT, P3082, DOI 10.1109/ICRA.2014.6907302; Zhang QS, 2014, PROC CVPR IEEE, P700, DOI 10.1109/CVPR.2014.95; Zhang QS, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2629701; Zhang QS, 2013, IEEE INT CONF ROBOT, P2685, DOI 10.1109/ICRA.2013.6630946; Zhang QS, 2013, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2013.32	42	5	6	0	34	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2016	38	3					532	545		10.1109/TPAMI.2015.2456892	http://dx.doi.org/10.1109/TPAMI.2015.2456892			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DE6JD	27046496				2022-12-18	WOS:000370738900009
J	Brandao, M; Ferreira, R; Hashimoto, K; Takanishi, A; Santos-Victor, J				Brandao, Martim; Ferreira, Ricardo; Hashimoto, Kenji; Takanishi, Atsuo; Santos-Victor, Jose			On Stereo Confidence Measures for Global Methods: Evaluation, New Model and Integration into Occupancy Grids	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Stereo vision; stereo matching; confidence; uncertainty; 3D reconstruction; occupancy grids		Stereo confidence measures are important functions for global reconstruction methods and some applications of stereo. In this article we evaluate and compare several models of confidence which are defined at the whole disparity range. We propose a new stereo confidence measure to which we call the Histogram Sensor Model (HSM), and show how it is one of the best performing functions overall. We also introduce, for parametric models, a systematic method for estimating their parameters which is shown to lead to better performance when compared to parameters as computed in previous literature. All models were evaluated when applied to two different cost functions at different window sizes and model parameters. Contrary to previous stereo confidence measure benchmark literature, we evaluate the models with criteria important not only to winner-take-all stereo, but also to global applications. To this end, we evaluate the models on a real-world application using a recent formulation of 3D reconstruction through occupancy grids which integrates stereo confidence at all disparities. We obtain and discuss our results on both indoors' and outdoors' publicly available datasets.	[Brandao, Martim] Waseda Univ 41 304, Grad Sch Adv Sci & Engn, Shinjuku Ku, Tokyo 1620044, Japan; [Hashimoto, Kenji] Waseda Univ, Fac Sci & Engn, Tokyo, Japan; [Takanishi, Atsuo] Waseda Univ, Dept Modern Mech Engn, Tokyo, Japan; [Takanishi, Atsuo] Waseda Univ, Humanoid Robot Inst, Tokyo, Japan; [Ferreira, Ricardo; Santos-Victor, Jose] Univ Lisbon, Inst Syst & Robot, Inst Super Tecn, P-1699 Lisbon, Portugal	Waseda University; Waseda University; Waseda University; Waseda University; Universidade de Coimbra; Universidade de Lisboa; Instituto Superior Tecnico	Brandao, M (corresponding author), Waseda Univ 41 304, Grad Sch Adv Sci & Engn, Shinjuku Ku, Kikui Cho, Tokyo 1620044, Japan.	mbrandao@fuji.waseda.jp; ricardo@isr.ist.utl.pt; k-hashimoto@takanishi.mech.waseda.ac.jp; takanisi@waseda.jp; jasv@isr.ist.utl.pt	Santos-Victor, José/K-2093-2012; Brandão, Martim/I-5522-2015; Hashimoto, Kenji/M-6442-2017; Brandao, Martim/O-3421-2019	Santos-Victor, José/0000-0002-9036-1728; Brandão, Martim/0000-0002-2003-0675; Hashimoto, Kenji/0000-0003-2300-6766; Brandao, Martim/0000-0002-2003-0675	JSPS KAKENHI [24360099, 25220005]; Strategic Young Researcher Overseas Visits Program for Accelerating Brain Circulation, JSPS, Japan; EU Project Poeticon++ [FP7-ICT-288382]; Grants-in-Aid for Scientific Research [15J06497] Funding Source: KAKEN	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Strategic Young Researcher Overseas Visits Program for Accelerating Brain Circulation, JSPS, Japan; EU Project Poeticon++; Grants-in-Aid for Scientific Research(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	The authors deeply thank the reviewers of this article for their invaluable comments and suggestions for improvement of the research. This study was conducted as part of the Research Institute for Science and Engineering, Waseda University, and Humanoid Robotics Institute, Waseda University. It was also supported in part by JSPS KAKENHI (Grant Number: 24360099 and 25220005), Strategic Young Researcher Overseas Visits Program for Accelerating Brain Circulation, JSPS, Japan, and the EU Project Poeticon++ FP7-ICT-288382.	Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Brandao M, 2014, IEEE INT C INT ROBOT, P1818, DOI 10.1109/IROS.2014.6942801; Brandao M, 2013, IEEE INT C INT ROBOT, P4681, DOI 10.1109/IROS.2013.6697030; Cech J., 2007, 2007 IEEE C COMP VIS, P1; Dima C, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P3347, DOI 10.1109/ROBOT.2002.1014228; Egnal G, 2004, IMAGE VISION COMPUT, V22, P943, DOI 10.1016/j.imavis.2004.03.018; Fusiello A, 1997, PROC CVPR IEEE, P858, DOI 10.1109/CVPR.1997.609428; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6_3; Gong ML, 2005, IEEE T PATTERN ANAL, V27, P998, DOI 10.1109/TPAMI.2005.120; Haeusler R, 2013, LECT NOTES COMPUT SC, V8142, P164, DOI 10.1007/978-3-642-40602-7_17; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Hirschmuller Heiko, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383248; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; MATTHIES L, 1989, INT J COMPUT VISION, V3, P209, DOI 10.1007/BF00133032; Matthies L., 1989, P SPIE SENSOR FUSION, P1; Mayoral R, 2006, IMAGE VISION COMPUT, V24, P1288, DOI 10.1016/j.imavis.2006.04.006; Merrell P, 2007, IEEE I CONF COMP VIS, P3012, DOI 10.1109/iccv.2007.4408984; Mordohai P, 2009, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2009.5459409; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Pal C. J., 2010, INT J COMPUT VISION, V99, P319; Pfeiffer D, 2013, PROC CVPR IEEE, P297, DOI 10.1109/CVPR.2013.45; Sabater N, 2012, IEEE T PATTERN ANAL, V34, P930, DOI 10.1109/TPAMI.2011.207; Sara R, 2002, LECT NOTES COMPUT SC, V2352, P900; Scharstein D, 1998, INT J COMPUT VISION, V28, P155, DOI 10.1023/A:1008015117424; Scharstein D, 2003, PROC CVPR IEEE, P195; SCOTT DW, 1979, BIOMETRIKA, V66, P605, DOI 10.1093/biomet/66.3.605; Shade Robbie, 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P2806, DOI 10.1109/ICRA.2011.5980121; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; Thrun S., 2005, PROBABILISTIC ROBOTI; Torabi A., 2011, 2011 IEEE International Symposium on Robotic and Sensors Environments (ROSE 2011), P143, DOI 10.1109/ROSE.2011.6058540	33	5	5	0	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2016	38	1					116	128		10.1109/TPAMI.2015.2437381	http://dx.doi.org/10.1109/TPAMI.2015.2437381			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CY8OW	26656581				2022-12-18	WOS:000366669200009
J	Perina, A; Jojic, N				Perina, Alessandro; Jojic, Nebojsa			Capturing Spatial Interdependence in Image Features: The Counting Grid, an Epitomic Representation for Bags of Features	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bag of features; spatial layout; scene analysis	SCENE	In recent scene recognition research images or large image regions are often represented as disorganized "bags" of features which can then be analyzed using models originally developed to capture co-variation of word counts in text. However, image feature counts are likely to be constrained in different ways than word counts in text. For example, as a camera pans upwards from a building entrance over its first few floors and then further up into the sky Fig. 1, some feature counts in the image drop while others rise-only to drop again giving way to features found more often at higher elevations. The space of all possible feature count combinations is constrained both by the properties of the larger scene and the size and the location of the window into it. To capture such variation, in this paper we propose the use of the counting grid model. This generative model is based on a grid of feature counts, considerably larger than any of the modeled images, and considerably smaller than the real estate needed to tile the images next to each other tightly. Each modeled image is assumed to have a representative window in the grid in which the feature counts mimic the feature distribution in the image. We provide a learning procedure that jointly maps all images in the training set to the counting grid and estimates the appropriate local counts in it. Experimentally, we demonstrate that the resulting representation captures the space of feature count combinations more accurately than the traditional models, not only when the input images come from a panning camera, but even when modeling images of different scenes from the same category.	[Perina, Alessandro] Ist Italiano Tecnol, Pattern Anal & Comp Vis Dept PAVIS, Genoa, Italy; [Perina, Alessandro; Jojic, Nebojsa] Microsoft Res, eSci Grp, Redmond, WA USA	Istituto Italiano di Tecnologia - IIT; Microsoft	Perina, A (corresponding author), Ist Italiano Tecnol, Pattern Anal & Comp Vis Dept PAVIS, Genoa, Italy.	alessandro.perina@gmail.com; jojic@microsoft.com						Amer MR, 2012, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2012.6247816; Bazzani L, 2012, PATTERN RECOGN LETT, V33, P898, DOI 10.1016/j.patrec.2011.11.016; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598; Bosch A, 2007, IEEE I CONF COMP VIS, P1863; Bosch A, 2006, LECT NOTES COMPUT SC, V3954, P517; Bouguila N, 2011, IEEE T NEURAL NETWOR, V22, P186, DOI 10.1109/TNN.2010.2091428; Boutell MR, 2007, IEEE T MULTIMEDIA, V9, P136, DOI 10.1109/TMM.2006.886372; Chu XQ, 2010, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2010.5540196; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Deng L., 2003, SIGNAL PROCESSING CO; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Frey BJ, 2003, IEEE T PATTERN ANAL, V25, P1, DOI 10.1109/TPAMI.2003.1159942; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; Jiang YN, 2012, LECT NOTES COMPUT SC, V7573, P730, DOI 10.1007/978-3-642-33709-3_52; Jojic N, 2001, PROC CVPR IEEE, P199; Jojic N, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P34; Jojic N., 2010, ADV NEURAL INFORM PR, P1027; Jojic N., 2011, P 27 C UNC ART INT, P547; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; LANGLEY P, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P223; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lovato P., 2012, P AS C COMP VIS, P45, DOI DOI 10.1007/978.3.642.37331.2_4; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; McCallum A, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P662; Neal R. M., 1999, LEARNING GRAPHICAL M, P355, DOI DOI 10.1007/978-94-011-5014-9; Ni K, 2009, IEEE T PATTERN ANAL, V31, P2158, DOI 10.1109/TPAMI.2009.165; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2; Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383; Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001; Perina A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1985, DOI 10.1109/CVPR.2011.5995742; Perina A., 2012, P 2 IEEE WORKSH EG V; Perina A, 2014, BIOCOMPUT-PAC SYM, P288; Perina A, 2013, PROC CVPR IEEE, P500, DOI 10.1109/CVPR.2013.71; Perina A, 2012, LECT NOTES COMPUT SC, V7577, P837, DOI 10.1007/978-3-642-33783-3_60; Perina A, 2012, IEEE T PATTERN ANAL, V34, P1249, DOI 10.1109/TPAMI.2011.241; Perina A, 2010, LECT NOTES COMPUT SC, V6316, P15, DOI 10.1007/978-3-642-15567-3_2; Perina A, 2010, IMAGE VISION COMPUT, V28, P927, DOI 10.1016/j.imavis.2009.11.007; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Turski A., 2014, P KDD WORKSH INT DAT; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1; Yang J, 2007, MULTIMEDIA C EXHIBIT, P197, DOI [10.1145/1290082.1290111, DOI 10.1145/1290082.1290111]	48	5	6	0	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2015	37	12					2374	2387		10.1109/TPAMI.2015.2424864	http://dx.doi.org/10.1109/TPAMI.2015.2424864			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CW2OK	26539844	Green Submitted			2022-12-18	WOS:000364831700002
J	Shih, KJ; Endres, I; Hoiem, D				Shih, Kevin J.; Endres, Ian; Hoiem, Derek			Learning Discriminative Collections of Part Detectors for Object Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Object recognition; part sharing; discriminative parts		We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC2010, we evaluate the part detectors' ability to discriminate and localize annotated keypoints and their effectiveness in detecting object categories.	[Shih, Kevin J.; Hoiem, Derek] Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA; [Endres, Ian] HERE Res Nokia, Chicago, IL USA	University of Illinois System; University of Illinois Urbana-Champaign; Nokia Corporation; Nokia Bell Labs	Shih, KJ (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA.	kjshih2@illinois.edu; ian.endres@here.com; dhoiem@illinois.edu			ONR MURI [N000141010934]; NSF [10-53768, IIS 09-04209]	ONR MURI(MURIOffice of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by ONR MURI grant N000141010934, NSF CAREER award 10-53768, and NSF award IIS 09-04209. The authors would also like to thank Johnston Jiaa for his intial contribution to this project.	Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; Bourdev L., 2012, POSELETS THEIR APPL; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Divvala S. K., 2012, P BRIT MACH VIS C, P601; Divvala SK, 2012, LECT NOTES COMPUT SC, V7585, P31, DOI 10.1007/978-3-642-33885-4_4; Doersch Carl, 2013, NIPS; Endres I, 2013, PROC CVPR IEEE, P939, DOI 10.1109/CVPR.2013.126; Endres I, 2012, PROC CVPR IEEE, P3130, DOI 10.1109/CVPR.2012.6248046; Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42; Everingham M., 2010, PASCAL VISUAL OBJECT; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Hoiem Derek, 2012, ECCV, P340, DOI [10.1007/978-3-642-33712-3_25, DOI 10.1007/978-3-642-33712-3_25]; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Ott P, 2011, PROC CVPR IEEE, P1513, DOI 10.1109/CVPR.2011.5995357; Park D, 2010, LECT NOTES COMPUT SC, V6314, P241, DOI 10.1007/978-3-642-15561-1_18; Platt JC, 2000, ADV NEUR IN, P61; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422; van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212; Zhu X., 2012, P BRIT MACH VIS C, P801	34	5	5	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2015	37	8					1571	1584		10.1109/TPAMI.2014.2366122	http://dx.doi.org/10.1109/TPAMI.2014.2366122			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CM3ON	26352996	hybrid			2022-12-18	WOS:000357591900004
J	Arora, C; Banerjee, S; Kalra, PK; Maheshwari, SN				Arora, Chetan; Banerjee, Subhashis; Kalra, Prem Kumar; Maheshwari, S. N.			Generalized Flows for Optimal Inference in Higher Order MRF-MAP	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Markov random field (MRF); maximum a posteriori (MAP); higher order cliques; optimal inference	GRAPH CUTS; APPROXIMATION ALGORITHMS; ENERGY MINIMIZATION; ROOF DUALITY	Use of higher order clique potentials in MRF-MAP problems has been limited primarily because of the inefficiencies of the existing algorithmic schemes. We propose a new combinatorial algorithm for computing optimal solutions to 2 label MRF-MAP problems with higher order clique potentials. The algorithm runs in time O(2(k)n(3)) in the worst case (k is size of clique and n is the number of pixels). A special gadget is introduced to model flows in a higher order clique and a technique for building a flow graph is specified. Based on the primal dual structure of the optimization problem, the notions of the capacity of an edge and a cut are generalized to define a flow problem. We show that in this flow graph, when the clique potentials are submodular, the max flow is equal to the min cut, which also is the optimal solution to the problem. We show experimentally that our algorithm provides significantly better solutions in practice and is hundreds of times faster than solution schemes like Dual Decomposition [1], TRWS [2] and Reduction [3], [4], [5]. The framework represents a significant advance in handling higher order problems making optimal inference practical for medium sized cliques.	[Arora, Chetan] Indraprastha Inst Informat Technol, Delhi, India; [Banerjee, Subhashis; Kalra, Prem Kumar; Maheshwari, S. N.] Indian Inst Technol, Delhi, India	Indraprastha Institute of Information Technology Delhi; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Arora, C (corresponding author), Indraprastha Inst Informat Technol, Delhi, India.	chetan@iiitd.ac.in; suban@cse.iitd.ac.in; pkalra@cse.iitd.ac.in; snm@cse.iitd.ac.in						Arora C, 2014, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2014.174; Arora C, 2012, LECT NOTES COMPUT SC, V7576, P17, DOI 10.1007/978-3-642-33715-4_2; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chekuri C, 2001, SIAM PROC S, P109; EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699; Fix A., 2011, HIGHER ORDER ENERGY; Fix A, 2011, IEEE I CONF COMP VIS, P1020, DOI 10.1109/ICCV.2011.6126347; Freedman D, 2005, PROC CVPR IEEE, P939; Gallagher A. C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1857, DOI 10.1109/CVPR.2011.5995452; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Goldberg A. V., 1986, P 18 ANN ACM S THEOR, P136, DOI [10.1145/12130.12144, DOI 10.1145/12130.12144]; Gould S., 2012, DARWIN 1 1 2; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014; Ishikawa H., 2011, HIGHER ORDER CLIQUE; Ishikawa H, 2011, IEEE T PATTERN ANAL, V33, P1234, DOI 10.1109/TPAMI.2010.91; Iwata S, 1996, PROCEEDINGS OF THE SEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P482; Iwata S, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1230; Kahl F, 2011, IEEE I CONF COMP VIS, P255, DOI 10.1109/ICCV.2011.6126250; Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268; Koller D., 2009, PROBABILISTIC GRAPHI; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P65; Kolmogorov V., 2011, QPBO VERSION 1 3; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025; Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P416, DOI 10.1016/j.dam.2011.10.026; Komodakis Nikos, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2985, DOI 10.1109/CVPRW.2009.5206846; Komodakis N, 2007, IEEE T PATTERN ANAL, V29, P1436, DOI 10.1109/TPAMI.2007.1061; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Mudenagudi U, 2011, IEEE T PATTERN ANAL, V33, P995, DOI 10.1109/TPAMI.2010.167; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Qian RJ, 1997, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.1997.609318; Ramalingam S., 2011, ABS11092304 CORR; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rother C, 2009, PROC CVPR IEEE, P1382, DOI 10.1109/CVPRW.2009.5206739; Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989; Sontag D., 2011, INTRO DUAL DECOMPOSI; Strandmark P., 2011, PSEUDOBOOLEAN OPTIMI; Werner T., 2008, P IEEE C COMP VIS PA, P1	41	5	5	0	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2015	37	7					1323	1335		10.1109/TPAMI.2014.2388218	http://dx.doi.org/10.1109/TPAMI.2014.2388218			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CK0YG	26352442				2022-12-18	WOS:000355931100002
J	Kumar, MP; Turki, H; Preston, D; Koller, D				Kumar, M. Pawan; Turki, Haithem; Preston, Dan; Koller, Daphne			Parameter Estimation and Energy Minimization for Region-Based Semantic Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantic segmentation; weakly supervised learning; energy minimization; LP relaxation	RANDOM-FIELDS	We consider the problem of parameter estimation and energy minimization for a region-based semantic segmentation model. The model divides the pixels of an image into non-overlapping connected regions, each of which is to a semantic class. In the context of energy minimization, the main problem we face is the large number of putative pixel-to-region assignments. We address this problem by designing an accurate linear programming based approach for selecting the best set of regions from a large dictionary. The dictionary is constructed by merging and intersecting segments obtained from multiple bottom-up over-segmentations. The linear program is solved efficiently using dual decomposition. In the context of parameter estimation, the main problem we face is the lack of fully supervised data. We address this issue by developing a principled framework for parameter estimation using diverse data. More precisely, we propose a latent structural support vector machine formulation, where the latent variables model any missing information in the human annotation. Of particular interest to us are three types of annotations: (i) images segmented using generic foreground or background classes; (ii) images with bounding boxes specified for objects; and (iii) images labeled to indicate the presence of a class. Using large, publicly available datasets we show that our methods are able to significantly improve the accuracy of the region-based model.	[Kumar, M. Pawan] Ecole Cent Paris, Ctr Visual Comp, F-92295 Chatenay Malabry, France; [Turki, Haithem; Preston, Dan; Koller, Daphne] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	UDICE-French Research Universities; Universite Paris Saclay; Stanford University	Kumar, MP (corresponding author), Ecole Cent Paris, Ctr Visual Comp, F-92295 Chatenay Malabry, France.	pawan.kumar@ecp.fr; hturki@cs.stanford.edu; dpreston@cs.stanford.edu; koller@cs.stanford.edu			European Research Council under the European Community's Seventh Framework Programme [259112]; INRIA-Stanford associate team SPLENDID; NSF [IIS 0917151]; MURI [N000140710747]; Boeing company	European Research Council under the European Community's Seventh Framework Programme; INRIA-Stanford associate team SPLENDID; NSF(National Science Foundation (NSF)); MURI(MURI); Boeing company	This work was partially funded by the European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC Grant agreement number 259112, INRIA-Stanford associate team SPLENDID, NSF grant IIS 0917151, MURI contract N000140710747, and the Boeing company. M. Pawan Kumar is the corresponding author of this paper.	Arbelaez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707; BARAHONA F, 1986, MATH PROGRAM, V36, P157, DOI 10.1007/BF02592023; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chekuri C, 2005, SIAM J DISCRETE MATH, V18, P608, DOI 10.1137/S0895480101396937; Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb P, 2008, 2008 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2008.4587597; Finley T., 2008, INT C MACHINE LEARNI, P304, DOI DOI 10.1145/1390156.1390195; Gonfaus JM, 2010, PROC CVPR IEEE, P3280, DOI 10.1109/CVPR.2010.5540048; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; He XM, 2004, PROC CVPR IEEE, P695; IVANESCU PL, 1965, OPER RES, V13, P388, DOI 10.1287/opre.13.3.388; JOACHIMS T, 1999, ADV KERNEL METHODS; Kohli P., 2008, PRECEEDINGS 2008 IEE, P1, DOI DOI 10.1109/CVPR.2008.4587417; Komodakis Nikos, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2985, DOI 10.1109/CVPRW.2009.5206846; Komodakis N, 2008, LECT NOTES COMPUT SC, V5304, P806, DOI 10.1007/978-3-540-88690-7_60; Komodakis N, 2007, IEEE I CONF COMP VIS, P488; Konishi S, 2000, PROC CVPR IEEE, P125, DOI 10.1109/CVPR.2000.855809; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Kumar M.P., 2008, P 25 INT C MACH LEAR, P680; Kumar MP, 2010, PROC CVPR IEEE, P3217, DOI 10.1109/CVPR.2010.5540072; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Larlus D., 2008, COMP VIS PATT REC 20, P1; Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262; Li FX, 2010, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2010.5539839; Nowozin S, 2009, PROC CVPR IEEE, P818, DOI 10.1109/CVPRW.2009.5206567; Pantofaru C, 2008, LECT NOTES COMPUT SC, V5304, P481, DOI 10.1007/978-3-540-88690-7_36; PEARL J, 1998, PROBABILISTIC REASON; Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986; Rother C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383203; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Shalev-Shwartz S., 2009, P 24 INT C MACH LEAR, P807; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Vicente S., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587440; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Wang HY, 2010, LECT NOTES COMPUT SC, V6314, P497; Weiss Y., 2008, P UAI, P503; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]	44	5	7	1	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2015	37	7					1373	1386		10.1109/TPAMI.2014.2372766	http://dx.doi.org/10.1109/TPAMI.2014.2372766			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CK0YG	26352446	Green Submitted			2022-12-18	WOS:000355931100006
J	Osokin, A; Vetrov, DP				Osokin, Anton; Vetrov, Dmitry P.			Submodular Relaxation for Inference in Markov Random Fields	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Markov random fields; energy minimization; combinatorial algorithms; relaxation; graph cuts	ENERGY MINIMIZATION; GRAPH CUTS; OPTIMIZATION; ALGORITHMS	In this paper we address the problem of finding the most probable state of a discrete Markov random field (MRF), also known as the MRF energy minimization problem. The task is known to be NP-hard in general and its practical importance motivates numerous approximate algorithms. We propose a submodular relaxation approach (SMR) based on a Lagrangian relaxation of the initial problem. Unlike the dual decomposition approach of Komodakis et al. [29] SMR does not decompose the graph structure of the initial problem but constructs a submodular energy that is minimized within the Lagrangian relaxation. Our approach is applicable to both pairwise and high-order MRFs and allows to take into account global potentials of certain types. We study theoretical properties of the proposed approach and evaluate it experimentally.	[Osokin, Anton] INRIA, SIERRA Team, Paris, France; [Osokin, Anton] Ecole Normale Super, F-75231 Paris, France; [Vetrov, Dmitry P.] Higher Sch Econ, Fac Comp Sci, Moscow, Russia; [Vetrov, Dmitry P.] Moscow MV Lomonosov State Univ, Moscow, Russia	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); HSE University (National Research University Higher School of Economics); Lomonosov Moscow State University	Osokin, A (corresponding author), INRIA, SIERRA Team, Paris, France.	anton.osokin@inria.fr; vetrovd@yandex.ru	Vetrov, Dmitry/H-4870-2015; Osokin, Anton/D-7398-2012	Vetrov, Dmitry/0000-0001-6863-9028; Osokin, Anton/0000-0002-8807-5132	Microsoft: Moscow State University Joint Research Center [RPD 1053945]; Russian Foundation for Basic Research [12-01-00938, 12-01-33085]; Skolkovo Institute of Science and Technology (Skoltech): Joint Laboratory [081-R]	Microsoft: Moscow State University Joint Research Center; Russian Foundation for Basic Research(Russian Foundation for Basic Research (RFBR)); Skolkovo Institute of Science and Technology (Skoltech): Joint Laboratory	The authors would like to thank Vladimir Kolmogorov and Bogdan Savchynskyy for valuable discussions and the anonymous reviewers for great comments. This work was supported by Microsoft: Moscow State University Joint Research Center (RPD 1053945), Russian Foundation for Basic Research (projects 12-01-00938 and 12-01-33085), Skolkovo Institute of Science and Technology (Skoltech): Joint Laboratory Agreement - No 081-R dated October 1, 2013, Appendix A 2.	Alahari K, 2010, IEEE T PATTERN ANAL, V32, P1846, DOI 10.1109/TPAMI.2009.194; Batra D, 2010, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2010.5539951; Bertsekas D., 2003, CONVEX ANAL OPTIMIZA; BESAG J, 1986, J R STAT SOC B, V48, P259; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Bottou L, 2012, OPTIMIZATION FOR MACHINE LEARNING, P351; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Darbon J, 2009, DISCRETE APPL MATH, V157, P3412, DOI 10.1016/j.dam.2009.02.026; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Globerson Amir, 2007, P NIPS, V20, P553; Haarala N, 2007, MATH PROGRAM, V109, P181, DOI 10.1007/s10107-006-0728-2; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Ishikawa H, 2011, IEEE T PATTERN ANAL, V33, P1234, DOI 10.1109/TPAMI.2010.91; Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Kappes JH, 2012, PROC CVPR IEEE, P1688, DOI 10.1109/CVPR.2012.6247863; KIWIEL KC, 1983, MATH PROGRAM, V27, P320, DOI 10.1007/BF02591907; Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268; Kohli P, 2007, IEEE T PATTERN ANAL, V29, P2079, DOI 10.1109/TPAMI.2007.1128; Kohli P, 2013, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2013.257; Kohli P, 2009, IEEE T PATTERN ANAL, V31, P1645, DOI 10.1109/TPAMI.2008.217; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V., 2005, P 21 C UNC ART INT, P316, DOI DOI 10.5555/3020336.3020376; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis Nikos, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2985, DOI 10.1109/CVPRW.2009.5206846; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Kropotov D., 2010, [Pattern Recognition and Image Analysis (Advances in Mathematical Theory and Applications), Pattern Recognition and Image Analysis. (Advances in Mathematical Theory and Applications)], V20, P324, DOI 10.1134/S1054661810030089; Ladicky L, 2014, IEEE T PATTERN ANAL, V36, P1056, DOI 10.1109/TPAMI.2013.165; Lewis AS, 2013, MATH PROGRAM, V141, P135, DOI 10.1007/s10107-012-0514-2; Osokin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1889, DOI 10.1109/CVPR.2011.5995361; Osokin A, 2012, LECT NOTES COMPUT SC, V7585, P305, DOI 10.1007/978-3-642-33885-4_31; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rother C, 2009, PROC CVPR IEEE, P1382, DOI 10.1109/CVPRW.2009.5206739; Savchynskyy Bogdan, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1817, DOI 10.1109/CVPR.2011.5995652; Savchynskyy B., 2014, ADV STRUCTURED PREDI; SCHLESINGER M, 1976, KIBERNETIKA, V4, P113; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Strandmark P, 2010, PROC CVPR IEEE, P2085, DOI 10.1109/CVPR.2010.5539886; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Woodford OJ, 2009, IEEE I CONF COMP VIS, P2319, DOI 10.1109/ICCV.2009.5459434; Yarkony J., 2011, UNCERTAINTY ARTIFICI; Yarkony J., 2011, P C UNC ART INT, P761; Zach C, 2012, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2012.6247860	47	5	5	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2015	37	7					1347	1359		10.1109/TPAMI.2014.2369046	http://dx.doi.org/10.1109/TPAMI.2014.2369046			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CK0YG	26352444	Green Submitted			2022-12-18	WOS:000355931100004
J	Rodriguez-Vaamonde, S; Torresani, L; Fitzgibbon, AW				Rodriguez-Vaamonde, Sergio; Torresani, Lorenzo; Fitzgibbon, Andrew W.			What Can Pictures Tell Us About Web Pages? Improving Document Search Using Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Web Pages; multimedia search; search engines; document ranking		Traditional Web search engines do not use the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the content of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using visual information extracted from the images contained in the pages. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We test our approach on one of the TREC Million Query Track benchmarks where we show that the exploitation of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark. We further validate our approach by collecting document relevance judgements on our search results using Amazon Mechanical Turk. The results of this experiment confirm the improvement in accuracy produced by our image-based reranker over a pure text-based system.	[Rodriguez-Vaamonde, Sergio] Tecnalia, Zamudio, Bizkaia, Spain; [Torresani, Lorenzo] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA; [Fitzgibbon, Andrew W.] Microsoft Res Cambridge, Cambridge CB1 2FB, England	Dartmouth College; Microsoft	Rodriguez-Vaamonde, S (corresponding author), Tecnalia, Zamudio, Bizkaia, Spain.	sergio.rodriguez@tecnalia.com; lt@dartmouth.edu; awf@microsoft.com			Microsoft Research; NSF [IIS-0952943, CNS-1205521]; Basque Government [IE11-316]	Microsoft Research(Microsoft); NSF(National Science Foundation (NSF)); Basque Government(Basque Government)	This work was supported in part by Microsoft Research, NSF CAREER award IIS-0952943 and NSF award CNS-1205521. This research was performed while the first author was a visiting student at Dartmouth College partly funded by the Basque Government under grant number IE11-316.	Alonso O, 2012, INFORM PROCESS MANAG, V48, P1053, DOI 10.1016/j.ipm.2012.01.004; Barnard K, 2005, ARTIF INTELL, V167, P13, DOI 10.1016/j.artint.2005.04.009; Bishop CM, 2006, PATTERN RECOGNITION; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Carterette B., 2009, TREC; Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Herbrich R, 2000, ADV NEUR IN, P115; Joachims T., 2006, P 12 ACM SIGKDD INT, V06, P217, DOI DOI 10.1145/1150402.1150429; Krapac J, 2010, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.2010.5540092; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Manning C.D., 2008, INTRO INFORM RETRIEV; Mohan A., 2011, PROC INT C YAHOO LEA, P77; Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133; Strohman Trevor, 2005, P INT C INT AN, V2, P2; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; TREC, 2009, TREC 2009 MILL QUER; TREC, 2009, UDMQAXQEWEB RANK LIS; Yeh T, 2008, P 16 ACM INT C MULT, P389, DOI DOI 10.1145/1459359.1459412; Yu Q, 2007, LECT NOTES COMPUT SC, V4425, P645; Zheng W., 2009, P TEXT RETR C; Zheng Zhaohui, 2007, P 20 INT C NEUR INF, P1697; Zhou ZH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2928	26	5	6	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2015	37	6					1274	1285		10.1109/TPAMI.2014.2366761	http://dx.doi.org/10.1109/TPAMI.2014.2366761			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CH9SR	26357348	hybrid, Green Published			2022-12-18	WOS:000354377100012
J	Zarrabeitia, LA; Qureshi, FZ; Aruliah, DA				Zarrabeitia, Luis A.; Qureshi, Faisal Z.; Aruliah, Dhavide A.			Stereo Reconstruction of Droplet Flight Trajectories	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Stereo reconstruction; multi-target tracking; multi-view geometry; nonlinear motion; parameter estimation	TRACKING; ALGORITHM; NUMBER	We developed a new method for extracting 3D flight trajectories of droplets using high-speed stereo capture. We noticed that traditional multi-camera tracking techniques fare poorly on our problem, in part due to the fact that all droplets have very similar shapes, sizes and appearances. Our method uses local motion models to track individual droplets in each frame. 2D tracks are used to learn a global, non-linear motion model, which in turn can be used to estimate the 3D locations of individual droplets even when these are not visible in any camera. We have evaluated the proposed method on both synthetic and real data and our method is able to reconstruct 3D flight trajectories of hundreds of droplets. The proposed technique solves for both the 3D trajectory of a droplet and its motion model concomitantly, and we have found it to be superior to 3D reconstruction via triangulation. Furthermore, the learned global motion model allows us to relax the simultaneity assumptions of stereo camera systems. Our results suggest that, even when full stereo information is available, our unsynchronized reconstruction using the global motion model can significantly improve the 3D estimation accuracy.	[Zarrabeitia, Luis A.; Qureshi, Faisal Z.; Aruliah, Dhavide A.] Univ Ontario, Inst Technol, Fac Sci, Oshawa, ON L1H 7K4, Canada	Ontario Tech University	Zarrabeitia, LA (corresponding author), Univ Ontario, Inst Technol, Fac Sci, Oshawa, ON L1H 7K4, Canada.	luis.zarrabeitia@uoit.ca; faisal.qureshi@uoit.ca; dhavide.aruliah@uoit.ca			Defence Research and Development Canada Centre for Security Science (Public Security Technical Program) [W7714-115037/001/sv]; National Science and Engineering Research Council	Defence Research and Development Canada Centre for Security Science (Public Security Technical Program); National Science and Engineering Research Council(Natural Sciences and Engineering Research Council of Canada (NSERC))	The authors acknowledge the financial support of the Defence Research and Development Canada Centre for Security Science (Public Security Technical Program Contract #: W7714-115037/001/sv) and of the National Science and Engineering Research Council. The authors would like to thank R. Murray and P. Prior for setting up the physical experimental setup and carrying out the experiments, as well as F. Gaspari for motivation, encouragement, and support. Finally, the authors extend their gratitude to the anonymous reviewers for their insightful suggestions.	[Anonymous], P IEEE C COMP VIS PA; Balch T., 2001, Proceedings of the Fifth International Conference on Autonomous Agents, P521, DOI 10.1145/375735.376434; Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667; Betke M, 2007, PROC CVPR IEEE, P192; Bijsterbosch J, 2010, ANN OPER RES, V181, P443, DOI 10.1007/s10479-010-0757-3; BOURGEOIS F, 1971, COMMUN ACM, V14, P802, DOI 10.1145/362919.362945; Buck U, 2011, FORENSIC SCI INT, V206, P22, DOI 10.1016/j.forsciint.2010.06.010; Burkard R., 2013, ASSIGNMENT PROBLEMS; Cecchetto B. T., 2011, P VIS MOD VIS WORKSH, P369; Dasgupta D., 2008, P 10 ANN C COMP GEN, P2129, DOI [10.1145/1388969.1389035, DOI 10.1145/1388969.1389035]; Grover D, 2008, J R SOC INTERFACE, V5, P1181, DOI 10.1098/rsif.2007.1333; Hartley R., 2004, ROBOTICA; Hestenes D., 1999, NEW FDN CLASSICAL ME, VSecond; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Kausler BX, 2012, LECT NOTES COMPUT SC, V7574, P144, DOI 10.1007/978-3-642-33712-3_11; Khan Z, 2005, PROC CVPR IEEE, P605; Khan Z, 2005, IEEE T PATTERN ANAL, V27, P1805, DOI 10.1109/TPAMI.2005.223; Khan Z, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P254; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Liu Y, 2012, LECT NOTES COMPUT SC, V7575, P730, DOI 10.1007/978-3-642-33765-9_52; Milan A, 2013, PROC CVPR IEEE, P3682, DOI 10.1109/CVPR.2013.472; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Murray R., 2012, THESIS U ONTARIO I T; Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158; Poore AB, 2006, MATH COMPUT MODEL, V43, P1074, DOI 10.1016/j.mcm.2005.05.026; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; Straw AD, 2011, J R SOC INTERFACE, V8, P395, DOI 10.1098/rsif.2010.0230; Tyagi A., 2007, P IEEE WORKSH MOT VI, P1; Veenman CJ, 2001, IEEE T PATTERN ANAL, V23, P54, DOI 10.1109/34.899946; Wu Z., 2009, P WORKSH MOT VID COM, P1, DOI [10.1109/WMVC.2009.5399245, DOI 10.1109/WMVC.2009.5399245]; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zarrabeitia L. A., 2012, Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods. ICPRAM 2012, P142; Zarrabeitia L. A., 2013, P 2 INT C PATT AN AP, P8; Zhang L, 2008, INT C WAVEL ANAL PAT, P11, DOI 10.1109/ICWAPR.2008.4635742; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	37	5	5	2	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2015	37	4					847	861		10.1109/TPAMI.2014.2353638	http://dx.doi.org/10.1109/TPAMI.2014.2353638			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CD6QF	26353298				2022-12-18	WOS:000351213400011
J	Kong, AWK				Kong, Adams Wai-Kin			A Statistical Analysis of IrisCode and Its Security Implications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Biometrics; iris recognition; statistical dependence; Daugman algorithm; template protection	RECOGNITION	IrisCode has been used to gather iris data for 430 million people. Because of the huge impact of IrisCode, it is vital that it is completely understood. This paper first studies the relationship between bit probabilities and a mean of iris images (The mean of iris images is defined as the average of independent iris images.) and then uses the Chi-square statistic, the correlation coefficient and a resampling algorithm to detect statistical dependence between bits. The results show that the statistical dependence forms a graph with a sparse and structural adjacency matrix. A comparison of this graph with a graph whose edges are defined by the inner product of the Gabor filters that produce IrisCodes shows that partial statistical dependence is induced by the filters and propagates through the graph. Using this statistical information, the security risk associated with two patented template protection schemes that have been deployed in commercial systems for producing application-specific IrisCodes is analyzed. To retain high identification speed, they use the same key to lock all IrisCodes in a database. The belief has been that if the key is not compromised, the IrisCodes are secure. This study shows that even without the key, application-specific IrisCodes can be unlocked and that the key can be obtained through the statistical dependence detected.	Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Kong, AWK (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.	adamskong@ntu.edu.sg			Academic Research Fund Tier 1 of the Ministry of Education of Singapore [RG6/10]	Academic Research Fund Tier 1 of the Ministry of Education of Singapore(Ministry of Education, Singapore)	The author would like to thank West Virginia University and the University of Beira Interior for sharing their databases. This work was partially supported by the Academic Research Fund Tier 1 (RG6/10) of the Ministry of Education of Singapore.	ALMOHAMAD HA, 1993, IEEE T PATTERN ANAL, V15, P522, DOI 10.1109/34.211474; Braithwaite M., 2002, Authentication using application-specific biometric templates, Patent No. [WO2002/095657, 2002095657]; Braithwaite Michael, 2002, IEEE WORKSHOP AUTOMA, P14; Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350; Daugman J, 2003, PATTERN RECOGN, V36, P279, DOI 10.1016/S0031-3203(02)00030-4; Daugman J., 2013, HIST IRIS RECOGNITIO; DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676; Daugman J, 2007, IEEE T SYST MAN CY B, V37, P1167, DOI 10.1109/TSMCB.2007.903540; Galbally J., 2012, IRISCODE IRIS NEW VU; Hollingsworth KP, 2011, IEEE T PATTERN ANAL, V33, P2465, DOI 10.1109/TPAMI.2011.89; Hollingsworth KP, 2009, IEEE T PATTERN ANAL, V31, P964, DOI 10.1109/TPAMI.2008.185; Jain AK, 2000, IEEE T IMAGE PROCESS, V9, P846, DOI 10.1109/83.841531; Kong AWK, 2010, IEEE T IMAGE PROCESS, V19, P522, DOI 10.1109/TIP.2009.2033427; Kong AWK, 2013, IEEE T IMAGE PROCESS, V22, P1148, DOI 10.1109/TIP.2012.2227770; Kong AWK, 2012, IEEE T PATTERN ANAL, V34, P506, DOI 10.1109/TPAMI.2011.159; Kong AWK, 2009, LECT NOTES COMPUT SC, V5627, P64, DOI 10.1007/978-3-642-02611-9_7; Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184; Kong WK, 2002, INT C PATT RECOG, P807; Krichen E., 2004, P INT C PATT REC, V4, P226; Ma L, 2004, IEEE T IMAGE PROCESS, V13, P739, DOI 10.1109/TIP.2004.827237; Matey J. R., 2007, ADV BIOMETRICS SENSO, P114; Noh SI, 2003, LECT NOTES COMPUT SC, V2688, P862; Park HA, 2007, PATTERN RECOGN LETT, V28, P2019, DOI 10.1016/j.patrec.2007.05.017; Proenca H., 2005, P 13 INT C IM AN PRO, V1, P790; Proenca H, 2007, IEEE T PATTERN ANAL, V29, P607, DOI 10.1109/TPAMI.2007.1016; Ross A., 2006, BIOMETRIC CONSORTIUM, P1, DOI DOI 10.1109/BCC.2006.4341625; Santos G., 2010, 2010 Proceedings of 3rd International Congress on Image and Signal Processing (CISP 2010), P1873, DOI 10.1109/CISP.2010.5647404; Sun ZN, 2009, IEEE T PATTERN ANAL, V31, P2211, DOI 10.1109/TPAMI.2008.240; Sun ZN, 2005, PROC CVPR IEEE, P279; UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778; Venugopalan S, 2011, IEEE T INF FOREN SEC, V6, P385, DOI 10.1109/TIFS.2011.2108288; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zhang L, 2010, PATTERN RECOGN, V43, P2560, DOI 10.1016/j.patcog.2010.01.020; [No title captured]	34	5	6	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2015	37	3					513	528		10.1109/TPAMI.2014.2343959	http://dx.doi.org/10.1109/TPAMI.2014.2343959			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CB4VK	26353258				2022-12-18	WOS:000349626200003
J	Gatta, C; Ciompi, F				Gatta, Carlo; Ciompi, Francesco			Stacked Sequential Scale-Space Taylor Context	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Contextual modeling; semantic image labeling; stacked sequential learning	AUTO-CONTEXT; SEGMENTATION	We analyze sequential image labeling methods that sample the posterior label field in order to gather contextual information. We propose an effective method that extracts local Taylor coefficients from the posterior at different scales. Results show that our proposal outperforms state-of-the-art methods on MSRC-21, CAMVID, eTRIMS8 and KAIST2 data sets.	[Gatta, Carlo] Ctr Visio Comp, Barcelona 08193, Spain; [Ciompi, Francesco] Univ Autonoma Barcelona, Dept Ciencies Comp, Bellaterra 08193, Spain; [Ciompi, Francesco] Ctr Visio Comp, Bellaterra 08193, Spain	Centre de Visio per Computador (CVC); Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Gatta, C (corresponding author), Ctr Visio Comp, Edifici O,Campus UAB, Barcelona 08193, Spain.	cgatta@cvc.uab.es; francesco.ciompi@radboudumc.nl	Ciompi, Francesco/P-5598-2015	Ciompi, Francesco/0000-0001-8327-9606	MICINN	MICINN(Ministry of Science and Innovation, Spain (MICINN)Spanish GovernmentEuropean Commission)	The authors want to thank the anonymous reviewers, J. van de Weijer and F. Vilarino for valuable comments and discussions. The work of Carlo Gatta was supported by MICINN under a Ramon y Cajal Fellowship. This paper is dedicated to the memory of Carlo Gatta's father Annibale Gatta.	Boix X, 2012, INT J COMPUT VISION, V96, P83, DOI 10.1007/s11263-011-0449-8; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Bulo SR, 2012, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2012.6248096; Carreira J, 2012, INT J COMPUT VISION, V98, P243, DOI 10.1007/s11263-011-0507-2; Chen YW, 2006, STUD FUZZ SOFT COMP, V207, P315; Ciompi F., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P710, DOI 10.1109/ICPR.2010.179; Cohen WW, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P671; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; Florack L, 1996, INT J COMPUT VISION, V18, P61, DOI 10.1007/BF00126140; Floros G., 2011, P BRIT MACH VIS C BM, P1; Frohlich B., 2012, 11 AS C COMP VIS DAE, P218; Gatta C, 2011, PATTERN RECOGN, V44, P2414, DOI 10.1016/j.patcog.2011.04.003; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Jiang JY, 2009, PROC CVPR IEEE, P1810, DOI 10.1109/CVPRW.2009.5206761; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Kontschieder P., 2012, P NEUR INF PROC SYST, V25, P440; Kontschieder P, 2011, IEEE I CONF COMP VIS, P2190, DOI 10.1109/ICCV.2011.6126496; Kou Z., 2007, P SIAM INT C DAT MIN; Ladicky L, 2010, LECT NOTES COMPUT SC, V6314, P424, DOI 10.1007/978-3-642-15561-1_31; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18; Lucchi A, 2011, IEEE I CONF COMP VIS, P9, DOI 10.1109/ICCV.2011.6126219; Maji S, 2013, IEEE T PATTERN ANAL, V35, P66, DOI 10.1109/TPAMI.2012.62; Martinovic A, 2012, LECT NOTES COMPUT SC, V7578, P416, DOI 10.1007/978-3-642-33786-4_31; Montillo A, 2011, LECT NOTES COMPUT SC, V6801, P184, DOI 10.1007/978-3-642-22092-0_16; Munoz D, 2010, LECT NOTES COMPUT SC, V6316, P57, DOI 10.1007/978-3-642-15567-3_5; Nowozin S, 2011, IEEE I CONF COMP VIS, P1668, DOI 10.1109/ICCV.2011.6126429; Qixing Huang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1953, DOI 10.1109/CVPR.2011.5995571; Seyedhosseini M, 2011, LECT NOTES COMPUT SC, V6891, P670, DOI 10.1007/978-3-642-23623-5_84; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Sturgess P., 2009, P BRIT MACH VIS C, P1; Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386; Weston J, 2001, ADV NEUR IN, V13, P668; Yang MY, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130243; Zhang CX, 2010, LECT NOTES COMPUT SC, V6314, P708, DOI 10.1007/978-3-642-15561-1_51; Zhang Q, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON ADVANCED FIBERS AND POLYMER MATERIALS, VOLS 1 AND 2, P497	41	5	5	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2014	36	8					1694	1700		10.1109/TPAMI.2013.2297706	http://dx.doi.org/10.1109/TPAMI.2013.2297706			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AM9HN	26353349	Green Published			2022-12-18	WOS:000340191900017
J	Zimmermann, K; Hurych, D; Svoboda, T				Zimmermann, Karel; Hurych, David; Svoboda, Tomas			Non-Rigid Object Detection with Local Interleaved Sequential Alignment (LISA)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Non-rigid object detection; alignment; regression; exploiting features; real-time; waldboost; sliding window; sequential decision process	FEATURES	This paper shows that the successively evaluated features used in a sliding window detection process to decide about object presence/absence also contain knowledge about object deformation. We exploit these detection features to estimate the object deformation. Estimated deformation is then immediately applied to not yet evaluated features to align them with the observed image data. In our approach, the alignment estimators are jointly learned with the detector. The joint process allows for the learning of each detection stage from less deformed training samples than in the previous stage. For the alignment estimation we propose regressors that approximate non-linear regression functions and compute the alignment parameters extremely fast.	[Zimmermann, Karel; Hurych, David; Svoboda, Tomas] Czech Tech Univ, Fac Elect Engn, Ctr Machine Percept, Dept Cybernet, Prague 12135, Czech Republic	Czech Technical University Prague	Zimmermann, K (corresponding author), Czech Tech Univ, Fac Elect Engn, Ctr Machine Percept, Dept Cybernet, Prague 12135, Czech Republic.	zimmerk@cmp.felk.cvut.cz; hurycd1@cmp.felk.cvut.cz; svoboda@cmp.felk.cvut.cz	Svoboda, Tomas/H-1627-2012; Zimmermann, Karel/G-7963-2014; Zimmermann, Karel/AAF-1596-2021	Svoboda, Tomas/0000-0002-7184-1785; Zimmermann, Karel/0000-0002-8898-4512; 	Czech Science Foundation [P103/11/P700, P103/10/1585]; EC [FP7-ICT-247870 NIFTi]	Czech Science Foundation(Grant Agency of the Czech Republic); EC(European CommissionEuropean Commission Joint Research Centre)	Karel Zimmermann and David Hurych were supported by the Czech Science Foundation Projects P103/11/P700 and P103/10/1585, respectively. Tomas Svoboda was supported by EC project FP7-ICT-247870 NIFTi. Any opinions expressed in this paper do not necessarily reflect the views of the European Community. The Community is not liable for any use that may be made of the information contained herein.	Ali K, 2012, IEEE T PATTERN ANAL, V34, P225, DOI 10.1109/TPAMI.2011.117; Amit Y, 2007, INT J COMPUT VISION, V75, P267, DOI 10.1007/s11263-006-0033-9; Babenko B., 2008, P EUR C COMP VIS ECC; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310; Buhlmann P, 2007, STAT SCI, V22, P477, DOI 10.1214/07-STS242; Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dollar P, 2012, LECT NOTES COMPUT SC, V7573, P645, DOI 10.1007/978-3-642-33709-3_46; Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094; Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906; Fleuret F, 2008, J MACH LEARN RES, V9, P2549; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Huang C, 2005, IEEE I CONF COMP VIS, P446; Koestinger M., 2011, P IEEE 1 INT WORKSH; Kokkinos I., 2011, ADV NEURAL INFORM PR, VVol. 24, P2681; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68; Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Pedersoli M, 2011, PROC CVPR IEEE, P1353, DOI 10.1109/CVPR.2011.5995668; Penrose R., 1955, P CAMBRIDGE PHILOS S, V51, P406, DOI [10.1017/S0305004100030401, DOI 10.1017/S0305004100030401]; SCHAPIRE RE, 2001, P MSRI WORKSH NONL E; Shrestha DL, 2006, NEURAL COMPUT, V18, P1678, DOI 10.1162/neco.2006.18.7.1678; Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055; Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734; Viola P, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P747; Viola P., 2003, TR200396 MERL; Wu H, 2008, PROC CVPR IEEE, P3200; Xiaoming L., 2007, P IEEE C COMP VIS PA, P1; Zhang J., 2007, P IEEE C COMP VIS PA; Zhu X., 2012, P IEEE C COMP VIS PA; Zimmermann K., 2011, 2011 5th International Conference on Automation, Robotics and Applications (ICARA 2011), P196, DOI 10.1109/ICARA.2011.6144881; Zimmermann K., 2012, P AS C COMP VIS ACCV, P437; Zimmermann K, 2009, IEEE T PATTERN ANAL, V31, P677, DOI 10.1109/TPAMI.2008.119	37	5	5	0	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2014	36	4					731	743		10.1109/TPAMI.2013.171	http://dx.doi.org/10.1109/TPAMI.2013.171			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AE6MX	26353196	hybrid, Green Submitted			2022-12-18	WOS:000334109000008
J	Chen, L				Chen, Liang			A Fair Comparison Should Be Based on the Same Protocol-Comments on "Trainable Convolution Filters and Their Application to Face Recognition"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Face recognition; filtering classifier; Volterra kernels; Volterrafaces		We comment on a paper describing an image classification approach called Volterra kernel classifier, which was called Volterrafaces when applied to face recognition. The performances were evaluated by the experiments on face recognition databases. We find that their comparisons with the state of the art of three databases were indeed based on unfair settings. The results with the settings of the standard protocol on three data sets are generated, which show that Volterrafaces achieves the state-of-the-art performance only in one database.	[Chen, Liang] Wenzhou Univ Adjunct, Coll Math & Informat Sci, Wenzhou, Zhejiang, Peoples R China; [Chen, Liang] Univ No British Columbia, Dept Comp Sci, Prince George, BC V2L 5P2, Canada	University of Northern British Columbia	Chen, L (corresponding author), Wenzhou Univ Adjunct, Coll Math & Informat Sci, Wenzhou, Zhejiang, Peoples R China.	lchen@ieee.org			Wenzhou University; NSERC Discovery Grant of Canada	Wenzhou University; NSERC Discovery Grant of Canada	The author receives an innovation grant from Wenzhou University for being an adjunct professor; the research is also supported by an NSERC Discovery Grant of Canada.	An S., 2008, P IEEE C COMP VIS PA; An S, 2007, P IEEE C COMP VIS PA; Cai D, 2007, P IEEE INT C COMP VI; Cai D., 2007, P IEEE C COMP VIS PA; Cai D, 2008, IEEE T KNOWL DATA EN, V20, P1, DOI 10.1109/TKDE.2007.190669; Cai D, 2006, IEEE T IMAGE PROCESS, V15, P3608, DOI 10.1109/TIP.2006.881945; Chen L, 2010, ARTIF INTELL REV, V33, P107, DOI 10.1007/s10462-009-9139-0; Fu Y, 2008, IEEE T IMAGE PROCESS, V17, P226, DOI 10.1109/TIP.2007.914203; HUA G, 2007, P IEEE C COMP VIS PA; Kumar R, 2012, IEEE T PATTERN ANAL, V34, P1423, DOI 10.1109/TPAMI.2011.225; Kumar R, 2009, PROC CVPR IEEE, P150, DOI 10.1109/CVPRW.2009.5206837; PHAM D.-S., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587408; Shan HG, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.955; Wang F, 2007, P IEEE C COMP VIS PA	14	5	6	0	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2014	36	3					622	623		10.1109/TPAMI.2013.187	http://dx.doi.org/10.1109/TPAMI.2013.187			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AA9YX	24101333	hybrid			2022-12-18	WOS:000331450100018
J	Yamada, M; Sigal, L; Raptis, M				Yamada, Makoto; Sigal, Leonid; Raptis, Michalis			Covariate Shift Adaptation for Discriminative 3D Pose Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional pose estimation; covariate shift adaptation; importance weight estimation; twin Gaussian processes		Discriminative, or (structured) prediction, methods have proved effective for variety of problems in computer vision; a notable example is 3D monocular pose estimation. All methods to date, however, relied on an assumption that training (source) and test (target) data come from the same underlying joint distribution. In many real cases, including standard data sets, this assumption is flawed. In the presence of training set bias, the learning results in a biased model whose performance degrades on the (target) test set. Under the assumption of covariate shift, we propose an unsupervised domain adaptation approach to address this problem. The approach takes the form of training instance reweighting, where the weights are assigned based on the ratio of training and test marginals evaluated at the samples. Learning with the resulting weighted training samples alleviates the bias in the learned models. We show the efficacy of our approach by proposing weighted variants of kernel regression (KR) and twin Gaussian processes (TGP). We show that our weighted variants outperform their unweighted counterparts and improve on the state-of-the-art performance in the public (HUMANEVA) data set.	[Sigal, Leonid] Disney Res Pittsburgh, Pittsburgh, PA 15213 USA			makotoy@yahoo-inc.com; lsigal@disneyresearch.com; mraptis@cable.comcast.com						Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; [Anonymous], 2006, PROC IEEE COMPUT SOC; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Bergamo A., 2010, ADV NEURAL INFORM PR, P181; Bissacco A., 2007, 2007 IEEE C COMP VIS, P1, DOI [10.1109/CVPR.2007.383129, DOI 10.1109/CVPR.2007.383129]; Black M. J., 2007, NIPS; Bo LF, 2008, PROC CVPR IEEE, P1833; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Ek C. H., 2007, MACHINE LEARNING MUL, P132; Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Ionescu C, 2009, IEEE I CONF COMP VIS, P1157, DOI 10.1109/ICCV.2009.5459346; Kanamori T, 2009, J MACH LEARN RES, V10, P1391; Kanaujia A., 2007, P IEEE C COMP VIS PA, P1; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Navaratnam R, 2007, IEEE I CONF COMP VIS, P1160; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Rosales R, 2002, ADV NEUR IN, V14, P1263; Rosales R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P378, DOI 10.1109/ICCV.2001.937543; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Salzmann Mathieu, 2010, P 13 INT C ART INT S, P701; Scholkopf B., 2002, LEARNING KERNELS; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Sigal Leonid, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2852, DOI 10.1109/CVPRW.2009.5206576; SIGAL L, 2006, CS0608 BROWN U; Sminchisescu C, 2005, PROC CVPR IEEE, P390; Sminchisescu Cristian, 2006, P INT C COMP VIS PAT, P1743; Stark M, 2009, IEEE I CONF COMP VIS, P373, DOI 10.1109/ICCV.2009.5459231; Sugiyama, 2011, ADV NEURAL INFORM PR, V25, P594; Sugiyama M., 2008, NIPS, P1433; Sugiyama M, 2007, J MACH LEARN RES, V8, P985; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Urtasun R, 2008, PROC CVPR IEEE, P149; Yamada M., 2012, P EUR C COMP VIS ECC; Yang HY, 2005, IEEE COMP SOC ANN, P71; Zhao X, 2008, INT C PATT RECOG, P2006	40	5	5	0	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2014	36	2					235	247		10.1109/TPAMI.2013.123	http://dx.doi.org/10.1109/TPAMI.2013.123			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	278OL	24356346				2022-12-18	WOS:000328899500003
J	Jacobs, N; Abrams, A; Pless, R				Jacobs, Nathan; Abrams, Austin; Pless, Robert			Two Cloud-Based Cues for Estimating Scene Structure and Camera Calibration	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Time lapse; depth map; nonmetric multidimensional scaling; image formation; shape from shadows; clouds		We describe algorithms that use cloud shadows as a form of stochastically structured light to support 3D scene geometry estimation. Taking video captured from a static outdoor camera as input, we use the relationship of the time series of intensity values between pairs of pixels as the primary input to our algorithms. We describe two cues that relate the 3D distance between a pair of points to the pair of intensity time series. The first cue results from the fact that two pixels that are nearby in the world are more likely to be under a cloud at the same time than two distant points. We describe methods for using this cue to estimate focal length and scene structure. The second cue is based on the motion of cloud shadows across the scene; this cue results in a set of linear constraints on scene structure. These constraints have an inherent ambiguity, which we show how to overcome by combining the cloud motion cue with the spatial cue. We evaluate our method on several time lapses of real outdoor scenes.	[Jacobs, Nathan] Univ Kentucky, Dept Comp Sci, Lexington, KY 40506 USA; [Abrams, Austin; Pless, Robert] Washington Univ, Dept Comp Sci, St Louis, MO 63130 USA	University of Kentucky; Washington University (WUSTL)	Jacobs, N (corresponding author), Univ Kentucky, Dept Comp Sci, 329 Rose St, Lexington, KY 40506 USA.	jacobs@cs.uky.edu	Jacobs, Nathan/AAC-3392-2019	Jacobs, Nathan/0000-0002-4242-8967	US National Science Foundation (NSF) [IIS-0546383, IIS-1111398]; US Defense Advanced Research Projects Agency (DARPA) [D11AP00255]; Emerging Frontiers [1065029, 1065074] Funding Source: National Science Foundation	US National Science Foundation (NSF)(National Science Foundation (NSF)); US Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Emerging Frontiers(National Science Foundation (NSF)NSF - Directorate for Biological Sciences (BIO))	The authors gratefully acknowledge the support of US National Science Foundation (NSF) grants (IIS-0546383, IIS-1111398) and US Defense Advanced Research Projects Agency (DARPA) grant (D11AP00255), which partially supported this work.	Billock VA, 2000, PHYSICA D, V137, P379, DOI 10.1016/S0167-2789(99)00197-9; BORG I., 2005, MODERN MULTIDIMENSIO, P207; BURTON GJ, 1987, APPL OPTICS, V26, P157, DOI 10.1364/AO.26.000157; Dare PM, 2005, PHOTOGRAMM ENG REM S, V71, P169, DOI 10.14358/PERS.71.2.169; Horprasert T., 1999, P IEEE INT C CCOMP V; Jacobs N., 2010, P IEEE C COMP VIS PA; Jacobs N., 2007, P IEEE C COMP VIS PA; Jacobs N., 2007, P IEEE INT C COMP VI; Kenney J., 1954, MATH STAT, V3; Kim S. J., 2008, P IEEE C COMP VIS PA; Koppal SJ, 2009, IEEE T PATTERN ANAL, V31, P1375, DOI 10.1109/TPAMI.2008.148; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Lalonde JF, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618477; LALONDE JFN, 2008, P EUR C COMP VIS; Lovejoy S., 1985, WATER RESOURCES RES; Mittal A, 2009, COMPUT VIS IMAGE UND, V113, P63, DOI 10.1016/j.cviu.2008.07.004; Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520; Stauffer C., 1999, P IEEE COMP SOC C CO, V2; Sun C.-H, 2000, P ATM RAD MEAS SCI T; Sunkavalli K., 2008, P IEEE C COMP VIS PA; Sunkavalli K, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239552, 10.1145/1276377.1276504]; THORNE LR, 1997, SAND979252 SAND NAT	22	5	5	0	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2013	35	10					2526	2538		10.1109/TPAMI.2013.55	http://dx.doi.org/10.1109/TPAMI.2013.55			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	201XB	23969394				2022-12-18	WOS:000323175200016
J	Twining, CJ; Taylor, CJ				Twining, Carole J.; Taylor, Christopher J.			Specificity: A Graph-Based Estimator of Divergence	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Specificity; generalization; assessment of modeling; graph-based estimators; entropy estimation; estimation of statistical distance; estimation of divergence; nearest-neighbor estimators; cross entropy; Kullback-Leibler divergence	SHAPE; DISTANCE; MODELS	In statistical modeling, there are various techniques used to build models from training data. Quantitative comparison of modeling techniques requires a method for evaluating the quality of the fit between the model probability density function (pdf) and the training data. One graph-based measure that has been used for this purpose is the specificity. We consider the large-numbers limit of the specificity, and derive expressions which show that it can be considered as an estimator of the divergence between the unknown pdf from which the training data was drawn and the model pdf built from the training data. Experiments using artificial data enable us to show that these limiting large-number relations enable us to obtain good quantitative and qualitative predictions of the behavior of the measured specificity, even for small numbers of training examples and in some extreme cases. We demonstrate that specificity can provide a more sensitive measure of difference between various modeling methods than some previous graph-based techniques. Key points are illustrated using real data sets. We thus establish a proper theoretical basis for the previously ad hoc concept of specificity, and obtain useful insights into the application of specificity in the analysis of real data.	[Twining, Carole J.; Taylor, Christopher J.] Univ Manchester, Imaging Sci Res Grp, Manchester M13 9PT, Lancs, England	University of Manchester	Twining, CJ (corresponding author), Univ Manchester, Imaging Sci Res Grp, Stopford Bldg,Oxford Rd, Manchester M13 9PT, Lancs, England.	carole.twining@manchester.ac.uk; chris.taylor@manchester.ac.uk	Taylor, Christopher J/A-3909-2009; Twining, Carole J/F-7423-2012	Taylor, Christopher J/0000-0001-7867-9533; 				ALI SM, 1966, J ROY STAT SOC B, V28, P131; Beardwood J., 1959, MATH PROC CAMBRIDGE, V55, P299, DOI [10.1017/s0305004100034095, DOI 10.1017/S0305004100034095, 10.1017\/S0305004100034095]; Bhattacharyya A., 1943, BULL CALCUTTA MATH S, V35, P99; Brakke K.A., 2005, STAT RANDOM PLANE VO; BRECHBUHLER C, 1995, COMPUT VIS IMAGE UND, V61, P154, DOI 10.1006/cviu.1995.1013; Cootes T. F., 1992, BMVC92. Proceedings of the British Machine Vision Conference, P9; Cootes TF, 2008, IMAGE VISION COMPUT, V26, P326, DOI 10.1016/j.imavis.2006.12.005; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Craw I., 1991, P BRIT MACH VIS C, V91, P367, DOI [https://doi.org/10.5244/c.5.52, DOI 10.1007/978-1-4471-1921-0_52, 10.1007/978-1-4471-1921-0_52]; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; Davies R, 2008, STAT MODELS SHAPE OP; Davies RH, 2003, LECT NOTES COMPUT SC, V2732, P38; Fu YL, 2010, COMPUT METH PROG BIO, V97, P199, DOI 10.1016/j.cmpb.2009.06.003; Hellinger E, 1909, J REINE ANGEW MATH, V136, P210, DOI 10.1515/crll.1909.136.210; Hero A, 1999, PROCEEDINGS OF THE IEEE SIGNAL PROCESSING WORKSHOP ON HIGHER-ORDER STATISTICS, P264, DOI 10.1109/HOST.1999.778739; Hero A. O., 2001, CSPL328 U MICH; Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; LANITIS A, 1994, ELECTRON LETT, V30, P1587, DOI 10.1049/el:19941110; Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539; LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079; MATUSITA K, 1955, ANN MATH STAT, V26, P631, DOI 10.1214/aoms/1177728422; Michel O. J. J., 2000, Traitement du Signal, V17, P287; Miller EG, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P297; Pizer SM, 1998, COMPUT VIS IMAGE UND, V69, P55, DOI 10.1006/cviu.1997.0563; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Rueckert D, 2003, IEEE T MED IMAGING, V22, P1014, DOI 10.1109/TMI.2003.815865; Schestowitz R, 2006, I S BIOMED IMAGING, P836; Shi L, 2007, LECT NOTES COMPUT SC, V4792, P818; Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8; Steele J.M., 1997, PROBABILITY THEORY C; STEELE JM, 1988, ANN PROBAB, V16, P1767, DOI 10.1214/aop/1176991596; Styner MA, 2003, LECT NOTES COMPUT SC, V2732, P63; TWINING C, 2006, P BRIT MACH VIS C ED, V2, P459; Wade AR, 2007, ADV APPL PROBAB, V39, P326, DOI 10.1239/aap/1183667613; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060	39	5	5	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2011	33	12					2492	2505		10.1109/TPAMI.2011.90	http://dx.doi.org/10.1109/TPAMI.2011.90			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	834RE	21576742				2022-12-18	WOS:000295980000014
J	Xu, YL; Roy-Chowdhury, AK				Xu, Yilei; Roy-Chowdhury, Amit K.			A Physics-Based Analysis of Image Appearance Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image appearance models; theoretical analysis; multilinear; deformation; face tracking	SHAPE; RECOGNITION; MOTION	Linear and multilinear models (PCA, 3DMM, AAM/ASM, and multilinear tensors) of object shape/appearance have been very popular in computer vision. In this paper, we analyze the applicability of these heuristic models from the fundamental physical laws of object motion and image formation. We prove that under suitable conditions, the image appearance space can be closely approximated to be multilinear, with the illumination and texture subspaces being trilinearly combined with the direct sum of the motion and deformation subspaces. This result provides a physics-based understanding of many of the successes and limitations of the linear and multilinear approaches existing in the computer vision literature, and also identifies some of the conditions under which they are valid. It provides an analytical representation of the image space in terms of different physical factors that affect the image formation process. Numerical analysis of the accuracy of the physics-based models is performed, and tracking results on real data are presented.	[Xu, Yilei] Navteq Corp, Chicago, IL 60606 USA; [Roy-Chowdhury, Amit K.] Univ Calif Riverside, Dept Elect Engn, Riverside, CA 92521 USA	Nokia Corporation; Nokia Bell Labs; University of California System; University of California Riverside	Xu, YL (corresponding author), Navteq Corp, 425 W Randolph St, Chicago, IL 60606 USA.	pkuelija@hotmail.com; amitrc@ee.ucr.edu	Xu, Yilei/F-5095-2012	Roy-Chowdhury, Amit/0000-0001-6690-9725	US National Science Foundation (NSF) [IIS 0712253]	US National Science Foundation (NSF)(National Science Foundation (NSF))	The work was partly supported under US National Science Foundation (NSF) grant IIS 0712253.	Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; BELHUMEUR P, 1996, P IEEE C COMP VIS PA; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; BROIDA TJ, 1991, IEEE T PATTERN ANAL, V13, P497, DOI 10.1109/34.87338; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Costantini R, 2008, IEEE T IMAGE PROCESS, V17, P42, DOI 10.1109/TIP.2007.910956; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; Doretto G, 2006, IEEE T PATTERN ANAL, V28, P2006, DOI 10.1109/TPAMI.2006.243; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Landau L.D., 1976, MECHANICS; LATHAUWER LD, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI DOI 10.1137/S0895479896305696; LEE C, 2003, P IEEE C COMP VIS PA, V1, P313; Lee KC, 2003, PROC CVPR IEEE, P313; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; MOSES Y, 1993, THESIS WEIZMANN I SC; OLIENSIS J, 1991, P SPIE C 1570 GEOM M; RAMAMOORTHI R, 2001, J OPTICAL SOC AM, V18; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sapiro G., 2001, GEOMETRIC PARTIAL DI; Shashua A, 1997, INT J COMPUT VISION, V21, P99, DOI 10.1023/A:1007975506780; Shirdhonkar S, 2005, IEEE I CONF COMP VIS, P1323; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L., 2002, P EUR C COMP VIS; VASILESCU M, 2005, IEEE CS C COMP VIS P; XU Y, 2008, P IEEE INT C COMP VI; Xu YL, 2007, IEEE T PATTERN ANAL, V29, P793, DOI 10.1109/TPAMI.2007.1047; Yang H, 2007, ADV INTEL SYS RES, DOI 10.2991/iske.2007.75; Zhang L., 2003, P IEEE INT C COMP VI	32	5	5	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2011	33	8					1681	1688		10.1109/TPAMI.2010.216	http://dx.doi.org/10.1109/TPAMI.2010.216			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	779UH	21135432				2022-12-18	WOS:000291807200015
J	Lerner, R; Rivlin, E				Lerner, Ronen; Rivlin, Ehud			Direct Method for Video-Based Navigation Using a Digital Terrain Map	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pose estimation; vision-based navigation; direct methods; DTM	DIRECT RECOVERY; MOTION; PARALLAX	A novel vision-based navigation algorithm is proposed. The gray levels of two images, together with a Digital Terrain Map (DTM), are directly utilized to define constraints on the navigation parameters. The feasibility of the algorithm is examined both under a simulated environment and using real flight data.	[Lerner, Ronen] Adv Tech Ctr Israel, Vis Syst Grp, IL-46725 Herzliyya, Israel; [Rivlin, Ehud] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Lerner, R (corresponding author), Adv Tech Ctr Israel, Vis Syst Grp, IL-46725 Herzliyya, Israel.	ronen.lerner@gm.com; ehudr@cs.technion.ac.il						Barron JL, 1996, PATTERN RECOGN, V29, P797, DOI 10.1016/0031-3203(95)00114-X; Foley James D, 1996, COMPUTER GRAPHICS PR, V12110; GRUEN A, 1985, REMOTE SENSING CARTO, V14, P175; Hoaglin D.C., 1983, UNDERSTANDING ROBUST; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; Irani M, 2002, IEEE T PATTERN ANAL, V24, P1528, DOI 10.1109/TPAMI.2002.1046174; KUMAR R, 1994, INT C PATT RECOG, P685, DOI 10.1109/ICPR.1994.576402; Lerner R, 2004, PROC CVPR IEEE, P604; Lerner R, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P2251, DOI 10.1109/IROS.2006.282569; Lerner R, 2006, IEEE T PATTERN ANAL, V28, P1404, DOI 10.1109/TPAMI.2006.192; Makadia A, 2005, IEEE INT CONF ROBOT, P3534; Sawhney HS, 1996, IEEE T PATTERN ANAL, V18, P814, DOI 10.1109/34.531801; Sim DG, 2002, IEEE T PATTERN ANAL, V24, P1, DOI 10.1109/34.982881; SZELISKI R, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P194, DOI 10.1109/CVPR.1994.323829; Tian TY, 1996, PROC CVPR IEEE, P315, DOI 10.1109/CVPR.1996.517091	15	5	5	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2011	33	2					406	411		10.1109/TPAMI.2010.171	http://dx.doi.org/10.1109/TPAMI.2010.171			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	694QR	20820078				2022-12-18	WOS:000285313200015
J	Davy, M; Tourneret, JY				Davy, Manuel; Tourneret, Jean-Yves			Generative Supervised Classification Using Dirichlet Process Priors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Supervised classification; Bayesian inference; Gibbs sampler; Dirichlet processes; altimetric signals	BAYESIAN-ANALYSIS; INFERENCE; MIXTURES	Choosing the appropriate parameter prior distributions associated to a given Bayesian model is a challenging problem. Conjugate priors can be selected for simplicity motivations. However, conjugate priors can be too restrictive to accurately model the available prior information. This paper studies a new generative supervised classifier which assumes that the parameter prior distributions conditioned on each class are mixtures of Dirichlet processes. The motivations for using mixtures of Dirichlet processes is their known ability to model accurately a large class of probability distributions. A Monte Carlo method allowing one to sample according to the resulting class-conditional posterior distributions is then studied. The parameters appearing in the class-conditional densities can then be estimated using these generated samples (following Bayesian learning). The proposed supervised classifier is applied to the classification of altimetric waveforms backscattered from different surfaces (oceans, ices, forests, and deserts). This classification is a first step before developing tools allowing for the extraction of useful geophysical information from altimetric waveforms backscattered from nonoceanic surfaces.	[Davy, Manuel] VEKIA, F-59000 Lille, France; [Tourneret, Jean-Yves] Univ Toulouse, ENSEEIHT IRIT TeSA, F-31071 Toulouse 7, France	Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite de Toulouse; Institut National Polytechnique de Toulouse	Davy, M (corresponding author), VEKIA, 165 Ave Bretagne, F-59000 Lille, France.	mdavy@vekia.fr; jean-yves.tourneret@enseeiht.fr						Anderson T. W., 2003, INTRO MULTIVARIATE S; ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; BAKER S, 2002, ENVISAT RA 2 MWR PRO; BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; BROWN GS, 1977, IEEE T ANTENN PROPAG, V25, P67, DOI [10.1109/JOE.1977.1145328, 10.1109/TAP.1977.1141536]; Caron F, 2008, IEEE T SIGNAL PROCES, V56, P71, DOI 10.1109/TSP.2007.900167; Davy M, 2002, IEEE T SIGNAL PROCES, V50, P377, DOI 10.1109/78.978392; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; DUMONT JP, 2001, ALT RET OCE 02 PERFO; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; FRALEY C, 2005, 05486 U WASH DEP STA; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Gelman A., 2004, BAYESIAN DATA ANAL, V2nd, DOI DOI 10.1201/9780429258411; Herbrich R., 2002, LEARNING KERNEL CLAS; Jackson E., 2007, P IEEE INT C AC SPEE; Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; KONONENKO I, 1991, LECT NOTES ARTIF INT, V482, P206, DOI 10.1007/BFb0017015; MCCALLUM A, 1998, P NAT C ART INT WORK, V752; Muller P, 2004, STAT SCI, V19, P95, DOI 10.1214/088342304000000017; Neal RM, 1998, 9815 U TOR DEP STAT; Oliver C., 1998, UNDERSTANDING SYNTHE; Raina R., 2004, ADV NEURAL INFORM PR, V16; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; Schneiderman H, 2000, PROC CVPR IEEE, P746, DOI 10.1109/CVPR.2000.855895; SETHURAMAN J, 1994, STAT SINICA, V4, P639; SEVERINI J, 2008, P IEEE INT GEOSC REM; SHAHBABA B, 2007, 0707 U TOR DEP STAT; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Theodoridis S., 2008, PATTERN RECOGN; TOURNERET JY, 2008, P IEEE INT GEOSC REM	34	5	8	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2010	32	10					1781	1794		10.1109/TPAMI.2010.21	http://dx.doi.org/10.1109/TPAMI.2010.21			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	639US	20724756				2022-12-18	WOS:000281000700005
J	Derpanis, KG; Wildes, RP				Derpanis, Konstantinos G.; Wildes, Richard P.			The Structure of Multiplicative Motions in Natural Imagery	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Multiplicative motion; translucency; dynamic occlusion; pseudotransparency; non-Fourier motion; spectral analysis; optical flow; multiple motion		A theoretical investigation of the frequency structure of multiplicative image motion signals is presented, e. g., as associated with translucency phenomena. Previous work has claimed that the multiplicative composition of visual signals generally results in the annihilation of oriented structure in the spectral domain. As a result, research has focused on multiplicative signals in highly specialized scenarios where highly structured spectral signatures are prevalent, or introduced a nonlinearity to transform the multiplicative image signal to an additive one. In contrast, in this paper, it is shown that oriented structure is present in multiplicative cases when natural domain constraints are taken into account. This analysis suggests that the various instances of naturally occurring multiple motion structures can be treated in a unified manner. As an example application of the developed theory, a multiple motion estimator previously proposed for translation, additive transparency, and occlusion is adapted to multiplicative image motions. This estimator is shown to yield superior performance over the alternative practice of introducing a nonlinear preprocessing step.	[Derpanis, Konstantinos G.; Wildes, Richard P.] York Univ, Dept Comp Sci & Engn, Toronto, ON M3J 1P3, Canada; [Derpanis, Konstantinos G.; Wildes, Richard P.] York Univ, Ctr Vis Res CVR, Toronto, ON M3J 1P3, Canada	York University - Canada; York University - Canada	Derpanis, KG (corresponding author), York Univ, Dept Comp Sci & Engn, CSB 1003,4700 Keele St, Toronto, ON M3J 1P3, Canada.	kosta@cse.yorku.ca; wildes@cse.yorku.ca			Natural Sciences and Engineering Research Council of Canada (NSERC)	Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	This research was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC). The authors thank M. Spetsakis and the anonymous reviewers for their helpful feedback.	ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; Adelson EH, 1990, P AAAI 90 WORKSH QUA, P77; AUSTVOLL I, 2005, P SCAND C IM AN; Baker S., 2007, P IEEE INT C COMP VI; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Beauchemin SS, 2000, IEEE T PATTERN ANAL, V22, P200, DOI 10.1109/34.825758; BERGEN JR, 1992, IEEE T PATTERN ANAL, V14, P886, DOI 10.1109/34.161348; Bracewell R., 1999, FOURIER TRANSFORM IT, P46; Brodatz P., 1966, TEXTURES; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Derpanis K., 2009, P AS C COMP VIS; Derpanis K., 2009, P IEEE C COMP VIS PA; FAHLE M, 1981, PROC R SOC SER B-BIO, V213, P451, DOI 10.1098/rspb.1981.0075; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; FLEET DJ, 1994, VISION RES, V34, P3057, DOI 10.1016/0042-6989(94)90278-X; Fleet DJ, 1992, MEASUREMENT IMAGE VE; HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837; HEEGER DJ, 1987, J OPT SOC AM A, V4, P1455, DOI 10.1364/JOSAA.4.001455; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Jepson A., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P760, DOI 10.1109/CVPR.1993.341161; Kersten D., 1991, COMPUTATIONAL MODELS, P209; Langley K, 1999, VISION RES, V39, P87, DOI 10.1016/S0042-6989(98)00093-5; Liu C., 2008, P IEEE C COMP VIS PA; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; Shizawa M., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P164, DOI 10.1109/WVM.1991.212811; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Szeliski R, 2000, PROC CVPR IEEE, P246, DOI 10.1109/CVPR.2000.855826; WATSON AB, 1985, J OPT SOC AM A, V2, P322, DOI 10.1364/JOSAA.2.000322; YU W, 1999, P COMP VIS PATT REC, P171; Yu WC, 2003, COMPUT VIS IMAGE UND, V90, P129, DOI 10.1016/S1077-3142(03)00011-0; Yu WC, 2002, IEEE T PATTERN ANAL, V24, P1286, DOI 10.1109/TPAMI.2002.1033220	32	5	5	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2010	32	7					1310	1316		10.1109/TPAMI.2010.64	http://dx.doi.org/10.1109/TPAMI.2010.64			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	595YC	20489232				2022-12-18	WOS:000277649100012
J	Garcia, HC; Villalobos, JR; Pan, R; Runger, GC				Garcia, Hugo C.; Villalobos, Jesus Rene; Pan, Rong; Runger, George C.			A Novel Feature Selection Methodology for Automated Inspection Systems	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature selection; misclassification error rate; quadratic discriminant function	DISCRIMINANT-ANALYSIS; CLASSIFICATION	This paper proposes a new feature selection methodology. The methodology is based on the stepwise variable selection procedure, but, instead of using the traditional discriminant metrics such as Wilks' Lambda, it uses an estimation of the misclassification error as the figure of merit to evaluate the introduction of new features. The expected misclassification error rate (MER) is obtained by using the densities of a constructed function of random variables, which is the stochastic representation of the conditional distribution of the quadratic discriminant function estimate. The application of the proposed methodology results in significant savings of computational time in the estimation of classification error over the traditional simulation and cross-validation methods. One of the main advantages of the proposed method is that it provides a direct estimation of the expected misclassification error at the time of feature selection, which provides an immediate assessment of the benefits of introducing an additional feature into an inspection/classification algorithm.	[Garcia, Hugo C.] L3, Electroopt Syst, Tempe, AZ 85281 USA; [Villalobos, Jesus Rene; Pan, Rong; Runger, George C.] Arizona State Univ, Dept Ind Engn, Tempe, AZ 85287 USA	Arizona State University; Arizona State University-Tempe	Garcia, HC (corresponding author), L3, Electroopt Syst, 1215 S 52nd St, Tempe, AZ 85281 USA.	Hugo.Garcia@L-3com.com; rene.villalobos@asu.edu; rong.pan@asu.edu; runger@asu.edu	Villalobos, Jesus Rene/F-8010-2010	Villalobos, Jesus Rene/0000-0003-2530-9760	US National Science Foundation [DMI-0300361]	US National Science Foundation(National Science Foundation (NSF))	The authors would like to acknowledge the support provided by the US National Science Foundation through grant DMI-0300361 for the realization of this research. They would also like to acknowledge the helpful suggestions of the anonymous reviewers of this paper.	Anderson T. W, 1984, INTRO MULTIVARIATE S; ASHIHARA M, 2006, P INT C ART NEUR N 2, P282; Duda Richard O., 2003, PATTERN CLASSIFICATI; GARCIA HC, 2008, THESIS ARIZONA STATE; GARCIA HC, 2007, P 5 INT C IND INF JU; Garcia HC, 2006, IEEE T AUTOM SCI ENG, V3, P394, DOI 10.1109/TASE.2006.877399; GUNYON I, 2005, FEATURE EXTRACTION F; Hua JP, 2005, PATTERN RECOGN, V38, P403, DOI 10.1016/j.patcog.2004.08.007; Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797; Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819; JENRICH R, 1997, STAT METHODS DIGITAL; Jiang SB, 2003, IEEE T AUTOMAT CONTR, V48, P369, DOI 10.1109/TAC.2003.809144; Kachigan S.K, 1982, MULTIVARIATE STAT AN; Khattree R., 2000, MULTIVARIATE DATA RE; Le CT., 1998, APPL CATEGORICAL DAT; Liu H, 2005, IEEE T KNOWL DATA EN, V17, P491, DOI 10.1109/TKDE.2005.66; LIU P, 2006, P INT C ADV DAT MIN, P457; Louw N, 2006, COMPUT STAT DATA AN, V51, P2043, DOI 10.1016/j.csda.2005.12.018; McFarland HR, 2002, J MULTIVARIATE ANAL, V82, P299, DOI 10.1006/jmva.2001.2034; MITRA P, 2002, IEEE T PATTERN ANAL, V24, P285; Park CY, 2008, COMPUT STAT DATA AN, V52, P3709, DOI 10.1016/j.csda.2007.12.011; RENCHER AC, 1993, BIOMETRICS, V49, P479, DOI 10.2307/2532560; Rencher AC, 2002, METHODS MULTIVARIATE; Saeys Y, 2007, BIOINFORMATICS, V23, P2507, DOI 10.1093/bioinformatics/btm344; SAMPATRAJ S, 2005, IIE T, V38, P309; Villalobos JR, 2003, J MANUF SYST, V22, P265; Wald A, 1944, ANN MATH STAT, V15, P145, DOI 10.1214/aoms/1177731280; Wei HL, 2007, IEEE T PATTERN ANAL, V29, P162, DOI 10.1109/TPAMI.2007.250607	29	5	5	0	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2009	31	7					1338	1344		10.1109/TPAMI.2008.276	http://dx.doi.org/10.1109/TPAMI.2008.276			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	447KB	19443930				2022-12-18	WOS:000266188900016
J	Domke, J; Aloimonos, Y				Domke, Justin; Aloimonos, Yiannis			Image Transformations and Blurring	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Reconstruction; restoration; sharpening and deblurring; smoothing	AFFINE; RECONSTRUCTION; SCALE; SHAPE	Since cameras blur the incoming light during measurement, different images of the same surface do not contain the same information about that surface. Thus, in general, corresponding points in multiple views of a scene have different image intensities. While multiple-view geometry constrains the locations of corresponding points, it does not give relationships between the signals at corresponding locations. This paper offers an elementary treatment of these relationships. We first develop the notion of "ideal" and "real" images, corresponding to, respectively, the raw incoming light and the measured signal. This framework separates the filtering and geometric aspects of imaging. We then consider how to synthesize one view of a surface from another; if the transformation between the two views is affine, it emerges that this is possible if and only if the singular values of the affine matrix are positive. Next, we consider how to combine the information in several views of a surface into a single output image. By developing a new tool called "frequency segmentation," we show how this can be done despite not knowing the blurring kernel.	[Domke, Justin; Aloimonos, Yiannis] Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Domke, J (corresponding author), Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.	domke@cs.umd.edu; yiannis@cfar.umd.edu	Aloimonos, Yiannis/AAI-2969-2020	Aloimonos, Yiannis/0000-0002-8152-4281	US National Science Foundation; European Union's Seventh Framework program on Cognitive Systems	US National Science Foundation(National Science Foundation (NSF)); European Union's Seventh Framework program on Cognitive Systems	The support of the US National Science Foundation under a collaborative project with Stanford University (EMT Bioinspired computing) and the European Union's Seventh Framework program on Cognitive Systems under the project POETICON is gratefully acknowledged.	BASCLE B, 1996, P EUR C COMP VIS, P573; BRACEWELL RN, 1993, ELECTRON LETT, V29, P304, DOI 10.1049/el:19930207; Bracewell RN., 1995, 2 DIMENSIONAL IMAGIN; Du H, 2004, APPL OPTICS, V43, P665, DOI 10.1364/AO.43.000665; Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669; Hartley R., 2004, ROBOTICA; JI H, 2006, P 9 EUR C COMP VIS; Lindeberg T, 1997, IMAGE VISION COMPUT, V15, P415, DOI 10.1016/S0262-8856(97)01144-X; LINDEBERG T, 1994, AXIOMATIC FDN LINEAR; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207; Ravela S, 2004, PROC CVPR IEEE, P725; STARCK J, 2002, DECONVOLUTION ASTRON; STONE JV, 1995, IEEE T PATTERN ANAL, V17, P713, DOI 10.1109/34.391414; STONE JV, 1990, P BRIT MACH VIS C, P181; Wang LF, 2001, PROC CVPR IEEE, P347	17	5	5	1	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2009	31	5					811	823		10.1109/TPAMI.2008.133	http://dx.doi.org/10.1109/TPAMI.2008.133			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	418JM	19299857				2022-12-18	WOS:000264144500004
J	Diaz, E; Sebastian, R; Ayala, G; Diaz, ME; Zoncu, R; Toomre, D; Gasman, S				Diaz, Ester; Sebastian, Rafael; Ayala, Guillermo; Elena Diaz, Maria; Zoncu, Roberto; Toomre, Derek; Gasman, Stephane			Measuring spatiotemporal dependencies in bivariate temporal random sets with applications to cell biology	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						random sets; Ripley K-function; pair correlation function; covariance function; clathrin mediated endocytosis	CLATHRIN-COATED PITS; MEDIATED ENDOCYTOSIS; BOOLEAN MODELS; BINDING; ACTIN; INVAGINATION; RECRUITMENT; MICROSCOPY; EPSIN; TIME	Analyzing spatiotemporal dependencies between different types of events is highly relevant to many biological phenomena (e. g., signaling and trafficking), especially as advances in probes and microscopy have facilitated the imaging of dynamic processes in living cells. For many types of events, the segmented areas can overlap spatially and temporally, forming random clumps. In this paper, we model the binary image sequences of two different event types as a realization of a bivariate temporal random set and propose a nonparametric approach to quantify spatial and spatiotemporal interrelations using the pair correlation, cross-covariance, and the Ripley K functions. Based on these summary statistics, we propose a randomization procedure to test independence between event types by applying random toroidal shifts and Monte Carlo tests. A simulation study assessed the performance of the proposed estimators and showed that these statistics capture the spatiotemporal dependencies accurately. The estimation of the spatiotemporal interval of interactions was also obtained. The method was successfully applied to analyze the interdependencies of several endocytic proteins using image sequences of living cells and validated the procedure as a new way to automatically quantify dependencies between proteins in a formal and robust manner.	[Diaz, Ester; Sebastian, Rafael; Elena Diaz, Maria] Univ Valencia, Dept Comp Sci, E-46100 Burjassot, Spain; [Ayala, Guillermo] Univ Valencia, Dept Estadist & Invest Operat, E-46100 Burjassot, Spain; [Toomre, Derek] Yale Univ, Dept Cell Biol, Sch Med, New Haven, CT 06520 USA; [Gasman, Stephane] Univ Strasbourg 1, Inst Neurosci Cellulaires & Integrat, Dept Neurotransmiss & Secret Neuroendocrine, CNRS, F-67084 Strasbourg, France	University of Valencia; University of Valencia; Yale University; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universites de Strasbourg Etablissements Associes; Universite de Strasbourg	Diaz, E (corresponding author), Univ Valencia, Dept Comp Sci, Avda Vicente Andres Estelles S-N, E-46100 Burjassot, Spain.	ester.diaz@uv.es; rafa.sebastian@uv.es; guillermo.ayala@uv.es; elena.diaz@uv.es; roberto.zoncu@yale.edu; derek.toomre@yale.edu; gasman@neurochem.u-strasbg.fr	Gasman, Stephane/D-5113-2017; Sebastian, Rafael/E-9386-2011; stéphane, gasman/Q-1145-2019; Ayala, Guillermo/N-5766-2019; Diaz, Maria Elena/H-5996-2011; Ayala, Guillermo/A-8077-2008	Gasman, Stephane/0000-0001-8415-1276; Sebastian, Rafael/0000-0001-6746-5740; stéphane, gasman/0000-0001-8415-1276; Ayala, Guillermo/0000-0002-6231-2865; Diaz, Maria Elena/0000-0002-6818-6943; Ayala, Guillermo/0000-0002-6231-2865				[Anonymous], 1988, STAT INFERENCE SPATI, DOI DOI 10.1017/CBO9780511624131; AYALA G, 1990, BIOMETRICAL J, V32, P73, DOI 10.1002/bimj.4710320112; Ayala G, 2006, IEEE T PATTERN ANAL, V28, P1707, DOI 10.1109/TPAMI.2006.199; Bellve KD, 2006, J BIOL CHEM, V281, P16139, DOI 10.1074/jbc.M511370200; Brett TJ, 2006, CURR OPIN CELL BIOL, V18, P395, DOI 10.1016/j.ceb.2006.06.014; Brodsky FM, 2001, ANNU REV CELL DEV BI, V17, P517, DOI 10.1146/annurev.cellbio.17.1.517; Chen H, 1998, NATURE, V394, P793, DOI 10.1038/29555; Diggle P., 2014, STAT ANAL SPATIAL SP; Edeling MA, 2006, NAT REV MOL CELL BIO, V7, P32, DOI 10.1038/nrm1786; Ehrlich M, 2004, CELL, V118, P591, DOI 10.1016/j.cell.2004.08.017; Engqvist-Goldstein AEY, 2001, J CELL BIOL, V154, P1209, DOI 10.1083/jcb.200106089; Epifanio I, 2002, IEEE T IMAGE PROCESS, V11, P859, DOI 10.1109/TIP.2002.801119; Ford MGJ, 2002, NATURE, V419, P361, DOI 10.1038/nature01020; Foxall R, 2002, J ROY STAT SOC C-APP, V51, P165, DOI 10.1111/1467-9876.00261; Goucher DR, 2005, BIOINFORMATICS, V21, P3248, DOI 10.1093/bioinformatics/bti531; Kaksonen M, 2005, CELL, V123, P305, DOI 10.1016/j.cell.2005.09.024; Matheron G., 1975, RANDOM SETS INTEGRAL; Merrifield CJ, 2002, NAT CELL BIOL, V4, P691, DOI 10.1038/ncb837; Merrifield CJ, 2005, CELL, V121, P593, DOI 10.1016/j.cell.2005.03.015; Metzler M, 2001, J BIOL CHEM, V276, P39271, DOI 10.1074/jbc.C100401200; MOLCHANOV I, 2005, THEORY RANDOM SETS P; *NAC CRESS, 1993, STAT SPAT DAT; Perera RM, 2006, P NATL ACAD SCI USA, V103, P19332, DOI 10.1073/pnas.0609795104; Rappoport JZ, 2003, J CELL SCI, V116, P847, DOI 10.1242/jcs.00289; Sebastian R, 2006, PATTERN RECOGN, V39, P2175, DOI 10.1016/j.patcog.2006.04.030; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Slepnev VI, 2000, NAT REV NEUROSCI, V1, P161, DOI 10.1038/35044540; Stoyan, 1994, FRACTALS RANDOM SHAP, V302; Stoyan D., 1995, STOCHASTIC GEOMETRY; Toomre D, 2001, TRENDS CELL BIOL, V11, P298, DOI 10.1016/S0962-8924(01)02027-X; Van Lieshout MNM, 1999, SCAND J STAT, V26, P511, DOI 10.1111/1467-9469.00165; Yao PJ, 2006, J COMP NEUROL, V494, P368, DOI 10.1002/cne.20810; Zoncu R, 2007, P NATL ACAD SCI USA, V104, P3793, DOI 10.1073/pnas.0611733104	33	5	5	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2008	30	9					1659	1671		10.1109/TPAMI.2007.70821	http://dx.doi.org/10.1109/TPAMI.2007.70821			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	324FZ	18617722				2022-12-18	WOS:000257504400012
J	Liu, CJ				Liu, Chengjun			Clarification of assumptions in the relationship between the Bayes decision rule and the whitened cosine similarity measure	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bayes decision rule; similarity measures; whitened cosine similarity measure		This paper first clarifies Assumption 3 (which misses a constant) and Assumption 4 (where the whitened pattern vectors represent the whitened means) in the paper "The Bayes Decision Rule Induced Similarity Measures" (IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 6, pp. 10861090, June 2007) and then provides examples to show that the assumptions after the clarification are consistent.	New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA	New Jersey Institute of Technology	Liu, CJ (corresponding author), New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA.	chengjun.liu@njit.edu						Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; FUKUNAGA K, 1990, INTRO STAT PATTERN; Liu CJ, 2007, IEEE T PATTERN ANAL, V29, P1086, DOI 10.1109/TPAMI.2007.1063; Loog M, 2008, IEEE T PATTERN ANAL, V30, P1114, DOI 10.1109/TPAMI.2007.70838; Mardia KV, 1979, MULTIVARIATE ANAL	5	5	5	2	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2008	30	6					1116	1117		10.1109/TPAMI.2008.66	http://dx.doi.org/10.1109/TPAMI.2008.66			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	286UW	18421116				2022-12-18	WOS:000254872500016
J	Corazza, A; Satta, G				Corazza, Anna; Satta, Giorgio			Probabilistic context-free grammars estimated from infinite distributions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article							FINITE-STATE MACHINES; MODELS	In this paper, we consider probabilistic context-free grammars, a class of generative devices that has been successfully exploited in several applications of syntactic pattern matching, especially in statistical natural language parsing. We investigate the problem of training probabilistic context-free grammars on the basis of distributions defined over an infinite set of trees or an infinite set of sentences by minimizing the cross-entropy. This problem has applications in cases of context-free approximation of distributions generated by more expressive statistical models. We show several interesting theoretical properties of probabilistic context-free grammars that are estimated in this way, including the previously unknown equivalence between the grammar cross-entropy with the input distribution and the so-called derivational entropy of the grammar itself. We discuss important consequences of these results involving the standard application of the maximum-likelihood estimator on finite tree and sentence samples, as well as other finite-state models such as Hidden Markov Models and probabilistic finite automata.	Univ Naples Federico II, Dept Phys, I-80126 Naples, Italy; Univ Padua, Dept Informat Engn, I-35131 Padua, Italy	University of Naples Federico II; University of Padua	Corazza, A (corresponding author), Univ Naples Federico II, Dept Phys, Via Cinthia, I-80126 Naples, Italy.	corazza@na.infn.it; satta@dei.unipd.it		SATTA, GIORGIO/0000-0001-7742-6438; CORAZZA, Anna/0000-0002-9156-5079				Baum LE, 1972, INEQUALITIES, V3, P1; BOOTH TL, 1973, IEEE T COMPUT, VC 22, P442, DOI 10.1109/T-C.1973.223746; CHAUDHURI R, 1983, IEEE T COMPUT, V32, P748, DOI 10.1109/TC.1983.1676313; Chelba C., 1998, P 36 ANN M ASS COMP, V1, P225; Chi ZY, 1999, COMPUT LINGUIST, V25, P131; Chi ZY, 1998, COMPUT LINGUIST, V24, P299; CORAZZA A, 1991, IEEE T PATTERN ANAL, V13, P936, DOI 10.1109/34.93811; Dupont P, 2005, PATTERN RECOGN, V38, P1349, DOI 10.1016/j.patcog.2004.03.020; HUTCHINS SE, 1972, INFORM SCIENCES, V4, P179, DOI 10.1016/S0020-0255(72)80010-0; Jelinek F., 1991, Computational Linguistics, V17, P315; Joshi A. K., 1997, HDB FORMAL LANGUAGES, P69, DOI DOI 10.1007/978-3-642-59126-6_2; Lari K., 1991, Computer Speech and Language, V5, P237, DOI 10.1016/0885-2308(91)90009-F; Lari K., 1990, Computer Speech and Language, V4, P35, DOI 10.1016/0885-2308(90)90022-X; Mohri Mehryar, 2001, REGULAR APPROXIMATIO, P153; Nederhof MJ, 2005, COMPUT LINGUIST, V31, P173, DOI 10.1162/0891201054223986; Roark B, 2001, COMPUT LINGUIST, V27, P249, DOI 10.1162/089120101750300526; Sanchez JA, 1997, IEEE T PATTERN ANAL, V19, P1052, DOI 10.1109/34.615455; Smith Noah A, 2005, P 43 ANN M ASS COMP, P354; Vidal E, 2005, IEEE T PATTERN ANAL, V27, P1013, DOI 10.1109/TPAMI.2005.147; Vidal E, 2005, IEEE T PATTERN ANAL, V27, P1026, DOI 10.1109/TPAMI.2005.148; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	28	5	6	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2007	29	8					1379	1393		10.1109/TPAMI.2007.1065	http://dx.doi.org/10.1109/TPAMI.2007.1065			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	177XT	17568142				2022-12-18	WOS:000247186500007
J	Miyagawa, I; Arakawa, K				Miyagawa, I; Arakawa, K			Motion and shape recovery based on iterative stabilization for modest deviation from planar motion	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						structure from motion; factorization method; planar motion; aerial image	PROJECTIVE STRUCTURE; FACTORIZATION METHOD	We describe an iterative stabilization method that can simultaneously recover camera motion and 3D shape from an image sequence captured under modest deviation from planar motion. This technique iteratively applies a factorization method based on planar motion and can approximate the observed image points to the 2D points projected under planar motion by stabilizing the camera motion. We apply the proposed method to aerial images acquired by a helicopter-borne camera and show better reconstruction of both motion and shape than Christy-Horaud's perspective factorization. Moreover, we confirm that the reprojection errors calculated from the recovered camera motion and 3D shape are very similar to the optimum results yielded by bundle adjustment.	Nippon Telegraph & Tel Corp, Cyber Space Labs, Nippon Telegraph & Tel Corp, Visual Media Commun Project,Image Proc Grp, Kanagawa 2390847, Japan	Nippon Telegraph & Telephone Corporation	Miyagawa, I (corresponding author), Nippon Telegraph & Tel Corp, Cyber Space Labs, Nippon Telegraph & Tel Corp, Visual Media Commun Project,Image Proc Grp, 1-1 Hikari No Oka, Kanagawa 2390847, Japan.	miyagawa.isao@lab.ntt.co.jp; kenichi.arakawa@lab.ntt.co.jp						BAILLARD C, 1999, P IEEE C COMP VIS PA, V2, P559; Bignone F., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P85; Bradski GR, 2000, PROC CVPR IEEE, P796, DOI 10.1109/CVPR.2000.854964; Christy S, 1996, IEEE T PATTERN ANAL, V18, P1098, DOI 10.1109/34.544079; CHRISTY S, 1994, 2421 INRIA; Frueh C, 2003, PROC CVPR IEEE, P562; Gruen A., 2002, INT ARCH PHOTOGRAMME, V34, P131; Han M, 2003, IEEE T PATTERN ANAL, V25, P884, DOI 10.1109/TPAMI.2003.1206517; Heyden A, 1999, IMAGE VISION COMPUT, V17, P981, DOI 10.1016/S0262-8856(99)00002-5; LI J, 2005, P IEEE WORKSH MOT VI, V2, P154, DOI DOI 10.1109/ACVMOT.2005.4; Lourakis M., 2004, 340 FORTH I COMP SCI; Mahamud S, 2000, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2000.854872; MICUSIK B, 2004, P AS C COMP VIS JEJ, V1, P545; Moons T., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P410, DOI 10.1007/BFb0055681; Oliensis J, 1996, PROC CVPR IEEE, P335, DOI 10.1109/CVPR.1996.517094; Oliensis J, 2001, IEEE T PATTERN ANAL, V23, P546, DOI 10.1109/34.927457; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Qian Chen, 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P55, DOI 10.1109/CVPR.1999.784608; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Stamos I, 2000, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2000.855865; Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; TELLER S, 1998, P IM UND WORKSH, P455; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170; UESHIBA T, 1998, P 5 ECCV, V1, P296; Zhao ZX, 2001, SOLID STATE SCI, V3, P339, DOI 10.1016/S1293-2558(00)01087-6	28	5	5	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2006	28	7					1176	1181		10.1109/TPAMI.2006.147	http://dx.doi.org/10.1109/TPAMI.2006.147			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	041AG	16792106				2022-12-18	WOS:000237424400014
J	Bazin, PL; Vezien, JM				Bazin, PL; Vezien, JM			Integration of geometric elements, Euclidean relations, and motion curves for parametric shape and motion estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						shape and motion recovery; model-based estimation; geometric relations; geometric reduction; constraint reconstruction; motion modeling; model selection; Bayesian estimation		This paper presents an approach to shape and motion estimation that integrates heterogeneous knowledge into a unique model-based framework. We describe the observed scenes in terms of structured geometric elements ( points, line segments, rectangles, 3D corners) sharing explicitly Euclidean relationships (orthogonality, parallelism, colinearity, coplanarity). Camera trajectories are represented with adaptative models which account for the regularity of usual camera motions. Two different strategies of automatic model building lead us to reduced models for shape and motion estimation with a minimal number of parameters. These models increase the robustness to noise and occlusions, improve the reconstruction, and provide a high-level representation of the observed scene. The parameters are optimally computed within a sequential Bayesian estimation procedure that gives accurate and reliable results on synthetic and real video imagery.	Johns Hopkins Univ Hosp, Lab Med Image Comp, Neuroradiol Div, Baltimore, MD 21287 USA; CNRS, LIMSI, F-91403 Orsay, France	Johns Hopkins University; Johns Hopkins Medicine; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay	Bazin, PL (corresponding author), Johns Hopkins Univ Hosp, Lab Med Image Comp, Neuroradiol Div, Phipps B100,600 N Wolfe St, Baltimore, MD 21287 USA.	bbazin@jhmi.edu; vezien@limsi.fr		Bazin, Pierre-Louis/0000-0002-0141-5510				[Anonymous], 1983, DATA STRUCTURES ALGO; Astrom K, 1999, IEEE T PATTERN ANAL, V21, P114, DOI 10.1109/34.748821; BALBIANI P, 1994, ELEMENTS GEOMETRIE M; BAZIN PL, 2001, IEICE T INFORM SYSTE; BAZIN PL, 2002, P 7 EUR C COMP VIS M; BAZIN PL, 2000, SPIE 45 ANN P VIS GE, V9; BAZIN PL, 2000, P 4 IR MACH VIS IM P, P43; BAZIN PL, 2002, SPIE 45 ANN P VIS GE, V9; Bondyfalat D., 1998, 3D Structure from Multiple Images of Large-Scale Environments. European Workshop, SMILE'98. Proceedings, P224; BOSE NK, 1985, MULTIDIMENSIONAL SYS; CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813; CHIUSO A, 2000, P EUR C COMP VIS JUN; COX D, 1992, IDEALS VARIEITES ALG; Debevec P E, 1996, P SIGGRAPH 96 C; DERICHE R, 1987, INT J COMPUT VISION, V1, P167, DOI 10.1007/BF00123164; DERICHE R, 1990, IMAGE VISION COMPUT, V8, P261, DOI 10.1016/0262-8856(90)80002-B; DICK A, 2002, P 7 EUR C COMP VIS M; DONIKIAN S, 1993, P 1 WORKSH PRINC PRA, P427; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FITZGIBBON AW, 1998, P EUR C COMP VIS, P311; GROSSMANN E, 2000, P BRIT MACH VIS C SE; HARALICK RM, 2000, PERFORMANCE CHARACTE; HARTLEY R, 2000, MULIPLE VIEW GEOMETR; HARTLEY RI, 1993, P DARPA ESPRIT WORKS; HU XP, 1993, IMAGE VISION COMPUT, V11, P549, DOI 10.1016/0262-8856(93)90021-8; INFANTINO I, 2000, P IAPR WORKSH MACH V, P427; KAHL F, 2001, P 8 INT C COMP VIS J; KANATANI K, 2000, MODEL SELECTION CRIT; KANATANI K, 2000, P WORKSH INF BAS IND, P45; Kwaiter G, 1998, IEEE INFOR VIS, P211, DOI 10.1109/IV.1998.694222; LEYTON M, 2000, LECT NOTES COMPUTER, V2145; MAYBANK SJ, 1999, P IEE C APPL STAT PA; McCarthy J.M., 1990, INTRO THEORETICAL KI; MORRIS DD, 1999, P C VIS ALG THEOR PR; Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705; Press WH., 1993, NUMERICAL RECIPES C; ROBERTSON D, 2002, P 17 EUR C COMP VIS; ROBERTSON D, 2000, P 11 BRIT MACH VIS C, P536; Rother C, 2002, INT J COMPUT VISION, V49, P117, DOI 10.1023/A:1020189404787; SPARR G, 1998, P SMILE WORKSH FREIB, P187; TORR P, 2000, INT J COMPUTER VISIO, V50; Torr PHS, 2000, DATA SEGMENTATION AND MODEL SELECTION FOR COMPUTER VISION, P143; TRIGGS B, 1999, P C VIS ALG THEOR PR	43	5	5	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2005	27	12					1960	1976		10.1109/TPAMI.2005.245	http://dx.doi.org/10.1109/TPAMI.2005.245			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	973ON	16355662				2022-12-18	WOS:000232532600010
J	Weiss, I; Ray, M				Weiss, I; Ray, M			Recognizing articulated objects using a region-based invariant transform	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						object recognition; invariance; range images; transform	3D OBJECTS; RECOGNITION	In this paper, we present a new method for representing and recognizing objects, based on invariants of the object's regions. We apply the method to articulated objects in low-resolution, noisy range images. Articulated objects such as a back-hoe can have many degrees of freedom, in addition to the unknown variables of viewpoint. Recognizing such an object in an image can involve a search in a high-dimensional space that involves all these unknown variables. Here, we use invariance to reduce this search space to a manageable size. The low resolution of our range images makes it hard to use common features such as edges to find invariants. We have thus developed a new "featureless" method that does not depend on feature detection. Instead of local features, we deal with whole regions of the object. We define a "transform" that converts the image into an invariant representation on a grid, based on invariant descriptors of entire regions centered around the grid points. We use these region-based invariants for indexing and recognition. While the focus here is on articulation, the method can be easily applied to other problems such as the occlusion of fixed objects.	Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Weiss, I (corresponding author), Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.	weiss@cfar.umd.edu; manjit@cfar.umd.edu						BESL PJ, 1988, IEEE T PATTERN ANAL, V10, P167, DOI 10.1109/34.3881; BINFORD TO, 1971, P IEEE C SYST CONTR; Bruckstein AM, 1997, IMAGE VISION COMPUT, V15, P335, DOI 10.1016/S0262-8856(96)01140-7; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Keren D, 2000, J MATH IMAGING VIS, V12, P5, DOI 10.1023/A:1008333805174; MOLER CB, 1973, SIAM J NUMERICAL ANA, V10; Mori G, 2001, PROC CVPR IEEE, P723; Mundy J., 1992, GEOMETRIC INVARIANCE; RENAUDIE D, 2000, P EUR C COMP VIS; RIVLIN E, 1995, IEEE T PATTERN ANAL, V17, P226, DOI 10.1109/34.368188; ROTHGANGER F, 2003, P INT C COMP VIS OCT; RUIZCORREA S, 2003, P INT C COMP VIS OCT; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; Weiss I., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P291, DOI 10.1109/CVPR.1988.196251; Weiss I, 1999, J MATH IMAGING VIS, V10, P175, DOI 10.1023/A:1008383224450; WEISS I, 1993, IEEE T PATTERN ANAL, V15, P943, DOI 10.1109/34.232081; WEISS I, 1993, INT J COMPUT VISION, V10, P207, DOI 10.1007/BF01539536; Weiss I, 2001, IEEE T PATTERN ANAL, V23, P116, DOI 10.1109/34.908963; WHEELER MD, 1995, IEEE T PATTERN ANAL, V17, P252, DOI 10.1109/34.368190	19	5	5	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2005	27	10					1660	1665		10.1109/TPAMI.2005.208	http://dx.doi.org/10.1109/TPAMI.2005.208			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	953OM	16237999				2022-12-18	WOS:000231086700012
J	Wu, CH; Chiu, YH; Cheng, KW				Wu, CH; Chiu, YH; Cheng, KW			Error-tolerant sign retrieval using visual features and maximum a posteriori estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Taiwanese Sign Language; alternative and augmentative communication; error tolerant retrieval; gesture feature	RECOGNITION	This paper proposes an efficient error-tolerant approach to retrieving sign words from a Taiwanese Sign Language (TSL) database. This database is tagged with visual gesture features and organized as a multilist code tree. These features are defined in terms of the visual characteristics of sign gestures by which they are indexed for sign retrieval and displayed using an anthropomorphic interface. The maximum a posteriori estimation is exploited to retrieve the most likely sign word given the input feature sequence. An error-tolerant mechanism based on mutual information criterion is proposed to retrieve a sign word of interest efficiently and robustly. A user-friendly anthropomorphic interface is also developed to assist learning TSL. Several experiments were performed in an educational environment to investigate the system's retrieval accuracy. Our proposed approach outperformed a dynamic programming algorithm in its task and shows tolerance to user input errors.	Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan	National Cheng Kung University	Wu, CH (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.	chwu@csie.ncku.edu.tw; chiuyh@csie.ncku.edu.tw; kungwei@csie.ncku.edu.tw	Wu, Chung-Hsien/E-7970-2013					ALONSO F, 1995, IEEE MULTIMEDIA, V2, P55, DOI 10.1109/93.482296; BAEZAYATES R, 1999, MODERN INFORMATION R; Berretti S, 2000, IEEE T MULTIMEDIA, V2, P225, DOI 10.1109/6046.890058; Chang S. K., 1992, Proceedings. 1992 IEEE Workshop on Visual Languages (Cat. No.92TH0471-3), P110, DOI 10.1109/WVL.1992.275776; Cook A. M., 1995, ASSISTIVE TECHNOLOGI, V1st; Cormen T.H., 1994, INTRO ALGORITHMS; Cross R., PRINCIPLES PRACTICES; Darragh J. J., 1992, CAMBRIDGE SERIES HUM; DeGroot M. H., 1970, OPTIMAL STAT DECISIO; DEMASCO PW, 1992, COMMUN ACM, V35, P68, DOI 10.1145/129875.129881; DONG ZG, 1999, COMPUTER SPEECH LANG, V13, P125; Gonnet G.H., 1984, HDB ALGORITHMS DATA; HAND DJ, 1989, DISCRIMINATION CLASS, P100; Kanji G., 1999, 100 STAT TESTS; LIANG R, 1997, THESIS NATL TAIWAN U; Liddell Scott, 1989, SIGN LANG STUD, V64, P195, DOI DOI 10.1353/SLS.1989.0027; MACKENZIE IS, 1993, SIGCHI B, V25, P58; *MIN ED DIV SPEC E, 2000, CHANG CIH SHOU HUAC, V1; MOHRI M, 1997, ASS COMPUTATIONAL LI, V23, P1; Oflazer K, 1997, IEEE T PATTERN ANAL, V19, P1376, DOI 10.1109/34.643897; Rabiner L., 1993, FUNDAMENTALS SPEECH; Ross, 1993, INTRO PROBABILITY MO; Sandler W, 1996, LINGUA, V98, P197, DOI 10.1016/0024-3841(95)00038-0; Simpson R C, 1999, IEEE Trans Rehabil Eng, V7, P464, DOI 10.1109/86.808950; SMITH WH, 1989, THESIS INDIANA U; SMITH WH, 1997, SHOU NENG SHENG CHYU, V1; SMITH WH, 1997, SHOU NENG SHENG CHYU, V2; Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811; SU MC, 2001, IEEE T NEURAL SYSTEM, V9; Swiffin AL, 1987, AUGMENTATIVE ALTERNA, V3, P181; THEODORIDIS S, 1999, PATTEN RECOGNITION; Valli C., 1995, LINGUISTICS AM SIGN, V2nd; Vogler C, 2001, COMPUT VIS IMAGE UND, V81, P358, DOI 10.1006/cviu.2000.0895; Vogler C, 1999, LECT NOTES ARTIF INT, V1739, P211; WEBSTER JG, 1985, ELECT DEVICES REHABI; [No title captured]	36	5	5	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2004	26	4					495	508		10.1109/TPAMI.2004.1265864	http://dx.doi.org/10.1109/TPAMI.2004.1265864			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	801NO	15382653				2022-12-18	WOS:000220102800005
J	Dong, YT; Carin, L				Dong, YT; Carin, L			Rate-distortion analysis of discrete-HMM pose estimation via multiaspect scattering data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						rate distortion; vector quantization; HIMM; pose estimation; underwater sensing	HIDDEN MARKOV-MODELS; IMAGE COMPRESSION; PURSUITS	We consider the problem of estimating the pose of a target based on a sequence of scattered waveforms measured at multiple target-sensor orientations. Using a hidden Markov model (HMM) representation of the scattered-waveform sequence, pose estimation reduces to estimating the underlying HMM states from a sequence of observations. It is assumed that each scattered waveform must be quantized via an encoding procedure. A distortion D is defined as the error in estimating the underlying HMM states, and the rate R represents the size of the discrete-HMM codebook. Rate-distortion theory is applied to define the minimum rate required to achieve a desired distortion, denoted as R(D). After deriving the rate-distortion function R(D), we demonstrate that discrete-HMM performance based on Lloyd encoding is far from this bound. Performance is improved via block coding, based on Bayes VQ. Results are presented for a canonical HMM problem, and then for multiaspect acoustic scattering from underwater elastic targets. Although the examples presented here are for multiaspect scattering and pose estimation, the results are of general applicability to discrete-HMM state estimation.	Guidant Corp, Cardiac Rhythm Management, St Paul, MN 55112 USA; Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Abbott Laboratories; Duke University	Dong, YT (corresponding author), Guidant Corp, Cardiac Rhythm Management, 4100 Hamline Ave N,MS E111, St Paul, MN 55112 USA.	Yanting.dong@guidant.com; lcarin@ee.duke.edu		Carin, Lawrence/0000-0001-6277-7948				AGAZZI OE, 1993, AT&T TECH J, V72, P60, DOI 10.1002/j.1538-7305.1993.tb00655.x; Berger T., 1971, RATE DISTORTION THEO; BLAHUT RE, 1972, IEEE T INFORM THEORY, V18, P460, DOI 10.1109/TIT.1972.1054855; CHOU PA, 1988, THESIS STANFORD U; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Dasgupta N, 2001, SIGNAL PROCESS, V81, P1303, DOI 10.1016/S0165-1684(00)00262-0; Dong YT, 2001, INT CONF ACOUST SPEE, P2841, DOI 10.1109/ICASSP.2001.940238; HANSON BA, 1990, INT CONF ACOUST SPEE, P857, DOI 10.1109/ICASSP.1990.115973; KIMBER D, 1990, P INT C AC SPEECH SI, P497; LEE KF, 1990, IEEE T ACOUST SPEECH, V38, P599, DOI 10.1109/29.52701; Li J, 1996, IEEE T SIGNAL PROCES, V44, P281, DOI 10.1109/78.485924; LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577; McClure MR, 1997, IEEE T SIGNAL PROCES, V45, P2912, DOI 10.1109/78.650250; Morgera S. D., 1988, Pattern Recognition and Artificial Intelligence. Towards an Integration. Proceedings of an International Workshop, P257; Neukirchen C, 2001, IEEE T SPEECH AUDI P, V9, P367, DOI 10.1109/89.917682; OEHLER KL, 1995, IEEE T PATTERN ANAL, V17, P461, DOI 10.1109/34.391396; Perlmutter KO, 1996, IEEE T IMAGE PROCESS, V5, P347, DOI 10.1109/83.480770; Principe JC, 1998, P SOC PHOTO-OPT INS, V3370, P218, DOI 10.1117/12.321826; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Ratches JA, 1997, IEEE T PATTERN ANAL, V19, P1004, DOI 10.1109/34.615449; Rigoll G, 1994, IEEE T SPEECH AUDI P, V2, P175, DOI 10.1109/89.260360; Runkle P, 1999, IEEE T PATTERN ANAL, V21, P1371, DOI 10.1109/34.817415; Runkle PR, 1999, IEEE T SIGNAL PROCES, V47, P2035, DOI 10.1109/78.771050; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; SIMS SRF, 1997, OPTICAL ENG, V36, P2671; Sorenson H. W, 1980, CONTROL SYSTEMS THEO; Trentin E, 2001, NEUROCOMPUTING, V37, P91, DOI 10.1016/S0925-2312(00)00308-8; YU G, 1990, P INT C AC SPEECH SI, P685	28	5	6	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2003	25	7					872	883		10.1109/TPAMI.2003.1206516	http://dx.doi.org/10.1109/TPAMI.2003.1206516			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	692NN					2022-12-18	WOS:000183667300009
J	Engbers, EA; Smeulders, AWM				Engbers, EA; Smeulders, AWM			Design considerations for generic grouping in vision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						grouping; design considerations; vision; perceptual grouping; clustering	PERCEPTUAL ORGANIZATION; EXTRACTION; INFERENCE; SEGMENTATION; BOUNDARY; CURVES	Grouping in vision can be seen as the process that organizes image entities into higher-level structures. Despite its importance, there is little consistency in the statement of the grouping problem in literature. In addition, most grouping algorithms in vision are inspired on a specific technique, rather than being based on desired characteristics, making it cumbersome to compare the behavior of various methods. This paper discusses six precisely formulated considerations for the design of generic grouping algorithms in vision: proper definition, invariance, multiple interpretations, multiple solutions, simplicity and robustness. We observe none of the existing algorithms for grouping in vision meet all the considerations. We present a simple algorithm as an extension of a classical algorithm, where the extension is based on taking the considerations into account. The algorithm is applied to three examples: grouping point sets, grouping poly-lines, and grouping flow-field vectors. The complexity of the greedy algorithm is O(nO(G)), where O-G is the complexity of the grouping measure.	Univ Amsterdam, Inst Informat, NL-1098 SJ Amsterdam, Netherlands	University of Amsterdam	Engbers, EA (corresponding author), Univ Amsterdam, Inst Informat, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.	engbers@science.uva.nl; smeulders@science.uva.nl						AHUJA N, 1989, COMPUT VISION GRAPH, V48, P304, DOI 10.1016/0734-189X(89)90146-1; Amir A, 1998, IEEE T PATTERN ANAL, V20, P168, DOI 10.1109/34.659934; August J., 2000, PERCEPTUAL ORG ARTIF, P265; BOLDT M, 1989, IEEE T SYST MAN CYB, V19, P1581, DOI 10.1109/21.44073; Casadei S, 1998, INT J COMPUT VISION, V27, P71, DOI 10.1023/A:1007905813513; COX IJ, 1993, INT J COMPUT VISION, V11, P5, DOI 10.1007/BF01420590; DOLAN J, 1989, P IMAGE UNDERSTANDIN, P1135; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Elder JH, 1998, VISION RES, V38, P143, DOI 10.1016/S0042-6989(97)00138-7; ENGBERS EA, 2001, 200117 ISIS U AMST; FISCHLER MA, 1986, IEEE T PATTERN ANAL, V8, P100, DOI 10.1109/TPAMI.1986.4767756; GRIMSON WEL, 1991, IEEE T PATTERN ANAL, V13, P1201, DOI 10.1109/34.106994; GRIMSON WEL, 1991, IEEE T PATTERN ANAL, V13, P920, DOI 10.1109/34.93810; Guy G, 1997, IEEE T PATTERN ANAL, V19, P1265, DOI 10.1109/34.632985; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; JACOBS DW, 1989, 1177 MIT AI; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; Jardine N., 1971, MATH TAXONOMY; Jonk A., 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P386, DOI 10.1109/ICDAR.1995.599019; Kanizsa G., 1979, ORG VISION; Krishnapuram R., 1993, IEEE Transactions on Fuzzy Systems, V1, P98, DOI 10.1109/91.227387; Kruger N, 1998, NEURAL PROCESS LETT, V8, P117, DOI 10.1023/A:1009688428205; LANCE GN, 1967, COMPUT J, V9, P373, DOI 10.1093/comjnl/9.4.373; LANCE GN, 1966, COMPUT J, V9, P60, DOI 10.1093/comjnl/9.1.60; LANCE GN, 1967, COMPUT J, V10, P271, DOI 10.1093/comjnl/10.3.271; LOWE D, 1984, THESIS STANFORD U; LU SY, 1978, IEEE T SYST MAN CYB, V8, P381, DOI 10.1109/TSMC.1978.4309979; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Marr D., 1982, VISION; MOHAN R, 1992, IEEE T PATTERN ANAL, V14, P616, DOI 10.1109/34.141553; NEEDHAM RM, 1962, P IFIP C, P284; Nitzberg M., 1993, LECT NOTES COMPUTER, V662; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; PRINCEN J, 1990, COMPUT VISION GRAPH, V52, P57, DOI 10.1016/0734-189X(90)90123-D; Rock I., 1983, LOGIC PERCEPTION; RUSPINI EH, 1969, INFORM CONTROL, V15, P22, DOI 10.1016/S0019-9958(69)90591-9; Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428; SARKAR S, 1993, IEEE T SYST MAN CYB, V23, P382, DOI 10.1109/21.229452; SARKAR S, 1993, IEEE T PATTERN ANAL, V15, P256, DOI 10.1109/34.204907; SAUND E, 1993, CVGIP-IMAG UNDERSTAN, V58, P327, DOI 10.1006/ciun.1993.1045; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481; Weiss R., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P489; Werbos PJ., 1991, ARTIFICIAL NEURAL NE, DOI [10.1016/b978-0-444-88740-5.50007-4, DOI 10.1016/B978-0-444-88740-5.50007-4]; WITKIN A, 1983, P 8 INT JOINT C ART, P1023; WRIGHT WE, 1973, PATTERN RECOGN, V5, P273, DOI 10.1016/0031-3203(73)90048-4; WU Z, 1993, IEEE T PATTERN ANAL, V15, P1101, DOI 10.1109/34.244673; Xiao J, 1998, INT C PATT RECOG, P1825, DOI 10.1109/ICPR.1998.712085; ZAHN CT, 1971, IEEE T COMPUT, VC 20, P68, DOI 10.1109/T-C.1971.223083; ZUCKER SW, 1987, VISION BRAIN COOPERA, P231	50	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2003	25	4					445	457		10.1109/TPAMI.2003.1190571	http://dx.doi.org/10.1109/TPAMI.2003.1190571			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	659BV		Green Submitted			2022-12-18	WOS:000181758100006
J	Brehelin, L; Gascuel, O; Caraux, G				Brehelin, L; Gascuel, O; Caraux, G			Hidden Markov models with patterns to learn Boolean vector sequences and application to the built-in self-test for integrated circuits	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Boolean vector sequence modeling; hidden Markov models; hybrid approach; structure (and parameters) learning; built-in self-test for integrated circuits	SPEECH RECOGNITION; MAXIMUM-LIKELIHOOD; ALGORITHM	We present a new model, derived from the Hidden Markov Model (HMM), to learn Boolean vector sequences. Our Hidden Markov Model with Patterns (HMMP) is a simple, hybrid, and interpretable model that uses Boolean patterns to define emission probability distributions attached to states. Vectors consistent with a given pattern are equiprobable, while inconsistent ones have probability zero to be emitted. We define an efficient learning algorithm for this model, which relies on the maximum likelihood principle, and proceeds by iteratively simplifying the structure and updating the parameters of an initial specific HMMP that represents the learning sequences. Each simplification involves merging two states of the current HMMP, while keeping the likelihood as high as possible and the algorithm stops when the HMMP has a sufficiently small structure. HMMPs and our learning algorithm are applied to the Built-in Self-Test (BIST) for integrated circuits, which is one of the key microelectronic problems. An HMMP is learned from a test sequence set (computed using a specific tool) that covers most of the potential faults of the circuit at hand. Then, this HMMP is used as test sequence generator. Our experiments, carried out with classical microelectronic benchmark circuits, show that learned HMMPs have a very high fault coverage. Furthermore, their small sizes combined with their simplicity allow these models to be easily implemented on the circuits for self-testing purposes.	Univ Montpellier 2, Dept Informat Fondamentale & Applicat, Lab Informat Robot & Microelect, F-34392 Montpellier 5, France	Universite de Montpellier	Brehelin, L (corresponding author), Univ Montpellier 2, Dept Informat Fondamentale & Applicat, Lab Informat Robot & Microelect, 161 Rue Ada, F-34392 Montpellier 5, France.	brehelin@lirmm.fr; gascuel@lirmm.fr; caraux@lirmm.fr						ABE N, 1992, MACH LEARN, V9, P205, DOI 10.1007/BF00992677; AGRAWAL VD, 1989, IEEE T COMPUT AID D, V8, P131, DOI 10.1109/43.21831; BAHL LR, 1983, IEEE T PATTERN ANAL, V5, P179, DOI 10.1109/TPAMI.1983.4767370; BAUM LE, 1970, ANN MATH STAT, V41, P164, DOI 10.1214/aoms/1177697196; Baum LE, 1972, INEQUALITIES, V3, P1; Brehelin L., 2000, Proceedings 18th IEEE VLSI Test Symposium, P359, DOI 10.1109/VTEST.2000.843866; BRGLEZ F, 1989, INT S CIRCUITS S MAY; CARRASCO RC, 1994, P 2 INT ICGI C GRAMM, V862, P139; CASACUBERTA F, 1990, IEEE T PATTERN ANAL, V12, P691, DOI 10.1109/34.56212; CHEN DL, 1994, INT J FATIGUE, V16, P485, DOI 10.1016/0142-1123(94)90199-6; CORNO F, 1996, LECT NOTES COMPUTER, V1067, P454; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Durbin R., 1998, BIOL SEQUENCE ANAL P; FORNEY GD, 1973, P IEEE, V61, P268, DOI 10.1109/PROC.1973.9030; Gascuel O, 1998, INT J PATTERN RECOGN, V12, P517, DOI 10.1142/S0218001498000336; Ghosh A., 1992, SEQUENTIAL LOGIC TES; Hopcroft John E., 1979, INTRO AUTOMATA THEOR; IBARRA OH, 1975, IEEE T COMPUTERS, V24; JUANG BH, 1990, IEEE T ACOUST SPEECH, V38, P1639, DOI 10.1109/29.60082; Kundu A., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P457, DOI 10.1109/CVPR.1988.196275; LANGAAS K, 1998, P 6 EUR C MATH OIL R, P1; ONCINA J, 1992, S MACH PERC, V1, P49; POVLOW BR, 1995, IEEE T PATTERN ANAL, V17, P1010, DOI 10.1109/34.464564; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rajski J., 1998, ARITHMETIC BUILT IN; Raphael C, 1999, IEEE T PATTERN ANAL, V21, P360, DOI 10.1109/34.761266; STOLCKE A, 1994, P 2 INT ICGI C GRAMM, V862, P106	27	5	5	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2001	23	9					997	1008		10.1109/34.955112	http://dx.doi.org/10.1109/34.955112			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	470RP					2022-12-18	WOS:000170885200005
J	Choudhury, R; Srivastava, JB; Chaudhury, S				Choudhury, R; Srivastava, JB; Chaudhury, S			Reconstruction-based recognition of scenes with translationally repeated quadrics	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						3D objects; reconstruction; recognition; projective invariants; translationally repeated objects; quadrics	OBJECT RECOGNITION; PAIRS; 3D	This paper addresses the problem of invariant-based recognition of quadric configurations from a single image. These configurations consist of a pair of rigidly connected translationally repeated quadric surfaces. This problem is approached via a reconstruction framework. A new mathematical framework, using relative affine structure, on the lines of Luong and Vieville [12], has been proposed. Using this mathematical framework, translationally repeated objects have been projectively reconstructed, from a single image, with four image point correspondences of the distinguished points on the object and its translate. This has been used to obtain a reconstruction of a pair of translationally repeated quadrics. We have proposed joint projective invariants of a pair of proper quadrics. For the purpose of recognition of quadric configurations, we compute these invariants for the pair of reconstructed quadrics. Experimental results on synthetic and real images, establish the discriminately power and stability of the proposed invariant-based recognition strategy. As a specific example, we have applied this technique for discriminating images of monuments which are characterized by translationally repeated domes modeled as quadrics.	INRIA Rhone Alpes, F-38330 St Martin Dheres, France; Indian Inst Technol, Dept Math, New Delhi 110016, India; Indian Inst Technol, Dept Elect Engn, New Delhi 110016, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Choudhury, R (corresponding author), INRIA Rhone Alpes, ZIRST 655 Ave Europe, F-38330 St Martin Dheres, France.	Ragini.Choudhury@inrialpes.fr; jbsrivas@maths.iitd.ernet.in; santanuc@cse.iitd.ernet.in						BOOKSTEIN FL, 1979, COMPUT VISION GRAPH, V9, P56, DOI 10.1016/0146-664X(79)90082-0; Choudhury R, 2001, J MATH IMAGING VIS, V14, P5, DOI 10.1023/A:1008373429426; CHOUDHURY R, 1999, THESIS INDIAN I TECH; CHOUDHURY R, 1998, P 1 IND C COMP VIS G, P101; Cross G, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P25, DOI 10.1109/ICCV.1998.710697; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; GROS P, 1992, RT90IMAG15LIFIA INRI; Heisterkamp DR, 1997, J MATH IMAGING VIS, V7, P253, DOI 10.1023/A:1008230528693; LIU J, 1995, P 2 AS C COMP VIS AC; Luong QT, 1996, COMPUT VIS IMAGE UND, V64, P193, DOI 10.1006/cviu.1996.0055; Moons T, 1996, IEEE T PATTERN ANAL, V18, P77, DOI 10.1109/34.476015; MUNDY JL, 1994, APPL INVARIANCE COMP, V825, P89; PILLOW N, 1995, IMAGE VISION COMPUT, V13, P355, DOI 10.1016/0262-8856(95)99722-D; QUAN L, 1992, IMAGE VISION COMPUT, V10, P319, DOI 10.1016/0262-8856(92)90049-9; QUAN L, 1993, COMPUTER VISION IMAG, V70, P111; QUAN L, 1995, J MATH IMAGING VIS, V5, P63; Rothwell C, 1997, COMPUT VIS IMAGE UND, V68, P37, DOI 10.1006/cviu.1997.0525; ROTHWELL CA, 1993, THESIS U OXFORD; Shashua A, 1996, IEEE T PATTERN ANAL, V18, P873, DOI 10.1109/34.537342; SHASHUA A, 1994, IEEE T PATTERN ANAL, V16, P778, DOI 10.1109/34.308472; Shashua A, 1997, INT J COMPUT VISION, V23, P185, DOI 10.1023/A:1007962930529; ZISSERMAN A, 1995, ARTIF INTELL, V78, P239, DOI 10.1016/0004-3702(95)00023-2	23	5	5	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2001	23	6					617	632		10.1109/34.927462	http://dx.doi.org/10.1109/34.927462			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	438DC					2022-12-18	WOS:000169037600006
J	Kiryati, N; Bruckstein, AM; Mizrahi, H				Kiryati, N; Bruckstein, AM; Mizrahi, H			Comments on: "Robust line fitting in a noisy image by the method of moments"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						line fitting; outliers; Hough transform		Qjidaa and Radouane have recently presented a method for robust line fitting and experimentally compared it to other methods, including a method suggested by us. The results attributed by Qjidaa and Radouane to our algorithm are incorrect. We apply are algorithm to the data used by Qjidaa and Radouane and demonstrate its robustness and accuracy.	Tel Aviv Univ, Dept Elect Engn Syst, IL-69978 Tel Aviv, Israel; Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Tel Aviv University; Technion Israel Institute of Technology	Kiryati, N (corresponding author), Tel Aviv Univ, Dept Elect Engn Syst, IL-69978 Tel Aviv, Israel.			Kiryati, Nahum/0000-0003-1436-2275; Bruckstein, Alfred/0000-0001-5669-0037				KIRYATI N, 1992, IEEE T PATTERN ANAL, V14, P496, DOI 10.1109/34.126810; Kiryati N, 2000, COMPUT VIS IMAGE UND, V78, P69, DOI 10.1006/cviu.1999.0828; Qjidaa H, 1999, IEEE T PATTERN ANAL, V21, P1216, DOI 10.1109/34.809115	3	5	6	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2000	22	11					1340	1341		10.1109/34.888720	http://dx.doi.org/10.1109/34.888720			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	374QE					2022-12-18	WOS:000165355200013
J	Bowyer, K; Flynn, P; Kasturi, R				Bowyer, K; Flynn, P; Kasturi, R			The 20th anniversary of the IEEE transactions on pattern analysis and machine intelligence	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article													Flynn, Patrick J/J-3388-2013	Flynn, Patrick J/0000-0002-5446-114X					0	5	5	1	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2000	22	1					1	3		10.1109/TPAMI.2000.824818	http://dx.doi.org/10.1109/TPAMI.2000.824818			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	286WR					2022-12-18	WOS:000085472300001
J	Marugame, A; Katto, J; Ohta, M				Marugame, A; Katto, J; Ohta, M			Structure recovery with multiple cameras from scaled orthographic and perspective views	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						structure recovery; scaled orthographic; perspective; Euclidean and projective geometry	MOTION; PROJECTIONS	This paper presents a novel framework for Euclidean structure recovery utilizing a scaled orthographic view and perspective views simultaneously. A scaled orthographic view is introduced in order to automatically obtain camera parameters such as camera positions, orientation, and focal length. Scaled orthographic properties enable ail camera parameters to be calculated implicitly and perspective properties enable a Euclidean structure to be recovered. The method can recover a Euclidean structure with at least seven point correspondences across a scaled orthographic view and perspective views. Experimental results for both computed and natural images verify that the method recovers structure with sufficient accuracy to demonstrate potential utility. The proposed method can be applied to an interface for 3D modeling, recognition, and tracking.	NEC Corp Ltd, C&C Med Res Labs, Kawasaki, Kanagawa 216, Japan	NEC Corporation	Marugame, A (corresponding author), NEC Corp Ltd, C&C Med Res Labs, Kawasaki, Kanagawa 216, Japan.			Katto, Jiro/0000-0002-1671-2614				FAUGERAS OD, 1992, P EUR C COMP VIS, P321; Hartley R. I., 1994, Proceedings 1994 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.94CH3405-8), P908, DOI 10.1109/CVPR.1994.323923; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P536, DOI 10.1109/34.24786; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; MCOAUCHLAN PF, 1994, P 3 EUR C COMP VIS S, P217; POELMAN CJ, 1994, P 3 EUR C COMP VIS S, P97; Pollefeys M., 1996, P INT C PATT REC, P31; SHASHUA A, 1995, T PATTERN ANAL MACHI, V17, P778; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1979, INTERPRETATION VISUA, P193; WEINSHALL D, 1993, P 4 INT C COMP VIS B, P675; ZHUANG X, 1988, IEEE J ROBOT AUTOM, V4, P236, DOI 10.1109/56.2089	14	5	6	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1999	21	7					628	633		10.1109/34.777373	http://dx.doi.org/10.1109/34.777373			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	216YT					2022-12-18	WOS:000081472600005
J	Sequeira, RE; Preteux, FJ				Sequeira, RE; Preteux, FJ			Discrete Voronoi diagrams and the SKIZ operator: A dynamic algorithm	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						incremental method; degenerate seed configuration; image segmentation; Euclidean distance function; point-location problem	TESSELLATION	The Voronoi diagram (VD) is a popular tool for partitioning the support of an image. An algorithm is presented for constructing VD when the seed set, which determines the Voronoi regions, can be modified by adding and removing seeds. The number of pixels and seeds revisited for updating the diagram and the neighbor relationships among seeds is minimized. A result on cocircular seeds is presented. The adjacency, or dual, graph of the VD is readily obtained. The use of the algorithm for constructing skeletons by influence zones is demonstrated.	INST NATL TELECOMMUN, DEPT SIM, F-91011 EVRY, FRANCE	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris	Sequeira, RE (corresponding author), MOTOROLA CIG, BLDG 1421, MAIL STOP 3-B8, 1501 W SHURE DR, ARLINGTON HTS, IL 60004 USA.							AHUJA N, 1985, COMPUT VISION GRAPH, V29, P286, DOI 10.1016/0734-189X(85)90126-4; ASANO T, 1985, COMPUTATIONAL GEOMET, P153; Bertin E, 1994, COMP IMAG VIS, V2, P209; Bertin E, 1994, THESIS U J FOURIER G, P1; BOISSONNAT JD, 1991, 1140 INRIA; BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0; BREU H, 1995, IEEE T PATTERN ANAL, V17, P529, DOI 10.1109/34.391389; Chassery J. M., 1991, Traitement du Signal, V8, P155; DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4; DEVILLERS O, 1991, 1349 INRIA; DEVILLERS O, 1992, 1619 INRIA; FANG TP, 1993, IEEE COMPUT GRAPH, V13, P36, DOI 10.1109/38.210490; Gersho A., 1991, VECTOR QUANTIZATION; GREEN PJ, 1978, COMPUT J, V21, P168, DOI 10.1093/comjnl/21.2.168; GUIBAS L, 1985, ACM T GRAPHIC, V4, P74, DOI 10.1145/282918.282923; GUIBAS LJ, 1992, ALGORITHMICA, V7, P381, DOI 10.1007/BF01758770; GUIBAS LJ, 1988, NATO ASI SERIES F, V40, P111; Luenberger D. G., 1969, OPTIMIZATION VECTOR; MELKEMI M, 1992, THESIS U J FOURIER G; Monga O., 1987, INT J PATTERN RECOGN, V1, P351, DOI [10.1142/s0218001487000242, DOI 10.1142/S0218001487000242]; Ogniewicz RL, 1995, PATTERN RECOGN, V28, P1839, DOI 10.1016/0031-3203(95)00059-3; OHYA T, 1984, INFORM PROCESS LETT, V18, P227, DOI 10.1016/0020-0190(84)90116-9; Okabe A., 2000, WILEY SERIES PROBABI; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; Sibson R., 1981, BRIEF DESCRIPTION NA, P21, DOI DOI 10.1002/ADVS.201700552; TUCERYAN M, 1990, IEEE T PATTERN ANAL, V12, P211, DOI 10.1109/34.44407; VINCENT L, 1989, SIGNAL PROCESS, V16, P365, DOI 10.1016/0165-1684(89)90031-5; VINCENT L, 1991, P IEEE C COMP VIS PA	29	5	6	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1997	19	10					1165	1170		10.1109/34.625128	http://dx.doi.org/10.1109/34.625128			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YB678					2022-12-18	WOS:A1997YB67800013
J	Kube, P; Perona, P				Kube, P; Perona, P			Scale-space properties of quadratic feature detectors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						feature detection; edge detection; scale space; nonlinear filtering; energy filters; quadratic filters; causality	EDGE-DETECTION; ENERGY; FILTERS; VISION	Feature detectors using a quadratic nonlinearity in the filtering stage:are known to have some advantages over linear detectors; here, we consider their scale-space properties. in particular, we investigate whether, like linear detectors, quadratic feature detectors permit a scale selection scheme with the ''causality property,'' which guarantees that features are never created as scale is coarsened. We concentrate on the design most common in practice, i.e., one-dimensional detectors with two constituent filters, with scale selection implemented as convolution with a scaling function. We consider two special cases of interest: constituent filter pairs related by the Hilbert transform, and by the first spatial derivative. We show that, under reasonable assumptions, Hilbert-pair quadratic detectors cannot have the causality property. In the case of derivative-pair detectors, we describe a family of scaling functions related to fractional derivatives of the Gaussian that are necessary and sufficient for causality. In addition, we report experiments that show the effects of these properties in practice. Thus we show that at least one class of quadratic feature detectors has the same desirable scaling property as the more familiar detectors based on linear filtering.	CALTECH, PASADENA, CA 91125 USA; UNIV PADUA, DIPARTIMENTO ELETTR & INFORMAT, PADUA, ITALY	California Institute of Technology; University of Padua	Kube, P (corresponding author), UNIV CALIF SAN DIEGO, DEPT COMP SCI & ENGN, LA JOLLA, CA 92093 USA.							ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; Anh V, 1996, IEEE T PATTERN ANAL, V18, P309, DOI 10.1109/34.485558; ARNOLD VI, 1981, SINGULARITY THEORY; BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749; BARVINOK AI, 1992, STOC; BRACEWELL RN, 1978, FOURIER TRANSFORM IT; CLARK JJ, 1988, IEEE T PATTERN ANAL, V10, P720, DOI 10.1109/34.6782; DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Granlund G.H., 1995, SIGNAL PROCESSING CO; GRANLUND GH, 1978, COMPUT VISION GRAPH, V8, P155, DOI 10.1016/0146-664X(78)90047-3; HUMMEL RA, 1989, IEEE T ACOUST SP DEC; KUBE P, 1992, P IEEE COMP SOC C CO; KUBE P, 1994, LNCS SERIES, V800, P115; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073; MORRONE MC, 1987, PATTERN RECOGN LETT, V6, P303, DOI 10.1016/0167-8655(87)90013-4; OWENS R, 1989, PATTERN RECOGN LETT, V9, P233, DOI 10.1016/0167-8655(89)90002-0; PERONA P, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P52; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; PERONA P, 1995, IEEE T PATTERN ANAL, V17, P488, DOI 10.1109/34.391394; PERONA P, 1992, P SPIE C APPL ART IN, V1, P708; Poston T., 2014, CATASTROPHE THEORY I; ROMENY BMT, 1994, GEOMETRY DRIVNE DIFF; RONSE C, 1993, IEEE T PATTERN ANAL, V15, P484, DOI 10.1109/34.211468; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI [10.1109/34.41383, 10.1109/TPAMI.1986.4767748]	29	5	5	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1996	18	10					987	999		10.1109/34.541408	http://dx.doi.org/10.1109/34.541408			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	VP691		Green Accepted			2022-12-18	WOS:A1996VP69100003
J	Garris, MD; Dimmick, DL				Garris, MD; Dimmick, DL			Form design for high accuracy optical character recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						form design; forms processing; handwriting recognition; idiosyncratic responses; neural networks; optical character recognition; tax		To successfully apply character recognition technology most of the forms currently hand-keyed will need to be redesigned. This paper presents results from a comprehensive study of three versions of a redesigned tax form. Analyses show that using separately spaced character boxes provides superior machine readability over fields containing combs and adjoining character boxes. It is shown that character boxes containing two vertically stacked ovals cause writers much more difficulty. Analyses provide proof that writer idiosyncratic responses on forms are the major source of errors, and proper form design can reduce these errors.			Garris, MD (corresponding author), NATL INST STAND & TECHNOL,GAITHERSBURG,MD 20899, USA.							BLUE JL, 1994, PATTERN RECOGN, V27, P485, DOI 10.1016/0031-3203(94)90031-0; David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; GARRIS MD, 1993, 5294 NISTIR NATL I S; GARRIS MD, 1994, 5364 NISTIR NATL I S; GARRIS MD, 1994, 5469 NISTIR NATL I S; GARRIS MD, 1992, 4950 NISTIR NATL I S; GROTHER P, 1995, HANDPRINTED FORMS CH; GROTHER PJ, 1992, P APPL ART NEU NETW, V3; SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q; WILKINSON RA, 1992, 4912 NISTIR NATL I S	10	5	6	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1996	18	6					653	656		10.1109/34.506417	http://dx.doi.org/10.1109/34.506417			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UR254					2022-12-18	WOS:A1996UR25400010
J	Zakarauskas, P; Ozard, JM				Zakarauskas, P; Ozard, JM			Complexity analysis for partitioning nearest neighbor searching algorithms	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						nearest-neighbor search; nearest-neighbor search; complexity analysis; cost analysis; Minkowski p-metric; k-d tree partitioning; ordered partitioning; product partitioning		In this paper we present cost estimates for finding the k-nearest neighbors to a test pattern according to a Minkowski p-metric, as a function of the size of the buckets in partitioning searching algorithms. The asymptotic expected number of operations to find the nearest neighbor is presented as a function of the average number of patterns per bucket n and is shown to contain a global minimum.	DEF RES ESTAB PACIFIC, FMO, ESQUIMALT DEF RES DETACHMENT, VICTORIA, BC V0S 1B0, CANADA		Zakarauskas, P (corresponding author), UNIV BRITISH COLUMBIA, DEPT OPHTHALMOL, VANCOUVER, BC V5Z 3N9, CANADA.							COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Dasarathy B.V., 1991, NEAREST NEIGHBOR NN; EASTMAN CM, 1981, INFORM PROCESS LETT, V12, P165, DOI 10.1016/0020-0190(81)90092-2; EASTMAN CM, 1987, INFORM SYST, V12, P375, DOI 10.1016/0306-4379(87)90029-9; Fix E., 1951, 4 USAF SCH AV; FIX E, 1952, 1 USAF SCH AV; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; FUKUNAGA K, 1975, IEEE T COMPUT, VC 24, P750, DOI 10.1109/T-C.1975.224297; HAMMING RW, 1980, CODING INFORMATION T; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; KAMGARPARSI B, 1985, PATTERN RECOGN LETT, V3, P7, DOI 10.1016/0167-8655(85)90036-4; KIM BS, 1986, IEEE T PATTERN ANAL, V8, P761, DOI 10.1109/TPAMI.1986.4767859; SHASHA D, 1990, ACM T INFORM SYST, V8, P140, DOI 10.1145/96105.96111; SHEN CW, 1978, IEEE P COMPS CHIC IL, P470; VIDAL E, 1994, PATTERN RECOGN LETT, V15, P1, DOI 10.1016/0167-8655(94)90094-9; YUNCK TP, 1976, IEEE T SYST MAN CYB, V6, P678, DOI 10.1109/TSMC.1976.4309418; ZAKARAUSKAS P, IN PRESS COMPLEXITY	17	5	6	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1996	18	6					663	668		10.1109/34.506419	http://dx.doi.org/10.1109/34.506419			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UR254					2022-12-18	WOS:A1996UR25400012
J	GROSSO, E; TISTARELLI, M				GROSSO, E; TISTARELLI, M			ACTIVE DYNAMIC STEREO VISION (VOL 17, PG 868, 1995)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction, Addition						ACTIVE VISION; DYNAMIC VISION; TIME-TO-IMPACT; STEREO VISION; MOTION ANALYSIS; NAVIGATION	BINOCULAR IMAGE FLOWS; ANIMATE VISION; MOBILE ROBOT; OPTICAL-FLOW; MOTION; DEPTH; CALIBRATION	Visual navigation is a challenging issue in automated robot control, In many robot applications, like object manipulation in hazardous environments or autonomous locomotion, it is necessary to automatically detect and avoid obstacles while planning a safe trajectory. In this context the detection of corridors of free space along the robot trajectory is a very important capability which requires nontrivial visual processing, In most cases it is possible to take advantage of the active control of the cameras. In this paper we propose a cooperative schema in which motion and stereo vision are used to infer scene structure and determine free space areas, Binocular disparity, computed on several stereo images over time, is combined with optical flow from the same sequence to obtain a relative-depth map of the scene. Both the time-to-impact and depth scaled by the distance of the camera from the fixation point in space are considered as good, relative measurements which are based on the viewer, but centered on the environment. The need for calibrated parameters is considerably reduced by using an active control strategy. The cameras track a point in space independently of the robot motion and the full rotation of the head, which includes the unknown robot motion, is derived from binocular image data. The feasibility of the approach in real robotic applications is demonstrated by several experiments performed on real image data acquired from an autonomous vehicle and a prototype camera head.			GROSSO, E (corresponding author), UNIV GENOA,DEPT COMMUN COMP & SYST SCI,INTEGRATED LAB ADV ROBOT,VIA OPERA PIA 13,I-16145 GENOA,ITALY.		Tistarelli, Massimo/AAH-9437-2021	Tistarelli, Massimo/0000-0002-3406-3048				AHUJA N, 1993, IEEE T PATTERN ANAL, V15, P1007, DOI 10.1109/34.254059; Aloimonos J., 1987, International Journal of Computer Vision, V1, P333, DOI 10.1007/BF00133571; AYACHE N, 1989, IEEE T ROBOTIC AUTOM, V5, P804, DOI 10.1109/70.88101; BAJCSY RK, 1985, 3RD P IEEE WORKSH CO, P13; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BALLARD DH, 1992, CVGIP-IMAG UNDERSTAN, V56, P3, DOI 10.1016/1049-9660(92)90081-D; BALLARD DH, 1989, OPT NEWS, V15, P17; Beer R. D., 1990, INTELLIGENCE ADAPTIV; BRIDWELL NJ, 1983, COMPUT VISION GRAPH, V21, P33, DOI 10.1016/S0734-189X(83)80028-0; BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032; BROOKS RA, 1988, P DARPA WORKSHOP IMA, P398; CHANG YL, 1989, IEEE T SYSTEMS MAN C, V19; CUTTING J, 1988, PERCEPTION EYE MOTIO; FERRARI F, 1990, JUL P INT WORKSH INT; GROSSO E, 1995, IEEE T PATTERN ANAL, V17, P868, DOI 10.1109/34.406652; GROSSO E, 1992, 2ND P EUR C COMP VIS, P516; GROSSO E, 1989, IEEE T SYSTEMS MAN C, V19; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1990, INT J COMPUTER VISIO, P59; IZAGUIRRE A, 1988, INT J ROBOT RES, P104; KAMGARPARSI B, 1986, CSTR1640 U MAR TECH; KRIEGMAN DJ, 1989, IEEE T ROBOTIC AUTOM, V5, P792, DOI 10.1109/70.88100; LEE DN, 1981, NATURE, V293, P293, DOI 10.1038/293293a0; LI LX, 1993, IEEE T PATTERN ANAL, V15, P657, DOI 10.1109/34.221167; MATTHIES L, 1989, INT J COMPUT VISION, V3, P209, DOI 10.1007/BF00133032; MATTHIES L, 1987, 4TH P INT S ROB RES, P120; NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5; NAGEL HH, 1992, ARTIFICIAL BIOL VISI, P193; NELSON RC, 1989, IEEE T PATTERN ANAL, V11, P1102, DOI 10.1109/34.42840; PUGET P, 1990, IMAGE VISION COMPUT, V8, P341, DOI 10.1016/0262-8856(90)80010-Q; SABATA B, 1991, CVGIP-IMAG UNDERSTAN, V54, P309, DOI 10.1016/1049-9660(91)90032-K; SANDINI G, 1990, IEEE T PATTERN ANAL, V12, P13, DOI 10.1109/34.41380; SANDINI G, 1990, OCT P IEEE INT WORKS, P396; TIRUMALAI AP, 1992, IEEE T PATTERN ANAL, V14, P1184, DOI 10.1109/34.177383; TISTARELLI M, 1990, IMAGE VISION COMPUT, V8, P271, DOI 10.1016/0262-8856(90)80003-C; TISTARELLI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P108, DOI 10.1016/1049-9660(92)90089-L; TISTARELLI M, 1994, 3RD P EUR C COMP VIS, P61; TISTARELLI M, 1991, JUN P INT C COMP VIS, P186; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; WAXMAN AM, 1986, IEEE T PATTERN ANAL, V8, P715, DOI 10.1109/TPAMI.1986.4767853	41	5	5	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1995	17	11					1117	1128						12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TD854					2022-12-18	WOS:A1995TD85400013
J	YANG, MCK; LEE, JS				YANG, MCK; LEE, JS			OBJECT IDENTIFICATION FROM MULTIPLE IMAGES BASED ON POINT MATCHING UNDER A GENERAL TRANSFORMATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						BAYES RULE; PATTERN RECOGNITION; POINT MATCHING; MULTIPLE IMAGE INFORMATION COMBINATION		This work is motivated by ship identification from a sequence of ISAR images. Maximum likelihood classification, based on point matching, is formulated when the observed images are subject to missing points and phantoms. The 3-D to 2-D transformation is assumed to be known only in a certain parametric form. Proper weights, based on the noise levels for all images, are derived for the classification formula. The new formulation simplifies the computation of matching and makes its extension to object identification from multiple images feasible. Moreover, some theoretical properties of the identification procedure can now be investigated. Guidelines on which groups of objects are easier to distinguish are found from statistical theory followed by intuitive explanation. This method is then applied to ship identification with simulated ISAR images.	USN,RES LAB,WASHINGTON,DC 20375	United States Department of Defense; United States Navy; Naval Research Laboratory	YANG, MCK (corresponding author), UNIV FLORIDA,DEPT STAT,225 GRIFFIN FLOYD HALL,GAINESVILLE,FL 32611, USA.							BAIRD HS, 1985, MODEL BASED IMAGE MA; CHEN CC, 1980, IEEE T AERO ELEC SYS, V16, P2, DOI 10.1109/TAES.1980.308873; CHU C, 1987, PATTERN RECOGN, V20, P403, DOI 10.1016/0031-3203(87)90064-1; COUHAT JL, 1986, COMBAT FLEET WORLD 1; FLICK TE, 1986, IEEE T PATTERN ANAL, V8, P482, DOI 10.1109/TPAMI.1986.4767812; GORIN AL, 1982, OPT ENG, V21, P858, DOI 10.1117/12.7972994; GRIFFIN PM, 1989, IEEE T SYST MAN CYB, V19, P1274, DOI 10.1109/21.44047; LAVINE D, 1983, PATTERN RECOGN, V16, P289, DOI 10.1016/0031-3203(83)90034-1; LEE JS, IN PRESS ISAR IMAGE; Press W. H., 1987, NUMERICAL RECIPES; Serfling RJ, 1980, APPROXIMATION THEORE; Stimson G. W., 1983, INTRO AIRBORNE RADAR; YANG MCK, 1993, 432 U FL DEP STAT TE	13	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1994	16	7					751	756		10.1109/34.297958	http://dx.doi.org/10.1109/34.297958			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NY134					2022-12-18	WOS:A1994NY13400010
J	SULL, S; AHUJA, N				SULL, S; AHUJA, N			INTEGRATED 3-D ANALYSIS AND ANALYSIS-GUIDED SYNTHESIS OF FLIGHT IMAGE SEQUENCES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						INTEGRATED SEGMENTATION AND MATCHING; INTEGRATED MOTION AND STRUCTURE ESTIMATION; CONSISTENCY OF STRUCTURE PARAMETERS; SEQUENTIAL-BATCH PROCESSING; ANALYSIS-GUIDED SYNTHESES; FLIGHT IMAGES; RECOGNITION OF VANISHING LINE	MOTION; SURFACE; LINE	This paper is concerned with three-dimensional (3-D) analysis, and analysis-guided syntheses, of images showing 3-D motion of an observer relative to a scene. There are two objectives of the paper. First, it presents an approach to recovering 3-D motion and structure parameters from multiple cues present in a monocular image sequence, such as point features, optical flow, regions, lines, texture gradient, and vanishing line. Second, it introduces the notion that the cues that contribute the most to 3-D interpretation are also the ones that would yield the most realistic synthesis, thus suggesting an approach to analysis-guided 3-D representation. For concreteness, the paper focuses on flight image sequences of a planar, textured surface. The integration of information in these diverse cues is carried out using optimization. For reliable estimation, a sequential batch method is used to compute motion and structure. Synthesis is done by using i) image attributes extracted from the image sequence, and ii) simple, artificial image attributes which are not present in the original images. For display, real and/or artificial attributes are shown as a monocular or a binocular sequence. Performance evaluation is done through experiments with one synthetic sequence, and two real image sequences digitized from a commercially available video tape and a laserdisc. The attribute based representation of these sequences compressed their sizes by 502 and 367. The visualization sequence appears very similar to the original sequence in informal, monocular as well as stereo viewing on a workstation monitor.	UNIV ILLINOIS,BECKMAN INST,URBANA,IL 61801; UNIV ILLINOIS,COORDINATED SCI LAB,URBANA,IL 61801	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign								ADIV G, 1985, IEEE T PATTERN ANAL, V7; BLOSTEIN D, 1989, IEEE T PATTERN ANAL, V11, P1233, DOI 10.1109/34.41363; CUI N, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P222; KANATANI K, 1985, COMPUT VISION GRAPH, V29, P13, DOI 10.1016/S0734-189X(85)90147-1; KUMAR RVR, 1989, P CVPR, P136; LIU Y, 1991, PATTERN RECOGN, V24, P489, DOI 10.1016/0031-3203(91)90016-X; LIU YC, 1988, COMPUT VISION GRAPH, V43, P37, DOI 10.1016/0734-189X(88)90041-2; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; Pavlidis T., 1977, STRUCTURAL PATTERN R; Rolfe J.M., 1986, FLIGHT SIMULATION; SUBBARAO M, 1986, COMPUT VISION GRAPH, V36, P208, DOI 10.1016/0734-189X(86)90076-9; SULL S, 1992, JAN P IM UND WORKSH, P21; SULL S, 1991, OCT P IEEE WORKSH VI, P274; SULL S, 1993, THESIS U ILLINOIS UR; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; Weng J., 1988, P INT C PATT REC, P247; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; [No title captured]	18	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	1994	16	4					357	372		10.1109/34.277590	http://dx.doi.org/10.1109/34.277590			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NH607					2022-12-18	WOS:A1994NH60700003
J	PAI, TW; HANSEN, JHL				PAI, TW; HANSEN, JHL			BOUNDARY-CONSTRAINED MORPHOLOGICAL SKELETON MINIMIZATION AND SKELETON RECONSTRUCTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter								A new algorithm for minimizing a morphological skeleton entitled boundary-constrained skeleton minimization (BCSM), as well as a new algorithm for reconstructing an original image from its minimized skeletal structure termed boundary-constrained skeleton reconstruction (BCSR), are proposed. The new algorithms are shown to reduce data storage requirements from (N + 1) binary images represented as separate skeleton subsets with their corresponding indices, to 2 binary images composed of a binary morphological skeleton and its corresponding morphological boundary structure. In addition to a reduction in memory storage, BCSM and BCSR result in substantial savings in computational complexity. The proposed algorithms are evaluated in the context of image analysis and coding, and their performance is compared to previous algorithms proposed by Serra and Maragos-Schafer. Sample evaluations indicate a greater than 22-fold savings in computational requirements and 11-fold reduction in memory requirements.			PAI, TW (corresponding author), DUKE UNIV,DEPT ELECT ENGN,DURHAM,NC 27708, USA.							BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; Blum Harry, 1967, TRANSFORMATION EXTRA, V43, P2; Feehs R. J., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V845, P285, DOI 10.1117/12.976517; Lantuejoul C., 1980, ISSUES DIGITAL IMAGE; Lantuejoul C., 1978, THESIS SCH MINES PAR; LEE JSJ, 1987, IEEE T ROBOTIC AUTOM, V3, P142, DOI 10.1109/JRA.1987.1087088; MARAGOS P, 1990, IEEE T PATTERN ANAL, V12, P498, DOI 10.1109/34.55110; MARAGOS P, 1986, IEEE T ACOUT SPEECH, P1228; MATHERON G., 1975, RANDOM SETS INTEGRAL; PAI TW, 1991, MAY P IEEE INT C AC, P2761; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Serra J, 1988, IMAGE ANAL MATH MORP	12	5	8	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1994	16	2					201	208		10.1109/34.273731	http://dx.doi.org/10.1109/34.273731			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NA631					2022-12-18	WOS:A1994NA63100007
J	GUIGO, R; SMITH, TF				GUIGO, R; SMITH, TF			INFERRING CORRELATION BETWEEN DATABASE QUERIES - ANALYSIS OF PROTEIN-SEQUENCE PATTERNS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						DATABASE MINING; FUNCTIONAL AND STOCHASTIC DEPENDENCES; INDUCTIVE MACHINE LEARNING; KNOWLEDGE ACQUISITION; MOLECULAR BIOLOGY DATABASES; PATTERN ANALYSIS; PROBABILISTIC INFERENCE; PROTEIN SEQUENCES; SET SIMILARITIES	MOTIFS	Given a subset P of a database, we address the problem of finding the query phi in a given database attribute having the closest extension to P. In the particular case that we outline, P is the set of protein sequences in a protein sequence database matching a given protein sequence pattern, whereas phi is a query in the annotation of the database. Ideally, phi is the description of a biological function. If the extension of phi is very similar to P, we may infer association between the pattern and the biological function described by the query. We have developed an algorithm that efficiently searches the query space when negation is not considered. Since the query language is a first-order language, the query space may be mapped into a set algebra in which a measure of stochastic dependence-an assymptotic approximation of the correlation coefficient-is used as a measure of set similarity. The algorithm uses the algebraic properties of such a measure to reduce the time required to search the query space. A prototype implementation of the algorithm has been tested in different collections of protein sequence patterns. Results obtained show that appropriate descriptions for known biologically meaningful protein sequence patterns are found efficiently. We finally describe an application of the algorithm in which the database is automatically and exhaustively searched for potentially relevant protein sequence patterns, and we report a few interesting discoveries obtained in a test case.	HARVARD UNIV,DANA FARBER CANC INST,CAMBRIDGE,MA 02138; HARVARD UNIV,DEPT BIOSTAT,CAMBRIDGE,MA 02138	Harvard University; Dana-Farber Cancer Institute; Harvard University			Guigo, Roderic/D-1303-2010	Guigo, Roderic/0000-0002-5738-4477				Aho A. V., 1988, AWK PROGRAMMING LANG; BAIROCH A, 1991, NUCLEIC ACIDS RES, V19, P2247, DOI 10.1093/nar/19.suppl.2247; BAIROCH A, 1991, NUCLEIC ACIDS RES, V19, P2241, DOI 10.1093/nar/19.suppl.2241; BARKER WC, 1991, NUCLEIC ACIDS RES, V19, P2231, DOI 10.1093/nar/19.suppl.2231; CRAMER H, 1946, MATH METHODS STATIST; DATE CJ, 1987, DATABASE SYSEMS; FIGGE J, 1988, NATURE, V334, P109, DOI 10.1038/334109a0; GRIBSKOV M, 1987, P NATL ACAD SCI USA, V84, P4355, DOI 10.1073/pnas.84.13.4355; GUIGO R, 1991, COMPUT APPL BIOSCI, V7, P309; GUIGO R, 1991, BIOCHEM J, V280, P833, DOI 10.1042/bj2800833; HART A, 1989, MACHINE LEARNING; HERTZ GZ, 1990, COMPUT APPL BIOSCI, V6, P81; KONAGAYA A, 1991, P AAAI WORKSHOP AL M; LATHROP RH, 1990, ARTIFICIAL INTELLIGE; Michalski R. S, 1983, MACHINE LEARNING; MOORE JF, 1988, STRUCTURAL FUNCTIONA; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; SMITH HO, 1990, P NATL ACAD SCI USA, V87, P826, DOI 10.1073/pnas.87.2.826; SMITH RF, 1990, P NATL ACAD SCI USA, V87, P118, DOI 10.1073/pnas.87.1.118; STRUHL K, 1989, TRENDS BIOCHEM SCI, V14, P137, DOI 10.1016/0968-0004(89)90145-X; Ullman J., 1988, PRINCIPLES DATABASE, V1; VALIANT LG, 1985, P IJCAI85, V85, P560; VEGA MA, 1990, NATURE, V345, P26, DOI 10.1038/345026a0; WALKER JE, 1982, EMBO J, V1, P945, DOI 10.1002/j.1460-2075.1982.tb01276.x; YAMANISHI K, 1991, 8TH P INT WORKSH MAC, P467	25	5	7	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1993	15	10					1030	1041		10.1109/34.254060	http://dx.doi.org/10.1109/34.254060			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MD775					2022-12-18	WOS:A1993MD77500005
J	PATEL, MAS; COHEN, FS				PATEL, MAS; COHEN, FS			LOCAL SURFACE SHAPE ESTIMATION OF 3-D TEXTURED SURFACES USING GAUSSIAN MARKOV RANDOM-FIELDS AND STEREO WINDOWS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						CONSISTENT PARENT; GAUSSIAN MARKOV RANDOM FIELDS; MARKOV RANDOM FIELDS; MAXIMUM LIKELIHOOD ESTIMATION; ORTHOGRAPHIC PROJECTION; SEMIPERSPECTIVE PROJECTION; SHAPE FROM TEXTURE; SLANT; STEREO IMAGING; STEREO-WINDOWS; TEXTURED SURFACE; TEXTURE TRANSFORMATION; TILT		This correspondence is concerned with the problem of extracting the local shape information of a 3-D textured surface from a single 2-D image by tracking the perceived systematic deformations the texture undergoes by virtue of being present on a 3-D surface and by virtue of being imaged. The surfaces of interest are planar and developable surfaces. The textured objects are viewed as originating by laying down a rubber planar sheet with a ''homogeneous'' parent texture on it onto the objects. The homogeneous planar parent texture is modeled by a stationary Gaussian Markov random field (GMRF). A probability distribution function for the texture data obtained by projecting tbe planar parent texture under a linear camera model is derived, which is an explicit function of the parent GMRF parameters, the surface shape parameters, and the camera geometry. The surface shape parameter estimation is posed as a maximum likelihood estimation (mle) problem. Aside from a statistical homogeneity assumption for the parent texture, there are no further constraining assumptions regarding the texture. A new ''stereo-windows'' concept is introduced to obtain a unique and ''consistent'' parent texture from the image data that, under appropriate transformations, yields the observed texture in the image. The theory is substantiated by experiments on synthesized as well as real images of textured surfaces.	DREXEL UNIV,DEPT ELECT & COMP ENGN,PHILADELPHIA,PA 19104	Drexel University	PATEL, MAS (corresponding author), UNIV MINNESOTA,DEPT RADIOL,MINNEAPOLIS,MN 55455, USA.							ALOIMONOS J, 1988, BIOL CYBERN, V58; BAJCSY R, 1976, COMPUT GRAPHICS IMAG, V5; BESAG J, 1975, BIOMETRIKA, V62; BESL PJ, 1985, P INT C COMPUT VISIO; BLOSTEIN D, 1989, IEEE T PATTERN ANAL, V11; BRODATZ P, TEXTURES; BROWN LG, 1990, IEEE T PATTERN ANAL, V12; BRZAKOVIC D, 1984, THESIS U FLORIDA; CHELLAPPA R, 1985, FEB IEEE AC SPEECH S, V33; CHOE Y, 1991, IEEE T PATTERN ANAL, V13; Cohen F., 1991, IEEE T PATTERN ANAL; COHEN FS, 1986, IEEE T PATT ANAL MAC; COHEN FS, 1991, THEORY APPLICATIONS, V53; DAVIS LS, 1983, IEEE T PATTERN ANAL, V5; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1975, PSYCHOL COMPUTER VIS, pCH4; IKEUCHI K, 1984, ARTIFICIAL INTELL, V22; JAU YC, 1990, COMPUT VISION GRAPHI, V52; KASHYAP RL, 1983, IEEE T INFORM THEORY, V29; KENDER JR, 1980, CMUCS81102 CARN MELL; LARIMORE W, 1977, P IEEE, V65; MARR D, 1977, P ROYAL SOC LON B, V197; Marr D, 1978, COMPUTER VISION SYST; OHTA Y, 1980, P INT JOINT C ART IN; PATEL MS, 1992, CVPR; PATEL MS, 1991, THESIS DREXEL U; RIMEY RD, 1988, IEEE T ROBOTICS AUTO; ROSENFELD A, 1992, DIGITAL PICTURE PROC, V2; STEVENS KA, 1981, ARTIFICIAL INTELL, V17; WITKIN AP, 1981, ARTIFICIAL INTELL, V17; WOODS J, 1972, IEEE T INFORM THEORY, V18	32	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1993	15	10					1091	1098		10.1109/34.254067	http://dx.doi.org/10.1109/34.254067			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MD775					2022-12-18	WOS:A1993MD77500012
J	SMITH, SP				SMITH, SP			THRESHOLD VALIDITY FOR MUTUAL NEIGHBORHOOD CLUSTERING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						CLUSTERING; POSSION POINT PROCESSES; RANDOM GRAPHS		Clustering algorithms have the annoying habit of finding clusters in random data. This note presents a theoretical analysis of the threshold of the mutual neighborhood clustering algorithm (MNCA) [1] under the hypothesis of random data. This yields a theoretical minimum value of this threshold below which even unclustered data is broken into separate clusters. To derive the threshold, a theorem about mutual near neighbors in a Poisson process is stated and proved. Simple experiments demonstrate the usefulness of the theoretical thresholds.			SMITH, SP (corresponding author), INTEL CORP,CHANDLER,AZ 85226, USA.							GOWDA KC, 1978, IEEE T SYST MAN CYB, V8, P888; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; JARVIS RA, 1973, IEEE T COMPUT, VC-22, P1025, DOI 10.1109/T-C.1973.223640; LING RF, 1976, J AM STAT ASSOC, V71, P293, DOI 10.2307/2285300; SMITH SP, 1984, IEEE T PATTERN ANAL, V6, P73, DOI 10.1109/TPAMI.1984.4767477	5	5	7	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1993	15	1					89	92		10.1109/34.184777	http://dx.doi.org/10.1109/34.184777			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KH085					2022-12-18	WOS:A1993KH08500007
J	BOLLE, RM; CALIFANO, A; KJELDSEN, R				BOLLE, RM; CALIFANO, A; KJELDSEN, R			A COMPLETE AND EXTENDIBLE APPROACH TO VISUAL RECOGNITION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						CONNECTIONIST NETWORKS; CONSTRAINT SATISFACTION NETWORKS; FEATURES; HOUGH TRANSFORM; INDEXING; OBJECT MATCHING; OBJECT MODELING; OBJECT RECOGNITION; PARAMETER TRANSFORMS; RANGE DATA	MODEL-BASED RECOGNITION; RANGE-DATA; VISION; SHAPE	We present a framework for 3-D object recognition. An important aspect of this framework is its flexibility and extendibility, which is accomplished through a uniform, parallel, and modular recognition architecture. Concurrent and stacked parameter transforms reconstruct a variety of features from the input scene. These transforms are either based on data, data and reconstructed features, or combinations of reconstructed features. At each stage, constraint satisfaction networks collect and fuse the evidence obtained through the parameter transforms. This process ensures a globally consistent interpretation of the input scene and allows for the integration of diverse types of information. The final interpretation of the scene is a small consistent subset of the many initial hypotheses about partial features, primitive features, feature assemblies, and 3D objects computed by the various parameter transforms. This paper reports on a complete, integrated (and implemented) system that extracts planar surfaces, patches of quadrics of revolution, and planar intersection curves of these surfaces (lines and conic sections in three space) from a depth map viewing 3-D objects. The reconstructed primitive features are used to index into an object model database to form hypotheses about objects in the scene. Integration of the various modules is a significant aspect of this work. Experimental results detailing the recognition behavior of the system are presented.	IBM CORP,THOMAS J WATSON RES CTR,DEPT COMP SCI,DEPT ARTIFICIAL INTELLIGENCE,YORKTOWN HTS,NY 10598	International Business Machines (IBM)	BOLLE, RM (corresponding author), IBM CORP,THOMAS J WATSON RES CTR,EXPLORATORY COMP VIS GRP,YORKTOWN HTS,NY 10598, USA.			Califano, Andrea/0000-0003-4742-3679				ALOIMONOS J, 1988, P IEEE, V76, P899, DOI 10.1109/5.5964; ALOIMONOS J, 1987, 1ST P INT C COMP VIS, P35; BAHNU B, 1984, IEEE T PATTERN ANAL, V8, P137; BAIRD HS, 1985, MODEL BASED IMAGE MA; BALLARD DH, 1981, 7TH P INT JOINT C AR, P1068; Besl P. J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P591, DOI 10.1109/CCV.1988.590039; Binford T. O., 1982, INT J ROBOT RES, V1, P18; BOLLE RM, 1986, IEEE T PATTERN ANAL, V8, P619, DOI 10.1109/TPAMI.1986.4767836; BOLLE RM, 1987, NOV P IEEE WORKSH CO, P324; BOLLE RM, 1989, JUN P IEEE C COMP VI, P625; BOLLE RM, 1986, RC12002 IBM TECH REP; BOLLES RC, 1983, 8TH P INT JOINT C AR, P1116; BOOKSTEIN FL, 1979, COMPUT VISION GRAPH, V9, P56, DOI 10.1016/0146-664X(79)90082-0; CALIFANO A, 1988, 7TH P NAT C ART INT, P831; CALIFANO A, 1989, JUN P IEEE C COMP VI, P192; CALIFANO A, 1990, 10TH P INT C PATT RE, P1; CALIFANO A, 1990, IBM RC16185 TECH REP; CALIFANO A, 1991, JUN P IEEE C COMP VI, P28; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; COHEN FS, 1988, JUN P IEEE COMP SOC, P964; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; Duda R.O., 1973, J ROYAL STAT SOC SER; ETTINGER GJ, 1988, IEEE C COMP VIS PATT, P32; FAHLMAN SE, 1987, IEEE COMPUT      JAN, P100; FAUGERAS OD, 1983, 8TH P INT JOINT C AR, P996; FELDMAN JA, 1982, COGNITIVE SCI, V6, P205, DOI 10.1207/s15516709cog0603_1; FLYNN PJ, 1991, JUN P IEEE WORKSH DI, P115; FU KS, 1983, ROBOTICS SENSING SYS, V441, P2; GOLDMAN RN, 1983, IEEE COMPUT GRAPH, V3, P68, DOI 10.1109/MCG.1983.263025; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HAKALA DG, P CAD CAM, V7, P363; HANSON AR, 1987, COMPUTER VISION SYST; HORAUD P, 1984, 1984 P IEEE INT C RO, P78; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; IKEUCHI K, 1981, 7TH P INT JOINT C AR, P595; KENDER JR, 1991, IBM RC16579 TECH REP; KNAPMAN J, 1987, 1ST P INT C COMP VIS, P547; KOENDERINK JJ, 1979, BIOL CYBERN, V32, P211, DOI 10.1007/BF00337644; LOWE DG, 1981, 7TH P INT JOINT C AR, P613; Marr D., 1982, VISION; MOHAN R, 1989, JUN P IEEE C COMP VI, P333; OSHIMA M, 1983, IEEE T PATTERN ANAL, V5, P353, DOI 10.1109/TPAMI.1983.4767405; SABBAH D, 1985, COGNITIVE SCI, V9, P25, DOI 10.1207/s15516709cog0901_3; SUGIHARA K, 1979, ARTIF INTELL, V12, P41, DOI 10.1016/0004-3702(79)90004-3; TAUBIN G, 1988, RC13873 IBM TECH REP; WALLACE TP, 1980, COMPUT VISION GRAPH, V13, P99, DOI 10.1016/S0146-664X(80)80035-9; 1986, 100X 3D SCANNER USER; [No title captured]	48	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1992	14	5					534	548		10.1109/34.134058	http://dx.doi.org/10.1109/34.134058			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HR650					2022-12-18	WOS:A1992HR65000003
J	GRIMSON, WEL; HUTTENLOCHER, DP				GRIMSON, WEL; HUTTENLOCHER, DP			INTRODUCTION TO THE SPECIAL ISSUE ON INTERPRETATION OF 3-D SCENES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									CORNELL UNIV, ITHACA, NY 14853 USA	Cornell University	GRIMSON, WEL (corresponding author), MIT, CAMBRIDGE, MA 02139 USA.								0	5	6	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1992	14	2					97	98						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HC029					2022-12-18	WOS:A1992HC02900001
J	WONG, PW				WONG, PW			ON QUANTIZATION ERRORS IN COMPUTER VISION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						COMPUTER VISION; ERROR DISTRIBUTION; ERROR MOMENTS; IMAGE PROCESSING; QUANTIZATION ERROR; TRANSFORM METHOD		We consider the error resulting in the computation of multivariable functions h(X1, X2,...,X(n)), where all the variables X(i)'s are only available in the quantized form. In image processing and computer vision problems, the variables are typically a mixture of the spatial coordinates and the intensity levels of objects in an image. A method is introduced using a first-order Taylor series expansion together with a periodic extension of the resulting expression and its Fourier series representation so that the moments and the probability distribution function of the error can be computed in closed form. This method only requires that the joint probability density function of X(i)'s be known and makes no assumption on the behavior on the quantization errors of the variables. Examples are also given where these results are applied.			WONG, PW (corresponding author), CLARKSON UNIV,DEPT ELECT & COMP ENGN,POTSDAM,NY 13699, USA.							BENNETT WR, 1948, AT&T TECH J, V27, P446, DOI 10.1002/j.1538-7305.1948.tb01340.x; BLOSTEIN SD, 1987, IEEE T PATTERN ANAL, V9, P752, DOI 10.1109/TPAMI.1987.4767982; BLOSTEIN SD, 1988, IEEE T PATTERN ANAL, V10, P765; DAVISSON LD, 1972, PR INST ELECTR ELECT, V60, P800, DOI 10.1109/PROC.1972.8779; Gradshteyn I. S, 1980, TABLES INTEGRALS SUM; GRAHAM RE, 1962, IRE T INFORM THEOR, V8, P129, DOI 10.1109/TIT.1962.1057690; KAMGARPARSI B, 1989, IEEE T PATTERN ANAL, V11, P929, DOI 10.1109/34.35496; MCVEY ES, 1982, IEEE T PATTERN ANAL, V4, P646, DOI 10.1109/TPAMI.1982.4767319; Pratt W. K., 1978, DIGITAL IMAGE PROCES; Rice SO, 1944, BELL SYST TECH J, V23, P282, DOI 10.1002/j.1538-7305.1944.tb00874.x; RICE SO, 1945, AT&T TECH J, V24, P46, DOI 10.1002/j.1538-7305.1945.tb00453.x; Roberts L, 1965, MACHINE PERCEPTION 3; SRIPAD AB, 1977, IEEE T ACOUST SPEECH, V25, P442, DOI 10.1109/TASSP.1977.1162977; Tolstov G.P., 1976, FOURIER SERIES; WIDROW B, 1960, T AIEE, V79, P555; WONG PW, 1990, IEEE T ACOUST SPEECH, V38, P286, DOI 10.1109/29.103065	16	5	8	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1991	13	9					951	956		10.1109/34.93812	http://dx.doi.org/10.1109/34.93812			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GJ180					2022-12-18	WOS:A1991GJ18000008
J	KENNY, P; LENNIG, M; MERMELSTEIN, P				KENNY, P; LENNIG, M; MERMELSTEIN, P			SPEAKER ADAPTATION IN A LARGE-VOCABULARY GAUSSIAN HMM RECOGNIZER	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									BELL NO RES,MONTREAL,QUEBEC,CANADA		KENNY, P (corresponding author), INRS TELECOMMUN,MONTREAL,QUEBEC,CANADA.							BAHL LR, 1988, IEEE WORKSHOP SPEECH; Baum LE, 1972, INEQUALITIES, V3, P1; DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420; DENG L, UNPUB COMPUT SPEECH; GUPTA VN, UNPUB J ACOUST SOC A; GUPTA VN, 1987, APR P IEEE INT C AC, P697; Jarre A., 1987, Proceedings: ICASSP 87. 1987 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.87CH2396-0), P1273; LEVINSON SE, 1983, AT&T TECH J, V62, P1035; Martin E. A., 1987, Proceedings: ICASSP 87. 1987 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.87CH2396-0), P709; Schwartz R., 1987, Proceedings: ICASSP 87. 1987 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.87CH2396-0), P633; SCHWARTZ R, 1988, IEEE WORKSHOP SPEECH; SUGAWARA K, 1986, P ICASSP, P2667	12	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1990	12	9					917	920		10.1109/34.57686	http://dx.doi.org/10.1109/34.57686			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DV778					2022-12-18	WOS:A1990DV77800007
J	ZUERNDORFER, B; WAKEFIELD, GH				ZUERNDORFER, B; WAKEFIELD, GH			EXTENSIONS OF SCALE-SPACE FILTERING TO MACHINE-SENSING SYSTEMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											ZUERNDORFER, B (corresponding author), UNIV MICHIGAN, DEPT ELECT ENGN & COMP SCI, ANN ARBOR, MI 48109 USA.							BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749; BISCHOF WF, 1988, COMPUT VISION GRAPH, V42, P192, DOI 10.1016/0734-189X(88)90163-6; Bracewell R., 1986, FOURIER TRANSFORM IT; CLARK JJ, 1989, IEEE T PATTERN ANAL, V11, P43, DOI 10.1109/34.23112; GEIGER D, 1987, NOV P WORKSH COMP VI, P211; Goffman C, 1965, CALCULUS SEVERAL VAR; KOENDERINK JJ, 1989, IEEE T PATTERN ANAL, V11, P1222, DOI 10.1109/34.42861; LU Y, 1989, IEEE T PATTERN ANAL, V11, P337, DOI 10.1109/34.19032; SCHUNCK B, GMR5586 GEN MOT RES; TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769; Ulaby F., 1981, MICROWAVE REMOTE SEN; VANWARMERDAM WLG, 1989, IEEE T PATTERN ANAL, V11, P973, DOI 10.1109/34.35500; WITKIN A, 1987, INT J COMPUT VISION, V1, P133, DOI 10.1007/BF00123162; WITKIN A, 1988, P INT JOINT C ART IN, P1019; WITKIN AP, 1984, IMAGE UNDERSTANDING; YUILLE AL, 1985, J OPT SOC AM A, V2, P683, DOI 10.1364/JOSAA.2.000683; YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI [10.1109/34.41383, 10.1109/TPAMI.1986.4767748]; YUILLE AL, 1988, ADV COMPUTER VISION, V2, P47	18	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1990	12	9					868	882		10.1109/34.57682	http://dx.doi.org/10.1109/34.57682			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DV778					2022-12-18	WOS:A1990DV77800003
J	WERKHOVEN, P; TOET, A; KOENDERINK, JJ				WERKHOVEN, P; TOET, A; KOENDERINK, JJ			DISPLACEMENT ESTIMATES THROUGH ADAPTIVE AFFINITIES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											WERKHOVEN, P (corresponding author), STATE UNIV UTRECHT,BUYS BALLOT LAB,POB 80000,3508 TA UTRECHT,NETHERLANDS.		Toet, Alexander/E-6733-2016	Toet, Alexander/0000-0003-1051-5422				HARTLEY R, 1985, PATTERN RECOGN LETT, V3, P253, DOI 10.1016/0167-8655(85)90005-4; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; JAIN JR, 1981, IEEE T COMMUN, V29, P1799, DOI 10.1109/TCOM.1981.1094950; KOENDERINK JJ, 1986, J OPT SOC AM A, V3, P242, DOI 10.1364/JOSAA.3.000242; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5; PRAGER JM, 1983, COMPUT VISION GRAPH, V24, P271, DOI 10.1016/0734-189X(83)90057-9; PRAZDNY K, 1983, COMPUT VISION GRAPH, V22, P239, DOI 10.1016/0734-189X(83)90067-1; THOMPSON WB, 1980, IEEE T PATTERN ANAL, V2, P543, DOI 10.1109/TPAMI.1980.6447701	10	5	5	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1990	12	7					658	663		10.1109/34.56208	http://dx.doi.org/10.1109/34.56208			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DK894					2022-12-18	WOS:A1990DK89400005
J	RANKA, S; SAHNI, S				RANKA, S; SAHNI, S			CONVOLUTION ON MESH CONNECTED MULTICOMPUTERS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV MINNESOTA,DEPT COMP SCI,MINNEAPOLIS,MN 55455	University of Minnesota System; University of Minnesota Twin Cities			Sahni, Sartaj K/A-7691-2009					BALLARD DH, 1985, COMPUTER VISION; Change J. H., 1987, Proceedings of the 1987 International Conference on Parallel Processing, P780; DEKEL E, 1981, SIAM J COMPUT, V10, P657, DOI 10.1137/0210049; FANG Z, 1985, NOV P IEEE WORKSH CO, P33; FANG Z, 1986, 1986 INT C PAR PROC, P262; HOROWITZ E, 1985, FUNDAMENTALS DATA ST; Kumar V. K. P., 1987, Proceedings of the 1987 International Conference on Parallel Processing, P765; LEE SY, 1987, IEEE T PATTERN ANAL, V9, P590, DOI 10.1109/TPAMI.1987.4767947; MARESCA M, 1986, 1986 P IEEE COMP SOC, P299; RANKA S, 1988, P INT C PAR PROC, V3, P84; RANKA S, 1988, P INT C PAR PROC, V3, P92; RANKA S, 1988, P INT C PAR PROC, V3, P212; Rosenfeld A., 1982, DIGITAL PICTURE PROC; [No title captured]	14	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1990	12	3					315	318		10.1109/34.49056	http://dx.doi.org/10.1109/34.49056			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CP943					2022-12-18	WOS:A1990CP94300007
J	GOEL, A; BYLANDER, T				GOEL, A; BYLANDER, T			COMPUTATIONAL FEASIBILITY OF STRUCTURED MATCHING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									OHIO STATE UNIV,DEPT COMP & INFORMAT SCI,ARTIFICIAL INTELLIGENCE RES LAB,COLUMBUS,OH 43210	University System of Ohio; Ohio State University								BYLANDER T, 1986, AI MAG, V7, P66; BYLANDER T, 1988, STRUCTURED MATCHING; CANTRELL HN, 1961, COMMUN ACM, V4, P272, DOI 10.1145/366573.366601; Chandrasekaran B., 1986, IEEE Expert, V1, P23, DOI 10.1109/MEX.1986.4306977; CHANDRASEKARAN B, 1983, ADV COMPUT, V22, P217, DOI 10.1016/S0065-2458(08)60130-8; CHAPMAN D, 1987, ARTIFICIAL INTELL, V32; CHARNIAK E, 1987, ARTIFICIAL INTELLIGE; CLANCEY WJ, 1985, ARTIF INTELL, V27, P289, DOI 10.1016/0004-3702(85)90016-5; Goel A., 1987, Third Annual Expert Systems in Government Conference Proceedings (Cat. No.87CH2467-9), P178; GOEL A, 1987, 6TH P NAT C ART INT, P421; JOHNSON TR, 1989, MAY P AAMSI C SAN FR; McDermott J., 1988, AUTOMATING KNOWLEDGE; Newell A, 1972, HUMAN PROBLEM SOLVIN; Sacerdoti E.D., 1977, STRUCTURE PLANS BEHA; SAMUEL AL, 1967, IBM J RES DEV, V11, P601, DOI 10.1147/rd.116.0601; SHUM SK, 1988, COMPUT CHEM ENG, V12, P27, DOI 10.1016/0098-1354(88)85003-8; Simon H. A., 1981, SCI ARTIFICIAL, V2nd, DOI 10.7551/mitpress/12107.001.0001; Smith J W Jr, 1985, J Med Syst, V9, P121, DOI 10.1007/BF00996197	18	5	6	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	1989	11	12					1312	1316		10.1109/34.41369	http://dx.doi.org/10.1109/34.41369			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CB716					2022-12-18	WOS:A1989CB71600007
J	CHENG, Y; KASHYAP, RL				CHENG, Y; KASHYAP, RL			A STUDY OF ASSOCIATIVE EVIDENTIAL REASONING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									PURDUE UNIV, SCH ELECT ENGN, W LAFAYETTE, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	CHENG, Y (corresponding author), UNIV CINCINNATI, DEPT COMP SCI, CINCINNATI, OH 45221 USA.							ACZEL J, 1966, LECTURES FUNCTIONAL; Clifford A. H., 1958, T AM MATH SOC, V88, P80; DOMBI J, 1982, FUZZY SET SYST, V8, P149, DOI 10.1016/0165-0114(82)90005-7; DUBOIS D, 1985, INFORM SCIENCES, V36, P85, DOI 10.1016/0020-0255(85)90027-1; FAUCETT WM, 1955, P AM MATH SOC, V6, P741; HAJEK P, 1985, INT J MAN MACH STUD, V22, P59, DOI 10.1016/S0020-7373(85)80077-8; KOHONEN T, 1988, COMPUTER, V21, P11, DOI 10.1109/2.28; LING CH, 1965, PUBL MATH-DEBRECEN, V12, P189; MOSTERT PS, 1957, ANN MATH, V65, P117, DOI 10.2307/1969668; Narens L, 1985, ABSTRACT MEASUREMENT; Schweitzer B, 1961, PUBL MATH-PARIS, V8, P169, DOI [10.5486/PMD, DOI 10.5486/PMD]; Shafer G., 1976, MATH THEORY EVIDENCE, VVolume 1; Shortliffe EH., 1975, MATH BIOSCI, V23, P351, DOI [10.1016/0025-5564(75)90047-4, DOI 10.1016/0025-5564(75)90047-4]; SILVERT W, 1979, IEEE T SYST MAN CYB, V9, P657; Zimmermann H.-J., 1978, Fuzzy Sets and Systems, V1, P45, DOI 10.1016/0165-0114(78)90031-3	15	5	6	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1989	11	6					623	631		10.1109/34.24796	http://dx.doi.org/10.1109/34.24796			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	U6749		Green Submitted			2022-12-18	WOS:A1989U674900007
J	KUMAR, VKP; KRISHNAN, V				KUMAR, VKP; KRISHNAN, V			EFFICIENT PARALLEL ALGORITHMS FOR IMAGE TEMPLATE MATCHING ON HYPERCUBE SIMD-MACHINES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											KUMAR, VKP (corresponding author), UNIV SO CALIF,DEPT ELECT ENGN SYST,LOS ANGELES,CA 90089, USA.							FANG Z, 1985, P IEEE WORKSH COMP A, P33; FANG Z, 1986, IEEE INT C PARALLEL; PRASANNA VK, 1987, INT C PARALLEL PROCE; PRASANNA VK, 1986, EFFICIENT IMAGE TEMP; Rosenfeld A., 1982, DIGITAL PICTURE PROC	5	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1989	11	6					665	669		10.1109/34.24802	http://dx.doi.org/10.1109/34.24802			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	U6749					2022-12-18	WOS:A1989U674900013
J	WALLACE, RS; HOWARD, MD				WALLACE, RS; HOWARD, MD			HBA VISION ARCHITECTURE - BUILT AND BENCHMARKED	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									HUGHERS ARTIFICIAL INTELLIGENCE CTR,CALABASAS,CA 91302					Howard, Mike/0000-0001-8353-5702				Ballard D.H., 1982, COMPUTER VISION; Chen J. S., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P293; CROWLEY JL, 1984, IEEE T PATTERN ANAL, V6, P212, DOI 10.1109/TPAMI.1984.4767504; FISHER A, 1985, P IEEE COMP SOC WORK, P484; Goldenberg R., 1987, Proceedings of the 1987 IEEE International Conference on Robotics and Automation (Cat. No.87CH2413-3), P1267; HAMEY LGC, 1987, PARALLEL COMPUTATION; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; HILLIS WD, 1982, CONNECTION MACHINE; HUERTAS A, 1983, IEEE T PATTERN ANAL, V5, P651; KUNG HT, 1986, DISTRIB COMPUT, V1, P246, DOI 10.1007/BF01660036; MORAVEC HP, 1987, 5TH P INT C ART INT, P584; NUSSMEIER T, 1983, HIERARCHIAL BUS ARCH; OLSON TJ, 1985, IMAGE PROCESSING PAC; PRINTZ H, 1987, COMMUNICATION; ROSENFELD A, 1987, P 1987 IM UND WORKSH, P298; SEITZ C, 1928, COMMUN ACM, V1, P22; STENTZ A, 1987, P IM UND WORKSH LOS, P440; STOUT QF, 1987, J PARALLEL DISTR COM, V4, P95, DOI 10.1016/0743-7315(87)90010-4; WALLACE R, 1986, SWATH ALGORITHMS VIS; WEEMS C, 1987, P IMAGE UNDERSTAND W, P483; 1987, MAX VIDEO FAMILY IMA; 1987, APA 512MX OEM BOARD	22	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1989	11	3					227	232		10.1109/34.21791	http://dx.doi.org/10.1109/34.21791			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	T3840					2022-12-18	WOS:A1989T384000002
J	CORDELLA, LP; DIBAJA, GS				CORDELLA, LP; DIBAJA, GS			GEOMETRIC-PROPERTIES OF THE UNION OF MAXIMAL NEIGHBORHOODS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									CNR,IST CIBERNET,I-80072 NAPLES,ITALY	Consiglio Nazionale delle Ricerche (CNR); Istituto di Cibernetica "Eduardo Caianiello" (ICIB-CNR)	CORDELLA, LP (corresponding author), UNIV NAPLES,DIPARTIMENTO INFORMAT & SISTEMIST,I-80125 NAPLES,ITALY.			Sanniti di Baja, Gabriella/0000-0003-2218-0412				ARCELLI C, 1984, COMPUT VISION GRAPH, V26, P61, DOI 10.1016/0734-189X(84)90130-0; ARCELLI C, 1985, IEEE T PATTERN ANAL, V7, P463, DOI 10.1109/TPAMI.1985.4767685; BATCHELOR BG, 1985, AUTOMATED VISUAL INS, P329; BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; CORDELLA LP, 1983, 3RD P SCAND C IM AN, P73; CORDELLA LP, 1988, PROPERTIES MAT LOCAL; Lipkin BS, 1970, PICTURE PROCESSING P, P241; PAVLIDIS T, 1986, COMPUT VISION GRAPH, V35, P111, DOI 10.1016/0734-189X(86)90128-3; ROSENFELD A, 1982, DIGITAL PICTURE PROC, V2, pCH11; SAMET H, 1984, MULTIRESOLUTION IMAG, P212; WU AY, 1986, COMPUT VISION GRAPH, V34, P76, DOI 10.1016/0734-189X(86)90049-6	11	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1989	11	2					214	217		10.1109/34.16718	http://dx.doi.org/10.1109/34.16718			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	R9989					2022-12-18	WOS:A1989R998900011
J	LEE, CH				LEE, CH			IMAGE SURFACE APPROXIMATION WITH IRREGULAR SAMPLES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											LEE, CH (corresponding author), USN,POSTGRAD SCH,DEPT ELECT & COMP ENGN,MONTEREY,CA 93943, USA.							[Anonymous], 1974, COMPUTER AIDED GEOME; BARSKY BA, 1980, COMPUT VISION GRAPH, V14, P203, DOI 10.1016/0146-664X(80)90053-2; BLOCH P, 1983, P IEEE, V71, P351, DOI 10.1109/PROC.1983.12593; De Boor C., 1978, PRACTICAL GUIDE SPLI, V27; DIERCKX P, 1982, COMPUT VISION GRAPH, V20, P171, DOI 10.1016/0146-664X(82)90043-0; Dierckx P., 1975, J COMPUT APPL MATH, V1, P165, DOI [DOI 10.1016/0771-050X(75)90034-0, 10.1016/0771-050X(75)90034-0]; DUCHON J, 1975, 231 U GREN REP; FRANKE R, 1982, COMPUT MATH APPL, V8, P273, DOI 10.1016/0898-1221(82)90009-8; FUCHS H, 1977, COMMUN ACM, V20, P693, DOI 10.1145/359842.359846; GRAHAM DN, 1967, PR INST ELECTR ELECT, V55, P336, DOI 10.1109/PROC.1967.5490; GRIMSON WEL, 1983, COMPUT VISION GRAPH, V22, P39, DOI 10.1016/0734-189X(83)90095-6; JANCAITUS JR, 1979, J GEOPHYS RES, V79, P3361; KEPPEL E, 1975, IBM J RES DEV, V19, P2, DOI 10.1147/rd.191.0002; Lawson CL, 1977, MATH SOFTWARE, P161, DOI [10.1016/B978-0-12-587260-7.50011-X, DOI 10.1016/B978-0-12-587260-7.50011-X]; MCCAUGHEY DG, 1981, IEEE T PATTERN ANAL, V3, P299, DOI 10.1109/TPAMI.1981.4767103; Meinguet J., 1979, J APPL MATH PHYS, V30, P292; PEUCKER TK, 1976, 10 OFF NAV RES TECH; Pratt W. K., 1978, DIGITAL IMAGE PROCES; RAM G, 1984, COMPUT VISION GRAPH, V26, P224, DOI 10.1016/0734-189X(84)90185-3; Schumaker LL, 1976, APPROXIMATION THEORY, VII, P203; SHANI U, 1984, COMPUT VISION GRAPH, V27, P129, DOI 10.1016/S0734-189X(84)80039-0; SLOAN KR, 1981, AUG P IEEE COMP SOC, P45; TERZOPOULOS D, 1983, COMPUT VISION GRAPH, V24, P52, DOI 10.1016/0734-189X(83)90020-8; TOUSSAINT GT, PRACTICAL USE BUCKET	24	5	6	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1989	11	2					206	212		10.1109/34.16716	http://dx.doi.org/10.1109/34.16716			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	R9989		Green Submitted			2022-12-18	WOS:A1989R998900009
J	BLOSTEIN, SD				BLOSTEIN, SD			CORRECTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction, Addition																		BLOSTEIN SD, 1987, IEEE T PATTERN ANAL, V9, P752, DOI 10.1109/TPAMI.1987.4767982	1	5	7	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1988	10	5					765	765						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	Q4255					2022-12-18	WOS:A1988Q425500022
J	PAUL, D; HATTICH, W; NILL, W; TATARI, S; WINKLER, G				PAUL, D; HATTICH, W; NILL, W; TATARI, S; WINKLER, G			VISTA - VISUAL INTERPRETATION SYSTEM FOR TECHNICAL APPLICATIONS - ARCHITECTURE AND USE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											PAUL, D (corresponding author), FRAUHOFER INST INFORMAT & DATA PROC,D-7500 KARLSRUHE 1,FED REP GER.							CONNERS RW, 1984, 7TH P INT C PATT REC; ENKELMANN W, 1986, MOTION UNDERSTANDING; GEISSELMANN H, 1986, FACHBERICHTE MESSEN, V14, P166; KORIES R, 1986, MAY P WORKSH MOT REP, P101; MILLER RK, 1985, MACHINE VISION ROBOT; MORING I, 1986, AUTOMATIC VISUAL WOO, V2, P290; POELZLEITNER W, 1986, 8TH P INT C PATT REC; TATARI S, 1987, SPIE INT SOC OPTICAL, V804, P229; TATARI S, 1987, APPL DIGITAL IMAGE P, V10, P86; YALAMANCHILI S, 1985, PROGR PATTERN RECOGN, V2, P1; 1985, MACHINE VISION SYSTE	11	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1988	10	3					399	407		10.1109/34.3904	http://dx.doi.org/10.1109/34.3904			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	N5337					2022-12-18	WOS:A1988N533700011
J	PERSOON, E				PERSOON, E			A PIPELINED IMAGE-ANALYSIS SYSTEM USING CUSTOM INTEGRATED-CIRCUITS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											PERSOON, E (corresponding author), PHILIPS RES LABS,POB 80000,5600 JA EINDHOVEN,NETHERLANDS.							BERNSEN J, 1986, 8 INT C PATT REC, P1251; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; PELGROM MJM, 1987, IEEE J SOLID STATE C, V22; PERSOON E, 1985, PATTERN RECOGNITION, V2; PERSOON E, 1980, PATTERN RECOGNITION; RUETZ PA, P ICASSP 86, P801; THISSEN FL, 1985, ROVISEC 5; INT C ACOUST SPEECH	8	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1988	10	1					110	116		10.1109/34.3874	http://dx.doi.org/10.1109/34.3874			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	L4366					2022-12-18	WOS:A1988L436600011
J	DIXIT, V; MOLDOVAN, DI				DIXIT, V; MOLDOVAN, DI			SEMANTIC NETWORK ARRAY PROCESSOR AND ITS APPLICATIONS TO IMAGE UNDERSTANDING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											DIXIT, V (corresponding author), UNIV SO CALIF, DEPT ELECT ENGN SYST, LOS ANGELES, CA 90089 USA.							BARROW HG, 1976, AI121 STANF RES I; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P60, DOI 10.1109/TPAMI.1979.4766876; Foster C. C., 1976, CONTENT ADDRESSABLE; HARALICK RM, 1979, IEEE T PATTERN ANAL, V1, P173, DOI 10.1109/TPAMI.1979.4766903; HARALICK RM, 1978, IEEE T SYST MAN CYB, V8, P600, DOI 10.1109/TSMC.1978.4310036; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; MOLDOVAN DI, 1983, P IEEE, V71, P113, DOI 10.1109/PROC.1983.12532; MOLDOVAN DI, 1984, J PARALLEL DISTR MAY; MOLDOVAN DI, 1984, PPP843 USC TECH REP; OHTA Y, 1983, CMUCS83162 CARN U TE; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; RUTKOWSKI WS, 1981, IEEE T PATTERN ANAL, V3, P368, DOI 10.1109/TPAMI.1981.4767123; ULLMANN JR, 1976, J ACM, V23, P31, DOI 10.1145/321921.321925; WALTZ D, 1976, APPLIED COMPUTATION, P468	15	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1987	9	1					153	160		10.1109/TPAMI.1987.4767882	http://dx.doi.org/10.1109/TPAMI.1987.4767882			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	F3785	21869387				2022-12-18	WOS:A1987F378500015
J	DALE, MB				DALE, MB			ON THE COMPARISON OF CONCEPTUAL CLUSTERING AND NUMERICAL TAXONOMY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											DALE, MB (corresponding author), CSIRO,DIV COMP RES,ST LUCIA,QLD 4067,AUSTRALIA.							DALE MB, 1973, AUST J BOT, V21, P253, DOI 10.1071/BT9730253; DALLWITZ MJ, 1974, SYST ZOOL, V23, P50, DOI 10.2307/2412239; GOODALL D. W., 1953, AUSTRALIAN JOUR BOT, V1, P39, DOI 10.1071/BT9530039; GOWER JC, 1974, BIOMETRICS, V30, P643, DOI 10.2307/2529229; HARTIGAN JA, 1972, J AM STAT ASSOC, V67, P123, DOI 10.2307/2284710; HILL MO, 1975, J ECOL, V63, P597, DOI 10.2307/2258738; JACKSON DM, 1970, INFORM STORAGE RET, V6, P187, DOI 10.1016/0020-0271(70)90061-6; Lance G. N., 1967, COMPUT J, V9, P60; Lance G. N., 1967, AUST COMPUT J, V1, P15; LANCE GN, 1968, COMPUT J, V11, P195, DOI 10.1093/comjnl/11.2.195; LANCE GN, 1967, COMPUT J, V9, P381, DOI 10.1093/comjnl/9.4.381; LU SY, 1978, IEEE T SYST MAN CYB, V8, P381, DOI 10.1109/TSMC.1978.4309979; MICHALSKI RS, 1983, IEEE T PATTERN ANAL, V5, P396, DOI 10.1109/TPAMI.1983.4767409; Openshaw S., 1980, COMPSTAT 1980. Proceedings in Computational Statistics, P419; PANKHURST RJ, 1983, MATH BIOSCI, V65, P209, DOI 10.1016/0025-5564(83)90062-7; Ross D., 1983, TAXON USERS MANUAL, VP4; VESELY A, 1981, KYBERNETIKA, V17, P82; WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967; WATANABE S., 1969, KNOWING GUESSING QUA; WILLIAMS W T, 1969, Taxon, V18, P369, DOI 10.2307/1218467; WILLIAMS WT, 1971, COMPUT J, V14, P157, DOI 10.1093/comjnl/14.2.157; WILLIAMS WT, 1959, J ECOL, V47, P83, DOI 10.2307/2257249; WILLIAMS WT, 1966, J ECOL, V54, P427, DOI 10.2307/2257960; YOLKINA VN, 1978, RAIRO-INF-COMPUT SCI, V12, P37; YU CT, 1977, J AM SOC INFORM SCI, V24, P345	25	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	2					241	244		10.1109/TPAMI.1985.4767647	http://dx.doi.org/10.1109/TPAMI.1985.4767647			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ACP84	21869260				2022-12-18	WOS:A1985ACP8400010
J	HARDAS, DM; SRIHARI, SN				HARDAS, DM; SRIHARI, SN			PROGRESSIVE REFINEMENT OF 3-D IMAGES USING CODED BINARY-TREES - ALGORITHMS AND ARCHITECTURE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									SUNY BUFFALO,DEPT COMP SCI,BUFFALO,NY 14260	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo			Srihari, Sargur N/E-8100-2011					BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; GARGANTINI I, 1982, COMMUN ACM, V25, P905, DOI 10.1145/358728.358741; Hill F. S.  Jr., 1983, Computer Graphics, V17, P323, DOI 10.1145/964967.801164; JACKINS CL, 1980, COMPUT VISION GRAPH, V14, P249, DOI 10.1016/0146-664X(80)90055-6; KNOWLTON K, 1980, P IEEE, V68, P885, DOI 10.1109/PROC.1980.11754; MEAGHER DJR, 1980, TRIPLTR80111 RENSS P; MEYEREBRECHT D, 1983, COMPUTER, V16, P19, DOI 10.1109/MC.1983.1654465; PAVLIDIS T, 1982, ALGORITHMS GRAPHICS, pCH6; SLOAN KR, 1979, IEEE T COMPUT, V28, P871, DOI 10.1109/TC.1979.1675269; SRIHARI SN, UNPUB INFORM SCI; SRIHARI SN, 1983, TR200 SUNY BUFF DEP; SRIHARI SN, 1980, TR162 SUNY BUFF DEP; SRIHARI SN, 1982, JUN P IEEECS C PATT, P485	13	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	6					748	757		10.1109/TPAMI.1984.4767598	http://dx.doi.org/10.1109/TPAMI.1984.4767598			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TX361	22499655				2022-12-18	WOS:A1984TX36100008
J	ROSENTHAL, DA; BAJCSY, R				ROSENTHAL, DA; BAJCSY, R			VISUAL AND CONCEPTUAL HIERARCHY - A PARADIGM FOR STUDIES OF AUTOMATED GENERATION OF RECOGNITION STRATEGIES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											ROSENTHAL, DA (corresponding author), UNIV PENN,DEPT COMP SCI & INFORMAT SCI,PHILADELPHIA,PA 19104, USA.							BAJCSY R, 1980, STRUCTURED COMPUTER; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; BROWN C, 1977, 5TH P IJCAI; BURT PJ, 1981, IEEE T SYST MAN CYB, V11, P802, DOI 10.1109/TSMC.1981.4308619; DAVIS R, 1975, AIM271 STANF ART INT; HANSON A, 1975, 75C1 U MASS DEP COMP; JOSHI AK, 1978, PATTERN DIRECTED INF; KELLY MD, 1971, MACHINE INTELL, V6; LEVINE MD, 1980, STRUCTURED COMPUTER; Rosenfeld A, 1976, IEEE T SYST MAN CYBE, VSMC-6; ROSENTHAL DA, 1981, INQUIRY DRIVEN VISIO; ROSENTHAL DA, 1978, STRIP FINDING FOURIE; SLOAN K, 1977, THESIS U PENNSYLVANI; TANIMOTO S, 1980, STRUCTURED COMPUTER; TIDHAR A, 1974, THESIS U PENNSYLVANI	15	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	3					319	325		10.1109/TPAMI.1984.4767524	http://dx.doi.org/10.1109/TPAMI.1984.4767524			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SR542	21869198				2022-12-18	WOS:A1984SR54200007
J	SHAPIRA, R				SHAPIRA, R			A NOTE ON SUGIHARA CLAIM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note																		Huffman D. A., 1971, Machine Intelligence Volume 6, P295; SHAPIRA R, 1979, COMMUN ACM, V22, P368, DOI 10.1145/359114.359129; SHAPIRA R, 1983, CHEAT EDGE LABELING; SUGIHARA K, 1982, IEEE T PATTERN ANAL, V4, P458, DOI 10.1109/TPAMI.1982.4767289	4	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	1					122	123		10.1109/TPAMI.1984.4767487	http://dx.doi.org/10.1109/TPAMI.1984.4767487			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SB213	21869177				2022-12-18	WOS:A1984SB21300018
J	PRESTON, K				PRESTON, K			GRAY LEVEL IMAGE-PROCESSING BY CELLULAR LOGIC TRANSFORMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV PITTSBURGH,DEPT RADIAT HLTH,PITTSBURGH,PA 15213	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	PRESTON, K (corresponding author), CARNEGIE MELLON UNIV,DEPT ELECT ENGN,PITTSBURGH,PA 15213, USA.							FOUNTAIN TJ, 1981, LANGUAGES ARCHITECTU; GOETCHERIAN V, 1980, PATTERN RECOGN, V12, P7, DOI 10.1016/0031-3203(80)90049-7; GRAHAM D, 1980, REAL TIME MED IMAGE; HUNT DJ, 1981, LANGUAGES ARCHITECTU; NAKAGAWA Y, 1978, IEEE T SYST MAN CYB, V8, P632; PRESTON K, 1980, PATTERN RECOGNITION; PRESTON K, 1981, PROGR PATTERN RECOGN	7	5	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	1					55	58		10.1109/TPAMI.1983.4767344	http://dx.doi.org/10.1109/TPAMI.1983.4767344			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PZ844	21869083				2022-12-18	WOS:A1983PZ84400007
J	SATO, Y; HONDA, I				SATO, Y; HONDA, I			PSEUDODISTANCE MEASURES FOR RECOGNITION OF CURVED OBJECTS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									KEIO UNIV,DEPT MATH,YOKOHAMA,KANAGAWA 223,JAPAN; TOKYO UNIV AGR & TECHNOL,FAC TECHNOL,DEPT ELECTR ENGN,KOGANEI,TOKYO 184,JAPAN	Keio University; Tokyo University of Agriculture & Technology			Rohlf, F J/A-8710-2008					AGIN GJ, 1976, IEEE T COMPUT, V25, P439, DOI 10.1109/TC.1976.1674626; COSGRIFF RL, 1960, AD254792; GRANLUND GH, 1972, IEEE T COMPUT, VC 21, P195, DOI 10.1109/TC.1972.5008926; JAIN R, 1979, P IEEE, V67, P805, DOI 10.1109/PROC.1979.11329; MARR D, 1976, MIT AI377 MEM; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; RICHARD CW, 1974, IEEE T SYST MAN CYB, VSMC4, P371, DOI 10.1109/TSMC.1974.5408458; SATO Y, 1982, IEEE T PATTERN ANAL, V4, P641, DOI 10.1109/TPAMI.1982.4767318; SOROKA BI, 1978, JUN P PATT REC IM PR, P331; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949	11	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	4					362	373		10.1109/TPAMI.1983.4767406	http://dx.doi.org/10.1109/TPAMI.1983.4767406			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	RA578	21869121				2022-12-18	WOS:A1983RA57800002
J	DATTA, AK; GANGULI, NR; RAY, S				DATTA, AK; GANGULI, NR; RAY, S			MAXIMUM-LIKELIHOOD METHODS IN VOWEL RECOGNITION - A COMPARATIVE-STUDY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											DATTA, AK (corresponding author), INDIAN STAT INST,ELECTR & COMMUN SCI UNIT,CALCUTTA 700035,W BENGAL,INDIA.							ANDERSON TW, 1958, INTRO MULTIVARIATE S, P147; BEZDEL, 1963, P IEEE, V112, P2000; CANNON MW, 1968, IEEE T ACOUST SPEECH, VAU16, P154, DOI 10.1109/TAU.1968.1161977; DATTA AK, 1980, IEEE T ACOUST SPEECH, V28, P85, DOI 10.1109/TASSP.1980.1163354; DATTA AK, 1978, 4TH P INT C PATT REC, P1047; Delattre P, 1952, WORD, V8, P195, DOI 10.1080/00437956.1952.11659431; DESOUZA P, 1977, IEEE T ACOUST SPEECH, V25, P554, DOI 10.1109/TASSP.1977.1163004; Fant G, 1960, ACOUSTIC THEORY SPEE; FORGIE JW, 1959, J ACOUST SOC AM, V31, P1480, DOI 10.1121/1.1907653; FRY DB, 1962, LANG SPEECH, V5, P171, DOI 10.1177/002383096200500401; FUJASIKI H, 1970, J ACOUST SOC JAPAN, V26, P152; GERSTMAN LJ, 1968, IEEE T AUDIO ELECTRO, V16; ITAKURA F, 1975, IEEE T ACOUST SPEECH, VAS23, P67, DOI 10.1109/TASSP.1975.1162641; ITAKURA F, 1968, 6TH P INT C ACOUST; JELINEK F, 1976, P IEEE, V64; KLEIN W, 1970, J ACOUST SOC AM, V40, P999; LADGEFOGED P, 1967, 3 AREAS EXPT PHONETI; LINDBLOM BE, 1967, J ACOUST SOC AM, V42, P830, DOI 10.1121/1.1910655; Majumder D. D., 1976, Journal of the Computer Society of India, V7, P14; MAJUMDER DD, 1973, INDIAN J PHYS, V47, P598; MAJUMDER DD, 1978, ACUSTICA, V41, P55; MILLAR JB, 1972, ACUSTICA, V27, P278; PAL SK, 1977, IEEE T SYST MAN CYB, V7, P625; PLOMP R, 1967, J ACOUST SOC AM, V41, P707, DOI 10.1121/1.1910398; POLS LCW, 1973, J ACOUST SOC AM, V53, P1093, DOI 10.1121/1.1913429; Sakai T., 1963, IEEE TRANS ELECTRON, VEC-12, P835, DOI [10.1109/PGEC.1963.263565, DOI 10.1109/PGEC.1963.263565]; SAMBUR MR, 1976, IEEE T ACOUST SPEECH, V24, P550, DOI 10.1109/TASSP.1976.1162879; STEVENS KN, 1963, J SPEECH HEAR RES, V6, P111, DOI 10.1044/jshr.0602.111	28	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	6					683	689		10.1109/TPAMI.1982.4767326	http://dx.doi.org/10.1109/TPAMI.1982.4767326			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PS237	22499647				2022-12-18	WOS:A1982PS23700018
J	FUKUNAGA, K; MANTOCK, JM				FUKUNAGA, K; MANTOCK, JM			A NONPARAMETRIC TWO-DIMENSIONAL DISPLAY FOR CLASSIFICATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											FUKUNAGA, K (corresponding author), PURDUE UNIV,DEPT ELECT ENGN,W LAFAYETTE,IN 47907, USA.							ANDERSON MW, 1970, IEEE T INFORM THEORY, V16, P541, DOI 10.1109/TIT.1970.1054532; BEAKLEY GW, 1972, IEEE T COMPUT, VC 21, P1337, DOI 10.1109/T-C.1972.223505; BROWN TA, 1979, IEEE T INFORM THEORY, V25, P617, DOI 10.1109/TIT.1979.1056092; CHIDANANDA K, 1978, PATTERN RECOGN, V10, P105; CHIEN Y, 1978, INTERACTIVE PATTERN; CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406; FUKUNAGA K, 1971, IEEE T COMPUT, VC 20, P1521, DOI 10.1109/T-C.1971.223165; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P285, DOI 10.1109/TIT.1975.1055373; FUKUNAGA K, 1971, IEEE T COMPUT, VC 20, P917, DOI 10.1109/T-C.1971.223371; FUKUNAGA K, 1978, TREE7848 PURD U TECH; FUKUNAGA K, 1972, INTRO STATISTICAL PA; HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155; LEE RCT, 1977, IEEE T COMPUT, V26, P288, DOI 10.1109/TC.1977.1674822; QUESENBERRY CP, 1968, ANN MATH STAT, V39, P664, DOI 10.1214/aoms/1177698425; SAMMON JW, 1970, IEEE T COMPUT, VC 19, P826, DOI 10.1109/T-C.1970.223047; SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678; TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P121, DOI 10.1109/TSMC.1976.5409182	17	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	4					427	436		10.1109/TPAMI.1982.4767276	http://dx.doi.org/10.1109/TPAMI.1982.4767276			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	NT735	21869059				2022-12-18	WOS:A1982NT73500011
J	KASHYAP, RL; OOMMEN, BJ				KASHYAP, RL; OOMMEN, BJ			A GEOMETRICAL APPROACH TO POLYGONAL DISSIMILARITY AND SHAPE-MATCHING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											KASHYAP, RL (corresponding author), PURDUE UNIV,SCH ELECT ENGN,W LAFAYETTE,IN 47907, USA.		Oommen, B. John/P-6323-2017; Rohlf, F J/A-8710-2008	Oommen, B. John/0000-0002-5105-1575; 				Aho A. V., 1972, SIAM Journal on Computing, V1, P305, DOI 10.1137/0201022; ALT FL, 1962, J ACM, V11, P240; BRIBIESCA E, 1980, PATTERN RECOGN, V12, P101, DOI 10.1016/0031-3203(80)90009-6; DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P60, DOI 10.1109/TPAMI.1979.4766876; DAVIS LS, 1981, IEEE T PATTERN ANAL, V3, P265, DOI 10.1109/TPAMI.1981.4767099; Duda R.O., 1973, J ROYAL STAT SOC SER; DUDANI SA, 1977, IEEE T COMPUT, V26, P39, DOI 10.1109/TC.1977.5009272; FEDER J, 1965, SEGMENT FITTING  MAR; Freeman H., 1974, Computing Surveys, V6, P57, DOI 10.1145/356625.356627; FU KS, 1982, SYNTACTIC PATTERN RE; GRANLUND GH, 1972, IEEE T COMPUT, VC 21, P195, DOI 10.1109/TC.1972.5008926; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; KASHYAP RL, 1981, IEEE T INFORM THEORY, V27; MONTNARI U, 1970, COMMUN ACM, V13, P41, DOI 10.1145/361953.361967; OOMMEN BJ, 1982, THESIS PURDUE U; PAVLIDIS T, 1979, IEEE T PATTERN ANAL, V1, P2, DOI 10.1109/TPAMI.1979.4766870; PAVLIDIS T, 1977, JUN P IEEE COMP SOC, P98; Pavlidis T., 1977, STRUCTURAL PATTERN R; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; RICHARD CW, 1974, IEEE T SYST MAN CYB, VSMC4, P371, DOI 10.1109/TSMC.1974.5408458; ROSENFELD A, 1979, COMPUT VISION GRAPH, V9, P354, DOI 10.1016/0146-664X(79)90100-X; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; SMITH S, 1981, P PATTERN RECOGNITIO, P168; Tsai W. H., 1980, Proceedings of the 5th International Conference on Pattern Recognition, P251; WALLACE TP, 1980, COMPUT VISION GRAPH, V13, P99, DOI 10.1016/S0146-664X(80)80035-9; WATSON LT, CS80004R STAT U DEP; YANG YH, 1981, P IEEE C PATTERN REC, P562; YOU KC, 1980, COMPUT VISION GRAPH, V13, P1, DOI 10.1016/0146-664X(80)90113-6; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949; 1953, NAT GEOGRAPHIC   DEC	30	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	6					649	654		10.1109/TPAMI.1982.4767320	http://dx.doi.org/10.1109/TPAMI.1982.4767320			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PS237	22499641				2022-12-18	WOS:A1982PS23700012
J	MIX, DF; JONES, RA				MIX, DF; JONES, RA			A DIMENSIONALITY REDUCTION TECHNIQUE BASED ON A LEAST SQUARED ERROR CRITERION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											MIX, DF (corresponding author), UNIV ARKANSAS,DEPT ELECT ENGN,FAYETTEVILLE,AR 72701, USA.							Bellman R., 1970, INTRO MATRIX ANAL, V2nd; CHIEN Y, 1978, INTERACTIVE PATTERN; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; FOLEY DH, 1975, IEEE T COMP, V24, P381; Tatsuoka, 1971, MULTIVARIATE ANAL; Van Trees H. L, 2004, DETECTION ESTIMATION; Wozencraft J. M., 1965, PRINCIPLES COMMUNICA; Young T. Y., 1974, CLASSIFICATION ESTIM; YOUNG TY, 1964, IEEE T BIO-MED ENG, VBM11, P60, DOI 10.1109/TBME.1964.4502308	9	5	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	5					537	544		10.1109/TPAMI.1982.4767299	http://dx.doi.org/10.1109/TPAMI.1982.4767299			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PG894	21869074				2022-12-18	WOS:A1982PG89400011
nullJ	Wang, JL; Jiao, JB; Bao, LC; He, SF; Liu, W; Liu, YH				Wang, Jiangliu; Jiao, Jianbo; Bao, Linchao; He, Shengfeng; Liu, Wei; Liu, Yun-hui			Self-Supervised Video Representation Learning by Uncovering Spatio-Temporal Statistics	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Three-dimensional displays; Neural networks; Image color analysis; Visualization; Training; Feature extraction; Self-supervised learning; representation learning; video understanding; 3D CNN	RECOGNITION; FLOW	This paper proposes a novel pretext task to address the self-supervised video representation learning problem. Specifically, given an unlabeled video clip, we compute a series of spatio-temporal statistical summaries, such as the spatial location and dominant direction of the largest motion, the spatial location and dominant color of the largest color diversity along the temporal axis, etc. Then a neural network is built and trained to yield the statistical summaries given the video frames as inputs. In order to alleviate the learning difficulty, we employ several spatial partitioning patterns to encode rough spatial locations instead of exact spatial Cartesian coordinates. Our approach is inspired by the observation that human visual system is sensitive to rapidly changing contents in the visual field, and only needs impressions about rough spatial locations to understand the visual contents. To validate the effectiveness of the proposed approach, we conduct extensive experiments with four 3D backbone networks, i.e., C3D, 3D-ResNet, R(2+1)D and S3D-G. The results show that our approach outperforms the existing approaches across these backbone networks on four downstream video analysis tasks including action recognition, video retrieval, dynamic scene recognition, and action similarity labeling. The source code is publicly available at: https://github.com/laura-wang/video_repres_sts.	[Wang, Jiangliu; Liu, Yun-hui] Chinese Univ Hong Kong CUHK, CUHK T Stone Robot Inst, Hong Kong Ctr Logist Robot, Hong Kong, Peoples R China; [Jiao, Jianbo] Univ Oxford, Dept Engn Sci, Oxford OX1 2JD, England; [Bao, Linchao; Liu, Wei] Tencent AI Lab, Shenzhen 518057, Guangdong, Peoples R China; [He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China	University of Oxford; Tencent; South China University of Technology	Liu, YH (corresponding author), Chinese Univ Hong Kong CUHK, CUHK T Stone Robot Inst, Hong Kong Ctr Logist Robot, Hong Kong, Peoples R China.; Bao, LC (corresponding author), Tencent AI Lab, Shenzhen 518057, Guangdong, Peoples R China.	jiangliuwang@link.cuhk.edu; jianbo@robots.ox.ac.uk; linchaobao@gmail.com; shengfenghe7@gmail.com; wl2223@columbia.edu; yhliu@cuhk.edu.hk	Bao, Linchao/AAG-9148-2020	Bao, Linchao/0000-0001-9543-3754; Liu, Wei/0000-0002-3865-8145; Jiao, Jianbo/0000-0003-0833-5115	Hong Kong RGC TRS [T42-409/18-R]; Hong Kong ITC [ITS/448/16FP]; VC Fund of the CUHK T Stone Robotics Institute [4930745]; Hong Kong Centre for Logistics Robotics; Hong Kong-Shenzhen Innovation and Technology Research Institute (Futian); National Natural Science Foundation of China [61972162]; EPSRC Programme Grant Seebibyte [EP/M013774/1]; Visual AI [EP/T028572/1]	Hong Kong RGC TRS; Hong Kong ITC; VC Fund of the CUHK T Stone Robotics Institute; Hong Kong Centre for Logistics Robotics; Hong Kong-Shenzhen Innovation and Technology Research Institute (Futian); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); EPSRC Programme Grant Seebibyte(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Visual AI	This work was supported by the Hong Kong RGC TRS under T42-409/18-R, the Hong Kong ITC under Grant ITS/448/16FP, the VC Fund 4930745 of the CUHK T Stone Robotics Institute, the Hong Kong Centre for Logistics Robotics, the Hong Kong-Shenzhen Innovation and Technology Research Institute (Futian), the National Natural Science Foundation of China (No. 61972162), the EPSRC Programme Grant Seebibyte EP/M013774/1, and Visual AI EP/T028572/1.	Alwassel Humam, 2020, NEURIPS; Arora S, 2019, PR MACH LEARN RES, V97; Bachman P, 2019, ADV NEUR IN, V32; Benaim S., 2020, CVPR, P9922; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Buchler U, 2018, LECT NOTES COMPUT SC, V11219, P797, DOI 10.1007/978-3-030-01267-0_47; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Chen T, 2020, PR MACH LEARN RES, V119; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Davies B., 2002, INTEGRAL TRANSFORMS, Vthird; Derpanis KG, 2012, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2012.6247815; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Epstein D, 2020, PROC CVPR IEEE, P916, DOI 10.1109/CVPR42600.2020.00100; Feichtenhofer C, 2014, PROC CVPR IEEE, P2681, DOI 10.1109/CVPR.2014.343; Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607; Gan C, 2018, PROC CVPR IEEE, P5589, DOI 10.1109/CVPR.2018.00586; Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232; Gidaris Spyros, 2018, ARXIV180307728; Giese MA, 2003, NAT REV NEUROSCI, V4, P179, DOI 10.1038/nrn1057; Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649; Hacohen G, 2019, PR MACH LEARN RES, V97; Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Henaff OJ, 2020, PR MACH LEARN RES, V119; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Huh Minyoung, 2016, ARXIV160808614; Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034; Jenni Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P425, DOI 10.1007/978-3-030-58604-1_26; Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; Kay W., 2017, ARXIV PREPRINT ARXIV; Kim D, 2019, AAAI CONF ARTIF INTE, P8545; Klaser Alexander, 2008, P BRIT MACH VIS C; Kliper-Gross O, 2012, IEEE T PATTERN ANAL, V34, P615, DOI 10.1109/TPAMI.2011.209; Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202; Korbar B, 2018, ADV NEUR IN, V31; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Liu Yang, 2019, ARXIV190713487; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701; Mathieu Michael, 2016, ICLR; Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990; Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Oord Avd, 2018, ARXIV 180703748; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Purushwalkam S., 2020, ADV NEURAL INFORM PR, V33, P3407; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Simonyan K, 2014, ADV NEUR IN, V27; Soomro K., 2012, ARXIV; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Tengda Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P312, DOI 10.1007/978-3-030-58580-8_19; Theriault C, 2013, PROC CVPR IEEE, P2603, DOI 10.1109/CVPR.2013.336; Tian Y., 2020, ECCV, P776, DOI [10.48550/arXiv.1906.05849, DOI 10.1007/978-3-030-58621-8_45]; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413; Wang Jiangliu, 2020, EUR C COMP VIS, P504, DOI DOI 10.1007/978-3-030-58520-430; Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wei DL, 2018, PROC CVPR IEEE, P8052, DOI 10.1109/CVPR.2018.00840; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058; Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658; Zagoruyko S., 2017, P INT C LEARN REPR, DOI DOI 10.1109/CVPR.2019.00271; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40	82	4	4	4	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3791	3806		10.1109/TPAMI.2021.3057833	http://dx.doi.org/10.1109/TPAMI.2021.3057833			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33566757				2022-12-18	WOS:000805820500033
J	Ye, LW; Rochan, M; Liu, Z; Zhang, XQ; Wang, Y				Ye, Linwei; Rochan, Mrigank; Liu, Zhi; Zhang, Xiaoqin; Wang, Yang			Referring Segmentation in Images and Videos With Cross-Modal Self-Attention Network	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Referring segmentation; actor and action segmentation; cross-modal; self-attention		We consider the problem of referring segmentation in images and videos with natural language. Given an input image (or video) and a referring expression, the goal is to segment the entity referred by the expression in the image or video. In this paper, we propose a cross-modal self-attention (CMSA) module to utilize fine details of individual words and the input image or video, which effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the visual input. We further propose a gated multi-level fusion (GMLF) module to selectively integrate self-attentive cross-modal features corresponding to different levels of visual features. This module controls the feature fusion of information flow of features at different levels with high-level and low-level semantic information related to different attentive words. Besides, we introduce cross-frame self-attention (CFSA) module to effectively integrate temporal information in consecutive frames which extends our method in the case of referring segmentation in videos. Experiments on benchmark datasets of four referring image datasets and two actor and action video segmentation datasets consistently demonstrate that our proposed approach outperforms existing state-of-the-art methods.	[Ye, Linwei; Zhang, Xiaoqin] Wenzhou Univ, Coll Comp Sci & Artificial Intelligence, Wenzhou 325035, Zhejiang, Peoples R China; [Rochan, Mrigank; Wang, Yang] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada; [Liu, Zhi] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China	Wenzhou University; University of Manitoba; Shanghai University	Wang, Y (corresponding author), Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.; Liu, Z (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.	yel3@cs.umanitoba.ca; mrochan@cs.umanitoba.ca; liuzhi@staff.shu.edu.cn; zhangxiaoqinnan@gmail.com; ywang@cs.umanitoba.ca	LIU, Zhi/D-4518-2012	LIU, Zhi/0000-0002-8428-1131	NSERC; National Natural Science Foundation of China [61771301, 61922064, U2033210]; Zhejiang Provincial Natural Science Foundation [LR17F030001]; FoS research chair program; GETS program at the University of Manitoba	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Provincial Natural Science Foundation(Natural Science Foundation of Zhejiang Province); FoS research chair program; GETS program at the University of Manitoba	This work was supported in part by the NSERC, in part by the National Natural Science Foundation of China under Grants 61771301, 61922064, and U2033210, in part by the Zhejiang Provincial Natural Science Foundation under Grant LR17F030001 and in part by the FoS research chair program and the GETS program at the University of Manitoba.	[Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bahdanau D., 2015, P 3 INT C LEARNING R; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cho K., 2014, P 2014 C EMP METH NA, P1724; Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7; Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Kalogeiton V, 2017, IEEE I CONF COMP VIS, P2001, DOI 10.1109/ICCV.2017.219; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787, DOI DOI 10.3115/V1/D14-1086; Khoreva A, 2019, LECT NOTES COMPUT SC, V11364, P123, DOI 10.1007/978-3-030-20870-7_8; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Li L. H., 2019, ARXIV 190803557; Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602; Li ZY, 2017, PROC CVPR IEEE, P7350, DOI 10.1109/CVPR.2017.777; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu JS, 2019, ADV NEUR IN, V32; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3; Shi XJ, 2015, ADV NEUR IN, V28; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang H, 2019, IEEE I CONF COMP VIS, P3938, DOI 10.1109/ICCV.2019.00404; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Xu CL, 2015, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2015.7298839; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yamaguchi M, 2017, IEEE I CONF COMP VIS, P1462, DOI 10.1109/ICCV.2017.162; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075; Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022; Ye SM, 2018, IEEE T IMAGE PROCESS, V27, P5514, DOI 10.1109/TIP.2018.2855406; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660	50	4	4	13	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3719	3732		10.1109/TPAMI.2021.3054384	http://dx.doi.org/10.1109/TPAMI.2021.3054384			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33497325	Green Submitted			2022-12-18	WOS:000805820500028
J	Zhang, HS; Zeng, Y; Lu, HC; Zhang, LH; Li, JH; Qi, JQ				Zhang, Hongshuang; Zeng, Yu; Lu, Huchuan; Zhang, Lihe; Li, Jianhua; Qi, Jinqing			Learning to Detect Salient Object With Multi-Source Weak Supervision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Saliency detection; Annotations; Image segmentation; Dogs; Feature extraction; Task analysis; Noise measurement; Saliency; salient object detection; weak supervision		High-cost pixel-level annotations makes it appealing to train saliency detection models with weak supervision. However, a single weak supervision source hardly contain enough information to train a well-performing model. To this end, we introduce a unified two-stage framework to learn from category labels, captions, web images and unlabeled images. In the first stage, we design a classification network (CNet) and a caption generation network (PNet), which learn to predict object categories and generate captions, respectively, meanwhile highlights the potential foreground regions. We present an attention transfer loss to transmit supervisions between two tasks and an attention coherence loss to encourage the networks to detect generally salient regions instead of task-specific regions. In the second stage, we create two complementary training datasets using CNet and PNet, i.e., natural image dataset with noisy labels for adapting saliency prediction network (SNet) to natural image input, and synthesized image dataset by pasting objects on background images for providing SNet with accurate ground-truth. During the testing phases, we only need SNet to predict saliency maps. Experiments indicate the performance of our method compares favorably against unsupervised, weakly supervised methods and even some supervised methods.	[Zhang, Hongshuang; Zeng, Yu; Lu, Huchuan; Zhang, Lihe; Li, Jianhua; Qi, Jinqing] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Liaoning, Peoples R China; [Lu, Huchuan] Dalian Univ Technol, Sch Informat & Commun Engn, Ningbo, Peoples R China; [Lu, Huchuan] Peng Cheng Tab, Shenzhen, Peoples R China	Dalian University of Technology; Dalian University of Technology	Lu, HC (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Liaoning, Peoples R China.	hszhang94@gmail.com; zengyu@mail.dlut.edu.cn; lhchuan@dlut.edu.cn; zhanglihe@dlut.edu.cn; jianhual@dlut.edu.cn; jinqing@dlut.edu.cn			National Key R&D Program of China [2018AAA0102001]; National Natural Science Foundation of China [61725202, U1903215, 61829102, 91538201, 61771088, 61751212]; Fundamental Research Funds for the Central Universities [DUT19GJ201]; Dalian Innovation leader's support Plan [2018RD07]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Dalian Innovation leader's support Plan	The work was supported in part by the National Key R&D Program of China under Grant No. 2018AAA0102001 and National Natural Science Foundation of China under grant No. 61725202, U1903215, 61829102, 91538201, 61771088, 61751212 and the Fundamental Research Funds for the Central Universities under Grant No. DUT19GJ201 and Dalian Innovation leader's support Plan under Grant No. 2018RD07. Hongshuang Zhang and Yu Zengy contributed equally to this work.	Achanta R., 2010, TECH REP; Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461; Bin Jin, 2017, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2017.185; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; Chen XL, 2014, PROC CVPR IEEE, P2035, DOI 10.1109/CVPR.2014.261; Craye C, 2016, IEEE INT CONF ROBOT, P2303, DOI 10.1109/ICRA.2016.7487379; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563; Hsu KJ, 2019, IEEE T IMAGE PROCESS, V28, P5435, DOI 10.1109/TIP.2019.2917224; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266; Kingma D.P, P 3 INT C LEARNING R; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78; Li GB, 2018, AAAI CONF ARTIF INTE, P7024; Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58; Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80; Long J. L., 2014, ADV NEURAL INFORM PR, V27, P1601; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Nguyen Duc Tam, 2019, DEEPUSPS DEEP ROBUST; Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780; Qian MY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.021; Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606; Rabinovich A, 2015, PROC INT C LEARN REP; Ramanishka V, 2017, PROC CVPR IEEE, P3135, DOI 10.1109/CVPR.2017.334; Suganuma M, 2019, PROC CVPR IEEE, P9031, DOI 10.1109/CVPR.2019.00925; Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31; Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304; Tsai D, 2011, IEEE I CONF COMP VIS, P611, DOI 10.1109/ICCV.2011.6126295; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404; Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938; Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50; Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330; Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929; Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu YY, 2019, IEEE I CONF COMP VIS, P3788, DOI 10.1109/ICCV.2019.00389; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732; Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623; Zeng Y, 2018, PROC CVPR IEEE, P1644, DOI 10.1109/CVPR.2018.00177; Zeng Y, 2018, IEEE T IMAGE PROCESS, V27, P4545, DOI 10.1109/TIP.2018.2838761; Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436; Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165; Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31; Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou DY, 2004, ADV NEUR IN, V16, P169; Zhuge YZ, 2019, AAAI CONF ARTIF INTE, P9340; Zund F, 2013, IEEE IMAGE PROC, P1845, DOI 10.1109/ICIP.2013.6738380	62	4	4	8	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3577	3589		10.1109/TPAMI.2021.3059783	http://dx.doi.org/10.1109/TPAMI.2021.3059783			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33591912				2022-12-18	WOS:000805820500018
J	Zhang, ZS; Wu, YW; Zhou, JR; Duan, SF; Zhao, H; Wang, R				Zhang, Zhuosheng; Wu, Yuwei; Zhou, Junru; Duan, Sufeng; Zhao, Hai; Wang, Rui			SG-Net: Syntax Guided Transformer for Language Representation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Syntactics; Task analysis; Bit error rate; Feature extraction; Context modeling; Linguistics; Knowledge discovery; Artificial intelligence; natural language processing; transformer; language representation; reading comprehension; machine translation		Understanding human language is one of the key themes of artificial intelligence. For language representation, the capacity of effectively modeling the linguistic knowledge from the detail-riddled and lengthy texts and getting ride of the noises is essential to improve its performance. Traditional attentive models attend to all words without explicit constraint, which results in inaccurate concentration on some dispensable words. In this work, we propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanisms for better linguistically motivated word representations. In detail, for self-attention network (SAN) sponsored Transformer-based encoder, we introduce syntactic dependency of interest (SDOI) design into the SAN to form an SDOI-SAN with syntax-guided self-attention. Syntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation. The proposed SG-Net is applied to typical Transformer encoders. Extensive experiments on popular benchmark tasks, including machine reading comprehension, natural language inference, and neural machine translation show the effectiveness of the proposed SG-Net design.	[Zhang, Zhuosheng; Wu, Yuwei; Zhou, Junru; Duan, Sufeng; Zhao, Hai; Wang, Rui] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China; [Zhang, Zhuosheng; Wu, Yuwei; Zhou, Junru; Duan, Sufeng; Zhao, Hai; Wang, Rui] Shanghai Jiao Tong Univ, Key Lab Shanghai Educ Commiss Intelligent Interac, Shanghai 200240, Peoples R China; [Zhang, Zhuosheng; Wu, Yuwei; Zhou, Junru; Duan, Sufeng; Zhao, Hai; Wang, Rui] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China	Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University	Zhao, H (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; Zhao, H (corresponding author), Shanghai Jiao Tong Univ, Key Lab Shanghai Educ Commiss Intelligent Interac, Shanghai 200240, Peoples R China.; Zhao, H (corresponding author), Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.	zhangzs@sjtu.edu.cn; will8821@sjtu.edu.cn; zhoujunru@sjtu.edu.cn; 1140339019dsf@sjtu.edu.cn; zhaohai@cs.sjtu.edu.cn; wangrui.nlp@gmail.com		Zhang, Zhuosheng/0000-0002-4183-3645	National Key Research and Development Program of China [2017YFB0304100]; Key Projects of National Natural Science Foundation of China [U1836222, 61733011]; Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model	National Key Research and Development Program of China; Key Projects of National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model	This work was supported in part by the National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. Part of this study has been accepted as "SG-Net: Syntax-Guided Machine Reading Comprehension" [1] in the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020). This paper extends the previous syntax-guided attention method to natural language comprehension, inference, and generation tasks. We further conduct comprehensive experiments, to verify the effectiveness, as well as generalization ability on different benchmarks with thorough case studies.	Ando RK, 2005, J MACH LEARN RES, V6, P1817; Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Blunsom P., 2010, P 2010 C EMP METH NA, P1204; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Bowman SR, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1466; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Brown P. F., 1992, Computational Linguistics, V18, P467; Chen KH, 2018, AAAI CONF ARTIF INTE, P4792; Chen KH, 2018, IEEE-ACM T AUDIO SPE, V26, P266, DOI 10.1109/TASLP.2017.2772846; Chen Qian, 2017, ARXIV170801353, DOI DOI 10.18653/V1/W17-5307; Clark K, 2019, BLACKBOXNLP WORKSHOP ON ANALYZING AND INTERPRETING NEURAL NETWORKS FOR NLP AT ACL 2019, P276, DOI 10.18653/v1/w19-4828; Collins M., 2005, P 43 ANN M ASS COMP, V43, P531, DOI DOI 10.3115/1219840.1219906; Conneau A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2126; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; Duan SF, 2019, INT CONF ASIAN LANG, P396, DOI 10.1109/IALP48816.2019.9037672; Dyer C, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P334; Eriguchi A, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P823; Ghaeini R., 2018, P C N AM ASS CHAPT A, V1, P1460, DOI DOI 10.18653/V1/N18-1132; Hendrycks Dan, 2016, ARXIV160608415; Hsu H., 2005, ENCY BIOSTATISTICS, V6, DOI [10.1002/0470011815.b2a15112, DOI 10.1002/0470011815.B2A15112]; Ivanova A., 2013, PROC 51 ANN MEET ING, P31; Kasai Jungo, 2019, LONG SHORT PAPERS, P701, DOI DOI 10.18653/V1/N19-1075; Kim Sang E.F.T., 2003, P 7 C NAT LANG LEARN; Kim S, 2019, AAAI CONF ARTIF INTE, P6586; Kingma D.P, P 3 INT C LEARNING R; Kitaev N, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2676; Kubler S., 2009, DEPENDENCY PARSING, DOI [10.2200/S00169ED1V01Y200901HLT002, DOI 10.2200/S00169ED1V01Y200901HLT002]; Lai G., 2017, EMNLP, P785, DOI [10.18653/v1/D17-1082, DOI 10.18653/V1/D17-1082]; Lan Z., 2020, ARXIV; Li JH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P688, DOI 10.18653/v1/P17-1064; Li ZC, 2020, AAAI CONF ARTIF INTE, V34, P8311; Li ZC, 2020, AAAI CONF ARTIF INTE, V34, P8319; Li Zuchao, 2018, P 27 INT C COMP LING, P3203; Liu XD, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4487; Liu Y., 2019, ARXIV; Ma CP, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1253; Ma XZ, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1403; McNemar Q, 1947, PSYCHOMETRIKA, V12, P153, DOI 10.1007/BF02295996; Metheniti E., 2019, PROC 5 INT C DEPENDE, P100; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mou LL, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P130; Mudrakarta PK, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1896; Nangia Nikita, 2017, P 2 WORKSH EV VECT S, P1, DOI DOI 10.18653/V1/W17-5301; Nivre J., 2005, THESIS VAXJO U SWEDE; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perelygin Socher A, 2013, P 2013 C EMP METH NA, P1631; Pollard Carl, 1994, HEAD DRIVEN PHRASE S; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Rajpurkar P., 2016, CORR, P2383, DOI [10.18653/v1/D16-1264, DOI 10.18653/V1/D16-1264]; Rajpurkar P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P784; Ran Q., 2019, ARXIV 190303033; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Seo Minjoon, 2016, ARXIV161101603; Shen Y., 2018, PROC INT C LEARN REP; Speer R, 2017, AAAI CONF ARTIF INTE, P4444; Strubell Emma, 2018, P 2018 C EMP METH NA, P5027, DOI [DOI 10.18653/V1/D18-1548, 10.18653/v1/D18-1548]; Sumita E., 2017, P 2017 C EMP METH NA, P2846; Sun K., 2019, P 2019 C N AM CHAP A, P2633; Vaswani A, 2017, ADV NEUR IN, V30; Wang Alex, 2018, ARXIV180407461, DOI DOI 10.18653/V1/W18-5446; Wang SN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4137; Wang WH, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2306; Wang XY, 2019, IEEE ACCESS, V7, P5014, DOI 10.1109/ACCESS.2018.2885032; Wang YS, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1061; Wu Wei, 2018, P 2018 C EMP METH NA, P3729; Yang ZW, 2021, ANIM BIOTECHNOL, V32, P67, DOI 10.1080/10495398.2019.1653901; Yu AW, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1880, DOI 10.18653/v1/P17-1172; Zhang SL, 2020, AAAI CONF ARTIF INTE, V34, P9563; Zhang Yue, 2011, P 49 ANN M ASS COMP, P188; Zhang Z, 2018, P 27 INT C COMP LING; Zhang Z., 2020, P 8 INT C LEARN REPR, P1; Zhang Z., 2018, PROC 27 INT C COMPUT, P1802; Zhang Z., 2019, P 33 PAC AS C LANG, P298; Zhang ZS, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1382; Zhang ZS, 2021, AAAI CONF ARTIF INTE, V35, P14506; Zhang ZS, 2019, IEEE-ACM T AUDIO SPE, V27, P1664, DOI 10.1109/TASLP.2019.2922537; Zhang Zhuosheng, 2018, P 27 INT C COMP LING, P3740; Zhang Zhuosheng, 2020, P 34 AAAI C ART INT, DOI DOI 10.1609/AAAI.V34I05.6511; Zhang Zhuosheng, 2020, P 34 AAAI C ART INT; Zhou JR, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2396; Zhou Junru, 2020, P FIND ASS COMP LING, P4450	84	4	4	13	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3285	3299		10.1109/TPAMI.2020.3046683	http://dx.doi.org/10.1109/TPAMI.2020.3046683			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33351752	Green Accepted			2022-12-18	WOS:000803117500036
J	Iquebal, AS; Bukkapatnam, S				Iquebal, Ashif Sikandar; Bukkapatnam, Satish			Consistent Estimation of the Max-Flow Problem: Towards Unsupervised Image Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Minimization; TV; Estimation; Tumors; Markov processes; Manuals; Continuous max-flow; unsupervised image segmentation; maximum a posteriori estimation; posterior consistency	EXPECTATION-MAXIMIZATION; BEHAVIOR; MODEL	Advances in the image-based diagnostics of complex biological and manufacturing processes have brought unsupervised image segmentation to the forefront of enabling automated, on the fly decision making. However, most existing unsupervised segmentation approaches are either computationally complex or require manual parameter selection (e.g., flow capacities in max-flow/min-cut segmentation). In this work, we present a fully unsupervised segmentation approach using a continuous max-flow formulation over the image domain while optimally estimating the flow parameters from the image characteristics. More specifically, we show that the maximum a posteriori estimate of the image labels can be formulated as a continuous max-flow problem given the flow capacities are known. The flow capacities are then iteratively obtained by employing a novel Markov random field prior over the image domain. We present theoretical results to establish the posterior consistency of the flow capacities. We compare the performance of our approach using brain tumor image segmentation, defect identification in additively manufactured components using electron microscopic images, and segmentation of multiple real-world images. Comparative results with several state-of-the-art supervised as well as unsupervised methods suggest that the present method performs statistically similar to the supervised methods, but results in more than 90 percent improvement in the Dice score when compared to the state-of-the-art unsupervised methods.	[Iquebal, Ashif Sikandar; Bukkapatnam, Satish] Texas A&M Univ, Dept Ind & Syst Engn, College Stn, TX 77843 USA	Texas A&M University System; Texas A&M University College Station	Iquebal, AS (corresponding author), Texas A&M Univ, Dept Ind & Syst Engn, College Stn, TX 77843 USA.	ashif_22@tamu.edu; satish@tamu.edu			National Science Foundation [ECCS 1547075]; Texas A&M President's Excellence Fund at Texas AM University	National Science Foundation(National Science Foundation (NSF)); Texas A&M President's Excellence Fund at Texas AM University	The authors would like to acknowledge the kind support from the National Science Foundation through award ECCS 1547075 and the X-Grants, part of the Texas A&M President's Excellence Fund at Texas A&M University.	Appleton B, 2006, IEEE T PATTERN ANAL, V28, P106, DOI 10.1109/TPAMI.2006.12; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Attar H, 2014, MAT SCI ENG A-STRUCT, V593, P170, DOI 10.1016/j.msea.2013.11.038; Bae E, 2011, INT J COMPUT VISION, V92, P112, DOI 10.1007/s11263-010-0406-y; Barron A., ANN STAT, V27, P536; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Boykov Y, 1998, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.1998.698673; Boykov Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P26; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2000, LECT NOTES COMPUT SC, V1935, P276; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800; Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Condat L., SIAM J IMAG SCI, V10, P1258; Couprie C, 2011, SIAM J IMAGING SCI, V4, P905, DOI 10.1137/100799186; Cour T, 2005, PROC CVPR IEEE, P1124; DERIN H, 1987, IEEE T PATTERN ANAL, V9, P39, DOI 10.1109/TPAMI.1987.4767871; Duval L, 2014, IEEE IMAGE PROC, P4862, DOI 10.1109/ICIP.2014.7025985; Dziugaite GK, 2018, ADV NEUR IN, V31; FREEDMAN DA, 1965, ANN MATH STAT, V36, P454, DOI 10.1214/aoms/1177700155; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Grimmett G. R, 1973, B LOND MATH SOC, V5, P81, DOI DOI 10.1112/BLMS/5.1.81; Koch LM, 2018, IEEE T PATTERN ANAL, V40, P1683, DOI 10.1109/TPAMI.2017.2711020; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Markou M, 2006, IEEE T PATTERN ANAL, V28, P1664, DOI 10.1109/TPAMI.2006.196; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Menze B., 2013, P MICCAI CHALL MULT, P57; Menze Bjoern H, 2015, IEEE Trans Med Imaging, V34, P1993, DOI 10.1109/TMI.2014.2377694; Nadler B, 2007, ADV NEURAL INFORM PR, P1017; REED TR, 1990, IEEE T PATTERN ANAL, V12, P1, DOI 10.1109/34.41379; Sheng Li, 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P469; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; STRANG G, 1983, MATH PROGRAM, V26, P123, DOI 10.1007/BF02592050; Strong D., 2003, INVERSE PROBL, V19; Nguyen TM, 2013, IEEE T CIRC SYST VID, V23, P621, DOI 10.1109/TCSVT.2012.2211176; Wang GT, 2019, IEEE T PATTERN ANAL, V41, P1559, DOI 10.1109/TPAMI.2018.2840695; Wei K., 2015, P 5 ECC THEM C COMP, P17; Yang WL, 2007, C IND ELECT APPL, P2550; Yuan J, 2014, NUMER MATH, V126, P559, DOI 10.1007/s00211-013-0569-x; Yuan J, 2010, LECT NOTES COMPUT SC, V6316, P379, DOI 10.1007/978-3-642-15567-3_28; Yuan J, 2010, PROC CVPR IEEE, P2217, DOI 10.1109/CVPR.2010.5539903; Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424	44	4	4	9	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2346	2357		10.1109/TPAMI.2020.3039745	http://dx.doi.org/10.1109/TPAMI.2020.3039745			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33226936	Green Submitted			2022-12-18	WOS:000792921400011
J	Lin, L; Gao, YM; Gong, K; Wang, M; Liang, XD				Lin, Liang; Gao, Yiming; Gong, Ke; Wang, Meng; Liang, Xiaodan			Graphonomy: Universal Image Parsing via Graph Reasoning and Transfer	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Task analysis; Cognition; Feature extraction; Annotations; Image segmentation; Faces; Image parsing; knowledge reasoning; transfer learning; panoptic segmentation		Prior highly-tuned image parsing models are usually studied in a certain domain with a specific set of semantic labels and can hardly be adapted into other scenarios (e.g.sharing discrepant label granularity) without extensive re-training. Learning a single universal parsing model by unifying label annotations from different domains or at various levels of granularity is a crucial but rarely addressed topic. This poses many fundamental learning challenges, e.g.discovering underlying semantic structures among different label granularity or mining label correlation across relevant tasks. To address these challenges, we propose a graph reasoning and transfer learning framework, named "Graphonomy," which incorporates human knowledge and label taxonomy into the intermediate graph representation learning beyond local convolutions. In particular, Graphonomy learns the global and structured semantic coherency in multiple domains via semantic-aware graph reasoning and transfer, enforcing the mutual benefits of the parsing across domains (e.g.different datasets or co-related tasks). The Graphonomy includes two iterated modules: Intra-Graph Reasoning and Inter-Graph Transfer modules. The former extracts the semantic graph in each domain to improve the feature representation learning by propagating information with the graph; the latter exploits the dependencies among the graphs from different domains for bidirectional knowledge transfer. We apply Graphonomy to two relevant but different image understanding research topics: human parsing and panoptic segmentation, and show Graphonomy can handle both of them well via a standard pipeline against current state-of-the-art approaches. Moreover, some extra benefit of our framework is demonstrated, e.g., generating the human parsing at various levels of granularity by unifying annotations across different datasets.	[Lin, Liang; Gao, Yiming; Gong, Ke; Liang, Xiaodan] Sun Yat Sen Univ, Guangzhou 510006, Peoples R China; [Lin, Liang; Gao, Yiming; Gong, Ke; Liang, Xiaodan] Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou, Peoples R China; [Lin, Liang; Gao, Yiming; Gong, Ke; Liang, Xiaodan] Minist Educ, Engn Res Ctr Adv Comp Engn Software, Guangzhou, Peoples R China; [Wang, Meng] Hefei Univ Technol, Hefei 230000, Peoples R China	Sun Yat Sen University; Hefei University of Technology	Liang, XD (corresponding author), Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.	linliang@ieee.org; gaoym9@mail2.sysu.edu.cn; kegong936@gmail.com; wangmeng@hfut.edu.cn; xdliang328@gmail.com			National Key Research and Development Program of China [2018YFC0830103]; National Natural Science Foundation of China (NSFC) [U1811463, 61836012]	National Key Research and Development Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key Research and Development Program of China under Grant No. 2018YFC0830103 and in part by the National Natural Science Foundation of China (NSFC) under Grants U1811463 and 61836012.	Bilinski P, 2018, PROC CVPR IEEE, P6596, DOI 10.1109/CVPR.2018.00690; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen Q, 2021, IEEE T CIRC SYST VID, V31, P2288, DOI 10.1109/TCSVT.2020.3020257; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong J, 2014, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2014.113; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fang HS, 2018, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2018.00015; Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47; Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715; He K, 2017, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.2017.322; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman Judy, 2014, NIPS; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Kipf TN, 2016, P INT C LEARN REPR; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170; Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789; Li QL, 2017, 2017 10TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI); Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234; Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347; Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8; Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163; Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360; Liang XH, 2016, IEEE INT CONF ELECTR, P14, DOI 10.1109/ICEIEC.2016.7589677; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Rebuffi SA, 2018, PROC CVPR IEEE, P8119, DOI 10.1109/CVPR.2018.00847; Velickovic P., 2018, P INT C LEARN REPR; Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717; Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544	55	4	4	5	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2504	2518		10.1109/TPAMI.2020.3043268	http://dx.doi.org/10.1109/TPAMI.2020.3043268			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33290211	Green Submitted			2022-12-18	WOS:000792921400022
J	Lin, MB; Ji, RR; Sun, XS; Zhang, BC; Huang, FY; Tian, YH; Tao, DC				Lin, Mingbao; Ji, Rongrong; Sun, Xiaoshuai; Zhang, Baochang; Huang, Feiyue; Tian, Yonghong; Tao, Dacheng			Fast Class-Wise Updating for Online Hashing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Hash functions; Optimization; Binary codes; Training data; Complexity theory; Boosting; Image retrieval; similarity preserving; online hashing; binary codes	ITERATIVE QUANTIZATION; PROCRUSTEAN APPROACH; BINARY	Online image hashing has received increasing research attention recently, which processes large-scale data in a streaming fashion to update the hash functions on-the-fly. To this end, most existing works exploit this problem under a supervised setting, i.e., using class labels to boost the hashing performance, which suffers from the defects in both adaptivity and efficiency: First, large amounts of training batches are required to learn up-to-date hash functions, which leads to poor online adaptivity. Second, the training is time-consuming, which contradicts with the core need of online learning. In this paper, a novel supervised online hashing scheme, termed Fast Class-wise Updating for Online Hashing (FCOH), is proposed to address the above two challenges by introducing a novel and efficient inner product operation. To achieve fast online adaptivity, a class-wise updating method is developed to decompose the binary code learning and alternatively renew the hash functions in a class-wise fashion, which well addresses the burden on large amounts of training batches. Quantitatively, such a decomposition further leads to at least 75 percent storage saving. To further achieve online efficiency, we propose a semi-relaxation optimization, which accelerates the online training by treating different binary constraints independently. Without additional constraints and variables, the time complexity is significantly reduced. Such a scheme is also quantitatively shown to well preserve past information during updating hashing functions. We have quantitatively demonstrated that the collective effort of class-wise updating and semi-relaxation optimization provides a superior performance comparing to various state-of-the-art methods, which is verified through extensive experiments on three widely-used datasets.	[Lin, Mingbao; Ji, Rongrong; Sun, Xiaoshuai] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Fujian, Peoples R China; [Ji, Rongrong] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Fujian, Peoples R China; [Zhang, Baochang] Beihang Univ, Beijing 100083, Peoples R China; [Huang, Feiyue] Tencent, Youtu Lab, Shanghai 200233, Peoples R China; [Tian, Yonghong] Peking Univ, Dept Comp Sci & Technol, Beijing 100871, Peoples R China; [Tao, Dacheng] Univ Sydney, Sch Comp Sci, Fac Engn, Darlington, NSW 2008, Australia	Xiamen University; Xiamen University; Beihang University; Tencent; Peking University; University of Sydney	Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Fujian, Peoples R China.	lmbxmu@stu.xmu.edu.cn; rrji@xmu.edu.cn; xssun@xmu.edu.cn; bczhang@buaa.edu.cn; garyhuang@tencent.com; yhtian@pku.edu.cn; dacheng.tao@sydney.edu.au			National Natural Science Foundation of China [62025603, U1705262, 617 72443, 61572410, 61802324, 61702136]; CCF-Baidu Open Fund; Australian Research Council [FL-170100117]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CCF-Baidu Open Fund; Australian Research Council(Australian Research Council)	This work was supported by the National Natural Science Foundation of China (No. 62025603, No. U1705262, No. 617 72443, No. 61572410, No. 61802324, and No. 61702136). This work was also supported by the CCF-Baidu Open Fund and Australian Research Council Project FL-170100117.	Babenko Boris, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1346, DOI 10.1109/ICCVW.2009.5457453; Cakir F, 2019, IEEE T PATTERN ANAL, V41, P2424, DOI 10.1109/TPAMI.2019.2914897; Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55; Cakir F, 2017, COMPUT VIS IMAGE UND, V156, P162, DOI 10.1016/j.cviu.2016.10.009; Cakir F, 2015, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2015.125; Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134; Chen XX, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Crammer K, 2006, J MACH LEARN RES, V7, P551; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432; Gui J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1485, DOI 10.1145/3219819.3219955; He K, 2018, PROC CVPR IEEE, P4023, DOI 10.1109/CVPR.2018.00423; Horadam K. J., 2012, HADAMARD MATRICES TH, DOI 10.1515/9781400842902; Huang L., 2013, IJCAI, P1422; Jiang JY, 2009, PROC CVPR IEEE, P1810, DOI 10.1109/CVPRW.2009.5206761; Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342; Jin L, 2019, IEEE T NEUR NET LEAR, V30, P1429, DOI 10.1109/TNNLS.2018.2869601; Kittler J., 2001, Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, pI, DOI 10.1109/CVPR.2001.990552; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Leng C, 2015, PROC CVPR IEEE, P2503, DOI 10.1109/CVPR.2015.7298865; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Lin MB, 2020, INT J COMPUT VISION, V128, P2279, DOI 10.1007/s11263-020-01332-z; Lin MB, 2020, IEEE T IMAGE PROCESS, V29, P5289, DOI 10.1109/TIP.2020.2981879; Lin MB, 2019, AAAI CONF ARTIF INTE, P8722; Lin MB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1635, DOI 10.1145/3240508.3240519; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1589, DOI 10.1145/3240508.3240684; Liu H, 2019, IEEE T PATTERN ANAL, V41, P941, DOI 10.1109/TPAMI.2018.2819978; Liu L, 2016, IEEE T NEUR NET LEAR, V27, P2526, DOI 10.1109/TNNLS.2015.2495345; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685; Ma C, 2017, IEEE T IMAGE PROCESS, V26, P1939, DOI 10.1109/TIP.2017.2675342; Qi MS, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P744, DOI 10.1145/3123266.3123311; Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63; Schapire R. E., 1997, MACH LEARN P 14 INT, P313; Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887; Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Simonyan K., 2015, INT C LEARN REPR ICL; Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344; Wang J, 2017, IEEE INFOCOM SER, DOI 10.1007/s12083-017-0556-6; Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48; Weiss Y., 2009, ADV NEURAL INFORM PR, P1753; Weng ZY, 2020, AAAI CONF ARTIF INTE, V34, P12354; Wu L, 2019, IEEE ACCESS, V7, P36489, DOI 10.1109/ACCESS.2019.2900489; Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133; Xie L, 2016, AAAI CONF ARTIF INTE, P294; Yao T, 2019, PATTERN RECOGN, V89, P1, DOI 10.1016/j.patcog.2018.12.012; Yu MY, 2017, IEEE T NEUR NET LEAR, V28, P2899, DOI 10.1109/TNNLS.2016.2609463; Zhang PF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1762, DOI 10.1145/3123266.3123320; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu X., 2013, P 21 ACM INT C MULTI, P143, DOI [10.1145/2502081.2502107, DOI 10.1145/2502081.2502107]	56	4	4	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2453	2467		10.1109/TPAMI.2020.3042193	http://dx.doi.org/10.1109/TPAMI.2020.3042193			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33270558	Green Submitted			2022-12-18	WOS:000792921400019
J	Mehta, S; Hajishirzi, H; Rastegari, M				Mehta, Sachin; Hajishirzi, Hannaneh; Rastegari, Mohammad			DiCENet: Dimension-Wise Convolutions for Efficient Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computer architecture; Tensors; Standards; Kernel; Convolutional codes; Task analysis; Object detection; Deep convolutional neural network; image classification; object detection; semantic segmentation; efficient networks		We introduce a novel and generic convolutional unit, DiCE unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. The DiCE unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the DiCE unit shows significant improvements across different architectures. When DiCE units are stacked to build the DiCENet model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4 percent higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet).	[Mehta, Sachin; Hajishirzi, Hannaneh; Rastegari, Mohammad] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Mehta, S (corresponding author), Univ Washington, Seattle, WA 98195 USA.	sacmehta@cs.washington.edu; hannaneh@cs.washington.edu; mrast@cs.washington.edu			ONR [N00014-18-1-2826]; DARPA [N66001-19-2-403]; NSF [IIS-1616112, IIS1252835]; Allen Distinguished Investigator Award; Samsung GRO	ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Allen Distinguished Investigator Award; Samsung GRO(Samsung)	This work was supported by ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF (IIS-1616112, IIS1252835), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, and Amazon. The authors would also like to thank the members of the PRIOR team at the Allen Institute for Artificial Intelligence (AI2), the H2Lab at the University of Washington, Seattle, and anonymous reviewers for their valuable feedback and comments.	Adam, 2017, MOBILENETS EFFICIENT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andri R, 2018, IEEE T COMPUT AID D, V37, P48, DOI 10.1109/TCAD.2017.2682138; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Han S., 2016, P 4 INT C LEARN REPR, P1; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291; Hubara I., 2016, P 30 INT C NEUR INF, P4114; Jin J., 2014, ARXIV PREPRINT ARXIV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li C, 2018, LECT NOTES COMPUT SC, V11214, P746, DOI 10.1007/978-3-030-01249-6_45; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Ma N., 2018, PROC EUR C COMPUT VI, P4510; Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941; Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan Mingxing, 2019, BRIT MACH VIS C BMVC; Veit A., 2018, PROC EUR C COMPUT VI, P730; Wang K, 2019, PROC CVPR IEEE, P8604, DOI 10.1109/CVPR.2019.00881; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wen W, 2016, ADV NEUR IN, V29; Wightman R, PYTORCH IMAGE MODELS; Wortsman M., 2019, ADV NEURAL INFORM PR, P2684; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	44	4	4	5	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2416	2425		10.1109/TPAMI.2020.3041871	http://dx.doi.org/10.1109/TPAMI.2020.3041871			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33264092	Green Submitted			2022-12-18	WOS:000792921400016
J	Peng, H; Hu, Y; Chen, JZ; Wang, HY; Li, Y; Cai, HM				Peng, Hong; Hu, Yu; Chen, Jiazhou; Wang, Haiyan; Li, Yang; Cai, Hongmin			Integrating Tensor Similarity to Enhance Clustering Performance	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Tensors; Matrix decomposition; Laplace equations; Clustering algorithms; Task analysis; Noise measurement; Manifolds; Tensor similarity; kronecker product; spectral clustering; under-sampled; imbalanced dataset; unsupervised learning	CONSISTENCY; ALGORITHM	The performance of most clustering methods hinges on the used pairwise affinity, which is usually denoted by a similarity matrix. However, the pairwise similarity is notoriously known for its vulnerability of noise contamination or the imbalance in samples or features, and thus hinders accurate clustering. To tackle this issue, we propose to use information among samples to boost the clustering performance. We proved that a simplified similarity for pairs, denoted by a fourth order tensor, equals to the Kronecker product of pairwise similarity matrices under decomposable assumption, or provide complementary information for which the pairwise similarity missed under indecomposable assumption. Then a high order similarity matrix is obtained from the tensor similarity via eigenvalue decomposition. The high order similarity capturing spatial information serves as a robust complement for the pairwise similarity. It is further integrated with the popular pairwise similarity, named by IPS2, to boost the clustering performance. Extensive experiments demonstrated that the proposed IPS2 significantly outperformed previous similarity-based methods on real-world datasets and it was capable of handling the clustering task over under-sampled and noisy datasets.	[Peng, Hong; Hu, Yu; Chen, Jiazhou; Wang, Haiyan; Li, Yang; Cai, Hongmin] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China	South China University of Technology	Cai, HM (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.	1641206766@qq.com; jasonscut@outlook.com; csjzchen@mail.scut.edu.cn; wanghy_ad@163.com; ylijyangr@gmail.com; hmcai@scut.edu.cn		Cai, Hongmin/0000-0002-2747-7234; Li, Yang/0000-0001-6970-3589	National Natural Science Foundation of China [61771007, 61472145]; Key-Area Research and Development of Guangdong Province [2020B010166002, 2020B1111190001]; Guangdong Natural Science Foundation [2017A030312008]; Health & Medical Collaborative Innovation Project of Guangzhou City [201803010021, 202002020049]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key-Area Research and Development of Guangdong Province; Guangdong Natural Science Foundation(National Natural Science Foundation of Guangdong Province); Health & Medical Collaborative Innovation Project of Guangzhou City	This work was supported in part by the National Natural Science Foundation of China (61771007, 61472145), the Key-Area Research and Development of Guangdong Province under Grant (2020B010166002, 2020B1111190001), Guangdong Natural Science Foundation (2017A030312008), and the Health & Medical Collaborative Innovation Project of Guangzhou City (201803010021, 202002020049).	Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; Agarwal S, 2005, PROC CVPR IEEE, P838; Agarwal S., 2006, ICML, P17, DOI DOI 10.1145/1143844.1143847; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Ghoshdastidar D., 2017, J MACH LEARN RES, V18, P1638; Huang JZX, 2005, IEEE T PATTERN ANAL, V27, P657, DOI 10.1109/TPAMI.2005.95; Huang YC, 2011, IEEE T PATTERN ANAL, V33, P1266, DOI 10.1109/TPAMI.2011.25; Jain S, 2013, IEEE I CONF COMP VIS, P3511, DOI 10.1109/ICCV.2013.436; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kumar Abhishek, 2011, NEURIPS, P2, DOI DOI 10.5555/2986459.2986617; Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274; Li P, 2017, ADV NEUR IN, V30; Li X, 2014, IEEE T KNOWL DATA EN, V26, P2588, DOI 10.1109/TKDE.2013.126; Lin WC, 2017, INFORM SCIENCES, V409, P17, DOI 10.1016/j.ins.2017.05.008; Lin YS, 2014, IEEE T KNOWL DATA EN, V26, P1575, DOI 10.1109/TKDE.2013.19; Liu FC, 2018, P NATL ACAD SCI USA, V115, P927, DOI 10.1073/pnas.1718449115; Liu H., 2010, ADV NEURAL INFORM PR, P1414; Mori U, 2016, IEEE T KNOWL DATA EN, V28, P181, DOI 10.1109/TKDE.2015.2462369; Ng AY, 2002, ADV NEUR IN, V14, P849; Nguyen CH, 2021, IEEE T PATTERN ANAL, V43, P2710, DOI [10.1007/s00170-020-05222-z, 10.1109/TPAMI.2020.2974746]; Ochs P, 2012, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2012.6247728; Purkait P, 2017, IEEE T PATTERN ANAL, V39, P1697, DOI 10.1109/TPAMI.2016.2614980; Sarkar S, 2020, IEEE T PATTERN ANAL, V42, P2257, DOI 10.1109/TPAMI.2019.2912599; Su LJ, 2020, IEEE T INFORM THEORY, V66, P324, DOI 10.1109/TIT.2019.2934157; Tao ZQ, 2017, AAAI CONF ARTIF INTE, P4285; Vazquez A, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/07/P07006; Wang HZ, 2019, IEEE T PATTERN ANAL, V41, P697, DOI 10.1109/TPAMI.2018.2803173; Yang MS, 2004, IEEE T PATTERN ANAL, V26, P434, DOI 10.1109/TPAMI.2004.1265860; Zhou D, 2006, P 2006 C ADV NEURAL, V19, DOI 10.7551/mitpress/7503.003.0205	29	4	4	8	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2582	2593		10.1109/TPAMI.2020.3040306	http://dx.doi.org/10.1109/TPAMI.2020.3040306			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33232225	Green Submitted			2022-12-18	WOS:000792921400027
J	You, C; Li, C; Robinson, DP; Vidal, R				You, Chong; Li, Chi; Robinson, Daniel P.; Vidal, Rene			Self-Representation Based Unsupervised Exemplar Selection in a Union of Subspaces	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Unsupervised exemplar selection; imbalanced data; large-scale data; subspace clustering	SIMULTANEOUS SPARSE APPROXIMATION; FACE RECOGNITION; ALGORITHMS	Finding a small set of representatives from an unlabeled dataset is a core problem in a broad range of applications such as dataset summarization and information extraction. Classical exemplar selection methods such as k-medoids work under the assumption that the data points are close to a few cluster centroids, and cannot handle the case where data lie close to a union of subspaces. This paper proposes a new exemplar selection model that searches for a subset that best reconstructs all data points as measured by the l(1) norm of the representation coefficients. Geometrically, this subset best covers all the data points as measured by the Minkowski functional of the subset. To solve our model efficiently, we introduce a farthest first search algorithm that iteratively selects the worst represented point as an exemplar. When the dataset is drawn from a union of independent subspaces, our method is able to select sufficiently many representatives from each subspace. We further develop an exemplar based subspace clustering method that is robust to imbalanced data and efficient for large scale data. Moreover, we show that a classifier trained on the selected exemplars (when they are labeled) can correctly classify the rest of the data points.	[You, Chong] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Li, Chi] Apple Inc, Cupertino, CA 95014 USA; [Robinson, Daniel P.] Lehigh Univ, Dept Ind & Syst Engn, Bethlehem, PA 18015 USA; [Vidal, Rene] Johns Hopkins Univ, Dept Biomed Engn, Baltimore, MD 21218 USA	University of California System; University of California Berkeley; Apple Inc; Lehigh University; Johns Hopkins University	You, C (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	cyou@berkeley.edu; chi_li@jhu.edu; daniel.p.robinson@gmail.edu; rvidal@cis.jhu.edu	You, Chong/AAV-3338-2020	You, Chong/0000-0001-7821-2378	Northrop Grumman Research in Applications for Learning Machines (REALM) Initiative;  [NSF 1618637];  [IARPA 127228];  [NSF 1934931]	Northrop Grumman Research in Applications for Learning Machines (REALM) Initiative; ; ; 	This work was supported in part by grants NSF 1618637, IARPA 127228, NSF 1934931, and the Northrop Grumman Research in Applications for Learning Machines (REALM) Initiative.	Abdolali M, 2019, SIGNAL PROCESS, V163, P166, DOI 10.1016/j.sigpro.2019.05.017; Adler A, 2015, IEEE T NEUR NET LEAR, V26, P2234, DOI 10.1109/TNNLS.2014.2374631; Akbani R, 2004, LECT NOTES COMPUT SC, V3201, P39, DOI 10.1007/978-3-540-30115-8_7; Aldroubi A., 2019, FRONT APPL MATH STAT, V4, P65; Aldroubi A, 2018, APPL COMPUT HARMON A, V45, P425, DOI 10.1016/j.acha.2017.08.006; Altschuler J, 2016, PR MACH LEARN RES, V48; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Borodin A., 2009, DETERMINANTAL POINT; Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968; Brodersen Kay H., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3121, DOI 10.1109/ICPR.2010.764; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Cevher V, 2011, IEEE J-STSP, V5, P979, DOI 10.1109/JSTSP.2011.2161862; CHAN TF, 1987, LINEAR ALGEBRA APPL, V88-9, P67, DOI 10.1016/0024-3795(87)90103-0; Chen X., 2011, P AAAI C ARTIFICIAL, DOI 10.1109/TCYB.2014.2358564; Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217; Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434; Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951; Croft H. T., 1991, SPRINGER SCI BUSINES; Dalal N., 2005, P IEEE COMP SOC C CO; Das A., 2011, P 28 INT C MACH LEAR, P1057; Donoho D. L., 2005, 20054 DEP STAT STANF; Dyer EL, 2013, J MACH LEARN RES, V14, P2487; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E., 2012, ADV NEURAL INFORM PR, P19; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852; Farahat AK, 2015, KNOWL INF SYST, V45, P1, DOI 10.1007/s10115-014-0801-8; Garcia S, 2012, IEEE T PATTERN ANAL, V34, P417, DOI 10.1109/TPAMI.2011.142; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Gillenwater J., 2014, ADV NEURAL INFORM PR, V2, P3149; Gillis N, 2015, SIAM J OPTIMIZ, V25, P677, DOI 10.1137/130940670; Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226; Heckel R, 2015, IEEE T INFORM THEORY, V61, P6320, DOI 10.1109/TIT.2015.2472520; Hoffman K, 1971, LINEAR ALGEBRA, P122; Kulesza Alex, 2011, ICML; Kumar A., 2013, P INT C MACH LEARN, P231; Liu C, 2017, KNOWL-BASED SYST, V116, P58, DOI 10.1016/j.knosys.2016.10.031; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26; Mairal J, 2010, J MACH LEARN RES, V11, P19; Meng JJ, 2016, PROC CVPR IEEE, P1039, DOI 10.1109/CVPR.2016.118; Park HS, 2009, EXPERT SYST APPL, V36, P3336, DOI 10.1016/j.eswa.2008.01.039; Ren H, 2003, IEEE T AERO ELEC SYS, V39, P1232, DOI 10.1109/TAES.2003.1261124; Shen J, 2016, PR MACH LEARN RES, V48; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Toth L. F., 1943, MATEMATIKAI FIZ LAPO, V50, P40; Traganitis PA, 2018, IEEE T SIGNAL PROCES, V66, P1663, DOI 10.1109/TSP.2017.2781649; Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030; Tropp JA, 2006, SIGNAL PROCESS, V86, P589, DOI 10.1016/j.sigpro.2005.05.031; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vershynin R., 2009, LECT GEOMETRIC FUNCT; Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z; Wang HX, 2017, PATTERN RECOGN, V63, P268, DOI 10.1016/j.patcog.2016.10.014; Wang YJ, 2016, J MACH LEARN RES, V17, P1; Wei K, 2015, PR MACH LEARN RES, V37, P1954; Williamson D. P., 2011, DESIGN APPROXIMATION, DOI DOI 10.1017/CBO9780511921735; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Xin B, 2018, IEEE T SIGNAL PROCES, V66, P449, DOI 10.1109/TSP.2017.2762279; Xiong H, 2009, IEEE T SYST MAN CY B, V39, P318, DOI 10.1109/TSMCB.2008.2004559; Yang Y., 2016, PROC EUR C COMPUT VI, P731; You C., 2018, THESIS JOHNS HOPKINS; You C, 2018, LECT NOTES COMPUT SC, V11213, P68, DOI 10.1007/978-3-030-01240-3_5; You C, 2017, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR.2017.460; You C, 2016, CONF REC ASILOMAR C, P1014, DOI 10.1109/ACSSC.2016.7869521; You C, 2016, PROC CVPR IEEE, P3918, DOI 10.1109/CVPR.2016.425; You C, 2016, PROC CVPR IEEE, P3928, DOI 10.1109/CVPR.2016.426; You C, 2015, PR MACH LEARN RES, V37, P1585; Zaeemzadeh A, 2019, PROC CVPR IEEE, P5409, DOI 10.1109/CVPR.2019.00556	72	4	4	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2698	2711		10.1109/TPAMI.2020.3035599	http://dx.doi.org/10.1109/TPAMI.2020.3035599			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33147685	Green Submitted			2022-12-18	WOS:000792921400035
J	Tran, C; Shin, WY; Spitz, A; Gertz, M				Tran, Cong; Shin, Won-Yong; Spitz, Andreas; Gertz, Michael			DeepNC: Deep Generative Network Completion	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Autoregressive generative model; deep generative model of graphs; inference; network completion; partially observable network	GRAPHS; DISTANCE	Most network data are collected from partially observable networks with both missing nodes and missing edges, for example, due to limited resources and privacy settings specified by users on social media. Thus, it stands to reason that inferring the missing parts of the networks by performing network completion should precede downstream applications. However, despite this need, the recovery of missing nodes and edges in such incomplete networks is an insufficiently explored problem due to the modeling difficulty, which is much more challenging than link prediction that only infers missing edges. In this paper, we present DeepNC, a novel method for inferring the missing parts of a network based on a deep generative model of graphs. Specifically, our method first learns a likelihood over edges via an autoregressive generative model, and then identifies the graph that maximizes the learned likelihood conditioned on the observable graph topology. Moreover, we propose a computationally efficient DeepNC algorithm that consecutively finds individual nodes that maximize the probability in each node generation step, as well as an enhanced version using the expectation-maximization algorithm. The runtime complexities of both algorithms are shown to be almost linear in the number of nodes in the network. We empirically demonstrate the superiority of DeepNC over state-of-the-art network completion approaches.	[Tran, Cong] Dankook Univ, Dept Comp Sci & Engn, Yongin 16890, South Korea; [Tran, Cong] Yonsei Univ, Machine Intelligence & Data Sci Lab, Seoul 03722, South Korea; [Shin, Won-Yong] Yonsei Univ, Sch Math & Comp Computat Sci & Engn, Seoul 03722, South Korea; [Spitz, Andreas] Ecole Polytech Fed Lausanne, Sch Comp & Commun Sci, CH-1015 Lausanne, Switzerland; [Gertz, Michael] Heidelberg Univ, Inst Comp Sci, D-69120 Heidelberg, Germany	Dankook University; Yonsei University; Yonsei University; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Ruprecht Karls University Heidelberg	Shin, WY (corresponding author), Yonsei Univ, Sch Math & Comp Computat Sci & Engn, Seoul 03722, South Korea.	congtran@ieee.org; wy.shin@yonsei.ac.kr; andreas.spitz@epfl.ch; gertz@informatik.uni-heidelberg.de			Republic of Korea's MSIT (Ministry of Science and ICT) [2020-0-01463]; Korea Health Industry Development Institute (KHIDI) - Ministry of Health & Welfare, Republic of Korea [HI20C0127]; Yonsei University [2020-22-0101]	Republic of Korea's MSIT (Ministry of Science and ICT)(Ministry of Science & ICT (MSIT), Republic of Korea); Korea Health Industry Development Institute (KHIDI) - Ministry of Health & Welfare, Republic of Korea; Yonsei University	This work was supported by the Republic of Korea's MSIT (Ministry of Science and ICT), under the High-Potential Individuals Global Training Program (No. 2020-0-01463) supervised by the IITP (Institute of Information and Communications Technology Planning Evaluation), by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (HI20C0127), and by the Yonsei University Research Fund of 2020 (2020-22-0101).	Acquisti A, 2015, SCIENCE, V347, P509, DOI 10.1126/science.aaa1465; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Bojchevski A, 2018, PR MACH LEARN RES, V80; Chung J., 2014, ARXIV14123555; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dey R., 2012, 2012 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), P346, DOI 10.1109/PerComW.2012.6197508; Dobson PD, 2003, J MOL BIOL, V330, P771, DOI 10.1016/S0022-2836(03)00628-4; Eyal R, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2536775; Fischer A, 2017, PATTERN RECOGN LETT, V87, P55, DOI 10.1016/j.patrec.2016.06.014; Gentile C, 2014, PR MACH LEARN RES, V32, P757; Haldar Justin P, 2009, IEEE Signal Process Lett, V16, P584, DOI 10.1109/LSP.2009.2018223; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jain P, 2010, P ADV NEUR INF PROC, P937; Kim M, 2011, P 2011 SIAM INT C DA, P47, DOI DOI 10.1137/1.9781611972818.5; Kingma D.P., 2015, ICLR, P1; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Klebanov L, 2007, BIOL DIRECT, V2, DOI 10.1186/1745-6150-2-9; Koskinen JH, 2013, SOC NETWORKS, V35, P514, DOI 10.1016/j.socnet.2013.07.003; Kossinets G, 2006, SOC NETWORKS, V28, P247, DOI 10.1016/j.socnet.2005.07.002; Lancichinetti A, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016118; Leskovec J., 2006, P 12 ACM SIGKDD INT, P631; Leskovec J, 2010, J MACH LEARN RES, V11, P985; Li Yujia, 2018, ARXIV180303324; Liao R, 2019, ADV NEUR IN, V32; LINIAL N, 1995, COMBINATORICA, V15, P215, DOI 10.1007/BF01200757; Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027; Mahadik K., 2020, P 34 ACM INT C SUP, P1; McCormick TH, 2010, J AM STAT ASSOC, V105, P59, DOI 10.1198/jasa.2009.ap08518; Menon AK, 2011, LECT NOTES ARTIF INT, V6912, P437, DOI 10.1007/978-3-642-23783-6_28; Monti F, 2017, ADV NEUR IN, V30; Newman MEJ, 2002, P NATL ACAD SCI USA, V99, P2566, DOI 10.1073/pnas.012582999; Park H, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2051, DOI 10.1145/3219819.3220123; Red V, 2011, SIAM REV, V53, P526, DOI 10.1137/080734315; SANFELIU A, 1983, IEEE T SYST MAN CYB, V13, P353, DOI 10.1109/TSMC.1983.6313175; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41; Sina Sigal, 2013, 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), P744; Tran C, 2017, ARXIV180100132; You JX, 2018, PR MACH LEARN RES, V80; You JX, 2018, ADV NEUR IN, V31; Zeng Zhiping, 2009, PROC VLDB ENDOW, V2, P25, DOI DOI 10.14778/1687627.1687631; Zhang MH, 2018, ADV NEUR IN, V31; Zhou DW, 2019, FRONT BIG DATA, V2, DOI 10.3389/fdata.2019.00003	43	4	4	6	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					1837	1852		10.1109/TPAMI.2020.3032286	http://dx.doi.org/10.1109/TPAMI.2020.3032286			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	33074806	Green Submitted			2022-12-18	WOS:000764815300015
J	Dang, ZY; Li, X; Gu, B; Deng, C; Huang, H				Dang, Zhiyuan; Li, Xiang; Gu, Bin; Deng, Cheng; Huang, Heng			Large-Scale Nonlinear AUC Maximization via Triply Stochastic Gradients	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						AUC maximization; random fourier features; kernel methods	NYSTROM METHOD; ONLINE	Learning to improve AUC performance for imbalanced data is an important machine learning research problem. Most methods of AUC maximization assume that the model function is linear in the original feature space. However, this assumption is not suitable for nonlinear separable problems. Although there have been some nonlinear methods of AUC maximization, scaling up nonlinear AUC maximization is still an open question. To address this challenging problem, in this paper, we propose a novel large-scale nonlinear AUC maximization method (named as TSAM) based on the triply stochastic gradient descents. Specifically, we first use the random Fourier feature to approximate the kernel function. After that, we use the triply stochastic gradients w.r. t the pairwise loss and random feature to iteratively update the solution. Finally, we prove that TSAM converges to the optimal solution with the rate of O(1/t) after t iterations. Experimental results on a variety of benchmark datasets not only confirm the scalability of TSAM. but also show a significant reduction of computational time compared with existing batch learning algorithms, while retaining the similar generalization performance.	[Dang, Zhiyuan; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China; [Dang, Zhiyuan] JD Digits, Beijing 100176, Peoples R China; [Li, Xiang] Univ Western Ontario, London, ON N6A 3K7, Canada; [Gu, Bin] Mohamed Bin Zayed Univ Artificial Intelligence, Dept Machine Learning, Abu Dhabi, U Arab Emirates; [Gu, Bin; Huang, Heng] JD Finance Amer Corp, Mountain View, CA 94043 USA; [Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA	Xidian University; Western University (University of Western Ontario); Mohamed Bin Zayed University of Artificial Intelligence; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.; Huang, H (corresponding author), Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.	zydang@stu.xidian.edu.cn; lxiang2@uwo.ca; bin.gu@mbzuai.ac.ae; chdeng@mail.xidian.edu.cn; heng.huang@pitt.edu		Deng, Cheng/0000-0003-2620-3247; dang, zhiyuan/0000-0003-4241-4116	National Key R&D Program of China [2017YFE0104100, 2016YFE0200400]	National Key R&D Program of China	The work of Z.Y. Dang and C. Deng were supported in part by the National Key R&D Program of China under Grants 2017YFE0104100 and 2016YFE0200400. Zhiyuan Dang, Xiang Li, and Bin Gu contributed equally to this work.	[Anonymous], 2009, DOGMA MATLAB TOOLBOX; Boissier M, 2016, JMLR WORKSH CONF PRO, V51, P204; Brefeld U., 2005, P ICML 2005 WORKSH R; Chen K, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/4629534; Cheng F, 2018, NEUROCOMPUTING, V318, P137, DOI 10.1016/j.neucom.2018.08.041; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Crammer K, 2004, ADV NEUR IN, V16, P225; Dai B., 2014, NIPS; Ding Y, 2017, IEEE DATA MINING, P91, DOI 10.1109/ICDM.2017.18; Ding Y, 2015, AAAI CONF ARTIF INTE, P2568; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fawcett T, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P131, DOI 10.1109/ICDM.2001.989510; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Freund Y, 2004, J MACH LEARN RES, V4, P933, DOI 10.1162/1532443041827916; Gao W., 2013, P 30 INT C MACHINE L, P906; Gao W, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P939; Gu B, 2020, PATTERN RECOGN; Gu BZ, 2021, IEEE T IND ELECTRON, V68, P10248, DOI 10.1109/TIE.2020.3026285; Gu B, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2483, DOI 10.1145/3394486.3403298; Gu B, 2018, AAAI CONF ARTIF INTE, P3085; Gu B, 2012, NEURAL NETWORKS, V27, P51, DOI 10.1016/j.neunet.2011.10.006; Gultekin S., 2020, IEEE T NEURAL NETW L, P1; HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747; Hu JJ, 2015, AAAI CONF ARTIF INTE, P2666; Hu JJ, 2018, IEEE T NEUR NET LEAR, V29, P882, DOI 10.1109/TNNLS.2016.2610465; Kakkar V., 2017, P SIAM INT C DAT MIN, P291; Kar P., 2013, ICML; Khalid Majdi, 2016, Advanced Data Mining and Applications. 12th International Conference, ADMA 2016. Proceedings: LNAI 10086, P35, DOI 10.1007/978-3-319-49586-6_3; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Kuo T. M., 2014, P 2014 SIAM INT C DA, P812; Le Quoc V., 2013, INT C MACH LEARN, P244; Lee CP, 2014, NEURAL COMPUT, V26, P781, DOI 10.1162/NECO_a_00571; Li X, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Long G., 2016, P 32 C UNC ART INT, P142; Orabona F, 2009, J MACH LEARN RES, V10, P2643; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Shi WL, 2020, AAAI CONF ARTIF INTE, V34, P5734; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Wang Yuyang, 2012, C LEARNING THEORY JM, P13; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zhao P., 2011, P 28 INT C MACHINE L, P233; Zhou D., 2018, ABS180805671	45	4	4	2	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1385	1398		10.1109/TPAMI.2020.3024987	http://dx.doi.org/10.1109/TPAMI.2020.3024987			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32946382				2022-12-18	WOS:000752018000023
J	Kachuee, M; Karkkainen, K; Goldstein, O; Darabi, S; Sarrafzadeh, M				Kachuee, Mohammad; Karkkainen, Kimmo; Goldstein, Orpaz; Darabi, Sajad; Sarrafzadeh, Majid			Generative Imputation and Stochastic Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Uncertainty; Generators; Stochastic processes; Training; Data models; Task analysis; Generative adversarial networks; Missing data; imputation; incomplete data; generative adversarial networks; classification uncertainty		In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 and MNIST image datasets as well as five real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.	[Kachuee, Mohammad; Karkkainen, Kimmo; Goldstein, Orpaz; Darabi, Sajad; Sarrafzadeh, Majid] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Kachuee, M (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	mkachuee@cs.ucla.edu; kimmo@cs.ucla.edu; orpgol@cs.ucla.edu; sajad.darabi@cs.ucla.edu; majid@cs.ucla.edu		Kachuee, Mohammad/0000-0002-0099-3466; Karkkainen, Kimmo/0000-0002-5188-9191; Darabi, Sajad/0000-0002-2187-8379; Goldstein, Orpaz/0000-0002-9764-1618				Aleryani A., 2020, SN COMPUT SCI, V1, P134, DOI [10.1007/s42979-020-00131-0, DOI 10.1007/S42979-020-00131-0]; ANDERSON TW, 1957, J AM STAT ASSOC, V52, P200, DOI 10.2307/2280845; Tran CT, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P521, DOI 10.1145/3071178.3071181; Chen PF, 2019, PR MACH LEARN RES, V97; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Dua D., 2017, UCI MACHINE LEARNING; Dumoulin Vincent, 2017, ICLR 2017, P4; Eisele G, 2022, ASSESSMENT, V29, P136, DOI 10.1177/1073191120957102; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hastie T., 1999, IMPUTING MISSING DAT; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Jang Eric, 2017, P 5 INT C LEARN REPR; Kachuee M., 2019, ARXIV PREPRINT ARXIV; King DB, 2015, ACS SYM SER, V1214, P1; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kochenderfer M., 2019, ARXIV PREPRINT ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; LeCun Y., 1998, MNIST DATABASE HANDW, V10; Li LC, 2020, KNOWL-BASED SYST, V194, DOI 10.1016/j.knosys.2020.105592; Lin SW, 2020, PROCEEDINGS OF THE 2020 SIAM INTERNATIONAL CONFERENCE ON DATA MINING (SDM), P46, DOI 10.1137/1.9781611976236.6; Little R. J, 2019, STAT ANAL MISSING DA, V793; Liu S, 2017, ADV NEUR IN, V30; Mattei P.-A., 2018, P 3 INT C NEUR INF P; Miyato T., 2018, INT C LEARN REPR, P2; Moody GA, 2001, IEEE ENG MED BIOL, V20, P45, DOI 10.1109/51.932724; Murray JS, 2018, STAT SCI, V33, P142, DOI 10.1214/18-STS644; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Nazabal A, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107501; Neapolitan R. E, 2004, LEARNING BAYESIAN NE, V38; Nielsen T.D., 2009, BAYESIAN NETWORKS DE; Pistoia G., 2018, BEHAV LITHIUM ION BA; REED S, 2014, AM GEOPH UN AGU FALL; RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.2307/2335739; Ryu S., 2020, IEEE ACCESS, V8, p40 656; Schafer JL, 2002, PSYCHOL METHODS, V7, P147, DOI 10.1037//1082-989X.7.2.147; Smieja M., 2020, P ICLR WORKSH INT DE; Sohn K, 2015, ADV NEUR IN, V28; van Buuren S, 2011, J STAT SOFTW, V45, P1; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Yi J., 2020, P ASIAN C COMPUTER V; Yoon Jaehong, 2018, LIFELONG LEARNING DY; Zhang HY, 2019, PR MACH LEARN RES, V97; Zhang SC, 2005, IEEE T KNOWL DATA EN, V17, P1689, DOI 10.1109/TKDE.2005.188	45	4	4	9	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1278	1288		10.1109/TPAMI.2020.3022383	http://dx.doi.org/10.1109/TPAMI.2020.3022383			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32894706	Green Submitted			2022-12-18	WOS:000752018000016
J	Gopinath, K; Desrosiers, C; Lombaert, H				Gopinath, Karthik; Desrosiers, Christian; Lombaert, Herve			Learnable Pooling in Graph Convolutional Networks for Brain Surface Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Brain; Convolution; Geometry; Task analysis; Surface treatment; Alzheimer's disease; Learnable pooling; graph convolutional networks; brain surface analysis; alzheimer classification	CORTICAL THICKNESS; MORPHOMETRY; DIAGNOSIS	Brain surface analysis is essential to neuroscience, however, the complex geometry of the brain cortex hinders computational methods for this task. The difficulty arises from a discrepancy between 3D imaging data, which is represented in Euclidean space, and the non-Euclidean geometry of the highly-convoluted brain surface. Recent advances in machine learning have enabled the use of neural networks for non-Euclidean spaces. These facilitate the learning of surface data, yet pooling strategies often remain constrained to a single fixed-graph. This paper proposes a new learnable graph pooling method for processing multiple surface-valued data to output subject-based information. The proposed method innovates by learning an intrinsic aggregation of graph nodes based on graph spectral embedding. We illustrate the advantages of our approach with in-depth experiments on two large-scale benchmark datasets. The ablation study in the paper illustrates the impact of various factors affecting our learnable pooling method. The flexibility of the pooling strategy is evaluated on four different prediction tasks, namely, subject-sex classification, regression of cortical region sizes, classification of Alzheimer's disease stages, and brain age regression. Our experiments demonstrate the superiority of our learnable pooling approach compared to other pooling techniques for graph convolutional networks, with results improving the state-of-the-art in brain surface analysis.	[Gopinath, Karthik; Desrosiers, Christian; Lombaert, Herve] ETS Montreal, Comp & Software Engn Dept, 1100 Notre Dame St West, Montreal, PQ H3C 1K3, Canada	University of Quebec; Ecole de Technologie Superieure - Canada	Gopinath, K (corresponding author), ETS Montreal, Comp & Software Engn Dept, 1100 Notre Dame St West, Montreal, PQ H3C 1K3, Canada.	karthik.gopinath.1@etsmtl.net; christian.desrosiers@etsmtl.ca; herve.lombaert@etsmtl.ca			Canada Research Chair on Shape Analysis in Medical Imaging; Research Council of Canada (NSERC); Fonds de recherche du Quebec -Nature et technologies (FRQNT); NVIDIA Corporation	Canada Research Chair on Shape Analysis in Medical Imaging; Research Council of Canada (NSERC); Fonds de recherche du Quebec -Nature et technologies (FRQNT); NVIDIA Corporation	This work was supported by the Canada Research Chair on Shape Analysis in Medical Imaging, the Research Council of Canada (NSERC), the Fonds de recherche du Quebec -Nature et technologies (FRQNT), and NVIDIA Corporation with the donation of a Titan Xp GPU. Part of the data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_ Acknowledgement_List.pdf.	Arbabshirani MR, 2017, NEUROIMAGE, V145, P137, DOI 10.1016/j.neuroimage.2016.02.079; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J., 2014, C TRACK P; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Oliveira PPD, 2010, J ALZHEIMERS DIS, V19, P1263, DOI 10.3233/JAD-2010-1322; Destrieux C., 2009, NEUROIMAGE, DOI [10.1016/S1053-8119(09)71561-7, DOI 10.1016/S1053-8119(09)71561-7]; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dolz J, 2018, NEUROIMAGE, V170, P456, DOI 10.1016/j.neuroimage.2017.04.039; Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097; Fischl B, 2004, CEREB CORTEX, V14, P11, DOI 10.1093/cercor/bhg087; Freeborough PA, 1998, IEEE T MED IMAGING, V17, P475, DOI 10.1109/42.712137; Gao H., 2019, PR MACH LEARN RES; Glaunes J, 2004, PROC CVPR IEEE, P712; Glocker B., 2017, P INT C MEDICAL IMAG, P177, DOI DOI 10.1007/978-3-319-66179-7_21; Gopinath K, 2019, LECT NOTES COMPUT SC, V11492, P86, DOI 10.1007/978-3-030-20351-1_7; Gopinath K, 2019, MED IMAGE ANAL, V54, P297, DOI 10.1016/j.media.2019.03.012; Hua X, 2013, NEUROIMAGE, V66, P648, DOI 10.1016/j.neuroimage.2012.10.086; Jack CR, 2008, J MAGN RESON IMAGING, V27, P685, DOI 10.1002/jmri.21049; Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Klein A, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005350; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledig C., 2014, P MED IM COMP ASS IN, P55; Lerch JP, 2005, CEREB CORTEX, V15, P995, DOI 10.1093/cercor/bhh200; Levie R, 2019, IEEE T SIGNAL PROCES, V67, P97, DOI 10.1109/TSP.2018.2879624; Lombaert H, 2015, LECT NOTES COMPUT SC, V9349, P547, DOI 10.1007/978-3-319-24553-9_67; Lombaert Herve, 2015, Inf Process Med Imaging, V24, P474, DOI 10.1007/978-3-319-19992-4_37; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Murphy DGM, 1996, ARCH GEN PSYCHIAT, V53, P585; Nair V., 2010, ICML, P807; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sowell ER, 2004, J NEUROSCI, V24, P8223, DOI 10.1523/JNEUROSCI.1798-04.2004; Styner Martin, 2006, Insight J, P242; Tang XY, 2014, HUM BRAIN MAPP, V35, P3701, DOI 10.1002/hbm.22431; Vemuri P, 2008, NEUROIMAGE, V39, P1186, DOI 10.1016/j.neuroimage.2007.09.073; Wachinger C, 2018, NEUROIMAGE, V170, P434, DOI 10.1016/j.neuroimage.2017.02.035; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yeo BTT, 2010, IEEE T MED IMAGING, V29, P650, DOI 10.1109/TMI.2009.2030797; Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697; Ying R, 2018, ADV NEUR IN, V31; Zhang TH, 2011, IEEE T MED IMAGING, V30, P1441, DOI 10.1109/TMI.2011.2114362	45	4	4	10	26	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					864	876		10.1109/TPAMI.2020.3028391	http://dx.doi.org/10.1109/TPAMI.2020.3028391			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YG0GP	33006927				2022-12-18	WOS:000742177000004
J	Koniusz, P; Zhang, HG				Koniusz, Piotr; Zhang, Hongguang			Power Normalizations in Fine-Grained Image, Few-Shot Image and Graph Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Covariance matrices; Visualization; Training; Laplace equations; Pipelines; Eigenvalues and eigenfunctions; CNN; second-order aggregation; eigenvalue power normalization; bilinear pooling; tensor pooling; heat diffusion	COVARIANCE; EIGENVALUES; DERIVATIVES	Power Normalizations (PN) are useful non-linear operators which tackle feature imbalances in classification problems. We study PNs in the deep learning setup via a novel PN layer pooling feature maps. Our layer combines the feature vectors and their respective spatial locations in the feature maps produced by the last convolutional layer of CNN into a positive definite matrix with second-order statistics to which PN operators are applied, forming so-called Second-order Pooling (SOP). As the main goal of this paper is to study Power Normalizations, we investigate the role and meaning of MaxExp and Gamma, two popular PN functions. To this end, we provide probabilistic interpretations of such element-wise operators and discover surrogates with well-behaved derivatives for end-to-end training. Furthermore, we look at the spectral applicability of MaxExp and Gamma by studying Spectral Power Normalizations (SPN). We show that SPN on the autocorrelation/covariance matrix and the Heat Diffusion Process (HDP) on a graph Laplacian matrix are closely related, thus sharing their properties. Such a finding leads us to the culmination of our work, a fast spectral MaxExp which is a variant of HDP for covariances/autocorrelation matrices. We evaluate our ideas on fine-grained recognition, scene recognition, and material classification, as well as in few-shot learning and graph classification.	[Koniusz, Piotr; Zhang, Hongguang] CSIRO, Data61, Canberra, ACT 2601, Australia; [Koniusz, Piotr; Zhang, Hongguang] Australian Natl Univ, Canberra, ACT 2601, Australia	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University	Koniusz, P (corresponding author), CSIRO, Data61, Canberra, ACT 2601, Australia.	piotr.koniusz@anu.edu.au; hongguang.zhang@anu.edu.au			CSIRO's Machine Learning and Artificial Intelligence Future Science Platform	CSIRO's Machine Learning and Artificial Intelligence Future Science Platform	This work was supported by CSIRO's Machine Learning and Artificial Intelligence Future Science Platform (MLAI FSP). The authors would like to thank Dr. Ke Sun for brainstorming, Ondrej Hlinka/Garry Swan for help with HPC, Hao Zhu for checks of some SOP codes, and Lei Wang for quick checks of text. Piotr Koniusz was mainly concerned with the mathematical analysis/modeling while H. Zhang with the deep learning modeling.	Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Bentley P. J, 2019, ARXIV190208399; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29; Boughorbel S, 2005, IEEE IMAGE PROC, P2629; Boureau Y.L., 2010, P 27 INT C MACH LEAR, P111; Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963; Carlsson S., 2016, IEEE T PATTERN ANAL, V38, P1790, DOI [10.1109/TPAMI.2015.2500224, DOI 10.1109/TPAMI.2015.2500224]; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Cherian A, 2013, IEEE T PATTERN ANAL, V35, P2161, DOI 10.1109/TPAMI.2012.259; Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007; Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461; Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325; Dryden IL, 2009, ANN APPL STAT, V3, P1102, DOI 10.1214/09-AOAS249; Fey M., 2019, ICLR WORKSH REPR LEA; Fink M, 2005, ADV NEURAL INFORM PR, P449; Finn C, 2017, PR MACH LEARN RES, V70; Guo K, 2013, IEEE T IMAGE PROCESS, V22, P2479, DOI 10.1109/TIP.2013.2252622; Harandi M, 2017, PR MACH LEARN RES, V70; Harandi M, 2015, INT J COMPUT VISION, V114, P113, DOI 10.1007/s11263-015-0833-x; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., NEURAL NETWORKS MACH; Hu GS, 2017, IEEE I CONF COMP VIS, P3764, DOI 10.1109/ICCV.2017.404; Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Ivanov S, 2018, PR MACH LEARN RES, V80; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609; Khan SH, 2017, IEEE I CONF COMP VIS, P5639, DOI 10.1109/ICCV.2017.601; Koniusz P., P INT C COMP VIS TUT; Koniusz P., 2013, TECHNICAL REPORT; Koniusz P, 2018, LECT NOTES COMPUT SC, V11220, P815, DOI 10.1007/978-3-030-01270-0_48; Koniusz P, 2018, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR.2018.00605; Koniusz P, 2017, PROC CVPR IEEE, P7139, DOI 10.1109/CVPR.2017.755; Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667; Koniusz P, 2016, PROC CVPR IEEE, P5395, DOI 10.1109/CVPR.2016.582; Koniusz P, 2013, COMPUT VIS IMAGE UND, V117, P479, DOI 10.1016/j.cviu.2012.10.010; Koniusz P, 2011, IEEE IMAGE PROC, P661; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li FF, 2002, P NATL ACAD SCI USA, V99, P9596, DOI 10.1073/pnas.092277599; Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin SY, 2018, LECT NOTES COMPUT SC, V11207, P639, DOI 10.1007/978-3-030-01219-9_38; Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu J, 2016, AAAI CONF ARTIF INTE, P4232; Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534; Magnus JR., 1985, ECONOMET THEOR, V1, P179, DOI [10.1017/S0266466600011129, DOI 10.1017/S0266466600011129]; Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Porikli F., 2006, CVPR; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Ravi S., 2017, INT C LEARN REPR, P12; ROGERS LC, 1970, AIAA J, V8, P943, DOI 10.2514/3.5795; Roy SK, 2018, PROC CVPR IEEE, P4460, DOI 10.1109/CVPR.2018.00469; RUDISILL CS, 1974, AIAA J, V12, P721, DOI 10.2514/3.49330; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schomburg I, 2004, NUCLEIC ACIDS RES, V32, pD431, DOI 10.1093/nar/gkh081; Sharan L, 2014, J VISION, V14, DOI 10.1167/14.9.12; Shih YF, 2017, PROC CVPR IEEE, P7302, DOI 10.1109/CVPR.2017.772; Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419; Simon M, 2020, IEEE T PATTERN ANAL, V42, P749, DOI 10.1109/TPAMI.2018.2885764; Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Verma Saurabh, 2018, ABS180508090 CORR; Vinyals O., 2016, P 30 INT C NEUR INF, P3637, DOI 10.5555/3157382.3157504; Wang L., 2015, PLACES205 VGGNET MOD; Wang Limin, 2015, ARXIV150702159; Wang ZZ, 2004, PROC CVPR IEEE, P228; Wertheimer D, 2019, PROC CVPR IEEE, P6551, DOI 10.1109/CVPR.2019.00672; Xie LX, 2017, INT J COMPUT VISION, V123, P226, DOI 10.1007/s11263-016-0970-x; Zhang C., 2015, MSRTR201531; Zhang HG, 2019, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR.2019.00288; Zhang HG, 2019, IEEE WINT CONF APPL, P1185, DOI 10.1109/WACV.2019.00131; Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438; Zhang Shan, 2020, P AS C COMP VIS; Zhang Y, 2016, INT C PATT RECOG, P3697, DOI 10.1109/ICPR.2016.7900209; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu H., 2021, ICLR, V2021, P1; Zhukov L. E., 2015, DIFFUSION RANDOM WAL	86	4	4	5	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					591	609		10.1109/TPAMI.2021.3107164	http://dx.doi.org/10.1109/TPAMI.2021.3107164			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	34428137	Green Submitted			2022-12-18	WOS:000740006100005
J	Koniusz, P; Wang, L; Cherian, A				Koniusz, Piotr; Wang, Lei; Cherian, Anoop			Tensor Representations for Action Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						CNN; 3D skeletons; action recognition; aggregation; kernels; higher-order tensors; HOSVD; power normalization	POSE; MOTION	Human actions in video sequences are characterized by the complex interplay between spatial features and their temporal dynamics. In this paper, we propose novel tensor representations for compactly capturing such higher-order relationships between visual features for the task of action recognition. We propose two tensor-based feature representations, viz. (i) sequence compatibility kernel (SCK) and (ii) dynamics compatibility kernel (DCK). SCK builds on the spatio-temporal correlations between features, whereas DCK explicitly models the action dynamics of a sequence. We also explore generalization of SCK, coined SCK circle plus, that operates on subsequences to capture the local-global interplay of correlations, which can incorporate multi-modal inputs e.g., skeleton 3D body-joints and per-frame classifier scores obtained from deep learning models trained on videos. We introduce linearization of these kernels that lead to compact and fast descriptors. We provide experiments on (i) 3D skeleton action sequences, (ii) fine-grained video sequences, and (iii) standard non-fine-grained videos. As our final representations are tensors that capture higher-order relationships of features, they relate to co-occurrences for robust fine-grained recognition (Lin, 2017), (Koniusz, 2018). We use higher-order tensors and so-called Eigenvalue Power Normalization (EPN) which have been long speculated to perform spectral detection of higher-order occurrences (Koniusz, 2013), (Koniusz, 2017), thus detecting fine-grained relationships of features rather than merely count features in action sequences. We prove that a tensor of order r, built from Z(*) dimensional features, coupled with EPN indeed detects if at least one higher-order occurrence is 'projected' into one of its (Z(*)/r) subspaces of dim. r represented by the tensor, thus forming a Tensor Power Normalization metric endowed with (Z(*)/r) such 'detectors'.	[Koniusz, Piotr; Wang, Lei] Data61 CSIRO, Canberra, ACT 2601, Australia; [Koniusz, Piotr; Wang, Lei] Australian Natl Univ, Canberra, ACT 2601, Australia; [Cherian, Anoop] Mitsubishi Elect Res Labs MERL, Cambridge, MA 02139 USA	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University	Koniusz, P (corresponding author), Data61 CSIRO, Canberra, ACT 2601, Australia.; Koniusz, P (corresponding author), Australian Natl Univ, Canberra, ACT 2601, Australia.	piotr.koniusz@anu.edu.au; lei.wang@data61.csiro.au; anoop.cherian@anu.edu.au	Wang, Lei/AAC-4422-2019	Wang, Lei/0000-0002-8600-7099	CSIRO's Machine Learning and Artificial Intelligence Future Science Platform (MLAI FSP)	CSIRO's Machine Learning and Artificial Intelligence Future Science Platform (MLAI FSP)	This work was supported by CSIRO's Machine Learning and Artificial Intelligence Future Science Platform (MLAI FSP).	Alex M, 2002, LECT NOTES COMPUT SC, V2350, P447; [Anonymous], 2018, P BRIT MACH VIS C; [Anonymous], 2016, P ECCV; Ben Tanfous A, 2018, PROC CVPR IEEE, P2840, DOI 10.1109/CVPR.2018.00300; Bo LF, 2011, PROC CVPR IEEE, P1729, DOI 10.1109/CVPR.2011.5995719; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cavazza J, 2016, INT C PATT RECOG, P408, DOI 10.1109/ICPR.2016.7899668; Cherian A, 2019, INT J COMPUT VISION, V127, P340, DOI 10.1007/s11263-018-1111-5; Cherian A, 2018, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2018.00234; Cherian A, 2017, PROC CVPR IEEE, P1581, DOI 10.1109/CVPR.2017.172; Cherian A, 2017, IEEE WINT CONF APPL, P130, DOI 10.1109/WACV.2017.22; Cheron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Elgammal A, 2009, IEEE T PATTERN ANAL, V31, P520, DOI 10.1109/TPAMI.2008.101; Gaidon A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.63; Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132; Hu JF, 2018, LECT NOTES COMPUT SC, V11211, P346, DOI 10.1007/978-3-030-01234-2_21; Hussein Mohamed E, 2013, 23 INT JOINT C ART I; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486; Kim Tae-Kyun, 2007, P IEEE C COMP VIS PA, P1; Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P591, DOI 10.1109/TPAMI.2021.3107164; Koniusz P, 2018, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR.2018.00605; Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667; Koniusz P, 2016, PROC CVPR IEEE, P5395, DOI 10.1109/CVPR.2016.582; Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3; Koniusz Piotr, 2013, HIGHER ORDER OCCURRE; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Li BL, 2012, PROC CVPR IEEE, P1362, DOI 10.1109/CVPR.2012.6247822; Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin SY, 2018, LECT NOTES COMPUT SC, V11207, P639, DOI 10.1007/978-3-030-01219-9_38; Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019; Lu HP, 2011, PATTERN RECOGN, V44, P1540, DOI 10.1016/j.patcog.2011.01.004; Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359; Mahmud T, 2017, IEEE I CONF COMP VIS, P5784, DOI 10.1109/ICCV.2017.616; Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007; Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76; Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shashua A., 2005, P 22 INT C MACHINE L, P792, DOI [10.1145/1102351.1102451, DOI 10.1145/1102351.1102451]; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Shechtman E, 2005, PROC CVPR IEEE, P405; Shi YM, 2017, IEEE I CONF COMP VIS, P716, DOI 10.1109/ICCV.2017.84; Shiri F, 2019, INT J COMPUT VISION, V127, P863, DOI 10.1007/s11263-019-01169-1; Shiri F, 2019, IEEE WINT CONF APPL, P406, DOI 10.1109/WACV.2019.00049; Shiri F, 2018, IEEE WINT CONF APPL, P102, DOI 10.1109/WACV.2018.00018; Shiri F, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING - TECHNIQUES AND APPLICATIONS (DICTA), P427; Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381; Simon Christian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P556, DOI 10.1007/978-3-030-58598-3_33; Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Sun K., 2019, P C UNC ART INT, P465; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Turaga Pavan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2435, DOI 10.1109/CVPRW.2009.5206710; Vasilescu MAO, 2004, ACM T GRAPHIC, V23, P336, DOI 10.1145/1015706.1015725; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang J, 2018, LECT NOTES COMPUT SC, V11208, P716, DOI 10.1007/978-3-030-01225-0_42; Wang L., P 29 ACM INT C MULT, P2021; Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879; Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Xudong Zhao, 2012, Digital-Forensics and Watermarking 10th International Workshop, IWDW 2011. Revised Selected Papers, P243, DOI 10.1007/978-3-642-32205-1_20; Yacoob Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P120, DOI 10.1109/ICCV.1998.710709; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang XD, 2014, J VIS COMMUN IMAGE R, V25, P2, DOI 10.1016/j.jvcir.2013.03.001; Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342; Zatsiorsky V.M., 1997, KINEMATIC HUMAN MOTI; Zhang Haoxian, 2020, EUR C COMP VIS, P2; Zhang HG, 2019, IEEE WINT CONF APPL, P1185, DOI 10.1109/WACV.2019.00131; Zhang JJ, 2021, INT J COMPUT VISION, V129, P300, DOI 10.1007/s11263-020-01376-1; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zhang Shan, 2020, P AS C COMP VIS; Zhang YP, 2012, IEEE VTS VEH TECHNOL; Zhou Y, 2015, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR.2015.7298953; Zhu H., P INT C LEARN REPR, P2021; Zhu Hao, 2021, ACM INT C INFORM KNO; Zuffi S., 2013, IJCV, V101, P437, DOI DOI 10.1007/s11263-012-0549-0	94	4	4	10	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					648	665		10.1109/TPAMI.2021.3107160	http://dx.doi.org/10.1109/TPAMI.2021.3107160			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	34428136	Green Submitted			2022-12-18	WOS:000740006100009
J	Liang, J; Liu, ZQ; Zhou, JY; Jiang, XQ; Zhang, CS; Wang, F				Liang, Jian; Liu, Ziqi; Zhou, Jiayu; Jiang, Xiaoqian; Zhang, Changshui; Wang, Fei			Model-Protected Multi-Task Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Covariance matrices; Privacy; Security; Data models; Resource management; Multi-task learning; model protection; differential privacy; covariance matrix; low-rank subspace learning	REGRESSION	Multi-task learning (MTL) refers to the paradigm of learning multiple related tasks together. In contrast, in single-task learning (STL) each individual task is learned independently. MTL often leads to better trained models because they can leverage the commonalities among related tasks. However, because MTL algorithms can "leak" information from different models across different tasks, MTL poses a potential security risk. Specifically, an adversary may participate in the MTL process through one task and thereby acquire the model information for another task. The previously proposed privacy-preserving MTL methods protect data instances rather than models, and some of them may underperform in comparison with STL methods. In this paper, we propose a privacy-preserving MTL framework to prevent information from each model leaking to other models based on a perturbation of the covariance matrix of the model matrix. We study two popular MTL approaches for instantiation, namely, learning the low-rank and group-sparse patterns of the model matrix. Our algorithms can be guaranteed not to underperform compared with STL methods. We build our methods based upon tools for differential privacy, and privacy guarantees, utility bounds are provided, and heterogeneous privacy budgets are considered. The experiments demonstrate that our algorithms outperform the baseline methods constructed by existing privacy-preserving MTL methods on the proposed model-protection problem.	[Liang, Jian; Zhang, Changshui] Tsinghua Univ THUAI, Inst Artificial Intelligence, Beijing 100084, Peoples R China; [Liang, Jian; Zhang, Changshui] Tsinghua Univ, Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China; [Liu, Ziqi] Ant Financial Serv Grp, AI Dept, Hangzhou 310013, Zhejiang, Peoples R China; [Zhou, Jiayu] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA; [Jiang, Xiaoqian] Univ Texas Hlth Sci Ctr Houston, Sch Biomed Informat, Houston, TX 77030 USA; [Wang, Fei] Weill Cornell Med Coll, Dept Populat Hlth Sci, New York, NY 10065 USA	Tsinghua University; Michigan State University; University of Texas System; University of Texas Health Science Center Houston; Cornell University	Liang, J (corresponding author), Tsinghua Univ THUAI, Inst Artificial Intelligence, Beijing 100084, Peoples R China.	liangjianzb12@gmail.com; ziqilau@gmail.com; jiayuz@msu.edu; jiayuz@msu.edu; zcs@mail.tsinghua.edu.cn; few2001@med.cornell.edu			Natural Science Fundation of China (NSFC); German Research Foundation (DFG) [DFG TRR-169]; Beijing Academy of Artificial Intelligence (BAAI); National Science Foundation (NSF) [IIS-1565596, III-1615597, IIS-1650723]; Office of Naval Research (ONR) [N00014-14-1-0631]; National Institutes of Health (NIH) [R00LM011392, R21LM012060]	Natural Science Fundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); German Research Foundation (DFG)(German Research Foundation (DFG)); Beijing Academy of Artificial Intelligence (BAAI); National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Office of Naval Research (ONR)(Office of Naval Research); National Institutes of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported in part by the Natural Science Fundation of China (NSFC), in part by the German Research Foundation (DFG) in Project Crossmodal Learning DFG TRR-169, in part by the Beijing Academy of Artificial Intelligence (BAAI), in part by the National Science Foundation (NSF) under grants IIS-1565596, III-1615597, and IIS-1650723, in part by the Office of Naval Research (ONR) under grant number N00014-14-1-0631, and in part by the National Institutes of Health (NIH) under grants R00LM011392 and R21LM012060. In addition, the authors would like to thank Yuxiang Wang from the Department of Computer Science at UC Santa Barbara for his valuable comments on improving the properties of iterative MP-MTL algorithms.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Almeida Miguel, 2013, P 51 ANN M ASS COMPU, P196; Amit Y., 2007, ICML 07 P 24 INT C M, P17, DOI DOI 10.1145/1273496.1273499; Ando RK, 2005, J MACH LEARN RES, V6, P1817; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Argyriou A., 2007, ADV NEURAL INFORM PR, P25; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Barak B., 2007, P 26 ACM SIGMOD SIGA, P273, DOI DOI 10.1145/1265530.1265569; Baytas IM, 2016, IEEE DATA MINING, P11, DOI [10.1109/ICDM.2016.0012, 10.1109/ICDM.2016.61]; Blocki J, 2012, ANN IEEE SYMP FOUND, P410, DOI 10.1109/FOCS.2012.67; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626; Chaudhuri K., 2008, PROC 22 ANN C NEURAL, P289; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chen D, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE OF MANAGEMENT SCIENCE AND INFORMATION SYSTEM, VOLS 1-4, P1375; Chen J., 2011, PROC 17 ACM SIGKDD I, P42; Chen JH, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086742; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P11, DOI 10.1145/2591796.2591883; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ACM S THEORY COMPUT, P715; Evgeniou A., 2007, ADV NEURAL INF PROCE, V19, P41, DOI DOI 10.2139/SSRN.1031158; Fei HL, 2013, KNOWL INF SYST, V35, P345, DOI 10.1007/s10115-012-0543-4; Finlayson SG, 2019, SCIENCE, V363, P1287, DOI 10.1126/science.aaw4399; Ganta S. R., 2008, P 14 ACM SIGKDD INT, P265, DOI DOI 10.1145/1401890.1401926; Goldreich O., 1998, PRELIMINARY VE UNPUB; Gong Pinghua, 2012, KDD, V2012, P895; Gu QQ, 2009, IEEE DATA MINING, P159, DOI 10.1109/ICDM.2009.32; Gupta A. K., 1999, MATRIX VARIATE DISTR, V104; Gupta Sunil Kumar, 2016, Intelligence and Security Informatics. 11th Pacific Asia Workshop, PAISI 2016. Proceedings: LNCS 9650, P101, DOI 10.1007/978-3-319-31863-9_8; Han L, 2016, AAAI CONF ARTIF INTE, P1638; Hollander M., 2013, NONPARAMETRIC STAT M, V751; Hsu J, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P341; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jalali A., 2010, ADV NEURAL INF PROCE, V23, P964; Ji S., 2009, P 26 ANN INT C MACH, P457, DOI DOI 10.1145/1553374.1553434; Ji Z., 2014, P TRANSL BIOINF C; Jiang WX, 2016, AAAI CONF ARTIF INTE, P1730; Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Kearns M., 2015, ROBUST MEDIATORS LAR; Kearns M, 2014, AM ECON REV, V104, P431, DOI 10.1257/aer.104.5.431; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Li Haoran, 2014, Adv Database Technol, V2014, P475; Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6; Liu J., 2009, P 25 C UNCERTAINTY A, P339, DOI DOI 10.5555/1795114.1795154; Mathew G, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P178, DOI 10.1109/ICMLA.2012.180; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Mopuri K. R., 2017, PROC BRIT MACH VIS C; Ng Kenney, 2015, AMIA Jt Summits Transl Sci Proc, V2015, P132; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Pathak M., 2010, ADV NEURAL INFORM PR, P1876; Pong TK, 2010, SIAM J OPTIMIZ, V20, P3465, DOI 10.1137/090763184; Schmidt M., 2011, ADV NEURAL INFORM PR, P1458; Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687; Su C, 2018, IEEE T PATTERN ANAL, V40, P1167, DOI 10.1109/TPAMI.2017.2679002; Sun ZN, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1145, DOI 10.1145/2783258.2783324; Szegedy Christian, 2014, P 2 INT C LEARNING R; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Vadhan S, 2017, INFORM SEC CRYPT TEX, P347, DOI 10.1007/978-3-319-57048-8_7; Wang Xiang, 2014, AMIA Annu Symp Proc, V2014, P1180; Wang YX, 2015, PR MACH LEARN RES, V37, P2493; Williams, 2007, NEURAL INFORM PROCES, P153, DOI DOI 10.5555/2981562.2981582; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892; Xu M., 2012, P 29 INT C MACH LEAR, P1479; Zantedeschi Valentina, 2017, P 10 ACM WORKSHOP AR, P39, DOI [10.1145/3128572.3140449, DOI 10.1145/3128572.3140449]; Zhang J, 2012, PROC VLDB ENDOW, V5, P1364, DOI 10.14778/2350229.2350253; Zhang Ping, 2014, AMIA Jt Summits Transl Sci Proc, V2014, P132; Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908; Zhang Y., 2017, ARXIV170708114V2; Zhang Y, 2010, PROCEEDINGS OF THE ASME 29TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2010, VOL 6, P733; Zhen XT, 2018, IEEE T PATTERN ANAL, V40, P497, DOI 10.1109/TPAMI.2017.2688363; Zhou Jiayu, 2011, Adv Neural Inf Process Syst, V2011, P702	78	4	4	1	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					1002	1019		10.1109/TPAMI.2020.3015859	http://dx.doi.org/10.1109/TPAMI.2020.3015859			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	32780696	Green Submitted			2022-12-18	WOS:000740006100033
J	Tiezzi, M; Marra, G; Melacci, S; Maggini, M				Tiezzi, Matteo; Marra, Giuseppe; Melacci, Stefano; Maggini, Marco			Deep Constraint-Based Propagation in Graph Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optimization; Computational modeling; Training; Graph neural networks; Data models; Biological neural networks; Convolution; Graph neural networks; constraint-based propagation; lagrangian optimization		The popularity of deep learning techniques renewed the interest in neural architectures able to process complex structures that can be represented using graphs, inspired by Graph Neural Networks (GNNs). We focus our attention on the originally proposed GNN model of Scarselli et al. 2009, which encodes the state of the nodes of the graph by means of an iterative diffusion procedure that, during the learning stage, must be computed at every epoch, until the fixed point of a learnable state transition function is reached, propagating the information among the neighbouring nodes. We propose a novel approach to learning in GNNs, based on constrained optimization in the Lagrangian framework. Learning both the transition function and the node states is the outcome of a joint process, in which the state convergence procedure is implicitly expressed by a constraint satisfaction mechanism, avoiding iterative epoch-wise procedures and the network unfolding. Our computational structure searches for saddle points of the Lagrangian in the adjoint space composed of weights, nodes state variables and Lagrange multipliers. This process is further enhanced by multiple layers of constraints that accelerate the diffusion process. An experimental analysis shows that the proposed approach compares favourably with popular models on several benchmarks.	[Tiezzi, Matteo; Marra, Giuseppe; Melacci, Stefano; Maggini, Marco] Univ Siena, Dept Informat Engn & Math, I-53100 Siena, Italy	University of Siena	Tiezzi, M (corresponding author), Univ Siena, Dept Informat Engn & Math, I-53100 Siena, Italy.	mtiezzi@diism.unisi.it; g.marra@unifi.it; mela@diism.unisi.it; marco.maggini@unisi.it		Tiezzi, Matteo/0000-0002-9133-8669; Marra, Giuseppe/0000-0001-5940-9562; Maggini, Marco/0000-0002-6428-1265	PRIN 2017 Project RexLearn - Italian Ministry of Education, University and Research [2017TWNMH2]; Research Foundation -Flanders; KU Leuven Research Fund	PRIN 2017 Project RexLearn - Italian Ministry of Education, University and Research; Research Foundation -Flanders(FWO); KU Leuven Research Fund(KU Leuven)	This work was supported in part by the DPRIN 2017 Project RexLearn, funded by the Italian Ministry of Education, University and Research (grant no. 2017TWNMH2). This work was also supported by the Research Foundation -Flanders and the KU Leuven Research Fund.	Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Bacciu D, 2020, NEURAL NETWORKS, V129, P203, DOI 10.1016/j.neunet.2020.06.006; Bianchini M, 2018, STUD COMPUT INTELL, V777, P29, DOI 10.1007/978-3-319-89629-8_2; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J., 2014, C TRACK P; Carreira-Perpinan MA, 2014, JMLR WORKSH CONF PRO, V33, P10; Chami I, 2020, MACHINE LEARNING GRA; Corso G., 2020, ARXIV200405718, V33, P13260; Dai HJ, 2018, PR MACH LEARN RES, V80; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dwivedi Vijay Prakash, 2020, ARXIV200300982; Errica Federico, 2020, ICLR; Frasconi P, 1998, IEEE T NEURAL NETWOR, V9, P768, DOI 10.1109/72.712151; Gama F, 2019, IEEE T SIGNAL PROCES, V67, P1034, DOI 10.1109/TSP.2018.2887403; Gilmer J, 2020, MACHINE LEARNING MEE, P199; Gilmer J, 2017, PR MACH LEARN RES, V70; Goller C, 1996, IEEE IJCNN, P347, DOI 10.1109/ICNN.1996.548916; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Gotmare A., 2018, P WORKSH EFF CRED AS, P1; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Henaff M, 2015, ARXIV150605163; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Ivanov S, 2018, PR MACH LEARN RES, V80; King DB, 2015, ACS SYM SER, V1214, P1; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Laurent T, 2017, ARXIV171107553 CORR; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; LeCun Y., 1988, P 1988 CONNECTIONIST, DOI DOI 10.3168/JDS.S0022-0302(88)79586-7; Li QM, 2018, AAAI CONF ARTIF INTE, P3538; Li Yujia, 2016, P INT C LEARN REPR I, P2; Loukas Andreas, 2020, INT C LEARN REPR; Maron H., 2019, P INT C LEARN REPR, P1; Marra G, 2020, IEEE IJCNN; Melacci S, 2011, J MACH LEARN RES, V12, P1149; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Morris C, 2019, AAAI CONF ARTIF INTE, P4602; Niepert M, 2016, PR MACH LEARN RES, V48; Platt J.C, 1988, NEURAL INFORM PROCES, P612; Ruiz L, 2020, IEEE T SIGNAL PROCES, V68, P127, DOI 10.1109/TSP.2019.2955832; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shchur O., 2018, ARXIV181105868; Sperduti A, 1997, IEEE T NEURAL NETWOR, V8, P714, DOI 10.1109/72.572108; Taylor G, 2016, PR MACH LEARN RES, V48; Velickovic P., 2018, P INT C LEARN REPR; Weisfeiler B., 1968, NAUCHNOTECHNICHESKAY, V2, P12; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752; Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438	53	4	4	3	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					727	739		10.1109/TPAMI.2021.3073504	http://dx.doi.org/10.1109/TPAMI.2021.3073504			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	33856980	Green Submitted, Green Accepted			2022-12-18	WOS:000740006100015
J	Chen, ZD; Deng, L; Wang, BY; Li, GQ; Xie, Y				Chen, Zhaodong; Deng, Lei; Wang, Bangyan; Li, Guoqi; Xie, Yuan			A Comprehensive and Modularized Statistical Framework for Gradient Norm Equality in Deep Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deep neural networks; free probability; gradient norm equality		The rapid development of deep neural networks (DNNs) in recent years can be attributed to the various techniques that address gradient explosion and vanishing. In order to understand the principle behind these techniques and develop new methods, plenty of metrics have been proposed to identify networks that are free of gradient explosion and vanishing. However, due to the diversity of network components and complex serial-parallel hybrid connections in modern DNNs, the evaluation of existing metrics usually requires strong assumptions, complex statistical analysis, or has limited application fields, which constraints their spread in the community. In this paper, inspired by the Gradient Norm Equality and dynamical isometry, we first propose a novel metric called Block Dynamical Isometry, which measures the change of gradient norm in individual blocks. Because our Block Dynamical Isometry is norm-based, its evaluation needs weaker assumptions compared with the original dynamical isometry. To mitigate challenging derivation, we propose a highly modularized statistical framework based on free probability. Our framework includes several key theorems to handle complex serial-parallel hybrid connections and a library to cover the diversity of network components. Besides, several sufficient conditions for prerequisites are provided. Powered by our metric and framework, we analyze extensive initialization, normalization, and network structures. We find that our Block Dynamical Isometry is a universal philosophy behind them. Then, we improve some existing methods based on our analysis, including an activation function selection strategy for initialization techniques, a new configuration for weight normalization, a depth-aware way to derive coefficients in SeLU, and initialization/weight normalization in DenseNet. Moreover, we propose a novel normalization technique named second moment normalization, which has 30 percent fewer computation overhead than batch normalization without accuracy loss and has better performance under micro batch size. Last but not least, our conclusions and methods are evidenced by extensive experiments on multiple models over CIFAR-10 and ImageNet.	[Chen, Zhaodong; Deng, Lei; Wang, Bangyan; Xie, Yuan] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA; [Li, Guoqi] Tsinghua Univ, Beijing Innovat Ctr Future Chip, Dept Precis Instrument, Ctr Brain Inspired Comp Res, Beijing, Peoples R China	University of California System; University of California Santa Barbara; Tsinghua University	Li, GQ (corresponding author), Tsinghua Univ, Beijing Innovat Ctr Future Chip, Dept Precis Instrument, Ctr Brain Inspired Comp Res, Beijing, Peoples R China.	chenzd15thu@ucsb.edu; leideng@ucsb.edu; bangyan@ucsb.edu; liguoqi@mail.tsinghua.edu.cn; yuanxie@ucsb.edu		Chen, Zhaodong/0000-0001-9601-4586; Wang, Bangyan/0000-0002-5240-7238	National Science Foundation [1725447, 1817037, 1730309]; National Natural Science Foundation of china [61876215]; Beijing Academy of Artificial Intelligence (BAAI); Institute for Guo Qiang, Tsinghua University; Key Scientific Technological Innovation Research Project by the Ministry of Education; Zhejiang laboratory	National Science Foundation(National Science Foundation (NSF)); National Natural Science Foundation of china(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI); Institute for Guo Qiang, Tsinghua University; Key Scientific Technological Innovation Research Project by the Ministry of Education; Zhejiang laboratory	This work was supported in part by National Science Foundation (Grant No. 1725447, 1817037, 1730309), National Natural Science Foundation of china (61876215), Beijing Academy of Artificial Intelligence (BAAI), and a grant from the Institute for Guo Qiang, Tsinghua University, and Key Scientific Technological Innovation Research Project by the Ministry of Education, and the open project of Zhejiang laboratory. Zhaodong Chen and Lei Deng contributed equally to this work.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arpit D, 2019, ARXIV PREPRINT ARXIV; Arpit D, 2016, PR MACH LEARN RES, V48; Cakmak B, 2012, NONHERMITIAN RANDOM; Carbonell, 2018, GRADIENTS EXPLODE DE; Chen Z, 2020, IEEE IFIP NETW OPER; Dubatovka, 2018, ABS180606362; Edelman A, 2012, ARXIV12042257; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Ganguli S., 2014, INT C LEARN REPR; Gitman I, 2017, ARXIV170908145; Glorot X., 2011, P 14 INT C ART INT S, P315; Haber E, 2018, INVERSE PROBL, V34, DOI 10.1088/1361-6420/aa9a90; Hayou S., 2019, ARXIV190513654; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Howard A.G, 2017, ARXIV170404861; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2016, ICLR, DOI DOI 10.1109/WCNC.2016.7564824; Ling ZN, 2019, IEEE ACCESS, V7, P105212, DOI 10.1109/ACCESS.2019.2931991; Luo P., 2019, ICLR, P1; Ma T, 2019, RESIDUAL LEARNING NO; Mingo JA, 2017, FIELD INST MONOGR, P1, DOI 10.1007/978-1-4939-6942-5_1; Park I. M, 2020, INFORM GEOMETRY ORTH; Perez-Cruz F, 2018, INT C ART INT STAT, V84, P1924; Poole B, 2016, ADV NEUR IN, V29; Qiao Siyuan, 2019, ARXIV190310520, P2; Schoenholz S. S., 2019, ARXIV190208129; Sheng T, 2018, 2018 1ST WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC2), P14, DOI 10.1109/EMC2.2018.00011; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tabor, 2019, P MACH LEARN RES, P2221; Ulyanov D., 2016, ARXIV160708022; Wu S, 2019, IEEE T NEUR NET LEAR, V30, P2043, DOI 10.1109/TNNLS.2018.2876179; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xiao LC, 2018, PR MACH LEARN RES, V80; Xu B, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/832093; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	44	4	4	5	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					13	31		10.1109/TPAMI.2020.3010201	http://dx.doi.org/10.1109/TPAMI.2020.3010201			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750821	Green Submitted			2022-12-18	WOS:000728561300003
J	Ding, YQ; Yang, J; Ponce, J; Kong, H				Ding, Yaqing; Yang, Jian; Ponce, Jean; Kong, Hui			Homography-Based Minimal-Case Relative Pose Estimation With Known Gravity Direction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Relative orientation; homography estimation; minimal solver; sensor fusion		In this paper, we propose a novel approach to two-view minimal-case relative pose problems based on homography with known gravity direction. This case is relevant to smart phones, tablets, and other camera-IMU (Inertial measurement unit) systems which have accelerometers to measure the gravity vector. We explore the rank-1 constraint on the difference between the euclidean homography matrix and the corresponding rotation, and propose an efficient two-step solution for solving both the calibrated and semi-calibrated (unknown focal length) problems. Based on the hidden variable technique, we convert the problems to the polynomial eigenvalue problems, and derive new 3.5-point, 3.5-point, 4-point solvers for two cameras such that the two focal lengths are unknown but equal, one of them is unknown, and both are unknown and possibly different, respectively. We present detailed analyses and comparisons with the existing 6- and 7-point solvers, including results with smart phone images.	[Ding, Yaqing; Yang, Jian; Kong, Hui] Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab, Nanjing, Peoples R China; [Ding, Yaqing; Yang, Jian; Kong, Hui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Jiangsu, Peoples R China; [Ponce, Jean] PSL Univ, Dept Informat ENS, CNRS, ENS INRIA, F-75012 Paris, France	Nanjing University of Science & Technology; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Ding, YQ (corresponding author), Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab, Nanjing, Peoples R China.	dingyaqing@njust.edu.cn; csjyang@njust.edu.cn; jean.ponce@inria.fr; konghui@njust.edu.cn	Ding, Yaqing/HHS-4745-2022	Ding, Yaqing/0000-0002-7448-6686	National Science Fund of China [U1713208, 61672287]; "111" Program [B13022]; Inria/NYU collaboration; Louis Vuitton/ENS chair on artificial intelligence; French government under Agence Nationale de la Recherche as part of the "Investissements d'avenir" program [ANR-19-P3IA-0001]	National Science Fund of China(National Natural Science Foundation of China (NSFC)); "111" Program(Ministry of Education, China - 111 Project); Inria/NYU collaboration; Louis Vuitton/ENS chair on artificial intelligence; French government under Agence Nationale de la Recherche as part of the "Investissements d'avenir" program(French National Research Agency (ANR))	The authors would like to thank the editor and the anonymous reviewers for their critical and constructive comments and suggestions. This work was supported by the National Science Fund of China (Grant Nos. U1713208 and 61672287), "111" Program B13022, the Inria/NYU collaboration and the Louis Vuitton/ENS chair on artificial intelligence. In addition, this work was funded in part by the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute).	Bai Zhaojun, 2000, TEMPLATES SOLUTION A; Barath D, 2019, IEEE I CONF COMP VIS, P1091, DOI 10.1109/ICCV.2019.00118; BAREISS EH, 1968, MATH COMPUT, V22, P565; Bougnoux S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P790, DOI 10.1109/ICCV.1998.710808; Bujnak M, 2009, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2009.5459402; Byrod M, 2009, INT J COMPUT VISION, V84, P237, DOI 10.1007/s11263-009-0235-z; Chum O, 2005, PROC CVPR IEEE, P772; Chum O., 2005, THESIS CZECH TECH U; Cox D. A., 2006, USING ALGEBRAIC GEOM, V185; Ding YQ, 2019, IEEE I CONF COMP VIS, P1655, DOI 10.1109/ICCV.2019.00174; Faugeras O. D., 1988, International Journal of Pattern Recognition and Artificial Intelligence, V2, P485, DOI 10.1142/S0218001488000285; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Frahm J.-M., 2006, P COMP VIS PATT REC, V1, P453, DOI DOI 10.1109/CVPR.2006.235; Fraundorfer F, 2012, IEEE ROBOT AUTOM MAG, V19, P78, DOI 10.1109/MRA.2012.2182810; Fraundorfer F, 2010, LECT NOTES COMPUT SC, V6314, P269, DOI 10.1007/978-3-642-15561-1_20; Grayson Daniel R, 2002, MACAULAY 2 SOFTWARE, P4; Guan BL, 2018, IEEE INT CONF ROBOT, P2320; Guan BL, 2018, COMPUT VIS IMAGE UND, V170, P79, DOI 10.1016/j.cviu.2018.03.001; Hartley R., 2003, MULTIPLE VIEW GEOMET; HARTLEY RI, 1992, LECT NOTES COMPUT SC, V588, P579; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1064; Hartley R, 2012, IEEE T PATTERN ANAL, V34, P2303, DOI 10.1109/TPAMI.2012.43; Horn R.A., 2013, MATRIX ANAL, VSecond; K_ustner H., 2012, VNR CONCISE ENCY MAT; Kahl F., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P366, DOI 10.1109/CVPR.1999.784661; Kalantari M, 2011, J MATH IMAGING VIS, V39, P259, DOI 10.1007/s10851-010-0234-2; Kukelova Zuzana, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P216, DOI 10.1007/978-3-642-19309-5_17; KUKELOVA Z, 2017, P IEEE C COMP VIS PA; Kukelova Z, 2008, LECT NOTES COMPUT SC, V5304, P302, DOI 10.1007/978-3-540-88690-7_23; Kukelova Z, 2016, PROC CVPR IEEE, P1799, DOI 10.1109/CVPR.2016.199; Kukelova Z, 2012, IEEE T PATTERN ANAL, V34, P1381, DOI 10.1109/TPAMI.2011.230; Larsson V., 2017, P IEEE C COMP VIS PA, P4; Larsson V, 2018, PROC CVPR IEEE, P3945, DOI 10.1109/CVPR.2018.00415; Larsson V, 2018, PROC CVPR IEEE, P2984, DOI 10.1109/CVPR.2018.00315; Larsson V, 2017, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2017.254; Lebeda K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.95; Li HD, 2006, LECT NOTES COMPUT SC, V3954, P200; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malis E, 2002, IEEE T PATTERN ANAL, V24, P1268, DOI 10.1109/TPAMI.2002.1033217; Malis E., 2007, DEEPER UNDERSTANDING; Malis E, 2009, IEEE INT CONF ROBOT, P1287; Naroditsky O, 2012, IEEE T PATTERN ANAL, V34, P818, DOI 10.1109/TPAMI.2011.226; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Saurer O, 2017, IEEE T PATTERN ANAL, V39, P327, DOI 10.1109/TPAMI.2016.2545663; Stewenius H, 2005, PROC CVPR IEEE, P789; Sweeney Chris, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P483, DOI 10.1109/3DV.2014.66; Torii Akihiko, 2010, Computer Vision - ACCV 2010 Workshops. ACCV 2010 International Workshops. Revised Selected Papers, P184, DOI 10.1007/978-3-642-22819-3_19; Vieville T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P591, DOI 10.1109/ICCV.1993.378157; Zhang ZF, 1996, IMAGE UNDERSTANDING WORKSHOP, 1996 PROCEEDINGS, VOLS I AND II, P1007	51	4	4	1	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					196	210		10.1109/TPAMI.2020.3005373	http://dx.doi.org/10.1109/TPAMI.2020.3005373			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750796				2022-12-18	WOS:000728561300015
J	Oh, SW; Lee, JY; Xu, N; Kim, SJ				Oh, Seoung Wug; Lee, Joon-Young; Xu, Ning; Kim, Seon Joo			Space-Time Memory Networks for Video Object Segmentation With User Guidance	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Video object segmentation; user-guided video object segmentation; semi-supervised video object segmentation; interactive video object segmentation; memory networks		We propose a novel and unified solution for user-guided video object segmentation tasks. In this work, we consider two scenarios of user-guided segmentation: semi-supervised and interactive segmentation. Due to the nature of the problem, available cues - video frame(s) with object masks (or scribbles) - become richer with the intermediate predictions (or additional user inputs). However, the existing methods make it impossible to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learning to read relevant information from all available sources. In the semi-supervised scenario, the previous frames with object masks form an external memory, and the current frame as the query is segmented using the information in the memory. Similarly, to work with user interactions, the frames that are given user inputs form the memory that guides segmentation. Internally, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. The abundant use of the guidance information allows us to better handle challenges such as appearance changes and occlusions. We validate our method on the latest benchmark sets and achieve state-of-the-art performance along with a fast runtime.	[Oh, Seoung Wug; Kim, Seon Joo] Yonsei Univ, Seoul 03722, South Korea; [Lee, Joon-Young; Xu, Ning] Adobe Res, San Jose, CA 95110 USA	Yonsei University; Adobe Systems Inc.	Kim, SJ (corresponding author), Yonsei Univ, Seoul 03722, South Korea.	sw.oh@yonsei.ac.kr; jolee@adobe.com; nxu@adobe.com; seonjookim@yonsei.ac.kr			ICT R&D program of MSIT/IITP [2017-0-01772]; Technology Innovation Program - Ministry of Trade, Industry& Energy (MOTIE, Korea) [10073129]; Graduate School of Yonsei University	ICT R&D program of MSIT/IITP; Technology Innovation Program - Ministry of Trade, Industry& Energy (MOTIE, Korea); Graduate School of Yonsei University	This work was supported in part by the ICT R&D program of MSIT/IITP (2017-0-01772, Development of QA systems for Video Story Understanding to pass the Video Turing Test), the Technology Innovation Program (10073129, Development of Driving and Manipulation Intelligence based on Deep Learning and Inverse Reinforcement Learning for Dual Arm Mobile Robot) funded By the Ministry of Trade, Industry& Energy (MOTIE, Korea), and the Graduate School of Yonsei University Research Scholarship Grants in 2020.	Agarwala A, 2004, ACM T GRAPHIC, V23, P584, DOI 10.1145/1015706.1015764; Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376; Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626; Benard A., 2017, ARXIV PREPRINT ARXIV; Bratt B., 2012, ROTOSCOPING TECHNIQU; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Caelles Sergi, 2018, ARXIV180300557; Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130; Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774; Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81; Ci H, 2018, LECT NOTES COMPUT SC, V11215, P524, DOI 10.1007/978-3-030-01252-6_31; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; GUO ZC, 1989, COMMUN ACM, V32, P359, DOI 10.1145/62065.62074; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4; Hu YT, 2017, ADV NEUR IN, V30; Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336; Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916; Khoreva A, 2019, INT J COMPUT VISION, V127, P1175, DOI 10.1007/s11263-019-01164-6; Kim, 2019, P C COMP VIS PATT RE; Kingma D.P, P 3 INT C LEARNING R; Kumar V, 2016, INT CONF ADVAN COMPU; Lee S, 2018, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2018.00153; Li WB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925973; Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu, 2019, P C COMP VIS PATT RE; Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35; Marki N, 2016, PROC CVPR IEEE, P743, DOI 10.1109/CVPR.2016.87; Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670; Miller A., 2016, ARXIV160603126, P1400; Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847; Na S, 2017, IEEE I CONF COMP VIS, P677, DOI 10.1109/ICCV.2017.80; Nagaraja NS, 2015, IEEE I CONF COMP VIS, P3235, DOI 10.1109/ICCV.2015.370; Oh SW, 2019, IEEE I CONF COMP VIS, P4402, DOI 10.1109/ICCV.2019.00450; Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/ICCV.2019.00932; Oh SW, 2019, PROC CVPR IEEE, P5242, DOI 10.1109/CVPR.2019.00539; Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770; Park CC, 2017, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2017.681; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372; Pont-Tuset J., 2017, ARXIV170400675; Price BL, 2009, IEEE I CONF COMP VIS, P779, DOI 10.1109/ICCV.2009.5459293; Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960; Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P175, DOI 10.1145/3130348.3130364; Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423; Van Gool L, ARXIV190500737; Vaswani A, 2017, ADV NEUR IN, V30; Ventura C, 2019, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2019.00542; Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971; Voigtlaender Paul, 2017, ARXIV170609364; Voigtlaender Paul, 2019, ARXIV190404552; Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Xu N., 2018, ARXIV180903327; Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36; Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47; Yang Jimei, 2015, NIPS; Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680; Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10; Yoon JS, 2017, IEEE I CONF COMP VIS, P2186, DOI 10.1109/ICCV.2017.238; Zheng W, 2019, P C COMP VIS PATT RE; Zhong F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366194	65	4	4	12	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					442	455		10.1109/TPAMI.2020.3008917	http://dx.doi.org/10.1109/TPAMI.2020.3008917			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750815				2022-12-18	WOS:000728561300032
J	Osawa, K; Tsuji, Y; Ueno, Y; Naruse, A; Foo, CS; Yokota, R				Osawa, Kazuki; Tsuji, Yohei; Ueno, Yuichiro; Naruse, Akira; Foo, Chuan-Sheng; Yokota, Rio			Scalable and Practical Natural Gradient for Large-Scale Deep Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Natural gradient descent; distributed deep learning; deep convolutional neural networks; image classification		Large-scale distributed training of deep neural networks results in models with worse generalization performance as a result of the increase in the effective mini-batch size. Previous approaches attempt to address this problem by varying the learning rate and batch size over epochs and layers, or ad hoc modifications of batch normalization. We propose scalable and practical natural gradient descent (SP-NGD), a principled approach for training models that allows them to attain similar generalization performance to models trained with first-order optimization methods, but with accelerated convergence. Furthermore, SP-NGD scales to large mini-batch sizes with a negligible computational overhead as compared to first-order methods. We evaluated SP-NGD on a benchmark task where highly optimized first-order methods are available as references: training a ResNet-50 model for image classification on ImageNet. We demonstrate convergence to a top-1 validation accuracy of 75.4 percent in 5.5 minutes using a mini-batch size of 32,768 with 1,024 GPUs, as well as an accuracy of 74.9 percent with an extremely large mini-batch size of 131,072 in 873 steps of SP-NGD.	[Osawa, Kazuki; Tsuji, Yohei; Ueno, Yuichiro; Yokota, Rio] Tokyo Inst Technol, Tokyo 1528550, Japan; [Ueno, Yuichiro; Yokota, Rio] AIST, AIST Tokyo Tech RWBC Oil, Tokyo 1528550, Japan; [Naruse, Akira] NVIDIA, Tokyo 1070052, Japan; [Foo, Chuan-Sheng] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore	Tokyo Institute of Technology; National Institute of Advanced Industrial Science & Technology (AIST); Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)	Tsuji, Y (corresponding author), Tokyo Inst Technol, Tokyo 1528550, Japan.	oosawa.k.ad@m.titech.ac.jp; tsuji.y.ae@m.titech.ac.jp; ueno.y.ai@m.titech.ac.jp; anaruse@nvidia.com; foo_chuan_sheng@i2r.a-star.edu.sg; rioyokota@gsic.titech.ac.jp			"ABCI Grand Challenge" Program, National Institute of Advanced Industrial Science and Technology (AIST); JSPS KAKENHI [JP18H03248, JP19J13477]; JST CREST, Japan [JPMJCR19F5]; "Joint Usage/Research Center for Interdisciplinary Large-scale Information Infrastructures" in Japan [jh180012-NAHI]; Tokyo Tech through the HPCI System Research Project [hp190122]	"ABCI Grand Challenge" Program, National Institute of Advanced Industrial Science and Technology (AIST); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST CREST, Japan(Core Research for Evolutional Science and Technology (CREST)); "Joint Usage/Research Center for Interdisciplinary Large-scale Information Infrastructures" in Japan; Tokyo Tech through the HPCI System Research Project	Computational resource of AI Bridging Cloud Infrastructure (ABCI) was awarded by "ABCI Grand Challenge" Program, National Institute of Advanced Industrial Science and Technology (AIST). This work was supported by JSPS KAKENHI Grant Number JP18H03248 and JP19J13477. This work was also supported by JST CREST Grant Number JPMJCR19F5, Japan. Part of this work was conducted as research activities of AIST -Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory (RWBC-OIL). This work was supported by "Joint Usage/Research Center for Interdisciplinary Large-scale Information Infrastructures" in Japan (Project ID: jh180012-NAHI). This research used computational resources of the HPCI system provided by Tokyo Tech through the HPCI System Research Project (Project ID: hp190122). The authors would like to thank Yaroslav Bulatov (South Park Commons) for helpful comments on the manuscript.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Akiba T., 2017, ARXIV PREPRINT ARXIV; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S., 2019, P INT C ART INT STAT, P694; Botev A, 2017, PR MACH LEARN RES, V70; Cheng, 2018, ARXIV 181106992; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ginsburg B., 2017, ARXIV 170803888; Grosse R, 2016, PR MACH LEARN RES, V48; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; HOFFER E, 2018, P INT C NEUR INF PRO, P2160; Jaggi M., 2020, PROC INT C LEARN REP; Jia X., 2018, ARXIV PREPRINT ARXIV; Johnson M., 2018, PROC INT C LEARN REP; Kunstner F., 2019, ARXIV190512558; Martens J., 2017, PROC INT C LEARN REP; Martens J., 2010, P 27 INT C MACH LEAR, P735; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Martens James, 2014, NEW INSIGHTS PERSPEC; Mikami H., 2018, ARXIV181105233; OSAWA K, 2019, PROC IEEE C COMPUT V; Osawa K, 2019, ADV NEUR IN, V32; Paszke A, 2019, ADV NEUR IN, V32; Roux N. L., 2019, ARXIV 190607774; Shallue CJ, 2019, J MACH LEARN RES, V20; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tang, 2017, P INT C LEARN REPR; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tokui S, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2002, DOI 10.1145/3292500.3330756; Tsuji Y, 2019, PROCEEDINGS OF THE 48TH INTERNATIONAL CONFERENCE ON PARALLEL PROCESSING WORKSHOPS (ICPP 2019), DOI 10.1145/3339186.3339202; Ueno Y, 2019, IEEE ACM INT SYMP, P430, DOI [10.1109/CCGRID.2019.000.57, 10.1109/CCGRID.2019.00057]; van Laarhoven, 2017, ARXIV 170605350; Wu YH, 2017, ADV NEUR IN, V30; Xu B., 2018, PROC INT C LEARN REP; Yamazaki Masafumi, 2019, ARXIV190312650; ZHANG G, 2019, PROC INT C NEURAL IN, P8196; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412	42	4	4	3	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					404	415		10.1109/TPAMI.2020.3004354	http://dx.doi.org/10.1109/TPAMI.2020.3004354			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750792	hybrid, Green Submitted			2022-12-18	WOS:000728561300029
J	Carlucci, FM; Porzi, L; Caputo, B; Ricci, E; Bulo, SR				Carlucci, Fabio Maria; Porzi, Lorenzo; Caputo, Barbara; Ricci, Elisa; Bulo, Samuel Rota			MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deep learning; Adaptation models; Computer architecture; Training; Visualization; Entropy; Data models; Unsupervised domain adaptation; visual recognition; batch normalization; domain alignment layers; entropy loss		One of the main challenges for developing visual recognition systems working in the wild is to devise computational models immune from the domain shift problem, i.e., accurate when test data are drawn from a (slightly) different data distribution than training samples. In the last decade, several research efforts have been devoted to devise algorithmic solutions for this issue. Recent attempts to mitigate domain shift have resulted into deep learning models for domain adaptation which learn domain-invariant representations by introducing appropriate loss terms, by casting the problem within an adversarial learning framework or by embedding into deep network specific domain normalization layers. This paper describes a novel approach for unsupervised domain adaptation. Similarly to previous works we propose to align the learned representations by embedding them into appropriate network feature normalization layers. Opposite to previous works, our Domain Alignment Layers are designed not only to match the source and target feature distributions but also to automatically learn the degree of feature alignment required at different levels of the deep network. Differently from most previous deep domain adaptation methods, our approach is able to operate in a multi-source setting. Thorough experiments on four publicly available benchmarks confirm the effectiveness of our approach.	[Carlucci, Fabio Maria] Sapienza Univ Rome, Dept Comp Control & Management Engn, I-00185 Rome, Italy; [Porzi, Lorenzo; Bulo, Samuel Rota] Mapillary Res, A-8010 Graz, Austria; [Caputo, Barbara] Italian Inst Technol, I-20133 Milan, Italy; [Ricci, Elisa] Fdn Bruno Kessler, I-38060 Trento, Italy; [Ricci, Elisa] Univ Trento, Dept Informat Engn & Comp Sci, I-38060 Trento, Italy	Sapienza University Rome; Istituto Italiano di Tecnologia - IIT; Fondazione Bruno Kessler; University of Trento	Carlucci, FM (corresponding author), Sapienza Univ Rome, Dept Comp Control & Management Engn, I-00185 Rome, Italy.	fabiom.carlucci@dis.uniroma1.it; lorenzo@mapillary.com; barbara.caputo@iit.it; eliricci@fbk.eu; samuel@mapillary.com		Rota Bulo, Samuel/0000-0002-2372-1367; Carlucci, Fabio Maria/0000-0003-4916-5706	project CHIST-ERA ALOOF, ERC [637076]; project DIGIMAP - Austrian Research Promotion Agency [860375]	project CHIST-ERA ALOOF, ERC; project DIGIMAP - Austrian Research Promotion Agency	This work was supported by the projects CHIST-ERA ALOOF, ERC #637076 RoboExNovo (F.M.C., B. C.), and DIGIMAP, funded under Grant #860375 by the Austrian Research Promotion Agency.	Aljundi R, 2016, LECT NOTES COMPUT SC, V9915, P508, DOI 10.1007/978-3-319-49409-8_43; [Anonymous], 2014, ABS14123474 CORR; Bousmalis K, 2016, ADV NEUR IN, V29; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233; Carlucci FM, 2017, LECT NOTES COMPUT SC, V10484, P357, DOI 10.1007/978-3-319-68560-1_32; Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chu WS, 2013, PROC CVPR IEEE, P3515, DOI 10.1109/CVPR.2013.451; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J, 2014, PR MACH LEARN RES, V32; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Grandvalet Y., 2004, P INT C NEUR INF PRO; Griffin G., 2007, 120 CAL I TECHN; Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman J, 2018, PR MACH LEARN RES, V80; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li Y., 2017, ICLR WORKSH; Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Long MS, 2013, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2013.59; Mancini M, 2018, IEEE INT C INT ROBOT, P1103, DOI 10.1109/IROS.2018.8593862; Mancini M, 2018, PROC CVPR IEEE, P3771, DOI 10.1109/CVPR.2018.00397; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; ONEILL TJ, 1978, J AM STAT ASSOC, V73, P821, DOI 10.2307/2286287; Pal D., 2010, INT C ART INT STAT, P129; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Saito K, 2017, PR MACH LEARN RES, V70; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Sha, 2013, P INT C MACH LEARN; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tommasi T, 2016, LECT NOTES COMPUT SC, V9915, P475, DOI 10.1007/978-3-319-49409-8_39; Tommasi T, 2015, LECT NOTES COMPUT SC, V8927, P18, DOI 10.1007/978-3-319-16199-0_2; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Xie JW, 2015, INT J COMPUT VISION, V114, P91, DOI 10.1007/s11263-014-0757-x; Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417; Yamada M, 2012, LECT NOTES COMPUT SC, V7575, P674, DOI 10.1007/978-3-642-33765-9_48; Zeng XY, 2014, LECT NOTES COMPUT SC, V8691, P472, DOI 10.1007/978-3-319-10578-9_31; Zhao H., 2018, P 6 INT C LEARN REPR, P1; Zhu X, 2017, WORLD, V10, P10	58	4	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4441	4452		10.1109/TPAMI.2020.3001338	http://dx.doi.org/10.1109/TPAMI.2020.3001338			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750781				2022-12-18	WOS:000714203900021
J	Li, ZQ; Dekel, T; Cole, F; Tucker, R; Snavely, N; Liu, C; Freeman, WT				Li, Zhengqi; Dekel, Tali; Cole, Forrester; Tucker, Richard; Snavely, Noah; Liu, Ce; Freeman, William T.			MannequinChallenge: Learning the Depths of Moving People by Watching Frozen People	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Three-dimensional displays; Cleaning; Internet; Image reconstruction; Geometry; Training; Depth prediction; mannequin challenge; dynamic scene reconstruction		We present a method for predicting dense depth in scenarios where both a monocular camera and people in the scene are freely moving (right). Existing methods for recovering depth for dynamic, non-rigid objects from monocular video impose strong assumptions on the objects' motion and may only recover sparse depth. In this paper, we take a data-driven approach and learn human depth priors from a new source of data: thousands of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a hand-held camera tours the scene (left). Because people are stationary, geometric constraints hold, thus training data can be generated using multi-view stereo reconstruction. At inference time, our method uses motion parallax cues from the static areas of the scenes to guide the depth prediction. We evaluate our method on real-world sequences of complex human actions captured by a moving hand-held camera, show improvement over state-of-the-art monocular depth prediction methods, and demonstrate various 3D effects produced using our predicted depth.	[Li, Zhengqi; Snavely, Noah] Cornell Univ, Cornell Tech, Dept Comp Sci, Ithaca, NY 14850 USA; [Dekel, Tali; Cole, Forrester; Tucker, Richard; Snavely, Noah; Liu, Ce; Freeman, William T.] Google Res, Mountain View, CA 94043 USA	Cornell University; Google Incorporated	Li, ZQ (corresponding author), Cornell Univ, Cornell Tech, Dept Comp Sci, Ithaca, NY 14850 USA.	zl548@cornell.edu; tdekel@google.com; fcole@google.com; richardt@google.com; snavely@cs.cornell.edu; celiu@google.com; wfreeman@google.com						[Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Basha T, 2013, INT J COMPUT VISION, V101, P6, DOI 10.1007/s11263-012-0542-7; Basha T, 2012, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2012.6247830; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chen Weifeng, 2016, NEURIPS, P2, DOI DOI 10.5555/3157096.3157178; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Eigen D, 2014, ADV NEUR IN, V27; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Howard I. P., 2002, SEEING DEPTH, V1; Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22; Irani M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P17, DOI 10.1007/BFb0015520; Jiang HQ, 2012, LECT NOTES COMPUT SC, V7573, P601, DOI 10.1007/978-3-642-33709-3_43; Kingma D.P, P 3 INT C LEARNING R; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500; Li Z., 2019, ARXIV191007454; Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218; Lv ZY, 2018, LECT NOTES COMPUT SC, V11209, P484, DOI 10.1007/978-3-030-01228-1_29; Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594; Mees O, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P151, DOI 10.1109/IROS.2016.7759048; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Ni BB, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Ranftl R, 2016, PROC CVPR IEEE, P4058, DOI 10.1109/CVPR.2016.440; Rematas K, 2018, PROC CVPR IEEE, P4738, DOI 10.1109/CVPR.2018.00498; Richardt C, 2016, INT CONF 3D VISION, P276, DOI 10.1109/3DV.2016.36; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Simon T, 2017, IEEE T PATTERN ANAL, V39, P2201, DOI 10.1109/TPAMI.2016.2638904; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; Vo M, 2016, PROC CVPR IEEE, P1710, DOI 10.1109/CVPR.2016.189; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wedel A, 2011, INT J COMPUT VISION, V95, P29, DOI 10.1007/s11263-010-0404-0; Wikipedia, 2018, FALS POS FALS NEG; Wulff J, 2017, PROC CVPR IEEE, P6911, DOI 10.1109/CVPR.2017.731; Xian K, 2018, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2018.00040; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xu D, 2019, IEEE T PATTERN ANAL, V41, P1426, DOI 10.1109/TPAMI.2018.2839602; Xu WP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181973; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zheng EL, 2015, IEEE I CONF COMP VIS, P4435, DOI 10.1109/ICCV.2015.504; Zhou HZ, 2018, LECT NOTES COMPUT SC, V11220, P851, DOI 10.1007/978-3-030-01270-0_50; Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhu Y, 2014, IMAGE VISION COMPUT, V32, P453, DOI 10.1016/j.imavis.2014.04.005; Zollhofer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601165	66	4	4	3	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4229	4241		10.1109/TPAMI.2020.2974454	http://dx.doi.org/10.1109/TPAMI.2020.2974454			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32078534				2022-12-18	WOS:000714203900008
J	Tan, Y; Zheng, HT; Zhu, YH; Yuan, XY; Lin, X; Brady, D; Fang, L				Tan, Yang; Zheng, Haitian; Zhu, Yinheng; Yuan, Xiaoyun; Lin, Xing; Brady, David; Fang, Lu			CrossNet plus plus : Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Reference-based super-resolution; camera array; light field imaging; image synthesis; image warping; optical flow	IMAGE SUPERRESOLUTION; ALGORITHM; ENHANCEMENT	The ability of camera arrays to efficiently capture higher space-bandwidth product than single cameras has led to various multiscale and hybrid systems. These systems play vital roles in computational photography, including light field imaging, 360 VR camera, gigapixel videography, etc. One of the critical tasks in multiscale hybrid imaging is matching and fusing cross-resolution images from different cameras under perspective parallax. In this paper, we investigate the reference-based super-resolution (RefSR) problem associated with dual-camera or multi-camera systems. RefSR consists of super-resolving a low-resolution (LR) image given an external high-resolution (HR) reference image, where they suffer both a significant resolution gap (8x) and large parallax (similar to 10% pixel displacement). We present CrossNet++, an end-to-end network containing novel two-stage cross-scale warping modules, image encoder and fusion decoder. The stage I learns to narrow down the parallax distinctively with the strong guidance of landmarks and intensity distribution consensus. Then the stage II operates more fine-grained alignment and aggregation in feature domain to synthesize the final super-resolved image. To further address the large parallax, new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss are proposed to regularize training and enable better convergence. CrossNet++ significantly outperforms the state-of-art on light field datasets as well as real dual-camera data. We further demonstrate the generalization of our framework by transferring it to video super-resolution and video denoising.	[Tan, Yang; Zhu, Yinheng; Fang, Lu] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 517000, Peoples R China; [Zheng, Haitian] Rochester Univ, Rochester, NY 14627 USA; [Yuan, Xiaoyun] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China; [Lin, Xing] Tsinghua Univ, Beijing Innovat Ctr Future Chip, Beijing 100084, Peoples R China; [Brady, David] Duke Univ, Elect & Comp Engn, Durham, NC 27708 USA	Tsinghua University; University of Rochester; Hong Kong University of Science & Technology; Tsinghua University; Duke University	Fang, L (corresponding author), Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 517000, Peoples R China.	tany19@mails.tsinghua.edu.cn; hzheng15@ur.rochester.edu; zhuyh19@mails.tsinghua.edu.cn; xyuanag@connect.ust.hk; lin-x@tsinghua.edu.cn; David.Brady@duke.edu; fanglu@tsinghua.edu.cn		Zheng, Haitian/0000-0003-0415-1765; Brady, David Jones/0000-0001-5655-2478; Yuan, Xiaoyun/0000-0002-7914-3658	Natural Science Foundation of China (NSFC) [61722209, 61860206003]; Shenzhen Science and Technology Research and Development Funds [JCYJ20180507183706645, ZDYBH201900000002]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Shenzhen Science and Technology Research and Development Funds	This work was supported in part by the Natural Science Foundation of China (NSFC) under contract No. 61722209 and 61860206003, in part by the Shenzhen Science and Technology Research and Development Funds (JCYJ20180507183706645 and ZDYBH201900000002). Yang Tan and Haitian Zheng contributed equally to thiswork.	Babacan SD, 2008, IEEE IMAGE PROC, P641, DOI 10.1109/ICIP.2008.4711836; Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941; Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21; Boominathan V, 2014, IEEE INT CONF COMPUT; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; Chen C, 2019, PROC CVPR IEEE, P1652, DOI 10.1109/CVPR.2019.00175; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Godard C, 2018, LECT NOTES COMPUT SC, V11219, P560, DOI 10.1007/978-3-030-01267-0_33; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713; Hensel M, 2017, ADV NEUR IN, V30; Ji DH, 2017, PROC CVPR IEEE, P7092, DOI 10.1109/CVPR.2017.750; Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537; Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu C, 2011, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2011.5995614; Liu C, 2010, LECT NOTES COMPUT SC, V6313, P706; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324; Nair V, 2010, P 27 INT C MACHINE L, P807; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693; Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481; Salimans T, 2016, ADV NEUR IN, V29; Salvador J, 2015, IEEE I CONF COMP VIS, P325, DOI 10.1109/ICCV.2015.45; Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003; Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Tai YW, 2010, PROC CVPR IEEE, P2400, DOI 10.1109/CVPR.2010.5539933; Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479; Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514; Varghese G, 2010, IEEE T CIRC SYST VID, V20, P1032, DOI 10.1109/TCSVT.2010.2051366; Wang TC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073614; Wang YW, 2017, IEEE T VIS COMPUT GR, V23, P2357, DOI 10.1109/TVCG.2016.2628743; Wu JD, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), DOI 10.1109/VCIP.2015.7457904; Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2; Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75; Yang J., 2008, 2008 IEEE C COMP VIS, DOI 10.1109/CVPR.2008.4587647; Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141; Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431; Yuan XY, 2017, IEEE INT CONF COMPUT, P33; Zhang JN, 2020, IEEE INT CONF COMPUT; Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817; Zhao MD, 2018, IEEE T COMPUT IMAG, V4, P406, DOI 10.1109/TCI.2018.2838457; Zheng H., 2017, P BMVC, P2; Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6; Zheng HT, 2017, IEEE INT CONF COMP V, P2481, DOI 10.1109/ICCVW.2017.292	68	4	4	8	26	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4291	4305		10.1109/TPAMI.2020.2997007	http://dx.doi.org/10.1109/TPAMI.2020.2997007			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750771				2022-12-18	WOS:000714203900012
J	Xia, C; Han, JW; Zhang, DW				Xia, Chen; Han, Junwei; Zhang, Dingwen			Evaluation of Saccadic Scanpath Prediction: Subjective Assessment Database and Recurrent Neural Network Based Metric	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Predictive models; Visualization; Feature extraction; Computational modeling; Visual databases; Visual attention; saccadic models; evaluation metrics; scanpath comparison; Long Short-Term Memory (LSTM) network; semantic hashing	VISUAL-ATTENTION; SALIENCY; MODEL	In recent years, predicting the saccadic scanpaths of humans has become a new trend in the field of visual attention modeling. Given various saccadic algorithms, determining how to evaluate their ability to model a dynamic saccade has become an important yet understudied issue. To our best knowledge, existing metrics for evaluating saccadic prediction models are often heuristically designed, which may produce results that are inconsistent with human subjective assessment. To this end, we first construct a subjective database by collecting the assessments on 5,000 pairs of scanpaths from ten subjects. Based on this database, we can compare different metrics according to their consistency with human visual perception. In addition, we also propose a data-driven metric to measure scanpath similarity based on the human subjective comparison. To achieve this goal, we employ a long short-term memory (LSTM) network to learn the inference from the relationship of encoded scanpaths to a binary measurement. Experimental results have demonstrated that the LSTM-based metric outperforms other existing metrics. Moreover, we believe the constructed database can be used as a benchmark to inspire more insights for future metric selection.	[Xia, Chen; Han, Junwei; Zhang, Dingwen] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China; [Zhang, Dingwen] Xidian Univ, Sch Machine Elect Engn, Xian 710071, Peoples R China	Northwestern Polytechnical University; Xidian University	Han, JW (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.	cxia@nwpu.edu.cn; junweihan2010@gmail.com; zhangdingwen2006yyy@gmail.com			National Key R&D Program of China [2017YFB1002201]; National Natural Science Foundation of China [61802314, 61876140]; Research Funds for Interdisciplinary Subject, NWPU; Natural Science Foundation of Shaanxi Province [2019JQ-296]; China Postdoctoral Science Foundation [2017M623242]; China Postdoctoral Support Scheme for Innovative Talents [BX20180236]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Research Funds for Interdisciplinary Subject, NWPU; Natural Science Foundation of Shaanxi Province(Natural Science Foundation of Shaanxi Province); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); China Postdoctoral Support Scheme for Innovative Talents	This work was supported in part by the National Key R&D Program of China under Grant 2017YFB1002201, in part by the National Natural Science Foundation of China under Grant 61802314 and Grant 61876140, in part by the Research Funds for Interdisciplinary Subject, NWPU, in part by the Project Supported by Natural Science Foundation of Shaanxi Province under Grant 2019JQ-296, in part by the China Postdoctoral Science Foundation under Grant 2017M623242, and in part by China Postdoctoral Support Scheme for Innovative Talents under Grant BX20180236.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Anderson NC, 2015, BEHAV RES METHODS, V47, P1377, DOI 10.3758/s13428-014-0550-3; Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275; Assens M, 2019, LECT NOTES COMPUT SC, V11133, P406, DOI 10.1007/978-3-030-11021-5_25; Boccignone G, 2004, PHYSICA A, V331, P207, DOI 10.1016/j.physa.2003.09.011; Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715; Borji A, 2013, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2013.118; Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Borji Ali, 2015, ARXIV150503581; Bruce N., 2005, P 18 INT C NEUR INF, P155; Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601; Chen ZZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P642; Cristino F, 2010, BEHAV RES METHODS, V42, P692, DOI 10.3758/BRM.42.3.692; DAI HP, 2019, ANN IEEE INT CONF SE; Emami M, 2013, IMAGE VISION COMPUT, V31, P796, DOI 10.1016/j.imavis.2013.08.004; Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875; Garcia-Diaz A, 2012, J VISION, V12, DOI 10.1167/12.6.17; Gide MS, 2016, IEEE T IMAGE PROCESS, V25, P3852, DOI 10.1109/TIP.2016.2577498; Gutierrez J, 2018, SIGNAL PROCESS-IMAGE, V69, P35, DOI 10.1016/j.image.2018.05.003; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jarodzka Halszka, 2010, P 2010 S EYE TRACK R, P211, DOI DOI 10.1145/1743666.1743718; Jiang M, 2016, IEEE T NEUR NET LEAR, V27, P1241, DOI 10.1109/TNNLS.2015.2496306; Judd T., 2012, MIT TECHNICAL REPORT; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kootstra G., 2008, P 19 BRIT MACH VIS C, P1115; Krizhevsky A., 2011, P 19 EUR S ART NEUR; Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620; Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86; Le Meur O, 2017, IEEE T IMAGE PROCESS, V26, P4777, DOI 10.1109/TIP.2017.2722238; Le Meur O, 2015, VISION RES, V116, P152, DOI 10.1016/j.visres.2014.12.026; Le Meur O, 2013, BEHAV RES METHODS, V45, P251, DOI 10.3758/s13428-012-0226-9; Li H, 2019, J VIS COMMUN IMAGE R, V64, DOI 10.1016/j.jvcir.2019.102611; Li J., 2018, ARXIV180610257; Li J, 2015, IEEE I CONF COMP VIS, P190, DOI 10.1109/ICCV.2015.30; Liu HY, 2013, IEEE I CONF COMP VIS, P3232, DOI 10.1109/ICCV.2013.401; Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047; Liu NA, 2018, IEEE T NEUR NET LEAR, V29, P392, DOI 10.1109/TNNLS.2016.2628878; NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4; Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520; Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3; Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147; SAUER T, 1991, J STAT PHYS, V65, P579, DOI 10.1007/BF01053745; Sharma P, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P654; Sun WJ, 2021, IEEE T PATTERN ANAL, V43, P2101, DOI 10.1109/TPAMI.2019.2956930; Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4; Tavakoli HR, 2017, NEUROCOMPUTING, V244, P10, DOI 10.1016/j.neucom.2017.03.018; Tavakoli HR, 2013, IMAGE VISION COMPUT, V31, P686, DOI 10.1016/j.imavis.2013.06.006; Ngo T, 2017, IEEE IMAGE PROC, P3435; Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358; Wang W, 2020, IEEE T CYBERNETICS, V50, P3973, DOI 10.1109/TCYB.2019.2917078; Wang W, 2011, PROC CVPR IEEE, P441, DOI 10.1109/CVPR.2011.5995423; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612; Wang YX, 2017, COGN PROCESS, V18, P87, DOI 10.1007/s10339-016-0781-6; Wilming N, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0024038; Wloka C, 2016, PROC CVPR IEEE, P525, DOI 10.1109/CVPR.2016.63; Wu Y, 2020, IEEE T IMAGE PROCESS, V29, P3984, DOI 10.1109/TIP.2020.2967584; Wu Y, 2017, IEEE INT CON MULTI, P529, DOI 10.1109/ICME.2017.8019456; Xia C, 2019, IEEE T IMAGE PROCESS, V28, P3502, DOI 10.1109/TIP.2019.2897966; Xia C, 2016, IEEE T NEUR NET LEAR, V27, P1227, DOI 10.1109/TNNLS.2015.2512898; Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28; Zanca D, 2020, IEEE T PATTERN ANAL, V42, P2983, DOI 10.1109/TPAMI.2019.2920636; Zanca D, 2017, ADV NEUR IN, V30; Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7	68	4	4	8	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4378	4395		10.1109/TPAMI.2020.3002168	http://dx.doi.org/10.1109/TPAMI.2020.3002168			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750785				2022-12-18	WOS:000714203900017
J	Bargal, SA; Zunino, A; Petsiuk, V; Zhang, JM; Saenko, K; Murino, V; Sclaroff, S				Bargal, Sarah Adel; Zunino, Andrea; Petsiuk, Vitali; Zhang, Jianming; Saenko, Kate; Murino, Vittorio; Sclaroff, Stan			Guided Zoom: Zooming into Network Evidence to Refine Fine-Grained Model Decisions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Explainable AI; grounding; saliency; fine-grained image classification; classification refinement; convolutional neural networks		In state-of-the-art deep single-label classification models, the top-k (k = 2,3,4,....) accuracy is usually significantly higher than the top-1 accuracy. This is more evident in fine-grained datasets, where differences between classes are quite subtle. Exploiting the information provided in the top k predicted classes boosts the final prediction of a model. We propose Guided Zoom, a novel way in which explainabitity could be used to improve model performance. We do so by making sure the model has "the right reasons" fora prediction. The reason/evidence upon which a deep neural network makes a prediction is defined to be the grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable the evidence used to make each of the top-k predictions is. Test time evidence is deemed reasonable if it is coherent with evidence used to make similar correct decisions at training time. This leads to better informed predictions. We explore a variety of grounding techniques and study their complementarity for computing evidence. We show that Guided Zoom results in an improvement of a model's classification accuracy and achieves state-of-the-art classification performance on four fine-grained classification datasets.	[Bargal, Sarah Adel; Petsiuk, Vitali; Saenko, Kate; Sclaroff, Stan] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA; [Zunino, Andrea] PAVIS, Ist Italiano Tecnol, Genoa, Italy; [Zunino, Andrea; Murino, Vittorio] Huawei Technol Co Ltd, Ireland Res Ctr, Dublin D02 R156, Ireland; [Zhang, Jianming] Adobe Res, San Jose, CA 95110 USA; [Murino, Vittorio] Ist Italiano Tecnol, Dept Pattern Anal & Comp Vis, I-16163 Genoa, Italy; [Murino, Vittorio] Univ Verona, Dept Comp Sci, I-37134 Verona, Italy	Boston University; Istituto Italiano di Tecnologia - IIT; Huawei Technologies; Adobe Systems Inc.; Istituto Italiano di Tecnologia - IIT; University of Verona	Bargal, SA (corresponding author), Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.	sbargal@bu.edu; andreazuna@hotmail.it; vpetsiuk@bu.edu; jianmzha@adobe.com; saenko@bu.edu; vittorio.murino@iit.it; sclaroff@bu.edu	; Zhang, Jianming/B-1665-2017	Petsiuk, Vitali/0000-0002-9565-4511; Saenko, Kate/0000-0002-7564-7218; Murino, Vittorio/0000-0002-8645-2328; Zhang, Jianming/0000-0002-9954-6294; Bargal, Sarah/0000-0003-3157-0412; Sclaroff, Stanley/0000-0002-0711-4313	Defense Advanced Research Projects Agency (DARPA) Explainable Artificial Intelligence (XAI) program; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) [D17PC00341]	Defense Advanced Research Projects Agency (DARPA) Explainable Artificial Intelligence (XAI) program(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC)	This work was supported in part by Defense Advanced Research Projects Agency (DARPA) Explainable Artificial Intelligence (XAI) program and Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00341. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, IARPA, DOI/IBC, or the U.S. Government. Sarah Adel Bargal and Andrea Zunino contributed equally to this work.	Bargal S. A., 2019, P BRIT MACH VIS C; Bargal SA, 2018, PROC CVPR IEEE, P1440, DOI 10.1109/CVPR.2018.00156; Branson S., 2014, BMVC; Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338; Chen CF, 2019, ADV NEUR IN, V32; Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132; Jaderberg M, 2015, ADV NEUR IN, V28; Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81; Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19; Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Li F.-F., 2011, PROC CVPR WORKSHOP F, V2; Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Maji S., 2013, TECH REP; Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684; Petsiuk Vitali, 2018, BRIT MACH VIS C 2018, P1; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Simon M, 2017, IEEE I CONF COMP VIS, P4970, DOI 10.1109/ICCV.2017.531; Simonyan K., 2014, WORKSH INT C LEARN R, P1; Springenberg J. T, 2015, ARXIV PREPRINT ARXIV; Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276; Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131; Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687; Welinder P., 2010, CNSTR2010001 CALTECH; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129; Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144; Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128; Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498; Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399; Zunino A, 2021, INT J COMPUT VISION, V129, P1139, DOI 10.1007/s11263-020-01422-y	43	4	4	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					4196	4202		10.1109/TPAMI.2020.3054303	http://dx.doi.org/10.1109/TPAMI.2020.3054303			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	33493111				2022-12-18	WOS:000702649700035
J	Chakraborty, R; Yang, L; Hauberg, S; Vemuri, BC				Chakraborty, Rudrasis; Yang, Liu; Hauberg, Soren; Vemuri, Baba C.			Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear Subspace Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Online subspace learning; robust; PCA; kernel PCA; grassmann manifold; frechet mean; frechet median	PRINCIPAL COMPONENT ANALYSIS; CENTER-OF-MASS; SPARSE; PCA; ALGORITHMS; UNIQUENESS; TRACKING; IMAGE	Principal component analysis (PCA) and Kernel principal component analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both (finite and infinite) situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by K-tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.	[Chakraborty, Rudrasis] Univ Florida, Gainesville, FL 32611 USA; [Vemuri, Baba C.] Univ Florida, Dept Comp Informat Sci & Engn, Engn, Gainesville, FL 32611 USA; [Chakraborty, Rudrasis; Vemuri, Baba C.] Univ Florida, Dept Stat, Gainesville, FL 32611 USA; [Yang, Liu] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Hauberg, Soren] Tech Univ Denmark, Sect Cognit Syst, DK-2800 Lyngby, Denmark	State University System of Florida; University of Florida; State University System of Florida; University of Florida; State University System of Florida; University of Florida; University of California System; University of California Berkeley; Technical University of Denmark	Vemuri, BC (corresponding author), Univ Florida, Dept Comp Informat Sci & Engn, Engn, Gainesville, FL 32611 USA.; Vemuri, BC (corresponding author), Univ Florida, Dept Stat, Gainesville, FL 32611 USA.	rudrasischa@gmail.com; liu-yang@berkeley.edu; sohau@dtu.dk; vemuri@ufl.edu	; Hauberg, Soren/L-2104-2016	Yang, Liu/0000-0002-3923-6593; Hauberg, Soren/0000-0001-7223-877X; Vemuri, Baba/0000-0002-1400-5844	NSF [IIS-1525431, IIS-1724174]; VILLUM FONDEN [15334]; European Research Council (ERC) under the European Union [757360]	NSF(National Science Foundation (NSF)); VILLUM FONDEN(Villum Fonden); European Research Council (ERC) under the European Union(European Research Council (ERC))	The authors would like to thank Chun-Hao Yang for several helpful suggestions. This work was supported in part by the NSF grants IIS-1525431 and IIS-1724174 to BCV. The work of S. Hauberg was supported by the research grant (15334) from VILLUM FONDEN. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no. 757360).	Abramowitz M., 1966, APPL MATH SERIES, V55, P62; Afsari B, 2011, P AM MATH SOC, V139, P655, DOI 10.1090/S0002-9939-2010-10541-5; Allen-Zhu Z., 2017, ANN IEEE SYMP FOUND, P487, DOI DOI 10.1109/FOCS.2017.51; Balzano L., 2010, P704; Balzano L, 2018, P IEEE, V106, P1293, DOI 10.1109/JPROC.2018.2847041; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Boutsidis C., 2015, P 26 ANN ACM SIAM S, P887; Boyd S, 2004, CONVEX OPTIMIZATION; Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707; BUNCH JR, 1978, NUMER MATH, V31, P111, DOI 10.1007/BF01397471; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Cappe O, 2011, MIXTURES ESTIMATION, V1, P31; Chakraborty R., 2015, IEEE I CONF COMP VIS, P4229, DOI DOI 10.1109/ICCV.2015.481; Chakraborty R., 2017, PROC CVPR IEEE, P801, DOI DOI 10.1109/CVPR.2017.92; Chi YJ, 2013, IEEE T SIGNAL PROCES, V61, P5947, DOI 10.1109/TSP.2013.2282910; Feng J., 2013, ADV NEURAL INFORM PR, P404; Feng J., 2012, P 29 INT C MACH LEAR, V1, P249; Fletcher PT, 2009, NEUROIMAGE, V45, pS143, DOI 10.1016/j.neuroimage.2008.10.052; Frechet M., 1948, ANN I H POINCARE, V10, P215; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Ghashami M, 2016, JMLR WORKSH CONF PRO, V51, P1365; Gunter S, 2007, J MACH LEARN RES, V8, P1893; Ha W., 2015, P ADV NEUR INF PROC, P1936; Hauberg S, 2016, IEEE T PATTERN ANAL, V38, P2298, DOI 10.1109/TPAMI.2015.2511743; He J., 2012, PROC CVPR IEEE, P1568; Honeine P, 2012, IEEE T PATTERN ANAL, V34, P1814, DOI 10.1109/TPAMI.2011.270; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Huber P. J., 2011, ROBUST STAT; Jiang W., 2015, P 24 INT JOINT C, P3590; Jolliffe IT, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0202; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; KENDALL WS, 1990, P LOND MATH SOC, V61, P371; Kennedy R., 2014, 2014 IEEE GLOB C, P507; Kennedy R, 2016, COMPUT VIS IMAGE UND, V150, P139, DOI 10.1016/j.cviu.2016.04.011; Kim KI, 2005, IEEE T PATTERN ANAL, V27, P1351, DOI 10.1109/TPAMI.2005.181; Lin Z., 2011, ADV NEURAL INF PROCE, P612; Lois B., 2015, IEEE INT SYMP INFO, P1826; Mahadevan V., 2010, PROC CVPR IEEE, V249; Nakagami M., 1960, STAT METHODS RADIO W, P3; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Qiu CL, 2014, IEEE T INFORM THEORY, V60, P5007, DOI 10.1109/TIT.2014.2331344; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Roman S., 2005, UMBRAL CALCULUS; Roweis S, 1998, ADV NEUR IN, V10, P626; Rudin W., 2017, FOURIER ANAL GROUPS; Sch_olkopf B., 1997, ARTIFICIAL NEURAL NE, P583; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; WONG YC, 1968, P NATL ACAD SCI USA, V60, P75, DOI 10.1073/pnas.60.1.75; Xu X., 2014, THESIS U ILLINOIS; YANG B, 1995, IEEE T SIGNAL PROCES, V43, P95, DOI 10.1109/78.365290; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	57	4	4	11	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					3904	3917		10.1109/TPAMI.2020.2992392	http://dx.doi.org/10.1109/TPAMI.2020.2992392			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32386140	Green Submitted			2022-12-18	WOS:000702649700016
J	Sun, MJ; Xiao, JM; Lim, EG; Liu, S; Goulermas, JY				Sun, Mingjie; Xiao, Jimin; Lim, Eng Gee; Liu, Si; Goulermas, John Y.			Discriminative Triad Matching and Reconstruction for Weakly Referring Expression Grounding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Training; Proposals; Visualization; Task analysis; Linguistics; Grounding; Referring expression grounding; weakly supervised training; discriminative triad matching		In this paper, we are tackling the weakly-supervised referring expression grounding task, for the localization of a referent object in an image according to a query sentence, where the mapping between image regions and queries are not available during the training stage. In traditional methods, an object region that best matches the referring expression is picked out, and then the query sentence is reconstructed from the selected region, where the reconstruction difference serves as the loss for back-propagation. The existing methods, however, conduct both the matching and the reconstruction approximately as they ignore the fact that the matching correctness is unknown. To overcome this limitation, a discriminative triad is designed here as the basis to the solution, through which a query can be converted into one or multiple discriminative triads in a very scalable way. Based on the discriminative triad, we further propose the triad-level matching and reconstruction modules which are lightweight yet effective for the weakly-supervised training, making it three times lighter and faster than the previous state-of-the-art methods. One important merit of our work is its superior performance despite the simple and neat design. Specifically, the proposed method achieves a new state-of-the-art accuracy when evaluated on RefCOCO (39.21 percent), RefCOCO+ (39.18 percent) and RefCOCOg (43.24 percent) datasets, that is 4.17, 4.08 and 7.8 percent higher than the previous one, respectively. The code is available at https://github.com/insomnia94/DTWREG.	[Sun, Mingjie] Univ Liverpool, Liverpool L69 3BX, Merseyside, England; [Sun, Mingjie; Xiao, Jimin; Lim, Eng Gee] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215123, Peoples R China; [Liu, Si] Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China; [Goulermas, John Y.] Univ Liverpool, Dept Comp Sci, Liverpool L69 3BX, Merseyside, England	University of Liverpool; Xi'an Jiaotong-Liverpool University; Beihang University; University of Liverpool	Xiao, JM (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215123, Peoples R China.	mingjie.sun@liverpool.ac.uk; jimin.xiao@xjtlu.edu.cn; enggee.lim@xjtlu.edu.cn; liusi@buaa.edu.cn; goulerma@liverpool.ac.uk	SUN, MINGJIE/GQQ-0374-2022	lim, enggee/0000-0003-0199-7386; SUN, MINGJIE/0000-0002-3697-7927; liu, si/0000-0002-9180-2935	National Natural Science Foundation of China [61972323]; Key Program Special Fund in XJTLU [KSF-T-02, KSF-P-02]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Program Special Fund in XJTLU	The work was supported by National Natural Science Foundation of China under Grant 61972323, and Key Program Special Fund in XJTLU under Grant KSF-T-02, KSF-P-02.	Chen DD, 2014, J ANAL METHODS CHEM, V2014, DOI 10.1155/2014/575246; Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425; De Marneffe M.-C., 2006, P LREC, V6, P449; Ge HW, 2019, IEEE I CONF COMP VIS, P1754, DOI 10.1109/ICCV.2019.00184; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066; Honnibal M., 2017, APPEAR; Jang E., 2017, ICLR; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787, DOI DOI 10.3115/V1/D14-1086; Kingma Diederik P., 2015, 3 INT C LEARN REPRES, V3; KUC R, 1987, IEEE T PATTERN ANAL, V9, P766, DOI 10.1109/TPAMI.1987.4767983; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205; Liu XJ, 2019, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2019.00270; Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074; Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Mikolov T., 2013, 1 INT LEARN REPR ICL; Pennington J., 2014, P 2014 C EMPIRICAL M, P1532; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49; Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206; Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246; Xiong Wenhan, 2020, INT C LEARN REPR; Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Zhukov D, 2019, PROC CVPR IEEE, P3532, DOI 10.1109/CVPR.2019.00365; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	30	4	4	1	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					4189	4195		10.1109/TPAMI.2021.3058684	http://dx.doi.org/10.1109/TPAMI.2021.3058684			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	33571088	Green Submitted			2022-12-18	WOS:000702649700034
J	Komeili, M; Armanfard, N; Hatzinakos, D				Komeili, Majid; Armanfard, Narges; Hatzinakos, Dimitrios			Multiview Feature Selection for Single-View Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Training; Dimensionality reduction; Correlation; Error analysis; Biomedical imaging; Feature selection; multiview; feature weighting; multiview training single view test; classification	MULTICLASS CLASSIFICATION; FRAMEWORK; INFORMATION; ALGORITHMS	In many real-world scenarios, data from multiple modalities (sources) are collected during a development phase. Such data are referred to as multiview data. While additional information from multiple views often improves the performance, collecting data from such additional views during the testing phase may not be desired due to the high costs associated with measuring such views or, unavailability of such additional views. Therefore, in many applications, despite having a multiview training data set, it is desired to do performance testing using data from only one view. In this paper, we present a multiview feature selection method that leverages the knowledge of all views and use it to guide the feature selection process in an individual view. We realize this via a multiview feature weighting scheme such that the local margins of samples in each view are maximized and similarities of samples to some reference points in different views are preserved. Also, the proposed formulation can be used for cross-view matching when the view-specific feature weights are pre-computed on an auxiliary data set. Promising results have been achieved on nine real-world data sets as well as three biometric recognition applications. On average, the proposed feature selection method has improved the classification error rate by 31 percent of the error rate of the state-of-the-art.	[Komeili, Majid] Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada; [Armanfard, Narges] McGill Univ, Dept Elect & Comp Engn, Montreal, PQ H3A 0G4, Canada; [Hatzinakos, Dimitrios] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 2E4, Canada	Carleton University; McGill University; University of Toronto	Komeili, M (corresponding author), Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.	majid.komeili@carleton.ca; armanfn@mcmaster.ca; dimitris@comm.utoronto.ca			Natural Sciences and Engineering Research Council of Canada (NSERC)	Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	This research was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC).	Abrahamsen T. J., 2013, KERNEL METHODS MACHI; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Alpaydin E, 2014, ADAPT COMPUT MACH LE, P1; Armanfard N, 2018, IEEE T NEUR NET LEAR, V29, P1396, DOI 10.1109/TNNLS.2017.2676101; Armanfard N, 2016, IEEE T PATTERN ANAL, V38, P1217, DOI 10.1109/TPAMI.2015.2478471; Banerjee M, 2015, IEEE T KNOWL DATA EN, V27, P3390, DOI 10.1109/TKDE.2015.2455509; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Brown G, 2012, J MACH LEARN RES, V13, P27; Chen B, 2009, IEEE T KNOWL DATA EN, V21, P1475, DOI 10.1109/TKDE.2008.238; Cheng QA, 2011, IEEE T PATTERN ANAL, V33, P1217, DOI 10.1109/TPAMI.2010.195; EL-Manzalawy Y, 2018, BMC MED GENOMICS, V11, DOI 10.1186/s12920-018-0388-0; Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634; Feng Y, 2012, ASIAN C COMPUTER VIS, P343, DOI DOI 10.1007/978-3-642-37331-226; Gilad-Bachrach R., 2004, P 21 INT C MACHINE L, P43, DOI DOI 10.1145/1015330.1015352; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Hernandez JCH, 2007, LECT NOTES COMPUT SC, V4447, P90; Hewett R, 2008, BMC GENOMICS, V9, DOI 10.1186/1471-2164-9-S2-S21; Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642; Hu RY, 2017, NEUROCOMPUTING, V220, P130, DOI 10.1016/j.neucom.2016.05.081; Jakulin A, 2005, THESIS U LJUBLJANA S; John G., DNA DATASET STATLOG; Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; KIRA K, 1992, MACHINE LEARNING /, P249; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Li SZ, 2009, PROC CVPR IEEE, P267, DOI 10.1109/FSKD.2009.137; Lin DH, 2006, LECT NOTES COMPUT SC, V3951, P68; Lin WY, 2018, PROC CVPR IEEE, P5784, DOI 10.1109/CVPR.2018.00606; Liu B, 2013, PATTERN RECOGN, V46, P2798, DOI 10.1016/j.patcog.2013.02.012; Liu HF, 2016, IEEE DATA MINING, P281, DOI [10.1109/ICDM.2016.37, 10.1109/ICDM.2016.0039]; Liu ZQ, 2011, BIOINFORMATICS, V27, P3242, DOI 10.1093/bioinformatics/btr547; Luo MN, 2018, IEEE T NEUR NET LEAR, V29, P944, DOI 10.1109/TNNLS.2017.2650978; Ma Y., 2007, P 24 INT C MACHINE L, P577, DOI DOI 10.1145/1273496.1273569; Meyer PE, 2006, LECT NOTES COMPUT SC, V3907, P91; Nag K, 2016, IEEE T CYBERNETICS, V46, P499, DOI 10.1109/TCYB.2015.2404806; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Nie FP, 2016, AAAI CONF ARTIF INTE, P1302; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Qian M., 2014, P 23 ACM INT C C INF, P1963; Shao WX, 2016, IEEE DATA MINING, P1203, DOI [10.1109/ICDM.2016.134, 10.1109/ICDM.2016.0160]; Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923; Stiglic G, 2010, J BIOMED BIOTECHNOL, DOI 10.1155/2010/616358; Sun SL, 2016, IEEE T CYBERNETICS, V46, P3272, DOI 10.1109/TCYB.2015.2502248; Sun YJ, 2007, IEEE T PATTERN ANAL, V29, P1035, DOI 10.1109/TPAMI.2007.1093; Sun YJ, 2010, IEEE T PATTERN ANAL, V32, P1610, DOI 10.1109/TPAMI.2009.190; Tang J, 2013, SIAM INT C DATA MINI, P270, DOI DOI 10.1137/1.9781611972832.30; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; Wang HT, 2004, IEEE IMAGE PROC, P1397; Wang L, 2008, IEEE T PATTERN ANAL, V30, P1534, DOI 10.1109/TPAMI.2007.70799; Wang WR, 2015, PR MACH LEARN RES, V37, P1083; Wang YX, 2005, LANCET, V365, P671, DOI 10.1016/S0140-6736(05)17947-1; Wangila KW, 2017, IEEE IMAGE PROC, P1930; Yang HH, 2000, ADV NEUR IN, V12, P687; Yang WQ, 2015, IEEE T NEUR NET LEAR, V26, P2801, DOI 10.1109/TNNLS.2015.2396937; Zhu PF, 2016, AAAI CONF ARTIF INTE, P2422; Zhu PF, 2017, PATTERN RECOGN, V66, P364, DOI 10.1016/j.patcog.2017.01.016; Zhu XF, 2018, IEEE T KNOWL DATA EN, V30, P517, DOI 10.1109/TKDE.2017.2763618	60	4	4	2	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2021	43	10					3573	3586		10.1109/TPAMI.2020.2987013	http://dx.doi.org/10.1109/TPAMI.2020.2987013			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UK8RG	32305902				2022-12-18	WOS:000692232400023
J	Yang, S; Wang, WJ; Liu, JY				Yang, Shuai; Wang, Wenjing; Liu, Jiaying			TE141K: Artistic Text Benchmark for Text Effect Transfer	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Benchmark testing; Feature extraction; Training; Image color analysis; Data models; Rendering (computer graphics); Decoding; Text effects; style transfer; deep neural network; large-scale dataset; model benchmarking		Text effects are combinations of visual elements such as outlines, colors and textures of text, which can dramatically improve its artistry. Although text effects are extensively utilized in the design industry, they are usually created by human experts due to their extreme complexity; this is laborious and not practical for normal users. In recent years, some efforts have been made toward automatic text effect transfer; however, the lack of data limits the capabilities of transfer models. To address this problem, we introduce a new text effects dataset, TE141K,(1) 1. Project page: https://daooshee.github.io/TE141K/. with 141,081 text effect/glyph pairs in total. Our dataset consists of 152 professionally designed text effects rendered on glyphs, including English letters, Chinese characters, and Arabic numerals. To the best of our knowledge, this is the largest dataset for text effect transfer to date. Based on this dataset, we propose a baseline approach called text effect transfer GAN (TET-GAN), which supports the transfer of all 152 styles in one model and can efficiently extend to new styles. Finally, we conduct a comprehensive comparison in which 14 style transfer models are benchmarked. Experimental results demonstrate the superiority of TET-GAN both qualitatively and quantitatively and indicate that our dataset is effective and challenging.	[Yang, Shuai; Wang, Wenjing; Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China	Peking University	Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.	williamyang@pku.edu.cn; daooshee@pku.edu.cn; liujiaying@pku.edu.cn			National Natural Science Foundation of China [61772043]; Beijing Natural Science Foundation [L182002]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work was supported in part by the National Natural Science Foundation of China under contract No.61772043, in part by Beijing Natural Science Foundation under contract No.L182002. Shuai Yang and Wenjing Wang contributed equally to this work.	Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789; Champandard Alex J., 2016, SEMANTIC STYLE TRANS, P2; Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296; Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Dumoulin V., 2017, P INT C LEARN REPR T; Gatys LA, 2015, ADV NEUR IN, V28; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230; Li YJ, 2017, ADV NEUR IN, V30; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703; Simonyan K, 2015, 3 INT C LEARN REPR I; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; Ulyanov D, 2016, PR MACH LEARN RES, V48; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759; Yang S, 2019, AAAI CONF ARTIF INTE, P1238; Yang S, 2019, IEEE T IMAGE PROCESS, V28, P952, DOI 10.1109/TIP.2018.2873064; Yang S, 2017, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR.2017.308; Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32; Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201285; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	32	4	4	1	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2021	43	10					3709	3723		10.1109/TPAMI.2020.2983697	http://dx.doi.org/10.1109/TPAMI.2020.2983697			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UK8RG	32248093	Green Submitted			2022-12-18	WOS:000692232400032
J	Ye, QL; Amini, AA; Zhou, Q				Ye, Qiaoling; Amini, Arash A.; Zhou, Qing			Optimizing Regularized Cholesky Score for Order-Based Learning of Bayesian Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bayes methods; Simulated annealing; Tuning; Directed acyclic graph; Annealing; Genetic algorithms; Bayesian networks; proximal gradient; regularized likelihood; simulated annealing; sparse Cholesky factorization; structure learning; topological sorts	COVARIANCE ESTIMATION; PENALIZED ESTIMATION; VARIABLE SELECTION; ALGORITHMS; INFERENCE	Bayesian networks are a class of popular graphical models that encode causal and conditional independence relations among variables by directed acyclic graphs (DAGs). We propose a novel structure learning method, annealing on regularized Cholesky score (ARCS), to search over topological sorts, or permutations of nodes, for a high-scoring Bayesian network. Our scoring function is derived from regularizing Gaussian DAG likelihood, and its optimization gives an alternative formulation of the sparse Cholesky factorization problem from a statistical viewpoint. We combine simulated annealing over permutation space with a fast proximal gradient algorithm, operating on triangular matrices of edge coefficients, to compute the score of any permutation. Combined, the two approaches allow us to quickly and effectively search over the space of DAGs without the need to verify the acyclicity constraint or to enumerate possible parent sets given a candidate topological sort. The annealing aspect of the optimization is able to consistently improve the accuracy of DAGs learned by greedy and deterministic search algorithms. In addition, we develop several techniques to facilitate the structure learning, including pre-annealing data-driven tuning parameter selection and post-annealing constraint-based structure refinement. Through extensive numerical comparisons, we show that ARCS outperformed existing methods by a substantial margin, demonstrating its great advantage in structure learning of Bayesian networks from both observational and experimental data. We also establish the consistency of our scoring function in estimating topological sorts and DAG structures in the large-sample limit. Source code of ARCS is available at https://github.com/yeqiaoling/arcs_bn.	[Ye, Qiaoling; Amini, Arash A.; Zhou, Qing] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Zhou, Q (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.	yeqiaoling@g.ucla.edu; aaamini@stat.ucla.edu; zhou@stat.ucla.edu		Ye, Qiaoling/0000-0003-0737-9001	NSF [IIS-1546098]	NSF(National Science Foundation (NSF))	This work was supported by the NSF Grant IIS-1546098.	Alonso-Barba JI, 2011, SOFT COMPUT, V15, P1881, DOI 10.1007/s00500-010-0623-x; Amestoy PR, 1996, SIAM J MATRIX ANAL A, V17, P886, DOI 10.1137/S0895479894278952; Aragam B, 2019, J STAT SOFTW, V91, P1, DOI 10.18637/jss.v091.i11; Aragam B, 2015, J MACH LEARN RES, V16, P2273; Bartlett M., 2013, UNCERTAINTY ARTIFICI, P182; Champion M, 2018, STAT COMPUT, V28, P905, DOI 10.1007/s11222-017-9769-z; Chen Z., STAT SINICA, V15, P1249; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; Cussens J, 2017, J ARTIF INTELL RES, V58, P185, DOI 10.1613/jair.5203; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Ellis B, 2008, J AM STAT ASSOC, V103, P778, DOI 10.1198/016214508000000193; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Friedman N, 2003, MACH LEARN, V50, P95, DOI 10.1023/A:1020249912095; Fu F, 2013, J AM STAT ASSOC, V108, P288, DOI 10.1080/01621459.2012.754359; Gamez JA, 2011, DATA MIN KNOWL DISC, V22, P106, DOI 10.1007/s10618-010-0178-6; GHOSHAL A, 2018, P MACHINE LEARNING R, V84, P1466; Gu JY, 2019, STAT COMPUT, V29, P161, DOI 10.1007/s11222-018-9801-y; Hauser A, 2012, J MACH LEARN RES, V13, P2409; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Larranaga P, 1996, IEEE T PATTERN ANAL, V18, P912, DOI 10.1109/34.537345; Lee C, 2017, LECT NOTES ARTIF INT, V10233, P129, DOI 10.1007/978-3-319-57351-9_17; Lee K., 2019, STAT SINICA; Li FQ, 2019, ACTA MATH SIN, V35, P619, DOI 10.1007/s10114-019-7326-8; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.1093/biomet/82.4.669; Pourahmadi M, 2011, STAT SCI, V26, P369, DOI 10.1214/11-STS358; Ramsey J. D., 2019, SCALING GREEDY CAUSA; ROBINS J, 1986, MATH MODELLING, V7, P1393, DOI 10.1016/0270-0255(86)90088-6; Scanagatta M., 2015, ADV NEURAL INFORM PR, P1864; Scanagatta M., 2017, P MACH LEARN RES, P45; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Scutari M., 2007, BAYESIAN NETWORK REP; Scutari M, 2010, J STAT SOFTW, V35, P1, DOI 10.18637/jss.v035.i03; Silander T., 2006, P 22 C UNC ART INT, P445; Spirtes P., 1991, Social Science Computer Review, V9, P62, DOI 10.1177/089443939100900106; Spirtes P., 2000, CAUSATION PREDICTION; Suzuki J., 1993, Uncertainty in Artificial Intelligence. Proceedings of the Ninth Conference (1993), P266; Teyssier M., 2005, P 21 C UNC ART INT, P584; Touchette S, 2016, J INTELL ROBOT SYST, V84, P859, DOI 10.1007/s10846-016-0367-7; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Vandenberghe L., 2014, FDN TRENDS OPTIM, V1, P241; Verzelen N, 2010, ELECTRON J STAT, V4, P1113, DOI 10.1214/10-EJS580; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zheng X., 2018, NEURIPS, P9492; Zhou Q, 2011, J AM STAT ASSOC, V106, P1317, DOI 10.1198/jasa.2011.ap10346	48	4	5	5	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2021	43	10					3555	3572		10.1109/TPAMI.2020.2990820	http://dx.doi.org/10.1109/TPAMI.2020.2990820			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UK8RG	32340938	Green Submitted			2022-12-18	WOS:000692232400022
J	Fang, JM; Sun, YZ; Zhang, Q; Peng, KJ; Li, Y; Liu, WY; Wang, XG				Fang, Jiemin; Sun, Yuzhu; Zhang, Qian; Peng, Kangjian; Li, Yuan; Liu, Wenyu; Wang, Xinggang			FNA plus plus : Fast Network Adaptation via Parameter Remapping and Architecture Search	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computer architecture; Task analysis; Object detection; Semantics; Image segmentation; Search problems; Pose estimation; Fast network adaptation; parameter remapping; neural architecture search		Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Network Adaptation (FNA++) method, which can adapt both the architecture and parameters of a seed network (e.g., an ImageNet pre-trained network) to become a network with different depths, widths, or kernel sizes via a parameter remapping technique, making it possible to use NAS for segmentation and detection tasks a lot more efficiently. In our experiments, we apply FNA++ on MobileNetV2 to obtain new networks for semantic segmentation, object detection, and human pose estimation that clearly outperform existing networks designed both manually and by NAS. We also implement FNA++ on ResNets and NAS networks, which demonstrates a great generalization ability. The total computation cost of FNA++ is significantly less than SOTA segmentation and detection NAS approaches: 1737x less than DPC, 6.8x less than Auto-DeepLab, and 8.0x less than DetNAS. A series of ablation studies are performed to demonstrate the effectiveness, and detailed analysis is provided for more insights into the working mechanism. Codes are available at https://github.com/JaminFong/FNA.	[Fang, Jiemin] Huazhong Univ Sci & Technol, Inst Artificial Intelligence, Wuhan 430074, Peoples R China; [Fang, Jiemin; Sun, Yuzhu; Liu, Wenyu; Wang, Xinggang] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China; [Zhang, Qian; Peng, Kangjian] Horizon Robot, Beijing, Peoples R China; [Li, Yuan] Google, Mountain View, CA 94043 USA	Huazhong University of Science & Technology; Huazhong University of Science & Technology; Google Incorporated	Wang, XG (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.	jaminfong@hust.edu.cn; yzsun@hust.edu.cn; qian01.zhang@horizon.ai; pengkangjian@gmail.com; liyu@google.com; liuwy@hust.edu.cn; xgwang@hust.edu.cn	Liu, Wenyu/AAG-1426-2019	Liu, Wenyu/0000-0002-4582-7488; Fang, Jiemin/0000-0002-0322-4582	NSFC [61876212, 61733007]; HUST-Horizon Computer Vision Research Center; Zhejiang Lab [2019NB0AB02]	NSFC(National Natural Science Foundation of China (NSFC)); HUST-Horizon Computer Vision Research Center; Zhejiang Lab	This work was supported in part by NSFC (No. 61876212 and No. 61733007), Zhejiang Lab (No. 2019NB0AB02), and HUST-Horizon Computer Vision Research Center. The authors would like to thank Liangchen Song, Wenqiang Zhang, Yingqing Rao and Jiapei Feng for the discussion and assistance. Jiemin Fang and Yuzhu Sun contributed equally to this work.	Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bender G, 2018, PR MACH LEARN RES, V80; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Brock Andrew, 2017, ARXIV170805344; Cai H., 2020, ICLR, P1; Cai H, 2018, PR MACH LEARN RES, V80; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai Han, 2019, INT C LEARN REPR; Chen K., ARXIV190607155, V2019; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen Liang-Chieh, 2017, 170605587 ARXIV; Chen Shoufa, 2019, ARXIV191204749; Chen T., 2016, P INT C LEARN REPR P INT C LEARN REPR; Chen Y., 2019, P 33 INT C NEUR INF P 33 INT C NEUR INF; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Elsken Thomas, 2019, INT C LEARN REPR; Fang J., 2019, P ICLR VANC BC CAN; Fang J, 2020, P INT C LEARN REPR P INT C LEARN REPR; Greff K., 2017, P INT C LEARN REPR P INT C LEARN REPR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Kaiming, 2018, ARXIV181108883; Howard A.G, 2017, ARXIV170404861; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jiemin Fang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10625, DOI 10.1109/CVPR42600.2020.01064; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li L, 2020, PR MACH LEARN RES, V115, P367; Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI 10.1007/978-3-030-01240-3_21; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu Hanxiao, 2017, INT C LEARN REPR ICL; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Liu Zili, 2019, ARXIV190900700; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Pham H, 2018, PR MACH LEARN RES, V80; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Stamoulis D., 2019, ARXIV PREPRINT ARXIV; Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tan M., 2018, CORR; Wang J., 2019, TPAMI; Wang R. J, 2018, NIPS; Wilber M, 2016, P ADV NEUR INF PROC, V30, P550; Wu Bichen, 2018, ARXIV181203443; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Yang A., 2020, PROC INT C LEARN REP; Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Yu Kaicheng, 2020, ICLR; Zhang HY, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277958; Zoph B, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	67	4	4	3	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					2990	3004		10.1109/TPAMI.2020.3044416	http://dx.doi.org/10.1109/TPAMI.2020.3044416			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	33315553	Green Submitted			2022-12-18	WOS:000681124300012
J	Mohr, F; Wever, M; Tornede, A; Hullermeier, E				Mohr, Felix; Wever, Marcel; Tornede, Alexander; Huellermeier, Eyke			Predicting Machine Learning Pipeline Runtimes in the Context of Automated Machine Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pipelines; Runtime; Prediction algorithms; Predictive models; Machine learning; Tools; Machine learning algorithms; Automated machine learning; runtime prediction for classifiers and pipelines; hierarchical runtime prediction		Automated machine learning (AutoML) seeks to automatically find so-called machine learning pipelines that maximize the prediction performance when being used to train a model on a given dataset. One of the main and yet open challenges in AutoMLis an effective use of computational resources: An AutoML process involves the evaluation of many candidate pipelines, which are costly but often ineffective because they are canceled due to a timeout. In this paper, we present an approach to predict the runtime of two-step machine learning pipelines with up to one pre-processor, which can be used to anticipate whether or not a pipeline will time out. Separate runtime models are trained offline for each algorithm that may be used in a pipeline, and an overall prediction is derived from these models. We empirically show that the approach increases successful evaluations made by an AutoML tool while preserving or even improving on the previously best solutions.	[Mohr, Felix] Univ La Sabana, Dept Comp Sci, Chia 250001, Cundinamarca, Colombia; [Wever, Marcel; Huellermeier, Eyke] Paderborn Univ, Dept Comp Sci, D-33098 Paderborn, Germany; [Tornede, Alexander] Paderborn Univ, Heinz Nixdorf Inst, D-33098 Paderborn, Germany	Universidad de La Sabana; University of Paderborn; University of Paderborn	Mohr, F (corresponding author), Univ La Sabana, Dept Comp Sci, Chia 250001, Cundinamarca, Colombia.	fmohr@mail.upb.de; marcel.wever@upb.de; alexander.tornede@upb.de; eyke@upb.de		Tornede, Alexander/0000-0002-2415-2186	German Research Foundation (DFG) within the Collaborative Research Center "On-The-Fly Computing" [SFB 901/3, 160364472]; Paderborn Center for Parallel Computing (PC2)	German Research Foundation (DFG) within the Collaborative Research Center "On-The-Fly Computing"(German Research Foundation (DFG)); Paderborn Center for Parallel Computing (PC2)	This work was supported in part by the German Research Foundation (DFG) within the Collaborative Research Center "On-The-Fly Computing" (SFB 901/3 Project no. 160364472). The authors gratefully acknowledge support by the Paderborn Center for Parallel Computing (PC2), which provided computational resources and computing time.	Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Chen BY, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P402, DOI 10.1145/3205455.3205586; de Sa AGC, 2017, LECT NOTES COMPUT SC, V10196, P246, DOI 10.1007/978-3-319-55696-3_16; Doan T, 2017, INT J MACH LEARN CYB, V8, P1929, DOI 10.1007/s13042-016-0571-6; Drori I., 2018, P INT WORKSH AUT MAC P INT WORKSH AUT MAC; Feurer Matthias, 2015, ADV NEURAL INFORM PR, P2962; Hall M., 2008, WEKA DATA MINING SOF, V11, P10, DOI [10.1145/1656274.1656278, DOI 10.1145/1656274.1656278]; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601; Hutter F, 2014, ARTIF INTELL, V206, P79, DOI 10.1016/j.artint.2013.10.003; Klein A, 2017, ELECTRON J STAT, V11, P4945, DOI 10.1214/17-EJS1335SI; Melnikov V., 2016, MACHINE LEARNING KNO, P756, DOI DOI 10.1007/978-3-319-46227-1_47; Mohr F, 2018, MACH LEARN, V107, P1495, DOI 10.1007/s10994-018-5735-z; Olson R.S., 2016, WORKSHOP AUTOMATIC M, V64, P66; Rakotoarison H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3296; Reif M, 2011, LECT NOTES ARTIF INT, V7006, P260, DOI 10.1007/978-3-642-24455-1_25; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Tornede Alexander, 2020, P MACHINE LEARNING R, V129, P737; Tornede T., 2020, IOT STREAMS DATA DRI, P106; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Wever M. D., 2018, P INT WORKSH AUT MAC P INT WORKSH AUT MAC; Yang CR, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1173, DOI 10.1145/3292500.3330909	22	4	4	3	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3055	3066		10.1109/TPAMI.2021.3056950	http://dx.doi.org/10.1109/TPAMI.2021.3056950			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	33539291				2022-12-18	WOS:000681124300016
J	Vo, M; Yumer, E; Sunkavalli, K; Hadap, S; Sheikh, Y; Narasimhan, SG				Vo, Minh; Yumer, Ersin; Sunkavalli, Kalyan; Hadap, Sunil; Sheikh, Yaser; Narasimhan, Srinivasa G.			Self-Supervised Multi-View Person Association and its Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Descriptor adaptation; self-supervised; people association; motion tracking; multi-angle video	MOTION CAPTURE; TRACKING; MULTITARGET	Reliable markerless motion tracking of people participating in a complex group activity from multiple moving cameras is challenging due to frequent occlusions, strong viewpoint and appearance variations, and asynchronous video streams. To solve this problem, reliable association of the same person across distant viewpoints and temporal instances is essential. We present a self-supervised framework to adapt a generic person appearance descriptor to the unlabeled videos by exploitingmotion tracking, mutual exclusion constraints, and multi-view geometry. The adapted discriminative descriptor is used in a tracking-by-clustering formulation. We validate the effectiveness of our descriptor learning on WILDTRACK T. Chavdarova et al., "WILDTRACK: Amulti-camera HD dataset for dense unscripted pedestrian detection," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 5030-5039. and three new complex social scenes captured bymultiple cameras with up to 60 people "in the wild". We report significant improvement in association accuracy (up to 18 percent) and stable and coherent 3D human skeleton tracking (5 to 10 times) over the baseline. Using the reconstructed 3D skeletons, we cut the input videos into a multi-angle videowhere the image of a specified person is shown fromthe best visible front-facing camera. Our algorithm detects inter-human occlusion to determine the camera switching moment while still maintaining the flow of the action well. Website: http://www.cs.cmu.edu/similar to ILIM/projects/IM/Association4Tracking	[Vo, Minh; Sheikh, Yaser; Narasimhan, Srinivasa G.] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Yumer, Ersin] Uber ATG, San Francisco, CA 94103 USA; [Sunkavalli, Kalyan] Adobe Res, San Jose, CA 95110 USA; [Hadap, Sunil] Amazon Lab 126, Sunnyvale, CA 94085 USA	Carnegie Mellon University; Adobe Systems Inc.	Vo, M (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	mpvo@cs.cmu.edu; yumer@uber.com; sunkaval@adobe.com; hadap@acm.org; yaser@cs.cmu.edu; srinivas@cs.cmu.edu			US National Science Foundation [CNS-1446601]; ONR [N00014-14-1-0595]; Heinz Endowments "Platform Pittsburgh"; Metro 21 grants; 2017 Qualcomm Innovation Fellowship	US National Science Foundation(National Science Foundation (NSF)); ONR(Office of Naval Research); Heinz Endowments "Platform Pittsburgh"; Metro 21 grants; 2017 Qualcomm Innovation Fellowship	This research was supported by US National Science Foundation CNS-1446601, ONR N00014-14-1-0595, Heinz Endowments "Platform Pittsburgh", Metro 21 grants, and an Adobe Research Gift. Minh Vo was partly supported by the 2017 Qualcomm Innovation Fellowship.	Agarwal S., CERES SOLVER; Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016; Alt N, 2010, PROC CVPR IEEE, P1355, DOI 10.1109/CVPR.2010.5539812; Arev I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601198; Assari SM, 2016, LECT NOTES COMPUT SC, V9906, P119, DOI 10.1007/978-3-319-46475-6_8; Baltieri D., 2011, P 2011 JOINT ACMWORK, P59, DOI DOI 10.1145/2072572.2072590; Baque P, 2017, IEEE I CONF COMP VIS, P271, DOI 10.1109/ICCV.2017.38; Barron JT, 2019, PROC CVPR IEEE, P4326, DOI 10.1109/CVPR.2019.00446; Ben Shitrit H, 2014, IEEE T PATTERN ANAL, V36, P1614, DOI 10.1109/TPAMI.2013.210; Ben Shitrit H, 2011, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2011.6126235; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Bialkowski A., 2012, DICTA; Brito MR, 1997, STAT PROBABIL LETT, V35, P33, DOI 10.1016/S0167-7152(96)00213-1; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chavdarova T, 2018, PROC CVPR IEEE, P5030, DOI 10.1109/CVPR.2018.00528; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Collins RT, 2012, PROC CVPR IEEE, P1744, DOI 10.1109/CVPR.2012.6247870; Das A, 2014, LECT NOTES COMPUT SC, V8690, P330, DOI 10.1007/978-3-319-10605-2_22; Elhayek A, 2017, IEEE T PATTERN ANAL, V39, P501, DOI 10.1109/TPAMI.2016.2557779; Elhayek A, 2012, PROC CVPR IEEE, P1870, DOI 10.1109/CVPR.2012.6247886; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174; Geng M., 2016, ARXIV161105244; Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21; Hermans Alexander, 2017, ARXIV, P1; Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Joo H, 2019, IEEE T PATTERN ANAL, V41, P190, DOI 10.1109/TPAMI.2017.2782743; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Kawanishi Y., 2014, P 20 KOR JAP JO WORK; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461; Liem MC, 2014, COMPUT VIS IMAGE UND, V128, P36, DOI 10.1016/j.cviu.2014.06.003; Liu YB, 2013, IEEE T PATTERN ANAL, V35, P2720, DOI 10.1109/TPAMI.2013.47; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190; Martinel N., 2012, IEEE COMPUTER SOC C, DOI DOI 10.1109/CVPRW.2012.6239203; McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148; Milan A, 2015, PROC CVPR IEEE, P5397, DOI 10.1109/CVPR.2015.7299178; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nguyen HT, 2004, IEEE T PATTERN ANAL, V26, P1099, DOI 10.1109/TPAMI.2004.45; Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794; Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139; Rhodin H, 2019, PROC CVPR IEEE, P7695, DOI 10.1109/CVPR.2019.00789; Rhodin H, 2015, IEEE I CONF COMP VIS, P765, DOI 10.1109/ICCV.2015.94; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Rozantsev A, 2017, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2017.266; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schwartz WR, 2009, SIBGRAPI, P322, DOI 10.1109/SIBGRAPI.2009.42; Shah SA, 2017, P NATL ACAD SCI USA, V114, P9814, DOI 10.1073/pnas.1700770114; Simi Wang, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1876, DOI 10.1109/ICCVW.2011.6130477; Stoll C, 2011, IEEE I CONF COMP VIS, P951, DOI 10.1109/ICCV.2011.6126338; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vo M, 2016, PROC CVPR IEEE, P1710, DOI 10.1109/CVPR.2016.189; Wu SH, 2019, INT J PAVEMENT ENG, V20, P33, DOI 10.1080/10298436.2016.1248204; Wu Z, 2012, PROC CVPR IEEE, P1948, DOI 10.1109/CVPR.2012.6247896; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Zhang L, 2008, INT C WAVEL ANAL PAT, P11, DOI 10.1109/ICWAPR.2008.4635742; Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52	77	4	4	1	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG 1	2021	43	8					2794	2808		10.1109/TPAMI.2020.2974726	http://dx.doi.org/10.1109/TPAMI.2020.2974726			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TF2YV	32086193	hybrid, Green Submitted			2022-12-18	WOS:000670578800020
J	Dundar, A; Liu, MY; Yu, ZD; Wang, TC; Zedlewski, J; Kautz, J				Dundar, Aysegul; Liu, Ming-Yu; Yu, Zhiding; Wang, Ting-Chun; Zedlewski, John; Kautz, Jan			Domain Stylization: A Fast Covariance Matching Framework Towards Domain Adaptation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Semantics; Training; Task analysis; Gallium nitride; Adaptation models; Data models; Domain adaptation; image stylization; semantic segmentation; object detection		Generating computer graphics (CG) rendered synthetic images has been widely used to create simulation environments for robotics/autonomous driving and generate labeled data. Yet, the problem of training models purely with synthetic data remains challenging due to the considerable domain gaps caused by current limitations on rendering. In this paper, we propose a simple yet effective domain adaptation framework towards closing such gap at image level. Unlike many GAN-based approaches, our method aims to match the covariance of the universal feature embeddings across domains, making the adaptation a fast, convenient step and avoiding the need for potentially difficult GAN training. To align domains more precisely, we further propose a conditional covariance matching framework which iteratively estimates semantic segmentation regions and conditionally matches the class-wise feature covariance given the segmentation regions. We demonstrate that both tasks can mutually refine and considerably improve each other, leading to state-of-the-art domain adaptation results. Extensive experiments under multiple synthetic-to-real settings show that our approach exceeds the performance of latest domain adaptation approaches. In addition, we offer a quantitative analysis where our framework shows considerable reduction in Frechet Inception distance between source and target domains, demonstrating the effectiveness of this work in bridging the synthetic-to-real domain gap.	[Dundar, Aysegul; Liu, Ming-Yu; Yu, Zhiding; Wang, Ting-Chun; Zedlewski, John; Kautz, Jan] Nvidia, Santa Clara, CA 95051 USA	Nvidia Corporation	Dundar, A (corresponding author), Nvidia, Santa Clara, CA 95051 USA.	adundar@nvidia.com; mingyul@nvidia.com; zhidingy@nvidia.com; tingchunw@nvidia.com; jzedlewski@nvidia.com; jkautz@nvidia.com		Liu, Ming-Yu/0000-0002-2951-2398				Alhaija H. Abu, 2018, P AS C COMP VIS, P85; [Anonymous], 2014, ABS14123474 CORR; Atapour-Abarghouei A, 2018, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2018.00296; Chen L.C., 2015, P INT C LEARN REPR; Chen L.-C., 2017, RETHINKING ATROUS CO; Chen YH, 2019, PROC CVPR IEEE, P1841, DOI 10.1109/CVPR.2019.00194; Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Darrell T., 2018, P INT C LEARN REPR W; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hensel M, 2017, ADV NEUR IN, V30; Hoffman J, 2016, FCNS WILD PIXELLEVEL; Hoffman J, 2018, PR MACH LEARN RES, V80; Hong WX, 2018, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2018.00145; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Li YH, 2018, PATTERN RECOGN, V80, P109, DOI 10.1016/j.patcog.2018.03.005; Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28; Li YJ, 2017, ADV NEUR IN, V30; Liu MY, 2016, ADV NEUR IN, V29; Liu MY, 2017, ADV NEUR IN, V30; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long MS, 2015, PR MACH LEARN RES, V37, P97; Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Peng XC, 2018, IEEE WINT CONF APPL, P1982, DOI 10.1109/WACV.2018.00219; Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151; Pitie F, 2005, IEEE I CONF COMP VIS, P1434; Quinonero-Candela J., 2008, COVARIATE SHIFT LOCA; Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Richter SR, 2015, PROC CVPR IEEE, P1128, DOI 10.1109/CVPR.2015.7298716; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Saleh FS, 2018, LECT NOTES COMPUT SC, V11206, P86, DOI 10.1007/978-3-030-01216-8_6; Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wu YD, 2018, IEEE IJCNN; Wu ZX, 2018, LECT NOTES COMPUT SC, V11209, P535, DOI 10.1007/978-3-030-01228-1_32; Xiao X., 2006, P ACM INT C VIRT REA, P305; Yu F., 2016, P ICLR 2016; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhang YH, 2018, PROC CVPR IEEE, P6810, DOI 10.1109/CVPR.2018.00712; Zhou DY, 2004, ADV NEUR IN, V16, P169; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XG, 2018, LECT NOTES COMPUT SC, V11211, P587, DOI 10.1007/978-3-030-01234-2_35; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	75	4	4	3	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2360	2372		10.1109/TPAMI.2020.2969421	http://dx.doi.org/10.1109/TPAMI.2020.2969421			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	31995476				2022-12-18	WOS:000692540900014
J	Kuo, MYJ; Kawahara, R; Nobuhara, S; Nishino, K				Kuo, Meng-Yu Jennifer; Kawahara, Ryo; Nobuhara, Shohei; Nishino, Ko			Non-Rigid Shape From Water	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Shape; Cameras; Image reconstruction; Absorption; Surface reconstruction; Imaging; Computational photography; underwater 3D shape recovery; non-rigid reconstruction; near-infrared light; camera calibration	CALIBRATION; MODEL	We introduce a novel 3D sensing method for recovering a consistent, dense 3D shape of a dynamic, non-rigid object in water. The method reconstructs a complete (or fuller) 3D surface of the target object in a canonical frame (e.g., rest shape) as it freely deforms and moves between frames by estimating underwater 3D scene flow and using it to integrate per-frame depth estimates recovered from two near-infrared observations. The reconstructed shape is refined in the course of this global non-rigid shape recovery by leveraging both geometric and radiometric constraints. We implement our method with a single camera and a light source without the orthographic assumption on either by deriving a practical calibration method that estimates the point source position with respect to the camera. Our reconstruction method also accounts for scattering by water. We prototype a video-rate imaging system and show 3D shape reconstruction results on a number of real-world static, deformable, and dynamic objects and creatures in real-world water. The results demonstrate the effectiveness of the method in recovering complete shapes of complex, non-rigid objects in water, which opens new avenues of application for underwater 3D sensing in the sub-meter range.	[Kuo, Meng-Yu Jennifer; Kawahara, Ryo; Nobuhara, Shohei; Nishino, Ko] Kyoto Univ, Grad Sch Informat, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan	Kyoto University	Kuo, MYJ (corresponding author), Kyoto Univ, Grad Sch Informat, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan.	jennifer@vision.ist.i.kyoto-u.ac.jp; kawahara@vision.ist.i.kyoto-u.ac.jp; nob@i.kyoto-u.ac.jp; kon@i.kyoto-u.ac.jp	Nobuhara, Shohei/GNP-2576-2022	Kawahara, Ryo/0000-0002-9819-3634; Nishino, Ko/0000-0002-3534-3447; Kuo, MengYu Jennifer/0000-0002-6705-7971; Nobuhara, Shohei/0000-0002-3204-8696	JSPS KAKENHI [17K20143, 18K19815, 20H05951, 21H04893]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported in part by the JSPS KAKENHI 17K20143, 18K19815, 20H05951, and 21H04893.	Agrawal A, 2012, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2012.6248073; [Anonymous], 2009, BMVC 20 BRIT MACHINE; Asano Y, 2016, LECT NOTES COMPUT SC, V9910, P635, DOI 10.1007/978-3-319-46466-4_38; Chadebecq F, 2020, INT J COMPUT VISION, V128, P1101, DOI 10.1007/s11263-019-01218-9; Chen XD, 2014, PROC CVPR IEEE, P524, DOI 10.1109/CVPR.2014.74; Fujimura Y, 2018, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2018.00777; Gregson J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185548; Grossberg MD, 2005, INT J COMPUT VISION, V61, P119, DOI 10.1023/B:VISI.0000043754.56350.10; HALE GM, 1973, APPL OPTICS, V12, P555, DOI 10.1364/AO.12.000555; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Ichimaru K, 2019, INT CONF 3D VISION, P524, DOI 10.1109/3DV.2019.00064; JAFFE JS, 1990, IEEE J OCEANIC ENG, V15, P101, DOI 10.1109/48.50695; Jordt-Sedlazeck A, 2013, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2013.14; Kang L, 2012, LECT NOTES COMPUT SC, V7575, P303, DOI 10.1007/978-3-642-33765-9_22; Kawahara Ryo, 2016, Methods in Oceanography, V17, P118, DOI 10.1016/j.mio.2016.08.002; Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x; Miraldo P, 2013, IEEE T PATTERN ANAL, V35, P2091, DOI 10.1109/TPAMI.2012.258; Morris NJW, 2011, IEEE T PATTERN ANAL, V33, P1518, DOI 10.1109/TPAMI.2011.24; Murai S, 2019, IEEE I CONF COMP VIS, P7829, DOI 10.1109/ICCV.2019.00792; Murez Z, 2015, IEEE I CONF COMP VIS, P3415, DOI 10.1109/ICCV.2015.390; Nakamae E., 1987, COMPUT GRAPH, V21, P303, DOI [10.1145/37402.37437, DOI 10.1145/37402.37437]; Narasimhan SG, 2006, ACM T GRAPHIC, V25, P1003, DOI 10.1145/1141911.1141986; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Reinhard E., 2008, COLOR IMAGING FUNDAM; Shibata A, 2015, IEEE INT CONF ROBOT, P5239, DOI 10.1109/ICRA.2015.7139929; Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531; Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309; Treibitz T, 2006, P IEEE COMP SOC C CO; Tsiotsios C, 2017, IMAGE VISION COMPUT, V57, P44, DOI 10.1016/j.imavis.2016.10.005; Tsiotsios C, 2014, PROC CVPR IEEE, P2259, DOI 10.1109/CVPR.2014.289; Verbiest Frank, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P397, DOI 10.1007/978-3-030-58539-6_24; Zhuo Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P246, DOI 10.1007/978-3-030-58548-8_15	32	4	4	2	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2220	2232		10.1109/TPAMI.2021.3075450	http://dx.doi.org/10.1109/TPAMI.2021.3075450			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	33900911				2022-12-18	WOS:000692540900005
J	Li, JN; Yang, JM; Hertzmann, A; Zhang, JM; Xu, TF				Li, Jianan; Yang, Jimei; Hertzmann, Aaron; Zhang, Jianming; Xu, Tingfa			LayoutGAN: Synthesizing Graphic Layouts With Vector-Wireframe Adversarial Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Layout; Generators; Rendering (computer graphics); Visualization; Three-dimensional displays; Optimization; Generative adversarial networks; graphic design; layout; wireframe	IMAGES	Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements, represented by vectors and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We, thus, propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation, tangram graphic design, mobile app layout design, and webpage layout optimization from hand-drawn sketches.	[Li, Jianan; Xu, Tingfa] Beijing Inst Technol, 5 South Zhongguancun St, Beijing 100081, Peoples R China; [Yang, Jimei; Hertzmann, Aaron; Zhang, Jianming] Adobe Res, 345 Pk Ave, San Jose, CA 95110 USA	Beijing Institute of Technology; Adobe Systems Inc.	Li, JN (corresponding author), Beijing Inst Technol, 5 South Zhongguancun St, Beijing 100081, Peoples R China.	20090964@bit.edu.cn; jimyang@adobe.com; hertzman@adobe.com; jianmzha@adobe.com; xutingfa@bit.edu.cn	Zhang, Jianming/B-1665-2017	Zhang, Jianming/0000-0002-9954-6294				Achlioptas P, 2018, PR MACH LEARN RES, V80; Bai XL, 2014, IEEE T IND INFORM, V10, P2135, DOI 10.1109/TII.2014.2359416; Beltramelli T, 2018, PROCEEDINGS OF THE ACM SIGCHI SYMPOSIUM ON ENGINEERING INTERACTIVE COMPUTING SYSTEMS (EICS'18), DOI 10.1145/3220134.3220135; Bylinskii Z, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P57, DOI 10.1145/3126594.3126653; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Deka B, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P845, DOI 10.1145/3126594.3126651; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Graves A, 2014, NEURAL TURING MACHIN; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Jaderberg M, 2015, ADV NEUR IN, V28; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339; King DB, 2015, ACS SYM SER, V1214, P1; Kipf TN, 2016, P INT C LEARN REPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar AN, 2018, PROC FRONT EDUC CONF; Kumar R, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2197; Li J., 2019, LAYOUTGAN GENERATING; Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982; O'Donovan P, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1221, DOI 10.1145/2702123.2702149; O'Donovan P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601110; O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958; Oord A.V.D., 2016, SSW; Pang XF, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982422; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618; Radford A., 2015, ARXIV PREPR ARXIV151; Reed Scott, 2015, ICLR, V1, P5; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Salimans T., 2016, ADV NEUR IN, P2234; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; van den Oord Aaron, 2016, ARXIV160605328; Vinyals Oriol, 2015, ARXIV151106391; Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Yang X., 2017, P IEEE C COMPUTER VI, P5315; Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256; Zitnick CL, 2016, IEEE T PATTERN ANAL, V38, P627, DOI 10.1109/TPAMI.2014.2366143; Zitnick CL, 2013, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2013.387	54	4	4	5	28	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2388	2399		10.1109/TPAMI.2019.2963663	http://dx.doi.org/10.1109/TPAMI.2019.2963663			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	31902756				2022-12-18	WOS:000692540900016
J	Wang, Y; Ding, Y; He, XJ; Fan, X; Lin, C; Li, FQ; Wang, TZ; Luo, ZX; Luo, JB				Wang, Yi; Ding, Yi; He, Xiangjian; Fan, Xin; Lin, Chi; Li, Fengqi; Wang, Tianzhu; Luo, Zhongxuan; Luo, Jiebo			Novelty Detection and Online Learning for Chunk Data Streams	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Data models; Linear systems; Fans; Hilbert space; Streaming media; Feature extraction; Data stream; feature selection; novelty detection; online learning	DISCRIMINANT-ANALYSIS; FAST IMPLEMENTATION; CLASSIFICATION	Datastream analysis aims at extracting discriminative information for classification from continuously incoming samples. It is extremely challenging to detect novel data while incrementally updating the model efficiently and stably, especially for high-dimensional and/or large-scale data streams. This paper proposes an efficient framework for novelty detection and incremental learning for unlabeled chunk data streams. First, an accurate factorization-free kernel discriminative analysis (FKDA-X) is put forward through solving a linear system in the kernel space. FKDA-X produces a Reproducing Kernel Hilbert Space (RKHS), in which unlabeled chunk data can be detected and classified by multiple known-classes in a single decision model with a deterministic classification boundary. Moreover, based on FKDA-X, two optimal methods FKDA-CX and FKDA-C are proposed. FKDA-CX uses the micro-cluster centers of original data as the input to achieve excellent performance in novelty detection. FKDA-C and incremental FKDA-C (IFKDA-C) using the class centers of original data as their input have extremely fast speed in online learning. Theoretical analysis and experimental validation on under-sampled and large-scale real-world datasets demonstrate that the proposed algorithms make it possible to learn unlabeled chunk data streams with significantly lower computational costs and comparable accuracies than the state-of-the-art approaches.	[Wang, Yi; Fan, Xin; Luo, Zhongxuan] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian 116620, Peoples R China; [Wang, Yi; Fan, Xin; Luo, Zhongxuan] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116620, Peoples R China; [Ding, Yi; Lin, Chi; Li, Fengqi] Dalian Univ Technol, Sch Software Technol, Dalian 116620, Peoples R China; [He, Xiangjian] Univ Technol Sydney, Sydney, NSW 2007, Australia; [Fan, Xin; Luo, Zhongxuan] Peng Cheng Lab, Shenfzhen 518066, Peoples R China; [Luo, Jiebo] Univ Rochester, Rochester, NY 14627 USA	Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; University of Technology Sydney; University of Rochester	Fan, X (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian 116620, Peoples R China.; Fan, X (corresponding author), Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116620, Peoples R China.; Fan, X (corresponding author), Peng Cheng Lab, Shenfzhen 518066, Peoples R China.	dlutwangyi@dlut.edu.cn; taika@mail.dlut.edu.cn; Xiangjian.He@uts.edu.au; xin.fan@dlut.edu.cn; c.lin@dlut.edu.cn; lifengqi@dlut.edu.cn; wangtz@126.com; zxluo@dlut.edu.cn; jluo@cs.rochester.edu	He, Xiangjian/CAA-1461-2022	He, Xiangjian/0000-0001-8962-540X; Luo, Jiebo/0000-0002-4516-9729	National Natural Science Foundation of China [61976037, 61733002]; National Key Research and Development Program of China [2017YFB1103704]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China	This work was supported by the National Natural Science Foundation of China (Grant No.61976037 and No.61733002) and the National Key Research and Development Program of China (Grant No.2017YFB1103704).	Aggarwal C., 2007, DATA STREAMS MODELS; Aggarwal CC, 2003, P 2003 VLDB C, V29, P81, DOI 10.1016/b978-012722442-8/50016-1; Bodesheim P, 2013, PROC CVPR IEEE, P3374, DOI 10.1109/CVPR.2013.433; Cai D, 2007, IEEE DATA MINING, P427, DOI 10.1109/ICDM.2007.88; Casimir R, 2006, ENG APPL ARTIF INTEL, V19, P169, DOI 10.1016/j.engappai.2005.07.004; Cevikalp H, 2006, IEEE T NEURAL NETWOR, V17, P1550, DOI 10.1109/TNN.2006.881485; Chu DL, 2015, IEEE T NEUR NET LEAR, V26, P2716, DOI 10.1109/TNNLS.2015.2391201; Chu DL, 2010, PATTERN RECOGN, V43, P1373, DOI 10.1016/j.patcog.2009.10.004; Clifton LA, 2006, LECT NOTES COMPUT SC, V3973, P836; de Faria ER, 2016, DATA MIN KNOWL DISC, V30, P640, DOI 10.1007/s10618-015-0433-y; de Faria ER, 2015, IEEE T KNOWL DATA EN, V27, P2961, DOI 10.1109/TKDE.2015.2441713; Faria ER, 2016, ARTIF INTELL REV, V45, P235, DOI 10.1007/s10462-015-9444-8; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Nguyen HL, 2015, KNOWL INF SYST, V45, P535, DOI 10.1007/s10115-014-0808-1; Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1023/B:AIRE.0000045502.10941.a9; Huang R, 2002, INT C PATT RECOG, P29, DOI 10.1109/ICPR.2002.1047787; Jumutc V, 2014, IEEE T PATTERN ANAL, V36, P2510, DOI 10.1109/TPAMI.2014.2327984; Karasuyama M, 2010, IEEE T NEURAL NETWOR, V21, P1048, DOI 10.1109/TNN.2010.2048039; Kemmler M, 2011, LECT NOTES COMPUT SC, V6493, P489; Kim TK, 2011, INT J COMPUT VISION, V91, P216, DOI 10.1007/s11263-010-0381-3; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Liu JC, 2017, PROC CVPR IEEE, P4123, DOI 10.1109/CVPR.2017.439; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Min HK, 2016, PATTERN RECOGN, V50, P45, DOI 10.1016/j.patcog.2015.08.021; Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026; Ramirez-Gallego S, 2017, NEUROCOMPUTING, V239, P39, DOI 10.1016/j.neucom.2017.01.078; Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300; Sharma A, 2012, PATTERN RECOGN, V45, P2205, DOI 10.1016/j.patcog.2011.11.018; Siahroudi SK, 2018, EXPERT SYST APPL, V91, P187, DOI 10.1016/j.eswa.2017.08.033; Silva JA, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2522968.2522981; Smola A. J., 2008, LECT NOTES COMPUT, V42, P1; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Xiong Tao, 2005, ADV NEURAL INFORM PR, P1529; Yang J, 2004, PATTERN RECOGN, V37, P2097, DOI 10.1016/j.patcog.2003.10.015; Ye JP, 2005, IEEE T KNOWL DATA EN, V17, P1208, DOI 10.1109/TKDE.2005.148; Yeung DY, 2002, INT C PATT RECOG, P385, DOI 10.1109/ICPR.2002.1047476	38	4	4	7	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2400	2412		10.1109/TPAMI.2020.2965531	http://dx.doi.org/10.1109/TPAMI.2020.2965531			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	31940520				2022-12-18	WOS:000692540900017
J	Zhao, YY; Raghuram, A; Kim, HK; Hielscher, AH; Robinson, JT; Veeraraghavan, A				Zhao, Yongyi; Raghuram, Ankit; Kim, Hyun K.; Hielscher, Andreas H.; Robinson, Jacob T.; Veeraraghavan, Ashok			High Resolution, Deep Imaging Using Confocal Time-of-Flight Diffuse Optical Tomography	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Time-of-flight imaging; diffuse optical tomography; confocal; time binning; fluorescence imaging	FLUORESCENCE; ALGORITHM; PHANTOMS	Light scattering by tissue severely limits how deep beneath the surface one can image, and the spatial resolution one can obtain from these images. Diffuse optical tomography (DOT) is one of the most powerful techniques for imaging deep within tissue - well beyond the conventional similar to 10-15 mean scattering lengths tolerated by ballistic imaging techniques such as confocal and twophoton microscopy. Unfortunately, existing DOT systems are limited, achieving only centimeter-scale resolution. Furthermore, they suffer from slow acquisition times and slow reconstruction speeds making real-time imaging infeasible. We show that time-of-flight diffuse optical tomography (ToF-DOT) and its confocal variant (CToF-DOT), by exploiting the photon travel time information, allow us to achieve millimeter spatial resolution in the highly scattered diffusion regime (>50 mean free paths). In addition, we demonstrate two additional innovations: focusing on confocal measurements, and multiplexing the illumination sources allow us to significantly reduce the measurement acquisition time. Finally, we rely on a novel convolutional approximation that allows us to develop a fast reconstruction algorithm, achieving a 100x speedup in reconstruction time compared to traditional DOT reconstruction techniques. Together, we believe that these technical advances serve as the first step towards real-time, millimeter resolution, deep tissue imaging using DOT.	[Zhao, Yongyi; Raghuram, Ankit; Robinson, Jacob T.; Veeraraghavan, Ashok] Rice Univ, Dept Elect Comp Engn, Houston, TX 77005 USA; [Kim, Hyun K.] Columbia Univ, Dept Radiol, New York, NY 10027 USA; [Kim, Hyun K.] New York Univ, Dept Biomed Engn, New York, NY 11201 USA; [Hielscher, Andreas H.] New York Univ, Dept Biomed Engn, New York, NY 11201 USA	Rice University; Columbia University; New York University; New York University	Zhao, YY (corresponding author), Rice Univ, Dept Elect Comp Engn, Houston, TX 77005 USA.	yongyi@rice.edu; ar89@rice.edu; hkk2107@cumc.columbia.edu; ahh4614@nyu.edu; jtrobinson@rice.edu; vashok@rice.edu		Zhao, Yongyi/0000-0002-4242-6910; Raghuram, Ankit/0000-0001-6689-501X; Veeraraghavan, Ashok/0000-0001-5043-7460	Defense Advanced Research Projects Agency (DARPA) [N66001-19-C-4020]; NSF Expeditions [1730147]; NLM Training Program [T15LM007093]	Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF Expeditions; NLM Training Program	The authors would like to thank Prof. Aswin Sankaranarayanan for his suggestions and edits to our paper. The authors would also like to thank Biorender for the use of its icons to help in figure design. This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant N66001-19-C-4020 and in part by the NSF Expeditions under Grant 1730147. The work of Yongyi Zhao was supported by a training fellowship through the NLM Training Program under Grant T15LM007093. The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or theU.S. Government. Yongyi Zhao and Ankit Raghuram contributed equally to thiswork.	Ahn B, 2019, IEEE I CONF COMP VIS, P7888, DOI 10.1109/ICCV.2019.00798; Azinovic D, 2019, PROC CVPR IEEE, P2442, DOI 10.1109/CVPR.2019.00255; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bevilacqua F, 1999, APPL OPTICS, V38, P4939, DOI 10.1364/AO.38.004939; Boas DA, 2001, OPT EXPRESS, V8, P263, DOI 10.1364/OE.8.000263; Boas DA, 2001, IEEE SIGNAL PROC MAG, V18, P57, DOI 10.1109/79.962278; Bouchard JP, 2010, OPT EXPRESS, V18, P11495, DOI 10.1364/OE.18.011495; Che Chengqian, 2018, ARXIV180910820; Chen J, 2011, BIOMED OPT EXPRESS, V2, P871, DOI 10.1364/BOE.2.000871; Cossairt O, 2013, IEEE T IMAGE PROCESS, V22, P447, DOI 10.1109/TIP.2012.2216538; Dempsey LA, 2017, BIOMED OPT EXPRESS, V8, P1754, DOI 10.1364/BOE.8.001754; Di Sieno L, 2017, J BIOMED OPT, V22, DOI 10.1117/1.JBO.22.8.085004; Farina A, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7121235; Frijia EM, 2021, NEUROIMAGE, V225, DOI 10.1016/j.neuroimage.2020.117490; Gibson AP, 2005, PHYS MED BIOL, V50, pR1, DOI 10.1088/0031-9155/50/4/R01; Gkioulekas I, 2016, LECT NOTES COMPUT SC, V9907, P685, DOI 10.1007/978-3-319-46487-9_42; Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508377; Haiyan Li, 2007, International Journal of Vehicle Safety, V2, P345, DOI 10.1504/IJVS.2007.016747; Hyde D., 2002, THESIS WORCESTER POL; Kak A.V., 1988, PRINCIPLES COMPUTERI, DOI DOI 10.1137/1.9780898719277.CH7; Kempe M, 1996, J OPT SOC AM A, V13, P46, DOI 10.1364/JOSAA.13.000046; Kim HK, 2017, INT J THERM SCI, V116, P265, DOI 10.1016/j.ijthermalsci.2017.03.004; Kim HK, 2009, INVERSE PROBL, V25, DOI 10.1088/0266-5611/25/1/015010; Levis A, 2017, PROC CVPR IEEE, P5797, DOI 10.1109/CVPR.2017.614; Levis A, 2015, IEEE I CONF COMP VIS, P3379, DOI 10.1109/ICCV.2015.386; Liebert A, 2008, OPT EXPRESS, V16, P13188, DOI 10.1364/OE.16.013188; Lindell DB, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-18346-3; Liu CX, 2021, IEEE T CYBERNETICS, V51, P2339, DOI 10.1109/TCYB.2020.2978003; Lyons A, 2019, NAT PHOTONICS, V13, P575, DOI 10.1038/s41566-019-0439-x; Mitra K, 2014, IEEE T PATTERN ANAL, V36, P1909, DOI 10.1109/TPAMI.2014.2313118; Mozumder M, 2020, J OPT SOC AM A, V37, P182, DOI 10.1364/JOSAA.37.000182; Naser MA, 2015, BIOMED PHYS ENG EXPR, V1, DOI 10.1088/2057-1976/1/4/045207; Nie LM, 2012, J BIOMED OPT, V17, DOI 10.1117/1.JBO.17.11.110506; Nimier-David M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392406; Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498; O'Toole M, 2018, NATURE, V555, P338, DOI 10.1038/nature25489; Oh BH, 2019, FRONT MED-LAUSANNE, V6, DOI 10.3389/fmed.2019.00274; Pediredla Adithya, 2019, 2019 IEEE INT C COMP, P1; Pediredla AK, 2016, J BIOMED OPT, V21, DOI 10.1117/1.JBO.21.12.126009; Pifferi A, 2016, J BIOMED OPT, V21, DOI 10.1117/1.JBO.21.9.091310; Pifferi A, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.138101; Puszka A, 2015, BIOMED OPT EXPRESS, V6, P1, DOI 10.1364/BOE.6.000001; Puszka A, 2013, BIOMED OPT EXPRESS, V4, P1351, DOI 10.1364/BOE.4.001351; Sankaranarayanan A. C, 2018, HADAMARD MULTIPLEXIN; Satat G, 2018, IEEE INT CONF COMPUT; Schechner YY, 2007, IEEE T PATTERN ANAL, V29, P1339, DOI 10.1109/TPAMI.2007.1151; Sergeeva EA, 2010, QUANTUM ELECTRON+, V40, P411, DOI 10.1070/QE2010v040n05ABEH014319; Tarvainen T, 2010, BIOMED OPT EXPRESS, V1, P209, DOI 10.1364/BOE.1.000209; Torricelli A, 2005, PHYS REV LETT, V95, DOI 10.1103/PhysRevLett.95.078101; Velten A, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1747; Wang L. V., 2007, BIOMEDICAL OPTICS PR; WANG LH, 1995, COMPUT METH PROG BIO, V47, P131, DOI 10.1016/0169-2607(95)01640-F; Wheelock MD, 2019, REV SCI INSTRUM, V90, DOI 10.1063/1.5086809; Xia J, 2014, PROG ELECTROMAGN RES, V147, P1, DOI 10.2528/PIER14032303; Yao RY, 2018, BIOMED OPT EXPRESS, V9, P4588, DOI 10.1364/BOE.9.004588; Zhao HB, 2018, NEUROPHOTONICS, V5, DOI 10.1117/1.NPh.5.1.011012	57	4	4	4	21	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2206	2219		10.1109/TPAMI.2021.3075366	http://dx.doi.org/10.1109/TPAMI.2021.3075366			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	33891548	Green Accepted, Green Submitted			2022-12-18	WOS:000692540900004
J	Xu, HY; Lv, XT; Wang, XY; Ren, Z; Bodla, N; Chellappa, R				Xu, Hongyu; Lv, Xutao; Wang, Xiaoyu; Ren, Zhou; Bodla, Navaneeth; Chellappa, Rama			Deep Regionlets: Blended Representation and Deep Learning for Generic Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Detectors; Object detection; Proposals; Machine learning; Deformable models; Strain; Object detection; deep learning; deep regionlets; spatial transformation		In this article, we propose a novel object detection algorithm named "Deep Regionlets" by integrating deep neural networks and a conventional detection schema for accurate generic object detection. Motivated by the effectiveness of regionlets for modeling object deformations and multiple aspect ratios, we incorporate regionlets into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Specifically, given a detection bounding box proposal, the region selection network provides guidance on where to select sub-regions from which features can be learned from. An object proposal typically contains three - 16 sub-regions. The regionlet learning module focuses on local feature selection and transformations to alleviate the effects of appearance variations. To this end, we first realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we design a "gating network" within the regionlet leaning module to enable instance dependent soft feature selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We present ablation studies and extensive experiments on the PASCAL VOC dataset and the Microsoft COCO dataset. The proposed method yields competitive performance over state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.	[Xu, Hongyu] Apple Inc, Cupertino, CA 95014 USA; [Lv, Xutao; Wang, Xiaoyu] Intellifusion, Redmond, WA 98052 USA; [Ren, Zhou] Wormpex AI Res, Bellevue, WA 98004 USA; [Bodla, Navaneeth; Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, UMIACS, College Pk, MD 20742 USA	Apple Inc; University System of Maryland; University of Maryland College Park	Xu, HY (corresponding author), Apple Inc, Cupertino, CA 95014 USA.	xuhongyu2006@gmail.com; lvxutao@gmail.com; fanghuaxue@gmail.com; renzhou200622@gmail.com; nbodla@umiacs.umd.edu; rama@umiacs.umd.edu	xu, hong/GSD-8903-2022	Wang, Xiaoyu/0000-0002-6431-8822	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) [D17PC00345]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC)	This research is based on work supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00345. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation theron. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied of IARPA, DOI/IBC, or the U.S. Government.	Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Bharat Singh, 2017, Arxiv, DOI arXiv:1704.04503; Bodla N, 2017, IEEE WINT CONF APPL, P586, DOI 10.1109/WACV.2017.71; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5; Cheng B., 2018, DECOUPLED CLASSIFICA; Cheng BW, 2018, LECT NOTES COMPUT SC, V11219, P473, DOI 10.1007/978-3-030-01267-0_28; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dollar P, 2012, LECT NOTES COMPUT SC, V7573, P645, DOI 10.1007/978-3-642-33709-3_46; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906; Fu C. -Y., 2017, ARXIV170106659; Gidaris S, 2016, PROC CVPR IEEE, P789, DOI 10.1109/CVPR.2016.92; Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gu JY, 2018, LECT NOTES COMPUT SC, V11216, P392, DOI 10.1007/978-3-030-01258-8_24; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351; Jaderberg M, 2015, ADV NEUR IN, V28; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48; Kim SW, 2018, LECT NOTES COMPUT SC, V11209, P239, DOI 10.1007/978-3-030-01228-1_15; Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Hengduo, 2019, ARXIV190405871; Li LJ, 2009, PROC CVPR IEEE, P2036, DOI 10.1109/CVPRW.2009.5206718; Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI 10.1007/978-3-030-01240-3_21; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; Mordan T., 2017, P BRIT MACH VIS C; Ouyang WL, 2017, IEEE T PATTERN ANAL, V39, P1320, DOI 10.1109/TPAMI.2016.2587642; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Ranjan R., 2018, ABS180401159 CORR; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren S., 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.169; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Singh B, 2018, ADV NEUR IN, V31; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang H, 2018, PROC CVPR IEEE, P1248, DOI 10.1109/CVPR.2018.00136; Wang XY, 2015, IEEE T PATTERN ANAL, V37, P2071, DOI 10.1109/TPAMI.2015.2389830; Wang XY, 2013, IEEE I CONF COMP VIS, P17, DOI 10.1109/ICCV.2013.10; Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207; Wei Y, 2018, LECT NOTES COMPUT SC, V11212, P274, DOI 10.1007/978-3-030-01237-3_17; Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929; Wu ZY, 2019, IEEE I CONF COMP VIS, P1201, DOI 10.1109/ICCV.2019.00129; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu HY, 2018, LECT NOTES COMPUT SC, V11215, P827, DOI 10.1007/978-3-030-01252-6_49; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhang ZS, 2018, PROC CVPR IEEE, P5813, DOI 10.1109/CVPR.2018.00609; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zhao XY, 2018, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2018.00427; Zhong YY, 2017, IEEE SIGNAL PROC LET, V24, P1213, DOI 10.1109/LSP.2017.2715076; Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062; Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444; Zou WB, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.78	80	4	4	11	38	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					1914	1927		10.1109/TPAMI.2019.2957780	http://dx.doi.org/10.1109/TPAMI.2019.2957780			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SA8YQ	31804929	Green Submitted			2022-12-18	WOS:000649590200008
J	Xue, N; Bai, S; Wang, FD; Xia, GS; Wu, TF; Zhang, LP; Torr, PHS				Xue, Nan; Bai, Song; Wang, Fu-Dong; Xia, Gui-Song; Wu, Tianfu; Zhang, Liangpei; Torr, Philip H. S.			Learning Regional Attraction for Line Segment Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Image edge detection; Detectors; Lattices; Machine learning; Electronic mail; Training; Line segment detection; low-level vision; deep learning	JUNCTION DETECTION; HOUGH TRANSFORM; EXTRACTION; ACCURATE	This paper presents regional attraction of line segment maps, and hereby poses the problem of line segment detection (LSD) as a problem of region coloring. Given a line segment map, the proposed regional attraction first establishes the relationship between line segments and regions in the image lattice. Based on this, the line segment map is equivalently transformed to an attraction field map (AFM), which can be remapped to a set of line segments without loss of information. Accordingly, we develop an end-to-end framework to learn attraction field maps for raw input images, followed by a squeeze module to detect line segments. Apart from existing works, the proposed detector properly handles the local ambiguity and does not rely on the accurate identification of edge pixels. Comprehensive experiments on the Wireframe dataset and the YorkUrban dataset demonstrate the superiority of our method. In particular, we achieve an F-measure of 0.831 on the Wireframe dataset, advancing the state-of-the-art performance by 10.3 percent.	[Xue, Nan; Xia, Gui-Song] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China; [Bai, Song; Torr, Philip H. S.] Univ Oxford, Oxford OX1 2JD, England; [Wang, Fu-Dong; Xia, Gui-Song; Zhang, Liangpei] Wuhan Univ, State Key Lab LIESMARS, Wuhan 430072, Hubei, Peoples R China; [Wu, Tianfu] North Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27695 USA	Wuhan University; University of Oxford; Wuhan University; University of North Carolina; North Carolina State University	Xia, GS (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.; Xia, GS (corresponding author), Wuhan Univ, State Key Lab LIESMARS, Wuhan 430072, Hubei, Peoples R China.	xuenan@whu.edu.cn; songbai.site@gmail.com; fudong-wang@whu.edu.cn; guisong.xia@whu.edu.cn; tianfu_wu@ncsu.edu; zlp62@whu.edu.cn; philip.torr@eng.ox.ac.uk	Xue, Nan/HCI-0300-2022	Xia, Gui-Song/0000-0001-7660-6090; Wang, Fudong/0000-0001-6416-4707; Wu, Tianfu/0000-0001-8911-5506	National Natural Science Foundation of China [61922065, 61771350, 41820104006]; EPSRC [Seebibyte EP/M013774/1]; EPSRC/MURI [EP/N019474/1]; China Scholarship Council; ARO [W911NF1810295]; NSF [IIS-1909644]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); China Scholarship Council(China Scholarship Council); ARO; NSF(National Science Foundation (NSF))	This work was supported by the National Natural Science Foundation of China under Grant 61922065, Grant 61771350, and Grant 41820104006. This work was also supported in part by EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI Grant EP/N019474/1. Nan Xue was also supported by the China Scholarship Council. T. Wu was supported in part by ARO Grant W911NF1810295 and NSF IIS-1909644. The views presented in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Almazan EJ, 2017, PROC CVPR IEEE, P5854, DOI 10.1109/CVPR.2017.620; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; BURNS JB, 1986, IEEE T PATTERN ANAL, V8, P425, DOI 10.1109/TPAMI.1986.4767808; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Cho NG, 2018, IEEE T PATTERN ANAL, V40, P1195, DOI 10.1109/TPAMI.2017.2703841; Denis P, 2008, LECT NOTES COMPUT SC, V5303, P197, DOI 10.1007/978-3-540-88688-4_15; Desolneux A, 2003, IEEE T PATTERN ANAL, V25, P508, DOI 10.1109/TPAMI.2003.1190576; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; Duan L, 2015, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2015.7298931; Estellers V, 2012, IEEE T IMAGE PROCESS, V21, P4722, DOI 10.1109/TIP.2012.2202674; Faugeras O. D., 1992, International Journal of Pattern Recognition and Artificial Intelligence, V6, P353, DOI 10.1142/S0218001492000229; Furukawa Y, 2003, COMPUT VIS IMAGE UND, V92, P1, DOI 10.1016/j.cviu.2003.07.002; Guerreiro RFC, 2012, IEEE T IMAGE PROCESS, V21, P4819, DOI 10.1109/TIP.2012.2202673; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hou Q., 3 BIRDS ONE STONE UN; Huang K, 2018, PROC CVPR IEEE, P626, DOI 10.1109/CVPR.2018.00072; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kimmel R, 1996, J MATH IMAGING VIS, V6, P223, DOI 10.1007/BF00119840; Kingma Diederik P., 2015, 3 INT C LEARN REPRES, V3; Kittler J, 1983, IMAGE VISION COMPUT, V1, P37, DOI DOI 10.1016/0262-8856(83)90006-9; Kokkinos I.., 2016, P 4 INT C LEARN REPR, P1; Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Matas J, 2000, COMPUT VIS IMAGE UND, V78, P119, DOI 10.1006/cviu.1999.0831; Nair V., 2010, ICML, P807; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sethian J. A., 1999, LEVEL SET METHODS FA; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shi DM, 2013, IEEE T IMAGE PROCESS, V22, P2500, DOI 10.1109/TIP.2013.2246522; Springenberg J. T, 2015, ARXIV PREPRINT ARXIV; TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769; von Gioi RG, 2008, J MATH IMAGING VIS, V32, P313, DOI 10.1007/s10851-008-0102-5; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Xia GS, 2014, INT J COMPUT VISION, V106, P31, DOI 10.1007/s11263-013-0640-1; Xiang TZ, 2018, PATTERN RECOGN, V83, P481, DOI 10.1016/j.patcog.2018.06.013; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xu C, 2017, IEEE T PATTERN ANAL, V39, P1209, DOI 10.1109/TPAMI.2016.2582162; Xu ZH, 2015, PATTERN RECOGN, V48, P4012, DOI 10.1016/j.patcog.2015.06.008; Xu ZZ, 2015, COMPUT VIS IMAGE UND, V138, P61, DOI 10.1016/j.cviu.2015.05.008; Xu ZZ, 2015, IEEE T IMAGE PROCESS, V24, P813, DOI 10.1109/TIP.2014.2387020; Xue N, 2019, PROC CVPR IEEE, P1595, DOI 10.1109/CVPR.2019.00169; Xue N, 2018, IEEE T IMAGE PROCESS, V27, P78, DOI 10.1109/TIP.2017.2754945; Yang K, 2011, COMPUT VIS IMAGE UND, V115, P1207, DOI 10.1016/j.cviu.2011.03.010; Yu Z, 2013, IEEE I CONF COMP VIS, P2792, DOI 10.1109/ICCV.2013.347; Yuan JY, 2018, IEEE T PATTERN ANAL, V40, P2793, DOI 10.1109/TPAMI.2017.2750680; Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401; Zollhofer M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766887; Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219	53	4	4	2	33	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					1998	2013		10.1109/TPAMI.2019.2958642	http://dx.doi.org/10.1109/TPAMI.2019.2958642			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SA8YQ	31831408	Green Published, Green Submitted			2022-12-18	WOS:000649590200013
J	Zhong, FW; Sun, P; Luo, WH; Yan, TY; Wang, YZ				Zhong, Fangwei; Sun, Peng; Luo, Wenhan; Yan, Tingyun; Wang, Yizhou			AD-VAT plus : An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Target tracking; Training; Visualization; Task analysis; Cameras; Object tracking; Active object tracking; adversarial training; reinforcement learning		Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. To learn a robust tracker for VAT, in this article, we propose a novel adversarial reinforcement learning (RL) method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT. In the mechanism, the tracker and target, viewed as two learnable agents, are opponents and can mutually enhance each other during the dueling/competition: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker. The dueling is asymmetric in that the target is additionally fed with the tracker's observation and action, and learns to predict the tracker's reward as an auxiliary task. Such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker. To improve the performance of the tracker in the case of challenging scenarios such as obstacles, we employ more advanced environment augmentation technique and two-stage training strategies, termed as AD-VAT+. For a better understanding of the asymmetric dueling mechanism, we also analyze the target's behaviors as the training proceeds and visualize the latent space of the tracker. The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios. The potential of the active tracker is also shown in real-world videos.	[Zhong, Fangwei; Yan, Tingyun] Peking Univ, Natl Engn Lab Video Technol, Key Lab Machine Percept MoE, Comp Sci Dept, Beijing 100871, Peoples R China; [Zhong, Fangwei] Beijing Film Acad, Adv Innovat Ctr Future Visual Entertainment AICFV, Beijing, Peoples R China; [Sun, Peng; Luo, Wenhan] Tencent AI Lab, Shenzhen 518057, Peoples R China; [Wang, Yizhou] Peking Univ, Ctr Frontiers Comp Studies CFCS, Comp Sci Dept, Beijing 100871, Peoples R China	Peking University; Tencent; Peking University	Zhong, FW (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Key Lab Machine Percept MoE, Comp Sci Dept, Beijing 100871, Peoples R China.; Zhong, FW (corresponding author), Beijing Film Acad, Adv Innovat Ctr Future Visual Entertainment AICFV, Beijing, Peoples R China.	zfw@pku.edu.cn; pengsun000@gmail.com; whluo.china@gmail.com; yanty18@pku.edu.cn; yizhou.wang@pku.edu.cn	Luo, Wenhan/GZL-0535-2022	Zhong, Fangwei/0000-0002-0428-4552	Tencent AI Lab Rhino-Bird Focused Research Program [MOST-2018AAA0102004, NSFC-61625201, NSFC-61527804]; QualcommUniversity Collaborative Research Program	Tencent AI Lab Rhino-Bird Focused Research Program; QualcommUniversity Collaborative Research Program	The authors would like to thank Weichao Qiu for his help in extending UnrealCV. Fangwei Zhong, Tingyun Yan, and YizhouWangwere supported in part by the following Grants: MOST-2018AAA0102004, NSFC-61625201, NSFC-61527804, Tencent AI Lab Rhino-Bird Focused Research Program, and QualcommUniversity Collaborative Research Program.	Abbeel P., 2018, P 35 INT C MACH LEAR, V80; Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737; Behzadan V., 2018, ARXIV180601368; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Brockman Greg, 2016, arXiv; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Brown N, 2019, PR MACH LEARN RES, V97; Choi J, 2017, IEEE UNDERWATER TECH; DENZLER J, 1994, IEEE IMAGE PROC, P635, DOI 10.1109/ICIP.1994.413812; Fergus R., 2018, INT C LEARN REPR ICL, P1; Griffis D., 2017, A3C LSTM ATARI PYTOR; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Heinrich J., 2016, ARXIV PREPRINT ARXIV; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hong ZW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4912; Hu WM, 2012, IEEE T PATTERN ANAL, V34, P2420, DOI 10.1109/TPAMI.2012.42; Huang S., 2017, ARXIV170202284; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kim KK, 2005, 7TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY, VOLS 1 AND 2, PROCEEDINGS, P817; Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982; Kylberg G., 2011, EXTERNAL REPORT BLUE, V35; Lanctot M, 2017, ADV NEUR IN, V30; Littman ML, 1994, ICML 1994, P157; Luo WH, 2018, PR MACH LEARN RES, V80; Luo WH, 2020, IEEE T PATTERN ANAL, V42, P1317, DOI 10.1109/TPAMI.2019.2899570; Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352; Mandlekar A, 2017, IEEE INT C INT ROBOT, P3932; Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Osborne MJ, 1994, COURSE GAME THEORY; Pan XL, 2019, IEEE INT CONF ROBOT, P8522, DOI 10.1109/ICRA.2019.8794293; Pinto L, 2017, PR MACH LEARN RES, V70; Qiu W., 2017, GYM UNRE ALCV REALIS; Qiu WC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1221, DOI 10.1145/3123266.3129396; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srinivasan S., 2018, ADV NEURAL INFORM PR, P3426; Sun T, 2008, STRATEG STUD, P63; Sutskever I., 2018, P INT C LEARN REPR; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Uther W., 1997, P AAAI FALL S MOD DI; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vlassis N., 2007, SYNTHESIS LECT ARTIF, V1, P71, DOI DOI 10.2200/S00091ED1V01Y200705AIM002; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Yan T., 2019, INT C LEARN REPR; Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108; Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7; Zinkevich M., 2008, ADV NEURAL INFORM PR, P1729	52	4	4	7	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1467	1482		10.1109/TPAMI.2019.2952590	http://dx.doi.org/10.1109/TPAMI.2019.2952590			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31722476				2022-12-18	WOS:000637533800001
J	Zampogiannis, K; Fermuller, C; Aloimonos, Y				Zampogiannis, Konstantinos; Fermueller, Cornelia; Aloimonos, Yiannis			Topology-Aware Non-Rigid Point Cloud Registration	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Topology; Dynamics; Motion estimation; Geometry; Estimation; Image reconstruction; Non-rigid registration; warp field; dense motion estimation; surface deformation; dynamic topology		In this paper, we introduce a non-rigid registration pipeline for pairs of unorganized point clouds that may be topologically different. Standard warp field estimation algorithms, even under robust, discontinuity-preserving regularization, tend to produce erratic motion estimates on boundaries associated with 'close-to-open' topology changes. We overcome this limitation by exploiting backward motion: in the opposite motion direction, a 'close-to-open' event becomes 'open-to-close', which is by default handled correctly. At the core of our approach lies a general, topology-agnostic warp field estimation algorithm, similar to those employed in recently introduced dynamic reconstruction systems from RGB-D input. We improve motion estimation on boundaries associated with topology changes in an efficient post-processing phase. Based on both forward and (inverted) backward warp hypotheses, we explicitly detect regions of the deformed geometry that undergo topological changes by means of local deformation criteria and broadly classify them as 'contacts' or 'separations'. Subsequently, the two motion hypotheses are seamlessly blended on a local basis, according to the type and proximity of detected events. Our method achieves state-of-the-art motion estimation accuracy on the MPI Sintel dataset. Experiments on a custom dataset with topological event annotations demonstrate the effectiveness of our pipeline in estimating motion on event boundaries, as well as promising performance in explicit topological event detection.	[Zampogiannis, Konstantinos; Aloimonos, Yiannis] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Fermueller, Cornelia] Univ Maryland, Inst Adv Comp Studies UMIACS, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Zampogiannis, K (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	kzampog@cs.umd.edu; fer@umiacs.umd.edu; yiannis@cs.umd.edu		Zampogiannis, Konstantinos/0000-0002-2494-9048	Northrop Grumman Mission Systems University Research Program, of ONR [N0001417-1-2622]; National Science Foundation [BCS 1824198, CNS 1544787]	Northrop Grumman Mission Systems University Research Program, of ONR; National Science Foundation(National Science Foundation (NSF))	The support of Northrop Grumman Mission Systems University Research Program, of ONR under grant award N0001417-1-2622, and the support of the National Science Foundation under grants BCS 1824198 and CNS 1544787 are greatly acknowledged.	Amberg B, 2007, IEEE I CONF COMP VIS, P1326; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Engels Chris, 2006, PHOTOGRAMMETRIC COMP, V2; Gao W., 2018, P ROB SCI SYST; Herbst E, 2013, IEEE INT CONF ROBOT, P2276, DOI 10.1109/ICRA.2013.6630885; Hornacek M, 2014, PROC CVPR IEEE, P3526, DOI 10.1109/CVPR.2014.451; Huang AS, 2017, SPRINGER TRAC ADV RO, V100, DOI 10.1007/978-3-319-29363-9_14; Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22; Jaimez Mariano, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3992, DOI 10.1109/ICRA.2017.7989459; Jaimez M, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P64, DOI 10.1109/3DV.2015.15; Jaimez M, 2015, IEEE INT CONF ROBOT, P98, DOI 10.1109/ICRA.2015.7138986; Kerl C, 2013, IEEE INT CONF ROBOT, P3748, DOI 10.1109/ICRA.2013.6631104; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Quiroga J, 2014, LECT NOTES COMPUT SC, V8695, P567, DOI 10.1007/978-3-319-10584-0_37; Runz Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4471, DOI 10.1109/ICRA.2017.7989518; Runz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Slavcheva M, 2018, PROC CVPR IEEE, P2646, DOI 10.1109/CVPR.2018.00280; Slavcheva M, 2018, INT J COMPUT VISION, V126, P615, DOI 10.1007/s11263-017-1057-z; Slavcheva M, 2017, PROC CVPR IEEE, P5474, DOI 10.1109/CVPR.2017.581; Sorkine Olga, 2007, P EUROGRAPHICS ACM S, V4, P109, DOI 10.1145/1281991.1282006; Spies H, 2002, COMPUT VIS IMAGE UND, V85, P209, DOI 10.1006/cviu.2002.0970; Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531; Sun D, 2015, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2015.7298653; Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293; Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Zampogiannis K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1364, DOI 10.1145/3240508.3243655	33	4	4	1	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2021	43	3					1056	1069		10.1109/TPAMI.2019.2940655	http://dx.doi.org/10.1109/TPAMI.2019.2940655			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QE6IS	31514126	Green Submitted			2022-12-18	WOS:000616309900021
J	Wang, J; Cherian, A				Wang, Jue; Cherian, Anoop			Discriminative Video Representation Learning Using Support Vector Classifiers	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Support vector machines; Feature extraction; Trajectory; Task analysis; Computer architecture; Image recognition; Deep learning; Video representation; video data mining; discriminative pooling; action recognition; deep learning		Most popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action-indeed, many are common across multiple actions-pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To identify these useful features, we resort to a negative bag consisting of features that are known to be irrelevant, for example, they are sampled either from datasets that are unrelated to our actions of interest or are CNN features produced via random noise as input. With the features from the video as a positive bag and the irrelevant features as the negative bag, we cast an objective to learn a (nonlinear) hyperplane that separates the unknown useful features from the rest in a multiple instance learning formulation within a support vector machine setup. We use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they can be treated as a weighted average pooling of the features from the bags, with zero weights given to non-support vectors. Our pooling scheme is end-to-end trainable within a deep learning framework. We report results from experiments on eight computer vision benchmark datasets spanning a variety of video-related tasks and demonstrate state-of-the-art performance across these tasks.	[Wang, Jue] Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia; [Cherian, Anoop] Mistubishi Elect Res Labs MERL, Cambridge, MA 02139 USA	Australian National University	Wang, J (corresponding author), Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia.	jue.wang@anu.edu.au; cherian@merl.com	Wang, Jue/ABD-1746-2021	Wang, Jue/0000-0001-8546-4522				Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653; Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4; Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Bunescu R.C., 2007, P ICML, P105, DOI DOI 10.1145/1273496.1273510; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Carreira Joao, 2018, ARXIV180801340; Cherian A, 2019, INT J COMPUT VISION, V127, P340, DOI 10.1007/s11263-018-1111-5; Cherian A, 2018, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2018.00234; Cherian A, 2017, PROC CVPR IEEE, P1581, DOI 10.1109/CVPR.2017.172; Cherian A, 2017, IEEE WINT CONF APPL, P130, DOI 10.1109/WACV.2017.22; Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; DONTCHEV AL, 2009, SPRINGER MONOGRAPHS, V208; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787; Feichtenhofer C, 2017, PROC CVPR IEEE, P7435, DOI 10.1109/CVPR.2017.786; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Feichtenhofer Christoph, 2016, NIPS; Fernando B, 2016, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR.2016.212; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Gartner T., 2002, ICML, P179; Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337; Gould Stephen, 2016, ARXIV160705447; Hayat M, 2017, INT J COMPUT VISION, V123, P479, DOI 10.1007/s11263-017-1000-3; Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010; Hilliges, 2016, P BRIT MACH VIS C BM; Hu JF, 2018, LECT NOTES COMPUT SC, V11211, P346, DOI 10.1007/978-3-030-01234-2_21; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604; Kay W., 2017, ARXIV PREPRINT ARXIV; Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288; LAZIMY R, 1982, MATH PROGRAM, V22, P332, DOI 10.1007/BF01581047; Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Li WX, 2015, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2015.7299056; Li WX, 2013, IEEE I CONF COMP VIS, P2728, DOI 10.1109/ICCV.2013.339; Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Monfort Mathew, 2018, ARXIV180103150; Nowozin S., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383171; Pascanu R., 2013, P 30 INT C MACH LEAR, V28; Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806; Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39; Schindler K, 2008, PROC CVPR IEEE, P3025; Shah, 2012, ABS12120404 CORR; Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312; Sigurdsson GA, 2017, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR.2017.599; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Simonyan K., 2015, P INT C LEARN REPR, P1, DOI DOI 10.48550/ARXIV.1409.1556; Simonyan K, 2014, 2 INT C LEARN REPR I, P14; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sun C, 2014, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2014.329; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Vahdat A, 2013, IEEE I CONF COMP VIS, P1185, DOI 10.1109/ICCV.2013.463; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang J, 2018, PROC CVPR IEEE, P1149, DOI 10.1109/CVPR.2018.00126; Wang J, 2017, IEEE WINT CONF APPL, P168, DOI 10.1109/WACV.2017.26; WANG L, 2016, P EUR C COMP VIS; Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Willems G., 2009, BMVC, V2, P3; Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968; Yi Y, 2016, PATTERN RECOGN, V53, P148, DOI 10.1016/j.patcog.2015.11.022; Yu Felix X., 2013, P 30 INT C MACH LEAR, P504; Zepeda J, 2015, PROC CVPR IEEE, P3052, DOI 10.1109/CVPR.2015.7298924; Zhang DW, 2015, IEEE I CONF COMP VIS, P594, DOI 10.1109/ICCV.2015.75; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054	93	4	4	3	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2021	43	2					420	433		10.1109/TPAMI.2019.2937292	http://dx.doi.org/10.1109/TPAMI.2019.2937292			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PR6ZZ	31449006	Green Submitted			2022-12-18	WOS:000607383300004
J	Yun, JS; Sim, JY				Yun, Jae-Seong; Sim, Jae-Young			Virtual Point Removal for Large-Scale 3D Point Clouds with Multiple Glass Planes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Glass; Three-dimensional displays; Laser radar; Trajectory; Image edge detection; Surface emitting lasers; Buildings; Large-scale 3D point clouds; glass reflection; virtual point removal; trajectory estimation; LiDAR	SEPARATION; REFLECTIONS	Large-scale 3D point clouds (LS3DPCs) captured by terrestrial LiDAR scanners often include virtual points which are generated by glass reflection. The virtual points may degrade the performance of various computer vision techniques when applied to LS3DPCs. In this paper, we propose a virtual point removal algorithm for LS3DPCs with multiple glass planes. We first estimate multiple glass regions by modeling the reliability with respect to each glass plane, respectively, such that the regions are assigned high reliability when they have multiple echo pulses for each emitted laser pulse. Then we detect each point whether it is a virtual point or not. For a given point, we recursively traverse all the possible trajectories of reflection, and select the optimal trajectory which provides a point with a similar geometric feature to a given point at the symmetric location. We evaluate the performance of the proposed algorithm on various LS3DPC models with diverse numbers of glass planes. Experimental results show that the proposed algorithm estimates multiple glass regions faithfully and detects the virtual points successfully. Moreover, we also show that the proposed algorithm yields a much better performance of reflection artifact removal compared with the existing method qualitatively and quantitatively.	[Yun, Jae-Seong; Sim, Jae-Young] Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 44919, South Korea	Ulsan National Institute of Science & Technology (UNIST)	Sim, JY (corresponding author), Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 44919, South Korea.	jsyun@unist.ac.kr; jysim@unist.ac.kr		Yun, Jae-Seong/0000-0002-2511-0674	National Research Foundation of Korea (NRF) within the Ministry of Science and ICT (MSIT) [2017R1A2B4011970]	National Research Foundation of Korea (NRF) within the Ministry of Science and ICT (MSIT)(National Research Foundation of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported by the National Research Foundation of Korea (NRF) within the Ministry of Science and ICT (MSIT) under Grant 2017R1A2B4011970.	Agrawal A, 2005, ACM T GRAPHIC, V24, P828, DOI 10.1145/1073204.1073269; Ahn JK, 2015, IEEE J-STSP, V9, P422, DOI 10.1109/JSTSP.2014.2370752; [Anonymous], 1997, CMURIT9747 ROB I CAR; Arvanitopoulos N, 2017, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2017.190; BESAG J, 1986, J R STAT SOC B, V48, P259; Bishop C. M, 2006, PATTERN RECOGNITION, P431; Caltagirone L, 2017, IEEE INT VEH SYM, P1019; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Chen YF, 2017, IEEE INT C INT ROBOT, P1343; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351; Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224; Gai K, 2012, IEEE T PATTERN ANAL, V34, P19, DOI 10.1109/TPAMI.2011.87; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Guo XJ, 2014, PROC CVPR IEEE, P2195, DOI 10.1109/CVPR.2014.281; Han BJ, 2018, IEEE T IMAGE PROCESS, V27, P4873, DOI 10.1109/TIP.2018.2849880; Han BJ, 2017, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2017.412; Hellinger E, 1909, J REINE ANGEW MATH, V136, P210, DOI 10.1515/crll.1909.136.210; HOUSEHOLDER AS, 1958, J ACM, V5, P339, DOI 10.1145/320941.320947; Kong NJ, 2014, IEEE T PATTERN ANAL, V36, P209, DOI 10.1109/TPAMI.2013.45; Levin A, 2004, PROC CVPR IEEE, P306; Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106; Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346; Li Y, 2013, IEEE I CONF COMP VIS, P2432, DOI 10.1109/ICCV.2013.302; Lumberyard A., 2017, AMAZON LUMBERYARD BI; Marton ZC, 2010, IEEE INT C INT ROBOT, P3700, DOI 10.1109/IROS.2010.5650434; Nandoriya A, 2017, IEEE I CONF COMP VIS, P2430, DOI 10.1109/ICCV.2017.264; RIEGL, RIEGL VZ 400 3D TERR; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Schechner YY, 2000, INT J COMPUT VISION, V39, P25, DOI 10.1023/A:1008166017466; Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939; Shtrom E, 2013, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2013.446; Sinha SN, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185596; Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41; Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502; Wieschollek P, 2018, LECT NOTES COMPUT SC, V11217, P90, DOI 10.1007/978-3-030-01261-8_6; Xue TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766940; Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40; Yun JS, 2018, PROC CVPR IEEE, P4597, DOI 10.1109/CVPR.2018.00483; Yun JS, 2016, IEEE IMAGE PROC, P4062, DOI 10.1109/ICIP.2016.7533123; Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503	41	4	4	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2021	43	2					729	744		10.1109/TPAMI.2019.2933818	http://dx.doi.org/10.1109/TPAMI.2019.2933818			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PR6ZZ	31398108				2022-12-18	WOS:000607383300023
J	Chen, LX; Zheng, YQ; Shi, BX; Subpa-Asa, A; Sato, I				Chen, Lixiong; Zheng, Yinqiang; Shi, Boxin; Subpa-Asa, Art; Sato, Imari			A Microfacet-Based Model for Photometric Stereo with General Isotropic Reflectance	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Photometric stereo; reflectance model; microfacet theory	SURFACE; ILLUMINATION; IMAGES; SHAPE	This paper presents a precise, stable, and invertible reflectance model for photometric stereo. This microfacet-based model is applicable to all types of isotropic surface reflectance, covering cases from diffusion to specular reflections. We introduce a single variable to physically quantify the surface smoothness, and by monotonically sliding this variable between 0 and 1, our model enables a versatile representation that can smoothly transform between an ellipsoid of revolution and the equation for Lambertian reflectance. In the inverse domain, this model offers a compact and physically interpretable formulation, for which we introduce a fast and lightweight solver that allows accurate estimations for both surface smoothness and surface shape. Finally, extensive experiments on the appearances of synthesized and real objects evidence that this model is state-of-the-art in our off-the-shelf solution.	[Chen, Lixiong; Zheng, Yinqiang; Subpa-Asa, Art; Sato, Imari] Natl Inst Informat, Tokyo 1010003, Japan; [Shi, Boxin] Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Shi, Boxin] Peng Cheng Lab, Shenzhen 518040, Peoples R China	Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan; Peking University; Peng Cheng Laboratory	Shi, BX (corresponding author), Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.; Shi, BX (corresponding author), Peng Cheng Lab, Shenzhen 518040, Peoples R China.	lchen@nii.ac.jp; yazheng@nii.ac.jp; shiboxin@pku.edu.cn; art@nii.ac.jp; imarik@nii.ac.jp			JSPS KAKENHI [JP15H05918]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported in part by JSPS KAKENHI grant number JP15H05918.	Alldrin N, 2008, PROC CVPR IEEE, P2447; Alldrin NG, 2007, IEEE I CONF COMP VIS, P417; Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522; Ashikhmin M, 2000, COMP GRAPH, P65, DOI 10.1145/344779.344814; Ashikhmin M., 2007, DISTRIBUTION B UNPUB, V2; Bagher MM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2907941; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Beckmann P., 1987, SCATTERING ELECTROMA, P511; Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484; Bilgili A, 2011, COMPUT GRAPH FORUM, V30, P2427, DOI 10.1111/j.1467-8659.2011.02072.x; Blinn James F., 1977, COMPUT GRAPHICS-US, V11, P192, DOI [DOI 10.1145/965141.563893, 10.1145/965141, DOI 10.1145/965141]; BruceWalter Stephen R., 2007, P 18 EUR C REND TECH, P195, DOI DOI 10.2312/EGWR/EGSR07/195-206; Burley Brent, 2012, PHYSICALLY BASED SHA, V2012, P1; Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1; Chen L, 2017, IEEE I CONF COMP VIS, P3181, DOI 10.1109/ICCV.2017.343; Chung H.-S., 2008, P IEEE C COMP VIS PA; Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819; Dong Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778835; Dong Z, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2815618; Einarsson P., 2006, RENDERING TECH; Gardner A, 2003, ACM T GRAPHIC, V22, P749, DOI 10.1145/882262.882342; Georghiades AS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P816; Goldman D. B., 2009, IEEE T PATTERN ANAL, V32, P1060, DOI DOI 10.1109/TPAMI.2009.102; Goldman R, 2005, COMPUT AIDED GEOM D, V22, P632, DOI 10.1016/j.cagd.2005.06.005; HEITZ E, 2014, J COMPUTER GRAPHICS, V3, P48; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084; Horn B. K. P, 1970, SHAPE SHADING METHOD; Hui Z., 2015, P IEEE INT C COMP PH, P1; Hui Z, 2017, IEEE T PATTERN ANAL, V39, P2060, DOI 10.1109/TPAMI.2016.2623613; Ikehata S, 2014, PROC CVPR IEEE, P2187, DOI 10.1109/CVPR.2014.280; Ikehata S, 2014, IEEE T PATTERN ANAL, V36, P1816, DOI 10.1109/TPAMI.2014.2299798; Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691; Kajiya J. T., 1985, Computer Graphics, V19, P15, DOI 10.1145/325165.325167; Lafortune E. P., 1994, CW REPORTS DEP COMPU, P19; Lafortune E. P. F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P117, DOI 10.1145/258734.258801; Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751; Mallick SP, 2005, PROC CVPR IEEE, P619; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; Nam G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980220; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; Ngan A., 2005, RENDERING TECH, V2005, P2; Oren M., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P239, DOI 10.1145/192161.192213; PHARR M., 2016, PHYS BASED RENDERING, V3rd; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204; Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66; SATO Y, 1994, J OPT SOC AM A, V11, P2990, DOI 10.1364/JOSAA.11.002990; SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233; Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222; Shi BX, 2012, LECT NOTES COMPUT SC, V7574, P455, DOI 10.1007/978-3-642-33712-3_33; Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196; Shi BX, 2012, PROC CVPR IEEE, P230, DOI 10.1109/CVPR.2012.6247680; SMITH BG, 1967, IEEE T ANTENN PROPAG, VAP15, P668, DOI 10.1109/TAP.1967.1138991; Taniai T, 2018, PR MACH LEARN RES, V80; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Tozza S, 2016, J MATH IMAGING VIS, V56, P57, DOI 10.1007/s10851-016-0633-0; TROWBRIDGE TS, 1975, J OPT SOC AM, V65, P531, DOI 10.1364/JOSA.65.000531; Vickers GT, 1996, POWDER TECHNOL, V86, P195, DOI 10.1016/0032-5910(95)03049-2; Walter B., 2016, ELLIPSOID NORMAL DIS; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Wu Z, 2013, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2013.197; Yan LQ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925915	65	4	4	1	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2021	43	1					48	61		10.1109/TPAMI.2019.2927909	http://dx.doi.org/10.1109/TPAMI.2019.2927909			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PC7WN	31295106				2022-12-18	WOS:000597206900004
J	Parashar, S; Pizarro, D; Bartoli, A				Parashar, Shaifali; Pizarro, Daniel; Bartoli, Adrien			Local Deformable 3D Reconstruction with Cartan's Connections	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deformable models; Three-dimensional displays; Surface reconstruction; Image reconstruction; Solid modeling; Mathematical model; Computer vision; 3D reconstruction; NRSfM; deformable reconstruction; connections; 3D computer vision	STRUCTURE-FROM-MOTION; SHAPE; TEMPLATE; SURFACES	3D reconstruction of deformable objects using inter-image visual motion from monocular images has been studied under Shape-from-Template (SfT) and Non-Rigid Structure-from-Motion (NRSfM). Most methods have been developed for simple deformation models, primarily isometry. They may treat a surface as a discrete set of points and draw constraints from the points only or they may use a non-parametric representation and use both points and differentials to express constraints. We propose a differential framework based on Cartans theory of connections and moving frames. It is applicable to SfT and NRSfM, and to deformation models other than isometry. It utilises infinitesimal-level assumptions on the surfaces geometry and mappings. It has the following properties. 1) It allows one to derive existing solutions in a simpler way. 2) It models SfT and NRSfM in a unified way. 3) It allows us to introduce a new skewless deformation model and solve SfT and NRSfM for it. 4) It facilitates a generic solution to SfT which does not require deformation modeling. Our framework is complete: it solves deformable 3D reconstruction for a whole class of algebraic deformation models including isometry. We compared our solutions with the state-of-the-art methods and show that ours outperform in terms of both accuracy and computation time.	[Parashar, Shaifali; Pizarro, Daniel; Bartoli, Adrien] Univ Auvergne, CNRS, EnCoV, F-63000 Clermont Ferrand, France; [Pizarro, Daniel] Univ Alcala, GEINTRA, Alcala De Henares 28801, Spain	Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA); Universidad de Alcala	Parashar, S (corresponding author), Univ Auvergne, CNRS, EnCoV, F-63000 Clermont Ferrand, France.	shaifali.parashar@gmail.com; Dani.Pizarro@gmail.com; adrien.bartoli@gmail.com		Pizarro, Daniel/0000-0003-0622-4884	EU's FP7 through ERC [307483 FLEXABLE]; Spanish Ministry of Economy, Industry and Competitiveness [TIN2016-80939-R]	EU's FP7 through ERC; Spanish Ministry of Economy, Industry and Competitiveness(Spanish Government)	This research has received funding from the EU's FP7 through the ERC grant 307483 FLEXABLE, the Spanish Ministry of Economy, Industry and Competitiveness under project ARTEMISA (TIN2016-80939-R).	Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Agudo A, 2015, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2015.7298830; Akhter Ijaz, 2009, P NIPS; Bartoli A, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P5, DOI [10.1109/ISMAR-Adjunct.2016.19, 10.1109/ISMAR-Adjunct.2016.0026]; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Brunet F, 2014, COMPUT VIS IMAGE UND, V125, P138, DOI 10.1016/j.cviu.2014.04.003; Cagniart C, 2010, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2010.5539814; Cartan E., 1970, DIFFERENTIAL FORMS; Cartan E., 1923, ANN LECOLE NORMALE S, V40, P325, DOI DOI 10.24033/ASENS.529; Cartan E., 1924, B SMF, V2, P205, DOI 10.24033/bsmf.1053; Cartan E., 1926, ACTA MATH-DJURSHOLM, V48, P1, DOI [10.1007/BF02629755, 10.1007/BF02629755.f1, DOI 10.1007/BF02629755.F1]; Cartan E, 1937, THEORIE GROUPES FINI; Chhatkuli A., 2014, P BRIT MACH VIS C; Chhatkuli A, 2018, IEEE T PATTERN ANAL, V40, P2428, DOI 10.1109/TPAMI.2017.2762669; Chhatkuli A, 2017, IEEE T PATTERN ANAL, V39, P833, DOI 10.1109/TPAMI.2016.2562622; Dai YC, 2014, INT J COMPUT VISION, V107, P101, DOI 10.1007/s11263-013-0684-2; Ngo DT, 2015, IEEE I CONF COMP VIS, P2273, DOI 10.1109/ICCV.2015.262; de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697; Del Bue A., 2004, P IEEE C COMP VIS PA; Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319; Gumerov N, 2004, LECT NOTES COMPUT SC, V3023, P482; Haouchine N, 2014, INT SYM MIX AUGMENT, P229, DOI 10.1109/ISMAR.2014.6948432; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Henrion D, 2003, ACM T MATH SOFTWARE, V29, P165, DOI 10.1145/779359.779363; Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22; Kock A., 2010, SYNTHETIC GEOMETRY M; Malti A, 2017, PROC CVPR IEEE, P143, DOI 10.1109/CVPR.2017.23; Malti A, 2013, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2013.200; Moreno-Noguer F, 2009, PROC CVPR IEEE, P1842, DOI 10.1109/CVPRW.2009.5206758; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Olsen SI, 2008, J MATH IMAGING VIS, V31, P233, DOI 10.1007/s10851-007-0060-3; Ozgur E, 2017, INT J COMPUT VISION, V123, P184, DOI 10.1007/s11263-016-0968-4; Parashar S, 2018, IEEE T PATTERN ANAL, V40, P2442, DOI 10.1109/TPAMI.2017.2760301; Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8; Pizarro D, 2016, INT J COMPUT VISION, V119, P93, DOI 10.1007/s11263-016-0882-9; Pumarola A, 2018, PROC CVPR IEEE, P4681, DOI 10.1109/CVPR.2018.00492; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Salzmann M, 2011, IEEE T PATTERN ANAL, V33, P931, DOI 10.1109/TPAMI.2010.158; Starck J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P915; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; Taylor J, 2010, PROC CVPR IEEE, P2761, DOI 10.1109/CVPR.2010.5540002; Tewari A, 2020, IEEE T PATTERN ANAL, V42, P357, DOI 10.1109/TPAMI.2018.2876842; TORRESANI L, 2001, P IEEE C COMP VIS PA; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Varol A, 2012, PROC CVPR IEEE, P2248, DOI 10.1109/CVPR.2012.6247934; Varol A, 2009, IEEE I CONF COMP VIS, P1811, DOI 10.1109/ICCV.2009.5459403; Vicente S, 2012, LECT NOTES COMPUT SC, V7574, P426, DOI 10.1007/978-3-642-33712-3_31; Zhou XW, 2019, IEEE T PATTERN ANAL, V41, P901, DOI 10.1109/TPAMI.2018.2816031; Zollhofer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386	52	4	4	1	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2020	42	12					3011	3026		10.1109/TPAMI.2019.2920821	http://dx.doi.org/10.1109/TPAMI.2019.2920821			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	OP2KH	31180886				2022-12-18	WOS:000587912800004
J	Eftekhari, A; Hauser, RA; Grammenos, A				Eftekhari, Armin; Hauser, Raphael A.; Grammenos, Andreas			MOSES: A Streaming Algorithm for Linear Dimensionality Reduction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Dimensionality reduction; Estimation; Optimization; Approximation algorithms; Principal component analysis; Ear; Principal component analysis; linear dimensionality reduction; subspace identification; streaming algorithms; non-convex optimisation	FREQUENT DIRECTIONS; MATRIX; SUBSPACE; APPROXIMATION; RECOVERY; PCA	This paper introduces Memory-limited Online Subspace Estimation Scheme (MOSES) for both estimating the principal components of streaming data and reducing its dimension. More specifically, in various applications such as sensor networks, the data vectors are presented sequentially to a user who has limited storage and processing time available. Applied to such problems, MOSES can provide a running estimate of leading principal components of the data that has arrived so far and also reduce its dimension. MOSES generalises the popular incremental Singular Vale Decomposition (iSVD) to handle thin blocks of data, rather than just vectors. This minor generalisation in part allows us to complement MOSES with a comprehensive statistical analysis, thus providing the first theoretically-sound variant of iSVD, which has been lacking despite the empirical success of this method. This generalisation also enables us to concretely interpret MOSES as an approximate solver for the underlying non-convex optimisation program. We find that MOSES consistently surpasses the state of the art in our numerical experiments with both synthetic and real-world datasets, while being computationally inexpensive.	[Eftekhari, Armin] Ecole Polytech Fed Lausanne, Inst Elect Engn, CH-1015 Lausanne, Switzerland; [Hauser, Raphael A.] Univ Oxford, Math Inst, Oxford OX1 2JD, England; [Hauser, Raphael A.] Alan Turing Inst London, London NW1 2DB, England; [Grammenos, Andreas] Univ Cambridge, Dept Comp Sci, Cambridge CB2 1TN, England; [Grammenos, Andreas] Alan Turing Inst London, London NW1 2DB, England	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Oxford; University of Cambridge	Eftekhari, A (corresponding author), Ecole Polytech Fed Lausanne, Inst Elect Engn, CH-1015 Lausanne, Switzerland.	armin.eftekhari@epfl.ch; hauser@maths.ox.ac.uk; ag926@cl.cam.ac.uk		Hauser, Raphael/0000-0002-1166-5329	Alan Turing Institute under the EPSRC [EP/N510129/1, TU/C/000003]; Turing Seed Funding grant [SF019]; EPSRC [EP/N510129/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Turing Seed Funding grant; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	For this project, AE was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1 and also by the Turing Seed Funding grant SF019. RAH is supported by EPSRC grant EP/N510129/1. AG is supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1 and TU/C/000003. AE is grateful to Chinmay Hedge, Mike Wakin, Jared Tanner, and Mark Davenport for insightful suggestions and valuable feedback. Parts of this project were completed when AE was a Leibniz Fellow at Oberwolfach Research Institute for Mathematics and AE is extremely grateful for their hospitality.	[Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; Ardekani BA, 1999, IEEE T MED IMAGING, V18, P101, DOI 10.1109/42.759109; Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976; Balzano L, 2013, 2013 IEEE 5TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP 2013), P1, DOI 10.1109/CAMSAP.2013.6713992; Berry M.W., 2005, HDB PARALLEL COMPUTI, P133; Boutsidis C., 2015, P 26 ANN ACM SIAM S, P887, DOI DOI 10.1137/1.9781611973730.61; Brand M, 2006, LINEAR ALGEBRA APPL, V415, P20, DOI 10.1016/j.laa.2005.07.021; Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707; BUNCH JR, 1978, NUMER MATH, V31, P31, DOI 10.1007/BF01396012; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chiu JW, 2013, SIAM J MATRIX ANAL A, V34, P1361, DOI 10.1137/110852310; COMON P, 1990, P IEEE, V78, P1327, DOI 10.1109/5.58320; Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100; De Sa Christopher, 2014, ARXIV14111134; Desai A, 2016, IEEE T KNOWL DATA EN, V28, P1678, DOI 10.1109/TKDE.2016.2539943; Deshpande A., 2004, VLDB, DOI DOI 10.1016/B978-012088469-8.50053-X; Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367; Eftekhari A., 2018, ARXIV161200904; Eftekhari A., 2016, ARXIV161206339; Eftekhari A, 2018, INF INFERENCE, V7, P581, DOI 10.1093/imaiai/iax020; Eftekhari A, 2018, IEEE T INFORM THEORY, V64, P4044, DOI 10.1109/TIT.2018.2816685; Eftekhari A, 2017, IEEE SIGNAL PROC LET, V24, P872, DOI 10.1109/LSP.2017.2684784; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Gilbert A. C., 2012, ARXIV12110361; Gittens A, 2016, J MACH LEARN RES, V17; Hastie T, 2013, ELEMENTS STAT LEARNI; Hauser R., 2017, ARXIV171010124; Hauser R. A., 2018, P 24 ACM SIGKDD INT; Hauser R. H., 2018, ARXIV180507459; Iwen MA, 2016, SIAM J MATRIX ANAL A, V37, P1699, DOI 10.1137/16M1058467; Jain Prateek, 2016, C LEARN THEOR, P1147; Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544; Kim KI, 2005, IEEE T PATTERN ANAL, V27, P1351, DOI 10.1109/TPAMI.2005.181; Krasulina T.P., 1969, USSR COMP MATH MATH, V9, P189, DOI DOI 10.1016/0041-5553(69)90135-9; Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899; Li YM, 2004, PATTERN RECOGN, V37, P1509, DOI 10.1016/j.patcog.2003.11.010; Luo L, 2019, J MACH LEARN RES, V20, P1; Mirsky L., 1966, Q J MATH OXFORD, V11, P1156; Mitliagkas I., 2014, STREAMING PCA MANY M; Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR, P2886; OJA E, 1985, J MATH ANAL APPL, V106, P69, DOI 10.1016/0022-247X(85)90131-3; Oja E., 1983, SUBSPACE METHODPAT; Pourkamali-Anaraki F., 2016, ARXIV161206470; Roweis S, 1998, ADV NEUR IN, V10, P626; SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tong L, 1998, P IEEE, V86, P1951, DOI 10.1109/5.720247; Tropp J., 2017, P NEUR INF PROC SYST, P1225; Van Overschee P., 2012, THEORY IMPLEMENTATIO; Vershynin R, 2012, J THEOR PROBAB, V25, P655, DOI 10.1007/s10959-010-0338-z; Vidal R., 2016, INTERDISCIP APPL MAT, V40, DOI [10.1007/978-0-387-87811-9_2, DOI 10.1007/978-0-387-87811-9_2]; Warmuth MK, 2008, J MACH LEARN RES, V9, P2287; Zhang DJ, 2016, JMLR WORKSH CONF PRO, V51, P1460	55	4	4	2	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2020	42	11					2901	2911		10.1109/TPAMI.2019.2919597	http://dx.doi.org/10.1109/TPAMI.2019.2919597			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NX0AD	31150336	Green Submitted			2022-12-18	WOS:000575381000012
J	Joo, K; Oh, TH; Kweon, IS; Bazin, JC				Joo, Kyungdon; Oh, Tae-Hyun; Kweon, In So; Bazin, Jean-Charles			Globally Optimal Inlier Set Maximization for Atlanta World Understanding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Estimation; Gravity; Cameras; Lasers; Optimization; Layout; Atlanta frame; RGB-D image; branch-and-bound; global optimization; scene understanding	CONSENSUS; SPACE	In this work, we describe man-made structures via an appropriate structure assumption, called the Atlanta world assumption, which contains a vertical direction (typically the gravity direction) and a set of horizontal directions orthogonal to the vertical direction. Contrary to the commonly used Manhattan world assumption, the horizontal directions in Atlanta world are not necessarily orthogonal to each other. While Atlanta world can encompass a wider range of scenes, this makes the search space much larger and the problem more challenging. Our input data is a set of surface normals, for example, acquired from RGB-D cameras or 3D laser scanners, as well as lines from calibrated images. Given this input data, we propose the first globally optimal method of inlier set maximization for Atlanta direction estimation. We define a novel search space for Atlanta world, as well as its parametrization, and solve this challenging problem using a branch-and-bound (BnB) framework. To alleviate the computational bottleneck in BnB, i.e., the bound computation, we present two bound computation strategies: rectangular bound and slice bound in an efficient measurement domain, i.e., the extended Gaussian image (EGI). In addition, we propose an efficient two-stage method which automatically estimates the number of horizontal directions of a scene. Experimental results with synthetic and real-world datasets have successfully confirmed the validity of our approach.	[Joo, Kyungdon; Kweon, In So; Bazin, Jean-Charles] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea; [Oh, Tae-Hyun] MIT, Comp Sci & Artificial Intelligence Lab CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Bazin, Jean-Charles] Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Daejeon 34141, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Massachusetts Institute of Technology (MIT); Korea Advanced Institute of Science & Technology (KAIST)	Joo, K (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.	kdjoo369@gmail.com; taehyun@csail.mit.edu; iskweon@kaist.ac.kr; bazinjc@kaist.ac.kr	Oh, Tae-Hyun/D-7854-2016	Oh, Tae-Hyun/0000-0003-0468-1571	Technology Innovation Program [2017-10069072]; Ministry of Trade, Industry & Energy (MOTIE, Korea); National Research Foundation of Korea [NRF-2017R1C1B5077030]; Korean government (MSIT)	Technology Innovation Program; Ministry of Trade, Industry & Energy (MOTIE, Korea); National Research Foundation of Korea(National Research Foundation of Korea); Korean government (MSIT)(Korean Government)	This work was partially supported by the Technology Innovation Program (No. 2017-10069072) funded by the Ministry of Trade, Industry & Energy (MOTIE, Korea). This research was also partially supported by the National Research Foundation of Korea (NRF-2017R1C1B5077030) funded by the Korean government (MSIT).	[Anonymous], 1987, MAP PROJECTIONS WORK; Antunes M, 2013, PROC CVPR IEEE, P1336, DOI 10.1109/CVPR.2013.176; Balakrishnan V., 1991, International Journal of Robust and Nonlinear Control, V1, P295, DOI 10.1002/rnc.4590010404; Balakrishnan V., 1991, INT J ROBUST NONLINE, V1, P295; Bazin JC, 2010, COMPUT VIS IMAGE UND, V114, P254, DOI 10.1016/j.cviu.2009.04.006; Bazin JC, 2013, IEEE T PATTERN ANAL, V35, P1565, DOI 10.1109/TPAMI.2012.264; Bazin JC, 2012, PROC CVPR IEEE, P638, DOI 10.1109/CVPR.2012.6247731; Bazin JC, 2012, INT J ROBOT RES, V31, P63, DOI 10.1177/0278364911421954; Bazin Jean-Charles, 2012, P AS C COMP VIS, P2; Coughlan J., 2000, P 13 INT C NEUR INF, P809; Denis P, 2008, LECT NOTES COMPUT SC, V5303, P197, DOI 10.1007/978-3-540-88688-4_15; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Ghanem B, 2015, PROC CVPR IEEE, P3772, DOI 10.1109/CVPR.2015.7299001; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054; Hartley R., 2004, ROBOTICA; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Hedau V, 2010, LECT NOTES COMPUT SC, V6316, P224, DOI 10.1007/978-3-642-15567-3_17; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; Horst R., 2006, GLOBAL OPTIMIZATION; Jia ZY, 2013, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2013.8; Joo K, 2018, PROC CVPR IEEE, P5726, DOI 10.1109/CVPR.2018.00600; Joo K, 2019, IEEE T PATTERN ANAL, V41, P682, DOI 10.1109/TPAMI.2018.2799944; Joo K, 2016, PROC CVPR IEEE, P1763, DOI 10.1109/CVPR.2016.195; Kim P, 2018, PROC CVPR IEEE, P4673, DOI 10.1109/CVPR.2018.00491; Li H, 2018, IEEE INT CONF ROBOT, P2518; Li HD, 2009, IEEE I CONF COMP VIS, P1074, DOI 10.1109/ICCV.2009.5459398; Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179; Osswald Stefan, 2011, 2011 11th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2011), P93, DOI 10.1109/Humanoids.2011.6100836; Posada D, 2004, SYST BIOL, V53, P793, DOI 10.1080/10635150490522304; Schindler G, 2004, PROC CVPR IEEE, P203; Seo S, 2018, PROCEEDINGS OF 2018 THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (CSAI 2018) / 2018 THE 10TH INTERNATIONAL CONFERENCE ON INFORMATION AND MULTIMEDIA TECHNOLOGY (ICIMT 2018), P350, DOI 10.1145/3297156.3297244; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Straub J, 2018, IEEE T PATTERN ANAL, V40, P235, DOI 10.1109/TPAMI.2017.2662686; Straub J, 2015, IEEE INT C INT ROBOT, P1913, DOI 10.1109/IROS.2015.7353628; Straub J, 2014, PROC CVPR IEEE, P3770, DOI 10.1109/CVPR.2014.488; Taylor C.J., 2013, ROBOT SCI SYST, V8; Triebel R, 2005, IEEE INT CONF ROBOT, P4437; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Zhang CX, 2010, LECT NOTES COMPUT SC, V6314, P708, DOI 10.1007/978-3-642-15561-1_51	42	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2020	42	10					2656	2669		10.1109/TPAMI.2019.2909863	http://dx.doi.org/10.1109/TPAMI.2019.2909863			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NL5QY	30969915				2022-12-18	WOS:000567471300024
J	Ren, ZL; Sudderth, EB				Ren, Zhile; Sudderth, Erik B.			Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and Indoor Scene Layouts	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Layout; Two dimensional displays; Solid modeling; Feature extraction; Detectors; Object detection; 3D scene understanding; object detection; room layout estimation; structured prediction; cascaded classification		We develop new representations and algorithms for three-dimensional (3D) object detection and spatial layout prediction in cluttered indoor scenes. We first propose a clouds of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image gradients. To better represent the 3D visual styles of large objects and provide contextual cues to improve the detection of small objects, we introduce latent support surfaces. We then propose a "Manhattan voxel" representation which better captures the 3D room layout geometry of common indoor environments. Effective classification rules are learned via a latent structured prediction framework. Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses that exceed the state-of-the-art on the SUN RGB-D database.	[Ren, Zhile] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA; [Sudderth, Erik B.] Univ Calif Irvine, Sch Informat & Comp Sci, Irvine, CA 92697 USA	University System of Georgia; Georgia Institute of Technology; University of California System; University of California Irvine	Ren, ZL (corresponding author), Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.	jrenzhile@gmail.com; sudderth@uci.edu	Ren, Zhile/AAD-4844-2019	Ren, Zhile/0000-0002-0302-795X; Sudderth, Erik/0000-0002-0595-9726	Office of Naval Research (ONR) [N00014-13-1-0644, N00014-17-1-2094]; Brown University Center for Vision Research	Office of Naval Research (ONR)(Office of Naval Research); Brown University Center for Vision Research	This research is supported in part by the Office of Naval Research (ONR) under Award Numbers N00014-13-1-0644 and N00014-17-1-2094, and by a pilot grant from the Brown University Center for Vision Research.	Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Bai JJ, 2012, PROC CVPR IEEE, P1728, DOI 10.1109/CVPR.2012.6247868; Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Buch N., 2009, P BMVC, P1, DOI DOI 10.5244/C.23.15; Chen CY, 2017, LECT NOTES COMPUT SC, V10115, P214, DOI 10.1007/978-3-319-54193-8_14; Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Coughlan J.M., 1999, P ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Del Pero L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2009, DOI 10.1109/CVPR.2011.5995737; Deng Z, 2017, PROC CVPR IEEE, P398, DOI 10.1109/CVPR.2017.50; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fidler S., 2012, ADV NEURAL INFORM PR; Fouhey DF, 2014, LECT NOTES COMPUT SC, V8694, P687, DOI 10.1007/978-3-319-10599-4_44; Geiger A, 2015, LECT NOTES COMPUT SC, V9358, P183, DOI 10.1007/978-3-319-24947-6_15; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Guo RQ, 2013, IEEE I CONF COMP VIS, P2144, DOI 10.1109/ICCV.2013.266; Gupta A, 2010, LECT NOTES COMPUT SC, V6314, P482, DOI 10.1007/978-3-642-15561-1_35; Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hedau V, 2010, LECT NOTES COMPUT SC, V6316, P224, DOI 10.1007/978-3-642-15567-3_17; Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411; Heitz Geremy, 2009, ADV NEURAL INFORM PR, V21, P641; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166; Jia ZY, 2013, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2013.8; Jiang H, 2013, PROC CVPR IEEE, P2171, DOI 10.1109/CVPR.2013.282; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kuo WC, 2015, IEEE I CONF COMP VIS, P2479, DOI 10.1109/ICCV.2015.285; Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Lee CY, 2017, IEEE I CONF COMP VIS, P4875, DOI 10.1109/ICCV.2017.521; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Lim JJ, 2014, LECT NOTES COMPUT SC, V8694, P478, DOI 10.1007/978-3-319-10599-4_31; Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372; Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Mallya A, 2015, IEEE I CONF COMP VIS, P936, DOI 10.1109/ICCV.2015.113; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959; Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Payet N, 2011, IEEE I CONF COMP VIS, P983, DOI 10.1109/ICCV.2011.6126342; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Ren Y., 2012, J VISUAL COMMUN IMAG, V34, P2189; Ren Z, 2016, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2016.169; Ren ZZ, 2018, PROC CVPR IEEE, P762, DOI [10.1109/CVPR.2018.00086, 10.1109/CVPR.2018.00104]; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Russell BC, 2009, PROC CVPR IEEE, P2703; Scherer M, 2010, WSCG 2010: FULL PAPERS PROCEEDINGS, P41; Schwing AG, 2013, IEEE I CONF COMP VIS, P353, DOI 10.1109/ICCV.2013.51; Schwing AG, 2012, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2012.6248006; Shao TJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661288; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tulsiani S, 2017, IEEE T PATTERN ANAL, V39, P719, DOI 10.1109/TPAMI.2016.2574713; Vedaldi A., 2009, P ADV NEUR INF PROC, P1928; Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xiang Y, 2015, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2015.7298800; Xiao J, 2012, ADV NEURAL INFORM PR, P746; Yao J, 2012, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2012.6247739; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Zhang J, 2013, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2013.161; Zhang YD, 2017, IEEE I CONF COMP VIS, P1201, DOI 10.1109/ICCV.2017.135; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219	85	4	4	3	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2020	42	10					2670	2683		10.1109/TPAMI.2019.2923201	http://dx.doi.org/10.1109/TPAMI.2019.2923201			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NL5QY	31217095	Green Submitted			2022-12-18	WOS:000567471300025
J	Cheng, KHM; Kumar, A				Cheng, Kevin H. M.; Kumar, Ajay			Contactless Biometric Identification Using 3D Finger Knuckle Patterns	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Two dimensional displays; Thumb; Fingerprint recognition; Databases; Feature extraction; Biometrics; finger knuckle identification; 3d finger dorsal matching; contactless hand identification	INDIVIDUALITY; RECOGNITION; SURFACE; SHAPE	Study on finger knuckle patterns has attracted increasing attention for the automated biometric identification. However, finger knuckle pattern is essentially a 3D biometric identifier and the usage or availability of only 2D finger knuckle databases in the literature is the key limitation to avail full potential from this biometric identifier. This paper therefore introduces (first) contactless 3D finger knuckle database in public domain, which is acquired from 130 different subjects in two-session imaging using photometric stereo approach. This paper investigates on the 3D information from the finger knuckle patterns and introduces a new feature descriptor to extract discriminative 3D features for more accurate 3D finger knuckle matching. An individuality model for the proposed feature descriptor is also presented. Comparative experimental results using the state-of-the-art feature extraction methods on this challenging 3D finger knuckle database validate the effectiveness of our approach. Although our feature descriptor is designed for 3D finger knuckle patterns, it is also attractive for other hand-based biometric identifiers with similar patterns such as the palmprint and fingerprint. This observation is validated from the outperforming results, using the state-of-the-art pixel-wise 3D palmprint and 3D fingerprint feature descriptors, on other publicly available datasets.	[Cheng, Kevin H. M.; Kumar, Ajay] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China	Hong Kong Polytechnic University	Cheng, KHM (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China.	kevin.hm.cheng@connect.polyu.hk; Ajay.Kumar@polyu.edu.hk			General Research Fund from Research Grant Council of Hong Kong [PolyU 152192/17E]	General Research Fund from Research Grant Council of Hong Kong	This work is supported by General Research Fund from Research Grant Council of Hong Kong, project number PolyU 152192/17E.	[Anonymous], 2002, SUMMARY NIST STANDAR; [Anonymous], 2012, ROL BIOM TECHN AADHA; [Anonymous], 2018, [No title captured], DOI DOI 10.1109/IWBF.2018.8401566; Aoyama S., 2013, P IEEE 6 INT C BIOM, P1; Bo Li, 2015, 2015 Asia-Pacific Microwave Conference (APMC). Proceedings, P1, DOI 10.1109/APMC.2015.7411621; Bolle RM, 2004, INT C PATT RECOG, P927, DOI 10.1109/ICPR.2004.1334411; Chang KI, 2005, IEEE T PATTERN ANAL, V27, P619, DOI 10.1109/TPAMI.2005.70; Dass SC, 2010, IEEE T INF FOREN SEC, V5, P62, DOI 10.1109/TIFS.2009.2039598; Daugman J, 2003, PATTERN RECOGN, V36, P279, DOI 10.1016/S0031-3203(02)00030-4; DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676; Daugman J, 2016, IEEE T INF FOREN SEC, V11, P400, DOI 10.1109/TIFS.2015.2500196; Dorai C, 1997, IEEE T PATTERN ANAL, V19, P1115, DOI 10.1109/34.625113; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Grother P., 2017, IJB FACE IDENTIFICAT; Ikehata S, 2014, PROC CVPR IEEE, P2187, DOI 10.1109/CVPR.2014.280; Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691; Ito K, 2008, IEICE T FUND ELECTR, VE91A, P1023, DOI 10.1093/ietfec/e91-a.4.1023; Jaswal G, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2938727; Kanhangad V, 2011, IEEE T INF FOREN SEC, V6, P1014, DOI 10.1109/TIFS.2011.2121062; KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F; Kumar A., 2018, CONTACTLESS 3D FINGE, DOI [10.1007/978-3-319-67681-4, DOI 10.1007/978-3-319-67681-4]; Kumar A, 2016, IEEE T INF FOREN SEC, V11, P2338, DOI 10.1109/TIFS.2016.2574309; Kumar A, 2015, IEEE T PATTERN ANAL, V37, P681, DOI 10.1109/TPAMI.2014.2339818; Kumar A, 2014, IEEE T INF FOREN SEC, V9, P1288, DOI 10.1109/TIFS.2014.2328869; Kumar A, 2013, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2013.441; Kumar A, 2009, IEEE T INF FOREN SEC, V4, P98, DOI 10.1109/TIFS.2008.2011089; Maltoni D., 2009, HDB FINGERPRINT RECO; Pankanti S, 2002, IEEE T PATTERN ANAL, V24, P1010, DOI 10.1109/TPAMI.2002.1023799; Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; Sricharan KK, 2006, PROC SPIE, V6202, DOI 10.1117/12.666438; Srihari SN, 2002, J FORENSIC SCI, V47, P856; Tistarelli M., 2019, HDB REMOTE BIOMETRIC; Wang YZ, 2009, BIOFABRICATION, V1, DOI 10.1088/1758-5082/1/1/015001; Woodard DL, 2005, COMPUT VIS IMAGE UND, V100, P357, DOI 10.1016/j.cviu.2005.06.003; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Yan P, 2007, IEEE T PATTERN ANAL, V29, P1297, DOI 10.1109/TPAMI.2007.1067; Zhang L, 2010, PATTERN RECOGN, V43, P2560, DOI 10.1016/j.patcog.2010.01.020; Zheng Q, 2016, IEEE T PATTERN ANAL, V38, P1272, DOI 10.1109/TPAMI.2015.2509968; Zheng Q, 2016, IEEE T INF FOREN SEC, V11, P633, DOI 10.1109/TIFS.2015.2503265; Zhu LQ, 2010, PATTERN RECOGN LETT, V31, P1641, DOI 10.1016/j.patrec.2010.05.010	41	4	4	3	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG. 1	2020	42	8					1868	1883		10.1109/TPAMI.2019.2904232	http://dx.doi.org/10.1109/TPAMI.2019.2904232			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MF5XR					2022-12-18	WOS:000545415400005
J	Ye, HJ; Zhan, DC; Li, N; Jiang, Y				Ye, Han-Jia; Zhan, De-Chuan; Li, Nan; Jiang, Yuan			Learning Multiple Local Metrics: Global Consideration Helps	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Redundancy; Euclidean distance; Task analysis; Training; Complexity theory; Semantics; Distance metric learning; similarity measures; multi-metric learning; global and local; generalization analysis		Learning distance metric between objects provides a better measurement for their relative comparisons. Due to the complex properties inside or between heterogeneous objects, multiple local metrics become an essential representation tool to depict various local characteristics of examples. Different from existing methods building more than one local metric directly, however in this paper, we emphasize the effect of the global metric when generating those local ones. Since local metrics can be considered as types of amendments which describe the biases towards localities based on some commonly shared characteristic, it is expected that the performance of every single local metric for a specified locality can be "lifted" when learning with the global jointly. Following this consideration, we propose the Local metrIcs Facilitated Transformation (Lift) framework, where an adaptive number of local transformations are constructed with the help of their global counterpart. Generalization analyses not only reveal the relationship between the global and local metrics but also indicate when and why the framework works theoretically. In the implementation of Lift, locality anchored centers assist the decomposition of multiple local views, and a diversity regularizer is proposed to reduce the redundancy among biases. Empirical classification comparisons reveal the superiority of the Lift idea. Numerical and visualization investigations on different domains validate its adaptability and comprehensibility as well.	[Ye, Han-Jia; Zhan, De-Chuan; Jiang, Yuan] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China; [Li, Nan] Alibaba Grp, DAMO Acad, Hangzhou 311121, Peoples R China	Nanjing University; Alibaba Group	Zhan, DC (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.	yehj@lamda.nju.edu.cn; zhandc@lamda.nju.edu.cn; linan.sz@gmail.com; jiangy@lamda.nju.edu.cn	jiang, anyi/GPT-0379-2022		National Key R&D Program of China [2018YFB1004300]; NSFC [61773198, 61632004]; NSFC-NRF [61861146001]; Collaborative Innovation Center ofNovel Software Technology and Industrialization	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); NSFC-NRF; Collaborative Innovation Center ofNovel Software Technology and Industrialization	This research was supported by National Key R&D Program of China (2018YFB1004300), NSFC (61773198, 61632004), NSFC-NRF (61861146001), Collaborative Innovation Center ofNovel Software Technology and Industrialization.	[Anonymous], 2016, P ADV NEURAL INFORM; [Anonymous], 2013, P AAAI; [Anonymous], 2004, ICML, DOI [10.1145/1015330.1015376, DOI 10.1145/1015330.1015376]; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bellet A., 2015, METRIC LEARNING; Bhattarai B, 2016, PROC CVPR IEEE, P4226, DOI 10.1109/CVPR.2016.458; Bishop C.M, 2006, PATTERN RECOGN; Bohne J, 2014, LECT NOTES COMPUT SC, V8690, P679, DOI 10.1007/978-3-319-10605-2_44; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Boyd S, 2004, CONVEX OPTIMIZATION; Cao Q, 2016, MACH LEARN, V102, P115, DOI 10.1007/s10994-015-5499-7; Changpinyo S, 2013, ADV NEURAL INFORM PR, P1511; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Everingham M, 2009, PASCAL VISUAL OBJECT; Fetaya E, 2015, PR MACH LEARN RES, V37, P162; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; Hu JJ, 2015, J SEISMOL, V20, P1; Jin R., 2009, ADV NEURAL INFORM PR, V22; Kong DG, 2016, AAAI CONF ARTIF INTE, P1765; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Kuzborskij I, 2017, MACH LEARN, V106, P171, DOI 10.1007/s10994-016-5594-4; Law MT, 2017, INT J COMPUT VISION, V121, P65, DOI 10.1007/s11263-016-0923-4; Li Y, 2006, INT J APPROX REASON, V41, P229, DOI 10.1016/j.ijar.2005.06.019; Lichman M., 2013, UCI MACHINE LEARNING; Maurer A, 2008, J MACH LEARN RES, V9, P1049; McFee B., 2010, P 27 INT C MACHINE L, P775; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Noh Yung-kyun, 2010, ADV NEURAL INFORM PR, V23, P1822; Parameswaran S., 2010, ADV NEURAL INFORM PR, V23, P1867; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Park M, 2015, ADV NEURAL INFORM PR, P154; Perrot M., 2015, ADV NEURAL INFORM PR, P1810; Perrot M, 2015, PR MACH LEARN RES, V37, P1708; Perrot M, 2014, LECT NOTES COMPUT SC, V8693, P96, DOI 10.1007/978-3-319-10602-1_7; Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995; Shi Y, 2014, AAAI CONF ARTIF INTE, P2078; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Verma N., 2015, ADV NEURAL INFORM PR, P2584; Wang Jun, 2012, ADV NEURAL INF PROCE, V25, P1601; Wang SM, 2013, ASIA PACIF MICROWAVE, P639, DOI 10.1109/APMC.2013.6694890; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Weinberger Kilian Q, 2006, ADV NEURAL INFORM PR, P1473, DOI DOI 10.1007/978-3-319-13168-9_; Xing E., 2002, ADV NEURAL INFORM PR, V15, P505, DOI DOI 10.5555/2968618.2968683; Ying YM, 2012, J MACH LEARN RES, V13, P1; Yu Yang, 2011, 22 INT JOINT C ART I; Zadeh PH, 2016, PR MACH LEARN RES, V48; Zhan D. C., 2009, P 26 INT C MACH LEAR, P1225, DOI DOI 10.1145/1553374.1553530	51	4	4	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2020	42	7					1698	1712		10.1109/TPAMI.2019.2901675	http://dx.doi.org/10.1109/TPAMI.2019.2901675			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MC0DH	30835209				2022-12-18	WOS:000542967200014
J	Clough, JR; Balfour, DR; Cruz, G; Marsden, PK; Prieto, C; Reader, AJ; King, AP				Clough, James R.; Balfour, Daniel R.; Cruz, Gastao; Marsden, Paul K.; Prieto, Claudia; Reader, Andrew J.; King, Andrew P.			Weighted Manifold Alignment using Wave Kernel Signatures for Aligning Medical Image Datasets	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Manifolds; Kernel; Biomedical imaging; Pipelines; Laplace equations; Two dimensional displays; Manifold alignment; graph descriptor; wave kernel signature; magnetic resonance imaging; slice stacking	DIMENSIONALITY REDUCTION	Manifold alignment (MA) is a technique to map many high-dimensional datasets to one shared low-dimensional space. Here we develop a pipeline for using MA to reconstruct high-resolution medical images. We present two key contributions. First, we develop a novel MA scheme in which each high-dimensional dataset can be differently weighted preventing noisier or less informative data from corrupting the aligned embedding. We find that this generalisation improves performance in our experiments in both supervised and unsupervised MA problems. Second, we use the wave kernel signature as a graph descriptor for the unsupervised MA case finding that it significantly outperforms the current state-of-the-art methods and provides higher quality reconstructed magnetic resonance volumes than existing methods.	[Clough, James R.; Balfour, Daniel R.; Cruz, Gastao; Marsden, Paul K.; Prieto, Claudia; Reader, Andrew J.; King, Andrew P.] Kings Coll London, Sch Biomed Engn & Imaging Sci, London WC2R 2LS, England	University of London; King's College London	Clough, JR (corresponding author), Kings Coll London, Sch Biomed Engn & Imaging Sci, London WC2R 2LS, England.	james.clough@kcl.ac.uk; daniel.r.balfour@kcl.ac.uk; gastao.cruz@kcl.ac.uk; paul.marsden@kcl.ac.uk; claudia.prieto@kcl.ac.uk; andrew.reader@kcl.ac.uk; andrew.king@kcl.ac.uk	Prieto, Claudia/F-8308-2013	Prieto, Claudia/0000-0003-4602-2523; Clough, James/0000-0002-9135-0545; Reader, Andrew/0000-0002-2726-3383; Marsden, Paul/0000-0001-9892-9640; King, Andrew/0000-0002-9965-7015; Cruz, Gastao/0000-0002-7397-9104	Engineering and Physical Sciences Research Council [EP/M009319/1]; Wellcome EPSRC Centre for Medical Engineering at King's College London [WT203148/Z/16/Z]; EPSRC [EP/N009258/1, EP/M009319/1] Funding Source: UKRI	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Wellcome EPSRC Centre for Medical Engineering at King's College London(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the Engineering and Physical Sciences Research Council under Grant EP/M009319/1 and by the Wellcome EPSRC Centre for Medical Engineering at King's College London (WT203148/Z/16/Z). The data used in this study is freely available and will be made available to download from https://kclmmag.org/downloads.html.	Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Balfour DR, 2018, I S BIOMED IMAGING, P599; Baumgartner CF, 2017, MED IMAGE ANAL, V35, P83, DOI 10.1016/j.media.2016.06.005; Baumgartner Christian F, 2015, Inf Process Med Imaging, V24, P363, DOI 10.1007/978-3-319-19992-4_28; Baumgartner CF, 2014, MED IMAGE ANAL, V18, P939, DOI 10.1016/j.media.2014.05.010; Baumgartner Christian F, 2013, Inf Process Med Imaging, V23, P232, DOI 10.1007/978-3-642-38868-2_20; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Chen DJ, 2009, 2009 ASIA PACIFIC CONFERENCE ON POSTGRADUATE RESEARCH IN MICROELECTRONICS AND ELECTRONICS (PRIMEASIA 2009), P53, DOI 10.1109/PRIMEASIA.2009.5397450; Chen X, 2017, IEEE T MED IMAGING, V36, P960, DOI 10.1109/TMI.2016.2636449; Clough JR, 2018, I S BIOMED IMAGING, P319; Cui Z, 2012, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2012.6247982; Guerrero R, 2014, LECT NOTES COMPUT SC, V8679, P77, DOI 10.1007/978-3-319-10581-9_10; Ham J., 2005, P ANN C UNC ART INT, P120; Hu N, 2014, PROC CVPR IEEE, P2313, DOI 10.1109/CVPR.2014.296; Hughes SM, 2009, INT CONF ACOUST SPEE, P1565, DOI 10.1109/ICASSP.2009.4959896; Modat M, 2010, COMPUT METH PROG BIO, V98, P278, DOI 10.1016/j.cmpb.2009.09.002; Nene A. S., 1996, CUCS00696; Paccanaro A, 2002, ADV NEUR IN, V14, P857; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Tuia D, 2014, IEEE T GEOSCI REMOTE, V52, P7708, DOI 10.1109/TGRS.2014.2317499; Wachinger C, 2012, MED IMAGE ANAL, V16, P806, DOI 10.1016/j.media.2011.11.008; Wang C, 2008, P 25 INT C MACH LEAR, P1120, DOI DOI 10.1145/1390156.1390297; Zhang LL, 2017, ADV BIO SCI RES, V4, P270	24	4	4	1	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2020	42	4					988	997		10.1109/TPAMI.2019.2891600	http://dx.doi.org/10.1109/TPAMI.2019.2891600			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LE2GI	30629492	Green Submitted			2022-12-18	WOS:000526541100013
J	Zhang, RZ; Zhu, SY; Shen, TW; Zhou, L; Luo, ZX; Fang, T; Quan, L				Zhang, Runze; Zhu, Siyu; Shen, Tianwei; Zhou, Lei; Luo, Zixin; Fang, Tian; Quan, Long			Distributed Very Large Scale Bundle Adjustment by Global Camera Consensus	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Bundle adjustment; Optimization; Convex functions; Convergence; Merging; Bundle adjustment; structure-from-motion; 3D reconstruction; distributed computing	ALTERNATING DIRECTION METHOD; PROXIMAL POINT ALGORITHM; NONCONVEX; CONVERGENCE	The increasing scale of Structure-from-Motion is fundamentally limited by the conventional optimization framework for the all-in-one global bundle adjustment. In this paper, we propose a distributed approach to coping with this global bundle adjustment for very large scale Structure-from-Motion computation. First, we derive the distributed formulation from the classical optimization algorithm ADMM, Alternating Direction Method of Multipliers, based on the global camera consensus. Then, we analyze the conditions under which the convergence of this distributed optimization would be guaranteed. In particular, we adopt over-relaxation and self-adaption schemes to improve the convergence rate. After that, we propose to split the large scale camera-point visibility graph in order to reduce the communication overheads of the distributed computing. The experiments on both public large scale SfM data-sets and our very large scale aerial photo sets demonstrate that the proposed distributed method clearly outperforms the state-of-the-art method in efficiency and accuracy.	[Zhang, Runze; Shen, Tianwei; Zhou, Lei; Luo, Zixin; Quan, Long] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Zhu, Siyu] Alibaba AI Labs, Hangzhou, Zhejiang, Peoples R China; [Fang, Tian] Shenzhen Zhuke Innovat Technol, Shenzhen, Peoples R China	Hong Kong University of Science & Technology; Alibaba Group	Zhang, RZ (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	rzhangaj@cse.ust.hk; siting.zsy@alibaba-inc.com; tshenaa@cse.ust.hk; lzhouai@cse.ust.hk; zluoag@cse.ust.hk; fangtian@altizure.com; quan@cse.ust.hk	Shen, Tianwei/AAF-7731-2019; Zhou, Lei/H-4799-2016; Zhang, Runze/N-3486-2015	Shen, Tianwei/0000-0002-3290-2258; Zhou, Lei/0000-0003-4988-5084; Zhang, Runze/0000-0001-9698-0178	Hong Kong RGC [16208614, T22-603/15N]; Hong Kong ITC [PSKL12EG02]; China 973 program [2012CB316300]	Hong Kong RGC(Hong Kong Research Grants Council); Hong Kong ITC; China 973 program(National Basic Research Program of China)	This work is supported by Hong Kong RGC 16208614, T22-603/15N, Hong Kong ITC PSKL12EG02, and China 973 program, 2012CB316300.	Agarwal S., CERES SOLVER; Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Agarwal S, 2010, LECT NOTES COMPUT SC, V6312, P29, DOI 10.1007/978-3-642-15552-9_3; [Anonymous], [No title captured]; [Anonymous], [No title captured]; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Bertsekas D.P., 1989, PARALLEL DISTRIBUTED, V23; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Cui ZP, 2015, IEEE I CONF COMP VIS, P864, DOI 10.1109/ICCV.2015.105; Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; ECKSTEIN J, 1994, J OPTIMIZ THEORY APP, V80, P39, DOI 10.1007/BF02196592; Eckstein J., 1998, INFORMS Journal on Computing, V10, P218, DOI 10.1287/ijoc.10.2.218; Eriksson A, 2016, PROC CVPR IEEE, P1754, DOI 10.1109/CVPR.2016.194; Fang T, 2010, LECT NOTES COMPUT SC, V6312, P1, DOI 10.1007/978-3-642-15552-9_1; Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27; FUKUSHIMA M, 1981, INT J SYST SCI, V12, P989, DOI 10.1080/00207728108963798; Gallego G, 2015, J MATH IMAGING VIS, V51, P378, DOI 10.1007/s10851-014-0528-x; He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434; Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949; Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Jiang NJ, 2013, IEEE I CONF COMP VIS, P481, DOI 10.1109/ICCV.2013.66; Kaplan A, 1998, J GLOBAL OPTIM, V13, P389, DOI 10.1023/A:1008321423879; Klingner B, 2013, IEEE I CONF COMP VIS, P953, DOI 10.1109/ICCV.2013.122; Kushal A, 2012, PROC CVPR IEEE, P1442, DOI 10.1109/CVPR.2012.6247832; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; Li GY, 2015, SIAM J OPTIMIZ, V25, P2434, DOI 10.1137/140998135; Lu Q, 2011, PR ELECTROMAGN RES S, P576; Ni K, 2007, IEEE I CONF COMP VIS, P2009; Ni K, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P144, DOI 10.1109/3DIMPVT.2012.47; Olsson C, 2009, PROC CVPR IEEE, P1216, DOI 10.1109/CVPRW.2009.5206864; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148; Shen TW, 2016, LECT NOTES COMPUT SC, V9907, P139, DOI 10.1007/978-3-319-46487-9_9; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Snavely N, 2008, PROC CVPR IEEE, P2617; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Sra S., 2012, ADV NEURAL INFORM PR, P530; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Wilson K, 2014, EXPERT REV VACCINES, V13, P969, DOI 10.1586/14760584.2014.928208; Yang L, 2017, SIAM J IMAGING SCI, V10, P74, DOI 10.1137/15M1027528; Yong-Dian Jian, 2012, Outdoor and Large-Scale Real-World Scene Analysis. 15th International Workshop on Theoretical Foundations of Computer Vision. Revised Selected Papers, P131, DOI 10.1007/978-3-642-34091-8_6; Zhang RZ, 2015, IEEE I CONF COMP VIS, P2084, DOI 10.1109/ICCV.2015.241; Zhu SY, 2014, PROC CVPR IEEE, P3938, DOI 10.1109/CVPR.2014.503	47	4	5	2	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2020	42	2					291	303		10.1109/TPAMI.2018.2840719	http://dx.doi.org/10.1109/TPAMI.2018.2840719			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KE2KB	29993533				2022-12-18	WOS:000508386100004
J	Kim, S; Min, D; Lin, S; Sohn, K				Kim, Seungryong; Min, Dongbo; Lin, Stephen; Sohn, Kwanghoon			Discrete-Continuous Transformation Matching for Dense Semantic Correspondence	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Optimization; Strain; Computational modeling; Optical imaging; Labeling; Convolution; Dense semantic correspondence; discrete optimization; continuous optimization; interative inference	ENERGY MINIMIZATION; IMAGE; FLOW	Techniques for dense semantic correspondence have provided limited ability to deal with the geometric variations that commonly exist between semantically similar images. While variations due to scale and rotation have been examined, there is a lack of practical solutions for more complex deformations such as affine transformations because of the tremendous size of the associated solution space. To address this problem, we present a discrete-continuous transformation matching (DCTM) framework where dense affine transformation fields are inferred through a discrete label optimization in which the labels are iteratively updated via continuous regularization. In this way, our approach draws solutions from the continuous space of affine transformations in a manner that can be computed efficiently through constant-time edge-aware filtering and a proposed affine-varying CNN-based descriptor. Furthermore, leveraging correspondence consistency and confidence-guided filtering in each iteration facilitates the convergence of our method. Experimental results show that this model outperforms the state-of-the-art methods for dense semantic correspondence on various benchmarks and applications.	[Kim, Seungryong; Sohn, Kwanghoon] Yonsei Univ, Sch Elect & Elect Engn, Seoul 120749, South Korea; [Min, Dongbo] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul 03760, South Korea; [Lin, Stephen] Microsoft Res, Beijing 100080, Peoples R China	Yonsei University; Ewha Womans University; Microsoft	Sohn, K (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 120749, South Korea.	srkim89@yonsei.ac.kr; dbmin@ewha.ac.kr; stevelin@microsoft.com; khsohn@yonsei.ac.kr		Min, Dongbo/0000-0003-4825-5240	Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT [NRF-2017M3C4A7069370]; Yonsei University; NRF [NRF-2018R1C1B6004622]	Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT; Yonsei University; NRF	This researchwas supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (NRF-2017M3C4A7069370). This work of S. Kim was supported (in part) by the Yonsei University Research Fund (Yonsei Frontier Lab. Young Researcher Supporting Program) of 2018. Thiswork of D. Minwas supported in part by the Young Researcher Program through the NRF under Grant NRF-2018R1C1B6004622.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Barnes C, 2010, LECT NOTES COMPUT SC, V6313, P29; Besse F, 2014, INT J COMPUT VISION, V110, P2, DOI 10.1007/s11263-013-0653-9; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bose NK, 2006, IEEE T IMAGE PROCESS, V15, P2239, DOI 10.1109/TIP.2006.877406; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Bristow H, 2015, IEEE I CONF COMP VIS, P4024, DOI 10.1109/ICCV.2015.458; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Choy C. B., 2016, P INT C NEUR INF PRO; Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145; Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227; Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964; Gaur U, 2017, IEEE I CONF COMP VIS, P1744, DOI 10.1109/ICCV.2017.192; HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965; Ham B, 2018, IEEE T PATTERN ANAL, V40, P1711, DOI 10.1109/TPAMI.2017.2724510; Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Han K, 2017, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2017.203; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Hassner T, 2013, IEEE I CONF COMP VIS, P3607, DOI 10.1109/ICCV.2013.448; Hassner T, 2012, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2012.6247842; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; He KM, 2010, LECT NOTES COMPUT SC, V6311, P1; Heise P, 2013, IEEE I CONF COMP VIS, P2360, DOI 10.1109/ICCV.2013.293; Hur J, 2015, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2015.7298745; Hwang Y, 2014, PROC CVPR IEEE, P3342, DOI 10.1109/CVPR.2014.427; Joliffe I. T., 1986, PRINCIPAL COMPONENT; Kanazawa A, 2016, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2016.354; Ke Y, 2004, PROC CVPR IEEE, P506; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kim S, 2019, IEEE T PATTERN ANAL, V41, P581, DOI 10.1109/TPAMI.2018.2803169; Kim S, 2017, IEEE I CONF COMP VIS, P4539, DOI 10.1109/ICCV.2017.485; Kim S, 2017, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2017.73; Kim S, 2015, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR.2015.7298822; Krishnan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461992; LANCASTER P, 1981, MATH COMPUT, V37, P141, DOI 10.1090/S0025-5718-1981-0616367-1; Lang M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185530; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li Y, 2015, IEEE I CONF COMP VIS, P4006, DOI 10.1109/ICCV.2015.456; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Lin WY, 2012, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2012.6247651; Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314; Lin YL, 2014, LECT NOTES COMPUT SC, V8692, P466, DOI 10.1007/978-3-319-10593-2_31; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Long J. L., 2014, ADV NEURAL INFORM PR, V27, P1601; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu JB, 2013, PROC CVPR IEEE, P1854, DOI 10.1109/CVPR.2013.242; Lu JB, 2012, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2012.6247705; Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600; Myronenko A., 2007, ADV NEURAL INFORM PR, V19, P1009, DOI DOI 10.1109/TPAMI.20; Novotny D, 2017, PROC CVPR IEEE, P2867, DOI 10.1109/CVPR.2017.306; Qiu WC, 2014, IEEE WINT CONF APPL, P1112, DOI 10.1109/WACV.2014.6835734; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372; Rocco I, 2018, PROC CVPR IEEE, P6917, DOI 10.1109/CVPR.2018.00723; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Shechtman E, 2007, PROC CVPR IEEE, P1744; Shekhovtsov A, 2007, PROC CVPR IEEE, P1800; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460; Tau M, 2016, IEEE T PATTERN ANAL, V38, P875, DOI 10.1109/TPAMI.2015.2474356; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Trulls E, 2013, PROC CVPR IEEE, P2890, DOI 10.1109/CVPR.2013.372; Ufer N, 2017, PROC CVPR IEEE, P5929, DOI 10.1109/CVPR.2017.628; Wah C., 2011, TECH REP; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Xu L, 2010, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2010.5539820; Yang F, 2017, PROC CVPR IEEE, P4151, DOI 10.1109/CVPR.2017.442; Yang HS, 2014, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2014.435; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Yuille A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P344, DOI 10.1109/CCV.1988.590011; Zbontar J, 2016, J MACH LEARN RES, V17; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723	88	4	4	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2020	42	1					59	73		10.1109/TPAMI.2018.2878240	http://dx.doi.org/10.1109/TPAMI.2018.2878240			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JV3VQ	30371354				2022-12-18	WOS:000502294300005
J	Vinogradska, J; Bischoff, B; Achterhold, J; Koller, T; Peters, J				Vinogradska, Julia; Bischoff, Bastian; Achterhold, Jan; Koller, Torsten; Peters, Jan			Numerical Quadrature for Probabilistic Policy Search	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Policy search; control; Gaussian processes; reinforcement learning	GAUSSIAN-PROCESSES; REINFORCEMENT	Learning control policies has become an appealing alternative to the derivation of control laws based on classic control theory. Model-based approaches have proven an outstanding data efficiency, especially when combined with probabilistic models to eliminate model bias. However, a major difficulty for these methods is that multi-step-ahead predictions typically become intractable for larger planning horizons and can only poorly be approximated. In this paper, we propose the use of numerical quadrature to overcome this drawback and provide significantly more accurate multi-step-ahead predictions. As a result, our approach increases data efficiency and enhances the quality of learned policies. Furthermore, policy learning is not restricted to optimizing locally around one trajectory, as numerical quadrature provides a principled approach to extend optimization to all trajectories starting in a specified starting state region. Thus, manual effort, such as choosing informative starting points for simultaneous policy optimization, is significantly decreased. Furthermore, learning is highly robust to the choice of initial policy and, thus, interaction time with the system is minimized. Empirical evaluations on simulated benchmark problems show the efficiency of the proposed approach and support our theoretical results.	[Vinogradska, Julia; Achterhold, Jan; Koller, Torsten] Bosch Ctr Artificial Intelligence, Renningen, BW, Germany; [Bischoff, Bastian] Bosch Ctr Artificial Intelligence, Res Grp Deep Learning Percept, Renningen, BW, Germany; [Vinogradska, Julia; Peters, Jan] Tech Univ Darmstadt, Intelligent Autonomous Syst Lab, D-64289 Darmstadt, Germany; [Peters, Jan] Max Planck Inst Intelligent Syst, Interdept Robot Learning Grp, D-70569 Stuttgart, Germany	Technical University of Darmstadt; Max Planck Society	Vinogradska, J (corresponding author), Bosch Ctr Artificial Intelligence, Renningen, BW, Germany.	julia.vinogradska@de.bosch.com; bastian.bischoff@de.bosch.com; mail@janachterhold.de; Koller.Torsten@web.de; mail@jan-peters.net	Peters, Jan R/D-5068-2009; Peters, Jan/P-6027-2019	Peters, Jan R/0000-0002-5266-8091; Peters, Jan/0000-0002-5266-8091				Atkeson CG, 1997, IEEE INT CONF ROBOT, P3557, DOI 10.1109/ROBOT.1997.606886; Bach F., 2012, P 29 INT C INT C MAC, P1355; Beckers T, 2016, 2016 EUROPEAN CONTROL CONFERENCE (ECC), P2275, DOI 10.1109/ECC.2016.7810630; Bischoff Bastian, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P49, DOI 10.1007/978-3-642-40988-2_4; Burkardt J., 2014, STROUD NUMERICAL INT; Candela JQ, 2003, INT CONF ACOUST SPEE, P701; Chatzilygeroudis K, 2017, IEEE INT C INT ROBOT, P51; Chen YH, 2010, ADV INTEL SOFT COMPU, V66, P109, DOI 10.1145/1866919.1866935; Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218; Deisenroth MP, 2014, IEEE INT CONF ROBOT, P3876, DOI 10.1109/ICRA.2014.6907421; Deisenroth MP, 2009, NEUROCOMPUTING, V72, P1508, DOI 10.1016/j.neucom.2008.12.019; Doerr A., 2017, P C ROB LEARN, P227; Nguyen-Tuong D, 2011, COGN PROCESS, V12, P319, DOI 10.1007/s10339-011-0404-1; Engel Y., 2006, ADV NEURAL INFORM PR, P347; Ghavamzadeh M., 2007, ADV NEURAL INFORM PR, V19, P457; Gradshteyn I. S., 1996, TABLE INTEGRALS SERI, V5th; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Klenske ED, 2013, ANN ALLERTON CONF, P486, DOI 10.1109/Allerton.2013.6736564; Ko J, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3471, DOI 10.1109/IROS.2008.4651188; Kocijan J, 2004, P AMER CONTR CONF, P2214; Kupcsik A, 2013, AAAI; Kuvayev L., 1997, P 9 YAL WORKSH AD LE, P101; Maciejowski JM, 2013, CONF CONTR FAULT-TOL, P1, DOI 10.1109/SysTol.2013.6693820; MOORE AW, 1993, MACH LEARN, V13, P103, DOI 10.1007/BF00993104; Moore AW, 1995, MACH LEARN, V21, P199, DOI 10.1007/BF00993591; Narendra K. S., 2012, STABLEADAPTIVESYSTEM; Pan Y., 2014, ADV NEURAL INFORM PR, P1907; Rasmussen CE, 2004, ADV NEUR IN, V16, P751; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rottmann Axel, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P2106, DOI 10.1109/ROBOT.2009.5152660; Schneider J.G, 1996, NEURAL INFORM PROCES, V9, P1047; Skogestad S, 2005, MULTIVARIABLE FEEDBA, P341; Skrainka B. S., 2013, HIGH PERFORMANCE QUA; Stroud A.H., 1971, APPROXIMATE CALCULAT; Suli E., 2003, INTRO NUMERICAL ANAL; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tao G., 2003, ADAPTIVE CONTROL DES; Vinogradska J, 2016, PR MACH LEARN RES, V48; Vinogradska J, 2017, J MACH LEARN RES, V18; Zhou K., 1998, ESSENTIALS ROBUST CO; [No title captured]; [No title captured]	43	4	4	4	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2020	42	1					164	175		10.1109/TPAMI.2018.2879335	http://dx.doi.org/10.1109/TPAMI.2018.2879335			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JV3VQ	30403621				2022-12-18	WOS:000502294300013
J	Cui, Z; Xiao, ST; Niu, ZH; Yan, SC; Zheng, WM				Cui, Zhen; Xiao, Shengtao; Niu, Zhiheng; Yan, Shuicheng; Zheng, Wenming			Recurrent Shape Regression	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Shape regression; cascaded regression; recurrent neural network; shape detection; face alignment	FACE ALIGNMENT	An end-to-end network architecture, the Recurrent Shape Regression (RSR). is presented to deal with the task of facial shape detection, a crucial step in many computer vision problems. The RSR generalizes the conventional cascaded regression into a recurrent dynamic network through abstracting common latent models with stage-to-stage operations. Instead of invariant regression transformation, we construct shape-dependent dynamic regressors to attain the recurrence of regression action itself. The regressors can be stacked into a high-order regression network to represent more complex shape regression. By further integrating feature learning as well as global shape constraint, the RSR becomes more controllable in entire optimization of shape regression, where the gradient computation can be efficiently back-propagated through time. To handle the possible partial occlusions of shapes. we propose a mimic virtual occlusion strategy by randomly disturbing certain point cliques without the requirement of any annotations of occlusion information or even occluded training data. Extensive experiments on five face datasets demonstrate that the proposed RSR outperforms the recent state-of-the-art cascaded approaches.	[Cui, Zhen] Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China; [Xiao, Shengtao; Niu, Zhiheng; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore; [Yan, Shuicheng] 360 AI Inst, Beijing 100016, Peoples R China; [Zheng, Wenming] Southeast Univ, Key Lab Child Dev & Learning Sci, Sch Biol Sci & Med Engn, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China	Nanjing University of Science & Technology; National University of Singapore; Southeast University - China	Yan, SC (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.; Yan, SC (corresponding author), 360 AI Inst, Beijing 100016, Peoples R China.; Zheng, WM (corresponding author), Southeast Univ, Key Lab Child Dev & Learning Sci, Sch Biol Sci & Med Engn, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China.	zhen.cui@njust.edu.cn; xstgavin124@gmail.com; niuzhiheng@gmail.com; eleyans@nus.edu.sg; wenming_zheng@seu.edu.cn	Yan, Shuicheng/HCI-1431-2022		National Natural Science Foundation of China [61772276, 61602244, U1713208, 61472187]; 973 Program [2014CB349303, 2015CB351704]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); 973 Program(National Basic Research Program of China)	This work was supported in part by the National Natural Science Foundation of China under Grant Nos. 61772276, 61602244, U1713208 and 61472187, and the 973 Program Nos. 2014CB349303 and 2015CB351704.	Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Cech J, 2016, IMAGE VISION COMPUT, V47, P60, DOI 10.1016/j.imavis.2015.11.003; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Courville A, 2016, DEEP LEARNING; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng JK, 2016, IMAGE VISION COMPUT, V47, P19, DOI 10.1016/j.imavis.2015.11.005; Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094; Fan HQ, 2016, IMAGE VISION COMPUT, V47, P27, DOI 10.1016/j.imavis.2015.11.004; Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jain V., 2010, UMCS2010009; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Lee D, 2015, PROC CVPR IEEE, P4204, DOI 10.1109/CVPR.2015.7299048; Lee HS, 2009, IEEE T PATTERN ANAL, V31, P1102, DOI 10.1109/TPAMI.2008.286; Martinez B, 2016, IMAGE VISION COMPUT, V47, P36, DOI 10.1016/j.imavis.2015.09.003; Martinez B, 2013, IEEE T PATTERN ANAL, V35, P1149, DOI 10.1109/TPAMI.2012.205; Martins P, 2012, LECT NOTES COMPUT SC, V7574, P57, DOI 10.1007/978-3-642-33712-3_5; Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Shi BG, 2018, IEEE T NEUR NET LEAR, V29, P183, DOI 10.1109/TNNLS.2016.2618340; Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241; Smith BM, 2014, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2014.225; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989; Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239; Uricar M, 2016, IMAGE VISION COMPUT, V47, P45, DOI 10.1016/j.imavis.2016.02.004; Valstar M, 2010, PROC CVPR IEEE, P2729, DOI 10.1109/CVPR.2010.5539996; Wang NN, 2018, NEUROCOMPUTING, V275, P50, DOI 10.1016/j.neucom.2017.05.013; Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4; Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yan JJ, 2014, LECT NOTES COMPUT SC, V8690, P568, DOI 10.1007/978-3-319-10605-2_37; Yang WM, 2017, IEEE T SYST MAN CY-S, V47, P2613, DOI 10.1109/TSMC.2016.2523930; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhao XW, 2014, PROC CVPR IEEE, P1765, DOI 10.1109/CVPR.2014.228; Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	46	4	5	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2019	41	5					1271	1278		10.1109/TPAMI.2018.2828424	http://dx.doi.org/10.1109/TPAMI.2018.2828424			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HS1FL	29993627				2022-12-18	WOS:000463607400018
J	Hunt, XJ; Willett, R				Hunt, Xin J.; Willett, Rebecca			Online Data Thinning via Multi-Subspace Tracking	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Subspace clustering; subspace tracking; online learning; Gaussian mixture model; anomaly detection; saliency detection	DIMENSIONALITY REDUCTION; ANOMALY DETECTION; MIXTURE; CLASSIFICATION; ALGORITHM; MODEL; CART	In an era of ubiquitous large-scale streaming data, the availability of data far exceeds the capacity of expert human analysts. In many settings, such data is either discarded or stored unprocessed in data centers. This paper proposes a method of online data thinning, in which large-scale streaming datasets are winnowed to preserve unique, anomalous, or salient elements for timely expert analysis. At the heart of this proposed approach is an online anomaly detection method based on dynamic, low-rank Gaussian mixture models. Specifically, the high-dimensional covariance matrices associated with the Gaussian components are associated with low-rank models. According to this model, most observations lie near a union of subspaces. The low-rank modeling mitigates the curse of dimensionality associated with anomaly detection for high-dimensional data, and recent advances in subspace clustering and subspace tracking allow the proposed method to adapt to dynamic environments. Furthermore, the proposed method allows subsampling, is robust to missing data, and uses a mini-batch online optimization approach. The resulting algorithms are scalable, efficient, and are capable of operating in real time. Experiments on wide-area motion imagery and e-mail databases illustrate the efficacy of the proposed approach.	[Hunt, Xin J.] SAS Inst Inc, Cary, NC 27513 USA; [Willett, Rebecca] Univ Wisconsin, Dept Elect & Comp Engn, 1415 Johnson Dr, Madison, WI 53706 USA	SAS Institute Inc; University of Wisconsin System; University of Wisconsin Madison	Hunt, XJ (corresponding author), SAS Inst Inc, Cary, NC 27513 USA.	chlorisjiang@gmail.com; willett@discovery.wisc.edu		Willett, Rebecca/0000-0002-8109-7582				Achtert E, 2006, LECT NOTES ARTIF INT, V3918, P119; Achtert Elke, 2007, 2007 International Conference on Scientific and Statistical Database Management, DOI 10.1109/SSDBM.2007.21; Achtert E, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P413; Agarwal D, 2007, KNOWL INF SYST, V11, P29, DOI 10.1007/s10115-006-0036-4; Aggarwal C. C., 2008, SDM, V483; Aggarwal C. C., 2012, P 2012 SIAM INT C DA, P624, DOI DOI 10.1137/1.9781611972825.54; Aggarwal CC, 2000, SIGMOD REC, V29, P70, DOI 10.1145/335191.335383; Agovic A, 2009, INTELL DATA ANAL, V13, P435, DOI 10.3233/IDA-2009-0375; Ahmed T, 2009, GLOB TELECOMM CONF, P1009; Allard WK, 2012, APPL COMPUT HARMON A, V32, P435, DOI 10.1016/j.acha.2011.08.001; ANSCOMBE FJ, 1948, BIOMETRIKA, V35, P246, DOI 10.2307/2332343; Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976; Battiato S, 2010, IEEE T MULTIMEDIA, V12, P622, DOI 10.1109/TMM.2010.2060474; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bellman R., 1961, ADAPTIVE CONTROL PRO; Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217; Bezier Doug, 2007, WASHINGTON POST; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bohm C., 2004, P 2004 ACM SIGMOD IN, P455, DOI DOI 10.1145/1007568.1007620; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Bourzac K., 2012, NATURE NEWS; Bradley PS, 2000, J GLOBAL OPTIM, V16, P23, DOI 10.1023/A:1008324625522; Brady DJ, 2012, NATURE, V486, P386, DOI 10.1038/nature11150; Bruce N., 2005, P 18 INT C NEUR INF, P155; Budalakoti S, 2006, TM2006214553 NASA; Chang HC, 2006, J VIS COMMUN IMAGE R, V17, P659, DOI 10.1016/j.jvcir.2005.10.004; Chen MH, 2010, IEEE T SIGNAL PROCES, V58, P6140, DOI 10.1109/TSP.2010.2070796; Chi YJ, 2013, IEEE T SIGNAL PROCES, V61, P5947, DOI 10.1109/TSP.2013.2282910; Chi YJ, 2012, INT CONF ACOUST SPEE, P3301, DOI 10.1109/ICASSP.2012.6288621; Clavin W., 2013, MANAGING DELUGE BIG; CNN, 2001, POW TURN CAL; CNN Money, 2000, ENR NAM NEW CEO; CNN Money, 2002, ENR SELLS MET UN; CNN Money, 2001, DYN SCRAPS ENR DEAL; CNN Money, 2002, ENR TAK COOP; Darling M. W., 2011, P 49 ANN M ASS COMP, P642; Diesner J., 2005, Computational & Mathematical Organization Theory, V11, P201, DOI 10.1007/s10588-005-5377-0; Donoho DL, 1997, ANN STAT, V25, P1870, DOI 10.1214/aos/1069362377; Dumbill E., 2012, WHAT IS BIG DATA INT; Dutta H, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P473; Edgeworth F.Y., 1887, PHILOS MAGAZINE J SC, V23, P364, DOI [10.1080/14786448708628471, DOI 10.1080/14786448708628471]; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Erturk S, 2002, REAL-TIME IMAGING, V8, P317, DOI 10.1006/rtim.2001.0278; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; Greenemeier L., 2011, DRAWTHE CURTAINS GIG; Hambling D., 2009, SPECIAL FORCES GIGAP; Hansen M., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P54, DOI 10.1109/ACV.1994.341288; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hastie T, 2009, ELEMENTS STAT LEARNI; Horn C, 2011, INT CONF ACOUST SPEE, P1936; Hou X, 2007, 2007 IEEE C COMP VIS, V800, P1, DOI DOI 10.1109/CVPR.2007.383267; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Ivezic Z, 2008, ARXIV08052366; Kriegel H. P., 2008, P 14 ACM SIGKDD INT, P444, DOI [10.1145/1401890.1401946, DOI 10.1145/1401890.1401946]; Lee KY, 2009, IEEE I CONF COMP VIS, P1397; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Mansour H, 2015, INT CONF ACOUST SPEE, P4065, DOI 10.1109/ICASSP.2015.7178735; Marks Robert, ENRON TIMELINE; Marlow J., 2012, WIRED; Matsushita Y, 2005, PROC CVPR IEEE, P50; McLachlan G., 2007, EM ALGORITHM EXTENSI, V382; Melnykov V, 2010, STAT SURV, V4, P80, DOI 10.1214/09-SS053; Norris F., 2001, ENRON OUSTS FINANCE; Olshen R., 1984, CLASSIFICATION REGRE; Park JC, 2008, J VISION, V8, DOI 10.1167/8.10.8; Pimentel-Alarcon D, 2016, 2016 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP); Pincus W., WASHINGTON POST; Pires A., 2005, P INT C ROB STAT; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Raginsky M, 2012, IEEE T INFORM THEORY, V58, P5544, DOI 10.1109/TIT.2012.2201375; Rao N, 2010, CONF REC ASILOMAR C, P80, DOI 10.1109/ACSSC.2010.5757471; Ratakonda K., 1998, ISCAS '98. Proceedings of the 1998 IEEE International Symposium on Circuits and Systems (Cat. No.98CH36187), P69, DOI 10.1109/ISCAS.1998.698760; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Same A, 2007, STAT COMPUT, V17, P209, DOI 10.1007/s11222-007-9017-z; Steinberg D, 2009, CH CRC DATA MIN KNOW, P179, DOI 10.1201/9781420089653.ch10; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; van der Maaten L., 2017, MATLAB TOOLBOX DIMEN; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vidal R., 2016, GEN PRINCIPAL COMPON, V40; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Wang YJ, 2016, J MACH LEARN RES, V17, P1; Willett R, 2005, INT CONF ACOUST SPEE, P1089; Willett RM, 2003, IEEE T MED IMAGING, V22, P332, DOI 10.1109/TMI.2003.809622; Xie Y, 2013, IEEE J-STSP, V7, P12, DOI 10.1109/JSTSP.2012.2234082; Yang JB, 2015, IEEE T IMAGE PROCESS, V24, P106, DOI 10.1109/TIP.2014.2365720	89	4	4	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2019	41	5					1173	1187		10.1109/TPAMI.2018.2829189	http://dx.doi.org/10.1109/TPAMI.2018.2829189			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HS1FL	29993736	Green Submitted			2022-12-18	WOS:000463607400011
J	Martin, MAB; Pujol, O; De la Torre, F; Escalera, S				Martin, Miguel Angel Bautista; Pujol, Oriol; De la Torre, Fernando; Escalera, Sergio			Error-Correcting Factorization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Error-correcting output codes; multi-class learning; matrix factorization	LEAST-SQUARES; DEPENDENT DESIGN; OUTPUT CODES; MATRIX; CONVERGENCE	Error Correcting Output Codes (ECOC) is a successful technique in multi-class classification, which is a core problem in Pattern Recognition and Machine Learning. A major advantage of ECOC over other methods is that the multi-class problem is decoupled into a set of binary problems that are solved independently. However, literature defines a general error-correcting capability for ECOCs without analyzing how it distributes among classes, hindering a deeper analysis of pair-wise error-correction. To address these limitations this paper proposes an Error-Correcting Factorization (ECF) method. Our contribution is three fold: (I) We propose a novel representation of the error-correction capability, called the design matrix, that enables us to build an ECOC on the basis of allocating correction to pairs of classes. (II) We derive the optimal code length of an ECOC using rank properties of the design matrix. (III) ECF is formulated as a discrete optimization problem, and a relaxed solution is found using an efficient constrained block coordinate descent approach. (IV) Enabled by the flexibility introduced with the design matrix we propose to allocate the error-correction on classes that are prone to confusion. Experimental results in several databases show that when allocating the error-correction to confusable classes ECF outperforms state-of-the-art approaches.	[Martin, Miguel Angel Bautista] Heidelberg Univ, Heidelberg Collaboratory Image Proc, D-69115 Heidelberg, BW, Germany; [Pujol, Oriol; Escalera, Sergio] Univ Barcelona, Dept Appl Math & Anal, E-08007 Barcelona, Spain; [Pujol, Oriol; Escalera, Sergio] Autonomous Univ Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain; [De la Torre, Fernando] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15216 USA	Ruprecht Karls University Heidelberg; University of Barcelona; Autonomous University of Barcelona; Centre de Visio per Computador (CVC); Carnegie Mellon University	Martin, MAB (corresponding author), Heidelberg Univ, Heidelberg Collaboratory Image Proc, D-69115 Heidelberg, BW, Germany.	miguel.bautista@iwr.uni-heidelberg.de; oriol_pujol@ub.edu; ftorre@cs.cmu.edu; sescalera@ub.edu	Escalera, Sergio/L-2998-2015; Pujol, Oriol/F-7146-2016	Escalera, Sergio/0000-0003-0617-8873; Pujol, Oriol/0000-0001-7573-009X	Spanish MINECO [TIN2013-43478-P, TIN2016-74946-P]; SUR-DEC of the Generalitat de Catalunya; FSE	Spanish MINECO(Spanish Government); SUR-DEC of the Generalitat de Catalunya(Generalitat de Catalunya); FSE(European Social Fund (ESF))	This work has been partially supported by the Spanish MINECO Grants TIN2013-43478-P and TIN2016-74946-P. The work of Miguel Angel is supported by SUR-DEC of the Generalitat de Catalunya and FSE.	Agarwal Sameer, 2007, J MACHINE LEARNING R, P11; Allwein E. L., 2001, Journal of Machine Learning Research, V1, P113, DOI 10.1162/15324430152733133; Bautista MA, 2011, LECT NOTES COMPUT SC, V6713, P227, DOI 10.1007/978-3-642-21557-5_25; Bautista MA, 2014, PATTERN RECOGN, V47, P865, DOI 10.1016/j.patcog.2013.06.019; Bosch A, 2007, IEEE I CONF COMP VIS, P1863; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Boyd S, 2005, SIAM J MATRIX ANAL A, V27, P532, DOI 10.1137/040609902; Boyd S, 2004, CONVEX OPTIMIZATION; Cayton L., 2006, P 23 INT C MACH LEAR, P169; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Crammer K, 2001, ADV NEUR IN, V13, P437; De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; Escalera S, 2008, IEEE T PATTERN ANAL, V30, P1041, DOI 10.1109/TPAMI.2008.38; Escalera S, 2010, IEEE T PATTERN ANAL, V32, P120, DOI 10.1109/TPAMI.2008.266; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; FUKUNAGA K, 1984, IEEE T PATTERN ANAL, V6, P314, DOI 10.1109/TPAMI.1984.4767523; Gao T., 2011, P 28 INT C MACH LEAR, P569; Gao TS, 2011, IEEE I CONF COMP VIS, P2072, DOI 10.1109/ICCV.2011.6126481; Garcia-Pedrajas N, 2008, IEEE T EVOLUT COMPUT, V12, P93, DOI 10.1109/TEVC.2007.894201; Globerson A, 2007, J MACH LEARN RES, V8, P2265; Griffin G, 2008, PROC CVPR IEEE, P533; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; GU YX, 1983, IEEE T PATTERN ANAL, V5, P83, DOI 10.1109/TPAMI.1983.4767349; Guan NY, 2012, IEEE T SIGNAL PROCES, V60, P2882, DOI 10.1109/TSP.2012.2190406; Hastie T., 1998, ADV NEURAL INFORM PR, P451; Higham NJ, 2002, IMA J NUMER ANAL, V22, P329, DOI 10.1093/imanum/22.3.329; Hsieh C.J., 2011, P 17 ACM SIGKDD INT, P1064; Kim H, 2008, SIAM J MATRIX ANAL A, V30, P713, DOI 10.1137/07069239X; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Kuncheva LI, 2003, MACH LEARN, V51, P181, DOI 10.1023/A:1022859003006; Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756; Lorena AC, 2007, J INTELL FUZZY SYST, V18, P445; Malick J, 2004, SIAM J MATRIX ANAL A, V26, P272, DOI 10.1137/S0895479802413856; Marszalek M, 2008, LECT NOTES COMPUT SC, V5305, P479, DOI 10.1007/978-3-540-88693-8_35; Miettinen P, 2008, IEEE T KNOWL DATA EN, V20, P1348, DOI 10.1109/TKDE.2008.53; Mukherjee I, 2013, J MACH LEARN RES, V14, P437; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Pujol O, 2006, IEEE T PATTERN ANAL, V28, P1007, DOI 10.1109/TPAMI.2006.116; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Rifkin R, 2004, J MACH LEARN RES, V5, P101; Rohde DLT, 2002, NEURAL COMPUT, V14, P1195, DOI 10.1162/089976602753633457; Saberian MJ, 2011, ADV NEURAL INFORM PR, P2124; Schapire R. E., 1997, MACH LEARN P 14 INT, P313; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Weiss Y, 2012, LECT NOTES COMPUT SC, V7576, P340, DOI 10.1007/978-3-642-33715-4_25; Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219; Zhang X, 2009, IEEE I CONF COMP VIS, P1111, DOI 10.1109/ICCV.2009.5459355; Zhao B, 2013, PROC CVPR IEEE, P3350, DOI 10.1109/CVPR.2013.430; Zhong G., 2013, P 23 INT JOINT C ART, P1932; Zhong GQ, 2012, NEURAL COMPUT APPL, V21, P715, DOI 10.1007/s00521-011-0653-z; Zhou JD, 2012, PATTERN RECOGN, V45, P1802, DOI 10.1016/j.patcog.2011.10.009; Zhu J, 2009, STAT INTERFACE, V2, P349	55	4	4	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2018	40	10					2388	2401		10.1109/TPAMI.2017.2763146	http://dx.doi.org/10.1109/TPAMI.2017.2763146			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GS7IZ	29035211	Green Submitted			2022-12-18	WOS:000443875500008
J	Huang, CHP; Allain, B; Boyer, E; Franco, JS; Tombari, F; Navab, N; Ilic, S				Huang, Chun-Hao Paul; Allain, Benjamin; Boyer, Edmond; Franco, Jean-Sebastien; Tombari, Federico; Navab, Nassir; Ilic, Slobodan			Tracking-by-Detection of 3D Human Shapes: From Surfaces to Volumes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Shape tracking; random forest; centroidal Voronoi tessellation; 3D tracking-by-detection; discriminative associations	MOTION CAPTURE; REGISTRATION; DEFORMATION; SIGNATURES	3D Human shape tracking consists in fitting a template model to temporal sequences of visual observations. It usually comprises an association step, that finds correspondences between the model and the input data, and a deformation step, that fits the model to the observations given correspondences. Most current approaches follow the Iterative-Closest-Point (ICP) paradigm, where the association step is carried out by searching for the nearest neighbors. It fails when large deformations occur and errors in the association tend to propagate over time. In this paper, we propose a discriminative alternative for the association, that leverages random forests to infer correspondences in one shot. Regardless the choice of shape parameterizations, being surface or volumetric meshes, we convert 3D shapes to volumetric distance fields and thereby design features to train the forest. We investigate two ways to draw volumetric samples: voxels of regular grids and cells from Centroidal Voronoi Tessellation (CVT). While the former consumes considerable memory and in turn limits us to learn only subject-specific correspondences, the latter yields much less memory footprint by compactly tessellating the interior space of a shape with optimal discretization. This facilitates the use of larger cross-subject training databases, generalizes to different human subjects and hence results in less overfitting and better detection. The discriminative correspondences are successfully integrated to both surface and volumetric deformation frameworks that recover human shape poses, which we refer to as 'tracking-by-detection of 3D human shapes.' It allows for large deformations and prevents tracking errors from being accumulated. When combined with ICP for refinement, it proves to yield better accuracy in registration and more stability when tracking over time. Evaluations on existing datasets demonstrate the benefits with respect to the state-of-the-art.	[Huang, Chun-Hao Paul; Tombari, Federico; Navab, Nassir] Tech Univ Munich, Comp Aided Med Procedures, D-80333 Munich, Germany; [Allain, Benjamin; Boyer, Edmond; Franco, Jean-Sebastien] INRIA, LJK, F-78153 Grenoble, France; [Ilic, Slobodan] Siemens AG, D-80333 Munich, Germany	Technical University of Munich; Inria; Siemens AG; Siemens Germany	Huang, CHP (corresponding author), Tech Univ Munich, Comp Aided Med Procedures, D-80333 Munich, Germany.	paulchhuang1109@gmail.com; benjaminallain@hotmail.com; edmond.boyer@inrialpes.fr; jean-sebastien.franco@inria.fr; federico.tombari@unibo.it; navab@cs.tum.edu; Slobodan.Ilic@in.tum.de		Ilic, Slobodan/0000-0002-3413-1936				Akenine-Moller T., 2005, P ACM SIGGRAPH; Allain B, 2015, PROC CVPR IEEE, P268, DOI 10.1109/CVPR.2015.7298623; [Anonymous], 2011, P EUR WORKSH 3D OBJ; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Baran I, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276467; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838; Chen QF, 2015, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2015.236; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; CHEN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2724, DOI 10.1109/ROBOT.1991.132043; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Corazza S, 2010, INT J COMPUT VISION, V87, P156, DOI 10.1007/s11263-009-0284-3; Criminisi A., 2013, DECISION FORESTCOM; Criminisi A., 2011, MED COMPUTER VISION; de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836; Duveau E, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P206, DOI 10.1109/3DIMPVT.2012.29; Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0; Franco JS, 2009, IEEE T PATTERN ANAL, V31, P414, DOI 10.1109/TPAMI.2008.104; Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y; Huang CH, 2016, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2016.419; Huang CH, 2016, INT J COMPUT VISION, V116, P115, DOI 10.1007/s11263-015-0832-y; Huang CH, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P371, DOI 10.1109/3DV.2015.49; Huang CH, 2015, PROC CVPR IEEE, P4027, DOI 10.1109/CVPR.2015.7299029; Huang CH, 2014, PROC CVPR IEEE, P3446, DOI 10.1109/CVPR.2014.440; Huang CH, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P287, DOI 10.1109/3DV.2013.45; Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223; Kehl W., 2014, P BRIT MACH VIS C; Klaudiny M, 2012, LECT NOTES COMPUT SC, V7575, P743, DOI 10.1007/978-3-642-33765-9_53; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862; Li H, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1618521, 10.1145/1618452.1618503]; Liu YB, 2013, IEEE T PATTERN ANAL, V35, P2720, DOI 10.1109/TPAMI.2013.47; Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Novatnack J, 2008, LECT NOTES COMPUT SC, V5304, P440, DOI 10.1007/978-3-540-88690-7_33; Osher S., 2006, LEVEL SET METHODS DY, V153; Petrelli A, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P403, DOI 10.1109/3DIMPVT.2012.51; Petrelli A, 2011, IEEE I CONF COMP VIS, P2244, DOI 10.1109/ICCV.2011.6126503; Pons-Moll G, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.4; Rodola E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Sorkine Olga., 2004, P EUR ACM SIGGRAPH S, P175; Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941; Starck J., 2007, IEEE 11 INT C COMP V, P1, DOI DOI 10.1109/OCEANSE.2007.4302251; Straka M, 2012, LECT NOTES COMPUT SC, V7572, P724, DOI 10.1007/978-3-642-33718-5_52; Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; TAYLOR J, 2012, PROC CVPR IEEE, P103; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Wang L, 2016, COMPUT GRAPH FORUM, V35, P152, DOI 10.1111/cgf.12716; Wang L, 2016, LECT NOTES COMPUT SC, V9907, P173, DOI 10.1007/978-3-319-46487-9_11; Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171; Wei XL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366207; Ye M, 2016, IEEE T PATTERN ANAL, V38, P1517, DOI 10.1109/TPAMI.2016.2557783; Zaharescu A, 2012, INT J COMPUT VISION, V100, P78, DOI 10.1007/s11263-012-0528-5; Zollhofer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601165	66	4	4	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2018	40	8					1994	2008		10.1109/TPAMI.2017.2740308	http://dx.doi.org/10.1109/TPAMI.2017.2740308			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GL6DT	28816656	Green Published			2022-12-18	WOS:000437271100015
J	Shekhovtsov, A; Swoboda, P; Savchynskyy, B				Shekhovtsov, Alexander; Swoboda, Paul; Savchynskyy, Bogdan			Maximum Persistency via Iterative Relaxed Inference in Graphical Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Persistency; partial optimality; LP relaxation; discrete optimization; WCSP; graphical models; energy minimization	ENERGY MINIMIZATION	We consider the NP-hard problem of MAP-inference for undirected discrete graphical models. We propose a polynomial time and practically efficient algorithm for finding a part of its optimal solution. Specifically, our algorithm marks some labels of the considered graphical model either as (i) optimal, meaning that they belong to all optimal solutions of the inference problem; (ii) non-optimal if they provably do not belong to any solution. With access to an exact solver of a linear programming relaxation to the MAP-inference problem, our algorithm marks the maximal possible (in a specified sense) number of labels. We also present a version of the algorithm, which has access to a suboptimal dual solver only and still can ensure the (non-) optimality for the marked labels, although the overall number of the marked labels may decrease. We propose an efficient implementation, which runs in time comparable to a single run of a suboptimal dual solver. Our method is well-scalable and shows state-of-the-art results on computational benchmarks from machine learning and computer vision.	[Shekhovtsov, Alexander] Graz Univ Technol, Inst Comp Graph & Vis ICG, Inffeldgasse 16, A-8010 Graz, Austria; [Swoboda, Paul] IST Austria, Discrete Optimizat Grp, Campus 1, A-3400 Klosterneuburg, Austria; [Savchynskyy, Bogdan] Tech Univ Dresden, Comp Vis Lab, Fac Comp Sci, Inst Artificial Intelligence, D-01062 Dresden, Germany	Graz University of Technology; Institute of Science & Technology - Austria; Technische Universitat Dresden	Shekhovtsov, A (corresponding author), Graz Univ Technol, Inst Comp Graph & Vis ICG, Inffeldgasse 16, A-8010 Graz, Austria.	shekhovtsov@icg.tugraz.at; pswoboda@ist.ac.at; bogdan.savchynskyy@tu-dresden.de		Shekhovtsov, Alexander/0000-0003-0678-8954	Austrian Science Fund (FWF) under the START project BIVISION [Y729]; German Research Foundation (DFG) within the program "Spatio-/Temporal Graphical Models and Applications in Image Analysis" [GRK 1653]; European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program [647769]	Austrian Science Fund (FWF) under the START project BIVISION(Austrian Science Fund (FWF)); German Research Foundation (DFG) within the program "Spatio-/Temporal Graphical Models and Applications in Image Analysis"; European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program(European Research Council (ERC))	Alexander Shekhovtsov was supported by the Austrian Science Fund (FWF) under the START project BIVISION, No. Y729. Paul Swoboda and Bogdan Savchynskyy were supported by the German Research Foundation (DFG) within the program "Spatio-/Temporal Graphical Models and Applications in Image Analysis", grant GRK 1653. Bogdan Savchynskyy was also supported by European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant agreement No 647769).	Adams WP, 1998, MATH OPER RES, V23, P359, DOI 10.1287/moor.23.2.359; AGGARWAL A, 1987, ALGORITHMICA, V2, P195, DOI 10.1007/BF01840359; Alahari K., 2008, P IEEE C COMP VIS PA, P1; [Anonymous], 2016, PROBABILISTIC INFERE; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Cooper MC, 2010, ARTIF INTELL, V174, P449, DOI 10.1016/j.artint.2010.02.001; de Givry S, 2013, LECT NOTES COMPUT SC, V8124, P263, DOI 10.1007/978-3-642-40627-0_22; DESMET J, 1992, NATURE, V356, P539, DOI 10.1038/356539a0; Freuder E. C., 1991, AAAI-91. Proceedings Ninth National Conference on Artificial Intelligence, P227; Globerson Amir, 2007, P NIPS, V20, P553; GOLDSTEIN RF, 1994, BIOPHYS J, V66, P1335, DOI 10.1016/S0006-3495(94)80923-3; Gridchyn I, 2013, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2013.288; Hirata T, 1996, INFORM PROCESS LETT, V58, P129, DOI 10.1016/0020-0190(96)00049-X; Hurkat Skand, 2015, 2015 25th International Conference on Field Programmable Logic and Applications (FPL), P1, DOI 10.1109/FPL.2015.7293934; ILOG Inc, 2016, ILOG CPLEX HIGH PERF; Jungwook Choi, 2012, 2012 22nd International Conference on Field Programmable Logic and Applications (FPL), P209, DOI 10.1109/FPL.2012.6339183; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kohli P., 2008, P 25 INT C MACH LEAR, P480, DOI DOI 10.1145/1390156.1390217; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kolmogorov V, 2015, IEEE T PATTERN ANAL, V37, P919, DOI 10.1109/TPAMI.2014.2363465; Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P416, DOI 10.1016/j.dam.2011.10.026; Kovtun I, 2003, LECT NOTES COMPUT SC, V2781, P402; KOVTUN I, 2011, CONTROL SYST COMPUT, V2, P35; Lecoutre Christophe, 2012, Principles and Practice of Constraint Programming. Proceedings 18th International Conference, CP 2012, P406, DOI 10.1007/978-3-642-33558-7_31; Li MT, 2016, LECT NOTES COMPUT SC, V9906, P834, DOI 10.1007/978-3-319-46475-6_51; Meijster A, 2000, COMP IMAG VIS, V18, P331; Rother C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383203; Savchynskyy Bogdan, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1817, DOI 10.1109/CVPR.2011.5995652; Savchynskyy B., 2012, UAI P, P746; Schlesinger MI, 2011, CYBERN SYST ANAL+, V47, P175, DOI 10.1007/s10559-011-9300-z; Schlesinger M. I., 1976, CYBERNET SYST, P113; Shekhar Aditya, 2016, 2016 18 EUR C POW EL, P1; Shekhovtsov A., 2013, THESIS; Shekhovtsov A., 2014, CORR; Shekhovtsov A., 2015, CORR; Shekhovtsov A, 2016, COMPUT VIS IMAGE UND, V143, P54, DOI 10.1016/j.cviu.2015.05.002; Shekhovtsov A, 2015, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2015.7298650; Shekhovtsov A, 2014, PROC CVPR IEEE, P1162, DOI 10.1109/CVPR.2014.152; Swoboda P., 2013, P 4 INT C SCAL SPAC, P477, DOI DOI 10.1007/978-3-642-38267-3_40; Swoboda P, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2484327; Swoboda P, 2014, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2014.153; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Vanderbei R. J., 2001, LINEAR PROGRAMMING F; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang C, 2016, PROC CVPR IEEE, P5830, DOI 10.1109/CVPR.2016.628; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Windheuser T., 2012, GEN ROOF DUALITY MUL, P400, DOI [10.1007/978-3-642-33783-3_29, DOI 10.1007/978-3-642-33783-3_29]; Yanover C, 2008, J COMPUT BIOL, V15, P899, DOI 10.1089/cmb.2007.0158; Zach C, 2014, PROC CVPR IEEE, P1330, DOI 10.1109/CVPR.2014.173	51	4	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2018	40	7					1668	1682		10.1109/TPAMI.2017.2730884	http://dx.doi.org/10.1109/TPAMI.2017.2730884			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GI3TS	28742030	Green Submitted			2022-12-18	WOS:000434294800010
J	Zhang, Y; Chen, XW; Li, J; Wang, C; Xia, CQ; Li, J				Zhang, Yu; Chen, Xiaowu; Li, Jia; Wang, Chen; Xia, Changqun; Li, Jun			Semantic Object Segmentation in Tagged Videos via Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Video segmentation; semantic object; detection-based segmentation; weakly supervised segmentation	MOVING-OBJECTS	Semantic object segmentation (SOS) is a challenging task in computer vision that aims to detect and segment all pixels of the objects within predefined semantic categories. In image-based SOS, many supervised models have been proposed and achieved impressive performances due to the rapid advances of well-annotated training images and machine learning theories. However, in video-based SOS it is often difficult to directly train a supervised model since most videos are weakly annotated by tags. To handle such tagged videos, this paper proposes a novel approach that adopts a segmentation-by-detection framework. In this framework, object detection and segment proposals are first generated using the models pre-trained on still images, which provide useful cues to roughly localize the semantic objects. Based on these proposals, we propose an efficient algorithm to initialize object tracks by solving a joint assignment problem. As such tracks provide rough spatiotemporal configurations of the semantic objects, a voting-based refinement algorithm is further proposed to improve their spatiotemporal consistency. Extensive experiments demonstrate that the proposed framework can robustly and effectively segment semantic objects in tagged videos, even when the image-based object detectors provide inaccurate proposals. On various public benchmarks, the proposed approach obtains substantial improvements over the state-of-the-arts.	[Zhang, Yu; Chen, Xiaowu; Wang, Chen; Xia, Changqun; Li, Jun] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China; [Li, Jia] Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China; [Li, Jia] Beihang Univ, Int Res Inst Multidisciplinary Sci, Beijing 100191, Peoples R China	Beihang University; Beihang University	Chen, XW (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.	zhangyulb@gmail.com; chen@buaa.edu.cn; jiali@buaa.edu.cn; wangc@buaa.edu.cn; xiacq@buaa.edu.cn; junmuzi@buaa.edu.cn	Li, Jia/AAB-6431-2019	Li, Jia/0000-0002-4346-8696	National Natural Science Foundation of China [61325011, 61532003, 61421003]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We would like to thank the anonymous reviewers for their help in improving this work. This work was supported in part by grants from the National Natural Science Foundation of China (61325011, 61532003 and 61421003). Earlier version of this work has been published in CVPR 2015 [73].	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Ahuja R. K., 1993, NETWORK FLOWS THEORY; [Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Brendel W, 2010, ADV NEURAL INFORM PR, P307; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Chen A.Y.C., 2011, P IEEE WORKSH APPL C, P614; Chen D.-J., 2012, P ACM MULT, P805, DOI DOI 10.1145/2393347.2396317; Chen XW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P321, DOI 10.1145/2733373.2806274; Chiu WC, 2013, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.2013.48; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dong J, 2014, LECT NOTES COMPUT SC, V8693, P299, DOI 10.1007/978-3-319-10602-1_20; Drayer B., 2016, ARXIV160803066; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Faktor A., 2014, P BRIT MACH VIS C, P34; Floros G, 2012, PROC CVPR IEEE, P2823, DOI 10.1109/CVPR.2012.6248007; Fu HZ, 2014, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2014.405; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; Hartmann G, 2012, LECT NOTES COMPUT SC, V7583, P198, DOI 10.1007/978-3-642-33863-2_20; I. Gurobi Optimization, 2016, GUR OPT REF MAN; Jain A, 2013, IEEE I CONF COMP VIS, P1865, DOI 10.1109/ICCV.2013.234; Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43; Jang WD, 2016, PROC CVPR IEEE, P696, DOI 10.1109/CVPR.2016.82; Kavukcuoglu Koray, 2010, ADV NEURAL INFORM PR, V23, P1090; Keuper M, 2015, IEEE I CONF COMP VIS, P3271, DOI 10.1109/ICCV.2015.374; Kundu A, 2014, LECT NOTES COMPUT SC, V8694, P703, DOI 10.1007/978-3-319-10599-4_45; Ladicky L, 2010, LECT NOTES COMPUT SC, V6314, P424, DOI 10.1007/978-3-642-15561-1_31; Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471; Li CL, 2015, PROC CVPR IEEE, P5519, DOI 10.1109/CVPR.2015.7299191; Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273; Li J, 2015, IEEE T PATTERN ANAL, V37, P2428, DOI 10.1109/TPAMI.2015.2424870; Liang X., 2015, ARXIV150902636; Liang XD, 2016, PROC CVPR IEEE, P633, DOI 10.1109/CVPR.2016.75; Lin L, 2015, IEEE T PATTERN ANAL, V37, P959, DOI 10.1109/TPAMI.2014.2359888; Liu BY, 2015, PROC CVPR IEEE, P4286, DOI 10.1109/CVPR.2015.7299057; Liu X, 2014, PROC CVPR IEEE, P57, DOI 10.1109/CVPR.2014.15; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu JS, 2015, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2015.7299000; Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Platt JC, 2000, ADV NEUR IN, P61; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Real E., 2017, IEEE C COMP VIS PATT; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rubio Jose C., 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P13, DOI 10.1007/978-3-642-37444-9_2; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schindler K, 2006, LECT NOTES COMPUT SC, V3852, P581; Seguin Guillaume, 2016, P IEEE C COMP VIS PA; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; Tang K., 2012, ADV NEURAL INFORM PR, P638; Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190; Tang K, 2013, PROC CVPR IEEE, P2483, DOI 10.1109/CVPR.2013.321; Taylor Brian, 2013, Energy Minimization Methods in Computer Vision and Pattern Recognition. 9th International Conference, EMMCVPR 2013. Proceedings. LNCS 8081, P195, DOI 10.1007/978-3-642-40395-8_15; Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386; Vijayanarasimhan S, 2012, LECT NOTES COMPUT SC, V7576, P496, DOI 10.1007/978-3-642-33715-4_36; Wang L, 2014, LECT NOTES COMPUT SC, V8692, P640, DOI 10.1007/978-3-319-10593-2_42; Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184; Xia W, 2013, IEEE I CONF COMP VIS, P2176, DOI 10.1109/ICCV.2013.271; Xie J, 2016, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2016.401; Xin B, 2015, PROC CVPR IEEE, P4676, DOI 10.1109/CVPR.2015.7299099; Yang Y, 2010, PROC CVPR IEEE, P3113, DOI 10.1109/CVPR.2010.5540070; Zhang D, 2014, LECT NOTES COMPUT SC, V8695, P551, DOI 10.1007/978-3-319-10584-0_36; Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87; Zhang Y, 2015, PROC CVPR IEEE, P3641, DOI 10.1109/CVPR.2015.7298987; Zhong BN, 2013, NEUROCOMPUTING, V103, P132, DOI 10.1016/j.neucom.2012.10.001; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	75	4	4	2	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2018	40	7					1741	1754		10.1109/TPAMI.2017.2727049	http://dx.doi.org/10.1109/TPAMI.2017.2727049			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GI3TS	28742028				2022-12-18	WOS:000434294800015
J	Gooya, A; Lekadir, K; Castro-Mateos, I; Pozo, JM; Frangi, AF				Gooya, Ali; Lekadir, Karim; Castro-Mateos, Isaac; Pozo, Jose Maria; Frangi, Alejandro F.			Mixture of Probabilistic Principal Component Analyzers for Shapes from Point Sets	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generative modeling; variational Bayes; model selection; graphical models; statistical shape models	MODELS; SEGMENTATION; STATISTICS; SURFACES; SPACES; ATLAS	Inferring a probability density function (pdf) for shape from a population of point sets is a challenging problem. The lack of point-to-point correspondences and the non-linearity of the shape spaces undermine the linear models. Methods based on manifolds model the shape variations naturally, however, statistics are often limited to a single geodesic mean and an arbitrary number of variation modes. We relax the manifold assumption and consider a piece-wise linear form, implementing a mixture of distinctive shape classes. The pdf for point sets is defined hierarchically, modeling a mixture of Probabilistic Principal Component Analyzers (PPCA) in higher dimension. A Variational Bayesian approach is designed for unsupervised learning of the posteriors of point set labels, local variation modes, and point correspondences. By maximizing the model evidence, the numbers of clusters, modes of variations, and points on the mean models are automatically selected. Using the predictive distribution, we project a test shape to the spaces spanned by the local PPCA's. The method is applied to point sets from: i) synthetic data, ii) healthy versus pathological heart morphologies, and iii) lumbar vertebrae. The proposed method selects models with expected numbers of clusters and variation modes, achieving lower generalization-specificity errors compared to state-of-the-art.	[Gooya, Ali; Lekadir, Karim; Castro-Mateos, Isaac; Pozo, Jose Maria; Frangi, Alejandro F.] Univ Sheffield, Ctr Computat Imaging & Simulat Technol Biomed, Sheffield S10 2TN, S Yorkshire, England	University of Sheffield	Gooya, A (corresponding author), Univ Sheffield, Ctr Computat Imaging & Simulat Technol Biomed, Sheffield S10 2TN, S Yorkshire, England.	a.gooya@sheffield.ac.uk; karim.lekadir@upf.edu; isaac.casm@kcl.ac.uk; j.pozo@sheffield.ac.uk; a.frangi@sheffield.ac.uk	Pozo, Jose M/M-7604-2016; Frangi, Alejandro F/C-6500-2008	Pozo, Jose M/0000-0002-0759-3510; Frangi, Alejandro F/0000-0002-2675-528X; Gooya, Ali/0000-0001-5135-4800	Marie Skodowska-Curie Individual Fellowship [625745]	Marie Skodowska-Curie Individual Fellowship	This project was funded by the Marie Skodowska-Curie Individual Fellowship (Contract Agreement 625745), granted to A. Gooya.	Beil F., 2002, C KNOWLEDGE DISCOVER, P436, DOI [DOI 10.1145/775047.775110, DOI 10.1145/3292500.3330672]; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Bishop C.M, 2006, PATTERN RECOGN; BRECHBUHLER C, 1995, COMPUT VIS IMAGE UND, V61, P154, DOI 10.1006/cviu.1995.1013; Bronstein A. M., 2008, ACM T GRAPHIC, V30, P1295; Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838; Cates J, 2007, LECT NOTES COMPUT SC, V4584, P333; Chui H, 2004, IEEE T PATTERN ANAL, V26, P160, DOI 10.1109/TPAMI.2004.1262178; Cootes TF, 2008, IMAGE VISION COMPUT, V26, P326, DOI 10.1016/j.imavis.2006.12.005; Cootes TF, 1999, IMAGE VISION COMPUT, V17, P567, DOI 10.1016/S0262-8856(98)00175-9; Cremers D, 2003, PATTERN RECOGN, V36, P1929, DOI 10.1016/S0031-3203(03)00056-6; Datar M, 2009, LECT NOTES COMPUT SC, V5762, P167, DOI 10.1007/978-3-642-04271-3_21; Davies RH, 2002, IEEE T MED IMAGING, V21, P525, DOI 10.1109/TMI.2002.1009388; Davies RH, 2010, IEEE T MED IMAGING, V29, P961, DOI 10.1109/TMI.2009.2035048; Durrleman S, 2009, MED IMAGE ANAL, V13, P793, DOI 10.1016/j.media.2009.07.007; Duta N, 2009, PATTERN RECOGN, V42, P2797, DOI 10.1016/j.patcog.2009.02.007; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Ghahramani Z, 2000, ADV NEUR IN, V12, P449; Gooya A., 2015, P INF PROC MED IM, P858; Gooya A, 2015, SIAM J IMAGING SCI, V8, P858, DOI 10.1137/140982039; Hefny MS, 2015, LECT NOTES COMPUT SC, V9350, P238, DOI 10.1007/978-3-319-24571-3_29; Heimann T, 2009, MED IMAGE ANAL, V13, P543, DOI 10.1016/j.media.2009.05.004; Hufnagel H, 2008, INT J COMPUT ASS RAD, V2, P265, DOI 10.1007/s11548-007-0138-9; Joshi S, 2004, NEUROIMAGE, V23, pS151, DOI 10.1016/j.neuroimage.2004.07.068; Kelemen A, 1999, IEEE T MED IMAGING, V18, P828, DOI 10.1109/42.811260; Kendall David G, 1989, STAT SCI, P6, DOI DOI 10.1214/SS/1177012582; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; Kurtek S, 2011, IEEE T MED IMAGING, V30, P849, DOI 10.1109/TMI.2010.2099130; LE HL, 1987, MATH PROC CAMBRIDGE, V101, P313, DOI 10.1017/S0305004100066688; Lestrel PE, 2013, HOMO, V64, P247, DOI 10.1016/j.jchb.2013.05.001; Lindner C, 2015, IEEE T PATTERN ANAL, V37, P1862, DOI 10.1109/TPAMI.2014.2382106; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; MARON BJ, 1987, NEW ENGL J MED, V316, P780, DOI 10.1056/NEJM198703263161305; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Rasoulian A, 2012, IEEE T MED IMAGING, V31, P2025, DOI 10.1109/TMI.2012.2202913; Spoor F, 2015, NATURE, V519, P83, DOI 10.1038/nature14224; Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86; Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Swift AJ, 2012, J CARDIOVASC MAGN R, V14, DOI 10.1186/1532-429X-14-40; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Veeraraghavan A, 2005, IEEE T PATTERN ANAL, V27, P1896, DOI 10.1109/TPAMI.2005.246; Voelkel NF, 2006, CIRCULATION, V114, P1883, DOI 10.1161/CIRCULATIONAHA.106.632208; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Yan P, 2007, IEEE T PATTERN ANAL, V29, P1297, DOI 10.1109/TPAMI.2007.1067; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685; Younes L, 2012, IMAGE VISION COMPUT, V30, P389, DOI 10.1016/j.imavis.2011.09.009; Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900	51	4	4	1	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2018	40	4					891	904		10.1109/TPAMI.2017.2700276	http://dx.doi.org/10.1109/TPAMI.2017.2700276			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FY2ZU	28475045	Green Accepted			2022-12-18	WOS:000426687100009
J	Gorelick, L; Boykov, Y; Veksler, O; Ben Ayed, I; Delong, A				Gorelick, Lena; Boykov, Yuri; Veksler, Olga; Ben Ayed, Ismail; Delong, Andrew			Local Submodularization for Binary Pairwise Energies	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Discrete optimization; graph cuts; trust region; auxiliary functions; local submodularization	SEGMENTATION	Many computer vision problems require optimization of binary non-submodular energies. We propose a general optimization framework based on local submodular approximations (LSA). Unlike standard LP relaxation methods that linearize the whole energy globally, our approach iteratively approximates the energy locally. On the other hand, unlike standard local optimization methods (e.g., gradient descent or projection techniques) we use non-linear submodular approximations and optimize them without leaving the domain of integer solutions. We discuss two specific LSA algorithms based on trust region and auxiliary function principles, LSA-TR and LSA-AUX. The proposedmethods obtain state-of-the-art results on a wide range of applications such as binary deconvolution, curvature regularization, inpainting, segmentation with repulsion and two types of shape priors. Finally, we discuss amove-making extension to the LSA-TR approach. While our paper is focused on pairwise energies, our ideas extend to higher-order problems. The code is available online.	[Gorelick, Lena; Boykov, Yuri; Veksler, Olga] Univ Western Ontario, Dept Comp Sci, London, ON N6A 3K7, Canada; [Delong, Andrew] Univ Toronto, Dept Elect Engn, Toronto, ON M5S, Canada; [Ben Ayed, Ismail] Ecole Technol Super, Dept Genie Prod Automatisee, Montreal, PQ H3C 1K3, Canada	Western University (University of Western Ontario); University of Toronto; University of Quebec; Ecole de Technologie Superieure - Canada	Gorelick, L (corresponding author), Univ Western Ontario, Dept Comp Sci, London, ON N6A 3K7, Canada.	lenagorelick@gmail.com; yuri@csd.uwo.ca; olga@csd.uwo.ca; ismail.benayed@gmail.com; andrew.delong@gmail.com	Veksler, Olga/B-6549-2015	Veksler, Olga/0000-0002-9664-6601	NSERC; NIH	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We greatly thank V. Kolmogorov for his feedback. We also thank NSERC and NIH for their grants supporting this project.	Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95; Ben Ayed I, 2013, PROC CVPR IEEE, P1304, DOI 10.1109/CVPR.2013.172; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2006, LECT NOTES COMPUT SC, V3953, P409, DOI 10.1007/11744078_32; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Brendel W, 2010, ADV NEURAL INFORM PR, P307; Das P, 2009, IMAGE VISION COMPUT, V27, P206, DOI 10.1016/j.imavis.2008.02.006; Delong A, 2009, IEEE I CONF COMP VIS, P285, DOI 10.1109/ICCV.2009.5459263; El-Zehiry NY, 2010, PROC CVPR IEEE, P3257, DOI 10.1109/CVPR.2010.5540057; Fletcher R, 1987, PRACTICAL METHODS OP, V1; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Gorelick L, 2014, LECT NOTES COMPUT SC, V8693, P675, DOI 10.1007/978-3-319-10602-1_44; Gorelick L, 2013, PROC CVPR IEEE, P1714, DOI 10.1109/CVPR.2013.224; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Kappes JH, 2016, COMPUT VIS IMAGE UND, V143, P104, DOI 10.1016/j.cviu.2015.11.005; Keuchel J, 2003, IEEE T PATTERN ANAL, V25, P1364, DOI 10.1109/TPAMI.2003.1240111; Kolmogorov V., 2012, ARXIV12056352; Kumar MP, 2011, J MACH LEARN RES, V12, P31; Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; LAZIMY R, 1982, MATH PROGRAM, V22, P332, DOI 10.1007/BF01581047; Leordeanu Marius, 2009, ADV NEURAL INFORM PR; Narasimhan M., 2005, UAI, P404; Nieuwenhuis C, 2014, PROC CVPR IEEE, P4098, DOI 10.1109/CVPR.2014.522; Olsson C, 2008, COMPUT VIS IMAGE UND, V112, P3, DOI 10.1016/j.cviu.2008.05.010; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Rother C, 2005, PROC CVPR IEEE, P589; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Rother C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383203; Schlesinger D, 2007, LECT NOTES COMPUT SC, V4679, P28; Tang M, 2014, LECT NOTES COMPUT SC, V8693, P691, DOI 10.1007/978-3-319-10602-1_45; Taniai T, 2015, PROC CVPR IEEE, P2030, DOI 10.1109/CVPR.2015.7298814; Ulen J, 2013, IEEE T MED IMAGING, V32, P178, DOI 10.1109/TMI.2012.2218117; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yuan YX., 2000, P 4 INT C IND APPL M, V99, P271	36	4	4	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2017	39	10					1985	1999		10.1109/TPAMI.2016.2630686	http://dx.doi.org/10.1109/TPAMI.2016.2630686			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FF3NI	27875215				2022-12-18	WOS:000408807600007
J	Xiao, YS; Liu, B; Hao, ZF				Xiao, Yanshan; Liu, Bo; Hao, Zhifeng			A Sphere-Description-Based Approach for Multiple-Instance Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Multiple-instance learning; classification	IMAGE	Multiple-instance learning (MIL) is a generalization of supervised learning which addresses the classification of bags. Similar to traditional supervised learning, most of the existing MIL work is proposed based on the assumption that a representative training set is available for a proper learning of the classifier. That is to say, the training data can appropriately describe the distribution of positive and negative data in the testing set. However, this assumption may not be always satisfied. In real-world MIL applications, the negative data in the training set may not sufficiently represent the distribution of negative data in the testing set. Hence, how to learn an appropriate MIL classifier when a representative training set is not available becomes a key challenge for real-world MIL applications. To deal with this problem, we propose a novel Sphere-Description-Based approach for Multiple-Instance Learning (SDB-MIL). SDB-MIL learns an optimal sphere by determining a large margin among the instances, and meanwhile ensuring that each positive bag has at least one instance inside the sphere and all negative bags are outside the sphere. Enclosing at least one instance from each positive bag in the sphere enables a more desirable MIL classifier when the negative data in the training set cannot sufficiently represent the distribution of negative data in the testing set. Substantial experiments on the benchmark and real-world MIL datasets show that SDB-MIL obtains statistically better classification performance than the MIL methods compared.	[Xiao, Yanshan; Hao, Zhifeng] Guangdong Univ Technol, Sch Comp, Guangzhou 510006, Guangdong, Peoples R China; [Liu, Bo] Guangdong Univ Technol, Sch Automat, Guangzhou 510006, Guangdong, Peoples R China	Guangdong University of Technology; Guangdong University of Technology	Liu, B (corresponding author), Guangdong Univ Technol, Sch Automat, Guangzhou 510006, Guangdong, Peoples R China.	xiaoyanshan@gmail.com; csbliu@gmail.com; mazfhao@scut.edu.cn			Natural Science Foundation of China [61472089, 61472090]; NSFC-Guangdong Joint Found [U1501254]; Natural Science Foundation of Guangdong Province [2014A030308008, 2015A030313486]; Guangdong Province Natural Science Funds for Distinguished Young Scholar [S2013050014133]; Key Technology Research and Development Programs of Guangdong Province [2015B010131015, 2015B010108006]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); NSFC-Guangdong Joint Found; Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); Guangdong Province Natural Science Funds for Distinguished Young Scholar; Key Technology Research and Development Programs of Guangdong Province	This work was supported in part by the Natural Science Foundation of China (61472089, 61472090), in part by the NSFC-Guangdong Joint Found (U1501254), in part by the Natural Science Foundation of Guangdong Province (2014A030308008, 2015A030313486), in part by the Guangdong Province Natural Science Funds for Distinguished Young Scholar (S2013050014133), and in part by the Key Technology Research and Development Programs of Guangdong Province (2015B010131015, 2015B010108006). Bo Liu is the corresponding author.	Andrews S., 2002, SUPPORT VECTOR MACHI, P561; Auer P, 2004, LECT NOTES COMPUT SC, V3201, P63; Bergeron C, 2012, IEEE T PATTERN ANAL, V34, P1068, DOI 10.1109/TPAMI.2011.194; Bunescu R.C., 2007, P ICML, P105, DOI DOI 10.1145/1273496.1273510; Carson C., 1998, P 3 INT C VIS INF SY, P509, DOI DOI 10.1007/3-540-48762-X_63; Chapelle O, 2008, J MACH LEARN RES, V9, P203; Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248; Chen YX, 2004, J MACH LEARN RES, V5, P913; Cheung P. M., 2006, P 23 INT C MACH LEAR, P193; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Dong L., 2006, COMP MULTIINSTANCE L; Fu ZY, 2011, IEEE T PATTERN ANAL, V33, P958, DOI 10.1109/TPAMI.2010.155; Gehler P.V., 2007, P 11 INT C ART INT S, P123; Grtner T., 2002, P 19 INT C MACH LEAR, P179; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Kubat M., 1997, P 14 INT C MACH LEAR, V97, P179; Kwok JT, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P901; Li WJ, 2010, IEEE T KNOWL DATA EN, V22, P76, DOI 10.1109/TKDE.2009.58; Maron O., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P341; Maron O, 1998, ADV NEUR IN, V10, P570; Rahmani R, 2008, IEEE T PATTERN ANAL, V30, P1902, DOI 10.1109/TPAMI.2008.112; Salton G., 1989, AUTOMATIC TEXT PROCE; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Smola AJ, 2005, P 10 INT WORKSH ART, P325; Tao QP, 2008, IEEE T PATTERN ANAL, V30, P2084, DOI 10.1109/TPAMI.2007.70846; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Wang F, 2010, IEEE T NEURAL NETWOR, V21, P319, DOI 10.1109/TNN.2009.2036998; Wang H., 2011, ARAB J CHEM, P1; Wang H., 2011, P 25 AAAI C ART INT, P507; Wang H., 2011, P APPL ART INT C; Wang XM, 2011, NEURAL NETWORKS, V24, P360, DOI 10.1016/j.neunet.2011.01.007; Wu G., 2003, ICML 2003 WORKSH LEA, P49, DOI DOI 10.5753/SBRC.2019.7416; Zafra A, 2010, INFORM SCIENCES, V180, P4496, DOI 10.1016/j.ins.2010.07.031; Zhang C, 2006, ADV NEURAL INFORM PR, P1417; Zhang D, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P149; Zhang D, 2011, IEEE T NEURAL NETWOR, V22, P739, DOI 10.1109/TNN.2011.2109011; Zhang K, 2009, IEEE T NEURAL NETWOR, V20, P583, DOI 10.1109/TNN.2008.2010620; Zhang ML, 2008, IEEE DATA MINING, P688, DOI 10.1109/ICDM.2008.27; Zhang Q, 2002, ADV NEUR IN, V14, P1073; Zhou Z.-H., 2007, P 24 INT C MACHINE L, P1167; Zhou Z.-H., 2009, ANN INT C MACH LEARN, P1249, DOI DOI 10.1145/1553374.1553534	41	4	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2017	39	2					242	257		10.1109/TPAMI.2016.2539952	http://dx.doi.org/10.1109/TPAMI.2016.2539952			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EM8HZ	26978553				2022-12-18	WOS:000395553400003
J	Henter, GE; Kleijn, WB				Henter, Gustav Eje; Kleijn, W. Bastiaan			Minimum Entropy Rate Simplification of Stochastic Processes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Markov processes; stochastic processes; information theory; signal analysis; synthesis; and processing; language generation; statistical models	INFORMATION; DIVERGENCE; SELECTION; CAPACITY	We propose minimum entropy rate simplification (MERS), an information-theoretic, parameterization-independent framework for simplifying generative models of stochastic processes. Applications include improving model quality for sampling tasks by concentrating the probability mass on the most characteristic and accurately described behaviors while de-emphasizing the tails, and obtaining clean models from corrupted data (nonparametric denoising). This is the opposite of the smoothing step commonly applied to classification models. Drawing on rate-distortion theory, MERS seeks the minimum entropy-rate process under a constraint on the dissimilarity between the original and simplified processes. We particularly investigate the Kullback-Leibler divergence rate as a dissimilarity measure, where, compatible with our assumption that the starting model is disturbed or inaccurate, the simplification rather than the starting model is used for the reference distribution of the divergence. This leads to analytic solutions for stationary and ergodic Gaussian processes and Markov chains. The same formulas are also valid for maximum-entropy smoothing under the same divergence constraint. In experiments, MERS successfully simplifies and denoises models from audio, text, speech, and meteorology.	[Henter, Gustav Eje] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh, Midlothian, Scotland; [Kleijn, W. Bastiaan] Victoria Univ Wellington, Commun & Signal Proc Grp, Wellington, New Zealand; [Kleijn, W. Bastiaan] Delft Univ Technol, Multimedia Comp Grp, Delft, Netherlands	University of Edinburgh; Victoria University Wellington; Delft University of Technology	Henter, GE (corresponding author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh, Midlothian, Scotland.	gustav.henter@ee.kth.se; bastiaan.kleijn@ecs.vuw.ac.nz	Henter, Gustav Eje/G-4097-2019	Henter, Gustav Eje/0000-0002-1643-1054; Kleijn, W./0000-0002-1973-3920	LISTA (Listening Talker) project; Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET-Open grant [256230]	LISTA (Listening Talker) project; Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET-Open grant	The authors wish to thank associate professor Richard Heusdens at TU Delft for pointing out some inaccuracies in an earlier version of the article. This research was supported by the LISTA (Listening Talker) project. The project LISTA acknowledges the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET-Open grant number: 256230. A major portion of this research took place while G. E. Henter was with the Communication Theory laboratory, School of Electrical Engineering at KTH Royal Institute of Technology, Stockholm, Sweden.	[Anonymous], 2008, 3253 EUR BROADC UN; Arora S, 2006, LECT NOTES COMPUT SC, V4110, P272; Attias H, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P21; BAHL LR, 1983, IEEE T PATTERN ANAL, V5, P179, DOI 10.1109/TPAMI.1983.4767370; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Beck C, 2009, CONTEMP PHYS, V50, P495, DOI 10.1080/00107510902823517; BLAHUT RE, 1972, IEEE T INFORM THEORY, V18, P460, DOI 10.1109/TIT.1972.1054855; BRAND M, 1998, P NEUR INF PROC SYST, P723; Burg J. P., 1978, NEW ANAL TECHNIQUE T, P42; Chen S.F., 1996, P 34 ANN M ASS COMPU, P310, DOI DOI 10.3115/981863.981904; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Del Corso G. M., 2005, TR0520 U PIS DEP INF; DINES J, 2009, P INTERSPEECH, P1391; Domingos P., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P37; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Georgii H-O., 2011, GIBBS MEASURES PHASE; Goldhirsch I., 1987, Journal of Scientific Computing, V2, P33, DOI 10.1007/BF01061511; Henter G.E., 2014, P INTERSPEECH, P1504; Ihara S., 1993, INFORM THEORY CONTIN; Itakura F., 1968, P 6 INT C AC, P17; Jacquet P, 2008, THEOR COMPUT SCI, V395, P203, DOI 10.1016/j.tcs.2008.01.012; Jelinek F., 1980, Pattern Recognition in Practice. Proceedings of an International Workshop, P381; KATZ SM, 1987, IEEE T ACOUST SPEECH, V35, P400, DOI 10.1109/TASSP.1987.1165125; KNESER R, 1995, INT CONF ACOUST SPEE, P181, DOI 10.1109/ICASSP.1995.479394; Mallat S., 2008, WAVELET TOUR SIGNAL; MARTON K, 1994, ISR J MATH, V86, P331, DOI 10.1007/BF02773685; Ozerov A, 2011, IEEE T COMMUN, V59, P1031, DOI 10.1109/TCOMM.2011.012711.100405; Page L., 1999, 199966 STANF U; Rached Z, 2004, IEEE T INFORM THEORY, V50, P917, DOI 10.1109/TIT.2004.826687; Rasanen O., 2008, P NORD PROS AUG, V10, P191; Ron D, 1996, MACH LEARN, V25, P117, DOI 10.1007/BF00114008; Shalizi CR, 2001, J STAT PHYS, V104, P817, DOI 10.1023/A:1010388907793; Stummer W, 2012, IEEE T INFORM THEORY, V58, P1277, DOI 10.1109/TIT.2011.2178139; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tishby Naftali, 1999, ALL C COMM CONTR COM; Tokuda K, 2000, INT CONF ACOUST SPEE, P1315, DOI 10.1109/ICASSP.2000.861820; VERTANEN K, 2004, OVERVIEW DISCRIMINAT; Wang Y., 2009, ADV NEURAL INFORM PR, P2008, DOI DOI 10.1097/EDE.0B013E318231D67A; Wold H., 1954, STUDY ANAL STATIONAR, V2; Young S., 2009, HTK BOOK; Zen H., 2007, P 6 ISCA WORKSH SPEE, P294	44	4	4	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2016	38	12					2487	2500		10.1109/TPAMI.2016.2533382	http://dx.doi.org/10.1109/TPAMI.2016.2533382			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EC2WJ	26929031	Green Accepted			2022-12-18	WOS:000387984700011
J	Meila, M; Chen, H				Meila, Marina; Chen, Harr			Bayesian Non-Parametric Clustering of Ranking Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Rank data; top-t rankings; generalized Mallows model; Dirichlet process mixture; non-parametric clustering	DIRICHLET; INFERENCE	This paper studies the estimation of Dirichlet process mixtures over discrete incomplete rankings. The generative model for each mixture component is the generalized Mallows (GM) model, an exponential family model for permutations which extends seamlessly to top-t rankings. While the GM is remarkably tractable in comparison with other permutation models, its conjugate prior is not. Our main contribution is to derive the theory and algorithms for sampling from the desired posterior distributions under this DPM. We introduce a family of partially collapsed Gibbs samplers, containing as one extreme point an exact algorithm based on slicesampling, and at the other a fast approximate sampler with superior mixing that is still very accurate in all but the lowest ranks. We empirically demonstrate the effectiveness of the approximation in reducing mixing time, the benefits of the Dirichlet process approach over alternative clustering techniques, and the applicability of the approach to exploring large real-world ranking datasets.	[Meila, Marina] Univ Washington, Stat, Seattle, WA 98195 USA; [Chen, Harr] Vat Labs, San Francisco, CA USA	University of Washington; University of Washington Seattle	Meila, M (corresponding author), Univ Washington, Stat, Seattle, WA 98195 USA.	mmp@stat.washington.edu; harr@gmail.com			US National Science foundation (NSF) [IIS-0535100]	US National Science foundation (NSF)(National Science Foundation (NSF))	The authors thank Brendan Murphy for his guidance regarding the college application data and for providing the data itself, and Yi Mao and Guy Lebanon for making available the KDE code. Last but not least, the insightful remarks of an anonymous reviewer challenged us to think harder of the possibilities of our sampling algorithms, and were at the root of several improvements in this manuscript. This work was partly supported by the US National Science foundation (NSF) award IIS-0535100.	Ali A., 2010, NIPS WORKSH COMP SOC; ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; Awasthi P., 2014, ADV NEURAL INFORM PR, P2609; Benter W., 1994, EFFICIENCY RACETRACK, P183; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Busse LM, 2007, ICML 07, P113; Caron F, 2014, ANN APPL STAT, V8, P1145, DOI 10.1214/14-AOAS717; Chierichetti F, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P85, DOI 10.1145/2688073.2688111; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; FLIGNER MA, 1986, J R STAT SOC B, V48, P359; FLIGNER MA, 1990, PSYCHOMETRIKA, V55, P53, DOI 10.1007/BF02294743; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; Gormley IC, 2006, J R STAT SOC A STAT, V169, P361, DOI 10.1111/j.1467-985X.2006.00412.x; Gormley IC, 2008, J AM STAT ASSOC, V103, P1014, DOI 10.1198/016214507000001049; Guiver J., 2009, P 26 ANN INT C MACHI, P377; Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001; Kurihara K., 2006, P ADV NEUR INF PROC, P761; Kurihara K, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2796; Lebanon G, 2008, J MACH LEARN RES, V9, P2401; Luce R, 1959, INDIVIDUAL CHOICE BE; McKay DJC., 2003, INFORM THEORY INFERE; Meila M., 2007, 515 UW STAT; Meila M., 2007, C ART INT UAI, P729; Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013; Meila M, 2010, J MACH LEARN RES, V11, P3481; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193; Rasmussen CE, 2009, IEEE ACM T COMPUT BI, V6, P615, DOI 10.1109/TCBB.2007.70269; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Torralba A, 2005, ADV NEURAL INF PROCE, P1297; Vlachos A., 2008, P WORKSH PRIOR KNOWL, P43	32	4	4	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2016	38	11					2156	2169		10.1109/TPAMI.2016.2515599	http://dx.doi.org/10.1109/TPAMI.2016.2515599			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DZ6AW	26761192	hybrid			2022-12-18	WOS:000385945000002
J	Inoue, N; Shinoda, K				Inoue, Nakamasa; Shinoda, Koichi			Fast Coding of Feature Vectors Using Neighbor-to-Neighbor Search	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Neighbor-to-neighbor search; image classification; video semantic indexing; vector quantization; Gaussian mixture model	IMAGE	Searching for matches to high-dimensional vectors using hard/soft vector quantization is the most computationally expensive part of various computer vision algorithms including the bag of visual word (BoW). This paper proposes a fast computation method, Neighbor-to-Neighbor (NTN) search [1], which skips some calculations based on the similarity of input vectors. For example, in image classification using dense SIFT descriptors, the NTN search seeks similar descriptors from a point on a grid to an adjacent point. Applications of the NTN search to vector quantization, a Gaussian mixture model, sparse coding, and a kernel codebook for extracting image or video representation are presented in this paper. We evaluated the proposed method on image and video benchmarks: the PASCALVOC 2007 Classification Challenge and the TRECVID 2010 Semantic Indexing Task. NTN-VQ reduced the coding cost by 77.4 percent, and NTN-GMM reduced it by 89.3 percent, without any significant degradation in classification performance.	[Inoue, Nakamasa; Shinoda, Koichi] Tokyo Inst Technol, Dept Comp Sci, Tokyo, Japan	Tokyo Institute of Technology	Inoue, N; Shinoda, K (corresponding author), Tokyo Inst Technol, Dept Comp Sci, Tokyo, Japan.	inoue@ks.cs.titech.ac.jp; shinoda@cs.titech.ac.jp	Shinoda, Koichi/D-3198-2014	Shinoda, Koichi/0000-0003-1095-3203				Beis JS, 1997, PROC CVPR IEEE, P1000, DOI 10.1109/CVPR.1997.609451; Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57; Campbell WM, 2006, IEEE SIGNAL PROC LET, V13, P308, DOI 10.1109/LSP.2006.870086; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Ellis C, 2013, INT J COMPUT VISION, V101, P420, DOI 10.1007/s11263-012-0550-7; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; Gao Chenqiang, 2014, P INT C MULT RETR, P305; Gersho A., 1992, VECTOR QUANTIZATION; Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26; Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253; Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913; Hartley R., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587638; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Inoue N., 2011, P ACM MULT, P1357, DOI DOI 10.1145/2072298.2072014; Inoue N., 2011, P TRECVID WORKSH, P380; Inoue N, 2013, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2013.156; Inoue N, 2013, J VIS COMMUN IMAGE R, V24, P1450, DOI 10.1016/j.jvcir.2013.10.005; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272; Krizhevsky A., 2010, Adv Neural Inf Process Syst, V25, P1097; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo JJ, 2013, IEEE I CONF COMP VIS, P1809, DOI 10.1109/ICCV.2013.227; Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Nister D, 2006, IEEE COMP SOC C COMP, V2, P2161, DOI DOI 10.1109/CVPR.2006.264; Omohundro S. M., 1987, Complex Systems, V1, P273; Omohundro S. M., 1989, TR89063 ICSI; Ovsjanikov M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964928; Perronnin F., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383266; Perronnin F, 2006, LECT NOTES COMPUT SC, V3954, P464; Perronnin F, 2015, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2015.7298998; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Simonyan K., 2013, NEURAL INFORM PROCES, P163; Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014; Snoek C. G. M., 2014, P TRECVID WORKSH, P165; Snoek C. G. M., 2013, P TRECVID WORKSH, P178; SPROULL RF, 1991, ALGORITHMICA, V6, P579, DOI 10.1007/BF01759061; Sydorov V, 2014, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2014.182; UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R; Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018; Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207; Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757; Zhao X., 2013, P 21 ACM INT C MULT, P23; Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11	56	4	4	0	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2016	38	6					1170	1184		10.1109/TPAMI.2015.2481390	http://dx.doi.org/10.1109/TPAMI.2015.2481390			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DL4LU	26415151	Green Published			2022-12-18	WOS:000375609000010
J	Domokos, C; Kato, Z				Domokos, Csaba; Kato, Zoltan			Realigning 2D and 3D Object Fragments without Correspondences	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image registration; affine puzzle; realignment of broken fragments; locally linear deformation	REGISTRATION; RECONSTRUCTION; SHAPE	This paper addresses the problem of simultaneous estimation of different linear deformations, resulting in a global non-linear transformation, between an original object and its broken fragments. A general framework is proposed without using correspondences, where the solution of a polynomial system of equations directly provides the parameters of the alignment. We quantitatively evaluate the proposed algorithm on a large synthetic dataset containing 2D and 3D images, where linear (rigid-body and affine) transformations are considered. We also conduct an exhaustive analysis of the robustness against segmentation errors and the numerical stability of the proposed method. Moreover, we present experiments on 2D real images as well as on volumetric medical images.	[Domokos, Csaba] Tech Univ Munich, Comp Vis & Pattern Recognit, D-85748 Garching, Germany; [Kato, Zoltan] Univ Szeged, Inst Informat, H-6701 Szeged, Hungary; [Kato, Zoltan] J Selye Univ, Dept Math & Informat, Komarno, Slovakia	Technical University of Munich; Szeged University; J. Selye University	Domokos, C (corresponding author), Tech Univ Munich, Comp Vis & Pattern Recognit, Boltzmannstr 3, D-85748 Garching, Germany.	c.domokos.res@gmail.com; kato@inf.u-szeged.hu	Kato, Zoltan/AAD-6406-2019		European Union; State of Hungary; European Social Fund through National Excellence Program [TAMOP-4.2.4.A/2-11-1-2012-0001]; National Innovation Office (NIH) & the Hungarian Scientific Research Fund (OTKA) [CNK80370]	European Union(European Commission); State of Hungary; European Social Fund through National Excellence Program; National Innovation Office (NIH) & the Hungarian Scientific Research Fund (OTKA)(Orszagos Tudomanyos Kutatasi Alapprogramok (OTKA))	This work was mainly done while Csaba Domokos was with the Institute of Informatics, University of Szeged. This work has been partially supported by the European Union and the State of Hungary, co-financed by the European Social Fund through project TAMOP-4.2.4.A/2-11-1-2012-0001 National Excellence Program and by the grant CNK80370 of the National Innovation Office (NIH) & the Hungarian Scientific Research Fund (OTKA). The CT images were obtained from the University of Szeged, Department of Trauma Surgery and were used with permission of Prof. Endre Varga, MD.	BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Birgin EG, 2001, ACM T MATH SOFTWARE, V27, P340, DOI 10.1145/502800.502803; Bronstein AM, 2008, LECT NOTES COMPUT SC, V5303, P143, DOI 10.1007/978-3-540-88688-4_11; Chowdhury AS, 2009, COMPUT MED IMAG GRAP, V33, P333, DOI 10.1016/j.compmedimag.2009.01.006; Domokos C, 2012, IEEE T PATTERN ANAL, V34, P943, DOI 10.1109/TPAMI.2011.200; Domokos C, 2010, LECT NOTES COMPUT SC, V6312, P777, DOI 10.1007/978-3-642-15552-9_56; Domokos C, 2010, PATTERN RECOGN, V43, P569, DOI 10.1016/j.patcog.2009.08.013; Elad M, 2004, IEEE T SIGNAL PROCES, V52, P1814, DOI 10.1109/TSP.2004.828919; Erdohelyi B., 2009, P INT C COMP ASS RAD, V4, pS98; Feldmar J, 1996, INT J COMPUT VISION, V18, P99, DOI 10.1007/BF00054998; Fornasier M, 2005, PATTERN RECOGN, V38, P2074, DOI 10.1016/j.patcog.2005.03.014; Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007; Hagege R. R., 2009, P IEEE INT WORKSH MU, P1; Hill DLG, 2001, PHYS MED BIOL, V46, pR1, DOI 10.1088/0031-9155/46/3/201; Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925; Kong WX, 2001, PROC CVPR IEEE, P583; Leitao HCD, 2002, IEEE T PATTERN ANAL, V24, P1239, DOI 10.1109/TPAMI.2002.1033215; Litany O, 2012, LECT NOTES COMPUT SC, V7583, P1, DOI 10.1007/978-3-642-33863-2_1; McBride J. C., 2003, IEEE COMPUTER VISION, V1, P3, DOI [10.1109/CVPRW.2003.10008, DOI 10.1109/CVPRW.2003.10008]; Papaioannou G, 2003, IMAGE VISION COMPUT, V21, P401, DOI 10.1016/S0262-8856(03)00008-8; Periaswamy S, 2006, MED IMAGE ANAL, V10, P452, DOI 10.1016/j.media.2005.03.006; Pettersson J, 2006, IEEE IMAGE PROC, P1185, DOI 10.1109/ICIP.2006.312695; Toler-Franklin C., 2010, TR87410 PRINC U; Ucoluk G, 1999, COMPUT GRAPH-UK, V23, P573, DOI 10.1016/S0097-8493(99)00075-8; Winkelbach S., 2003, P ANN S GERM ASS PAT, P556; Winkelbach S, 2008, INT J COMPUT VISION, V78, P1, DOI 10.1007/s11263-007-0121-5	26	4	4	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2016	38	1					195	202		10.1109/TPAMI.2015.2450726	http://dx.doi.org/10.1109/TPAMI.2015.2450726			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CY8OW	26656587				2022-12-18	WOS:000366669200015
J	Wang, S; Wang, YZ; Zhu, SC				Wang, Shuo; Wang, Yizhou; Zhu, Song-Chun			Learning Hierarchical Space Tiling for Scene Modeling, Parsing and Attribute Tagging	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Scene representation; hierarchical space tiling; scene attributes	OBJECTS	A typical scene category contains an enormous number of distinct scene configurations that are composed of objects and regions of varying shapes in different layouts. In this paper, we first propose a representation named hierarchical space tiling (HST) to quantize the huge and continuous scene configuration space. Then, we augment the HST with attributes (nouns and adjectives) to describe the semantics of the objects and regions inside a scene. We present a weakly supervised method for simultaneously learning the scene configurations and attributes from a collection of natural images associated with descriptive text. The precise locations of attributes are unknown in the input and are mapped to the HST nodes through learning. Starting with a full HST, we iteratively estimate the HST model under a learning-by-parsing framework. Given a test image, we compute the most probable parse tree with the associated attributes by dynamic programming. We quantitatively analyze the representative efficiency of HST, show the learned representation is less ambiguous and has semantically meaningful inner concepts. In applications, we apply our model to four tasks: scene classification, attribute recognition, attribute localization, and pixel-wise scene labeling, and show the performance improvements as well as higher efficiency.	[Wang, Shuo; Wang, Yizhou] Peking Univ, Natl Engn Lab Video Technol, Cooperat Medianet Innovat Ctr, Key Lab Machine Percept MoE,Dept EECS, Beijing 100871, Peoples R China; [Zhu, Song-Chun] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Zhu, Song-Chun] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	Peking University; University of California System; University of California Los Angeles; University of California System; University of California Los Angeles	Wang, S (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Cooperat Medianet Innovat Ctr, Key Lab Machine Percept MoE,Dept EECS, Beijing 100871, Peoples R China.	shuowang@pku.edu.cn; Yizhou.Wang@pku.edu.cn; sczhu@stat.ucla.edu			NSF [CNS-1028381]; MURI ONR [N00014-10-1-0933]; China Scholarship Council;  [NSFC-61272027];  [NSFC-61231010];  [973-2015CB351800]	NSF(National Science Foundation (NSF)); MURI ONR(MURIOffice of Naval Research); China Scholarship Council(China Scholarship Council); ; ; 	This work was supported by 973-2015CB351800, NSFC-61272027, NSFC-61231010, NSF CNS-1028381, MURI ONR N00014-10-1-0933 and China Scholarship Council. The supplementary material can be downloaded at http://vcla.stat.ucla.edu/people/similar to shuo.wang/HST_att.html.	Berg M.d., 2008, COMPUTATIONAL GEOMET, DOI DOI 10.1007/978-3-540-77974-2; Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48; Blaschko MB, 2008, PROC CVPR IEEE, P93, DOI 10.1109/cvpr.2008.4587586; Burkard R., 2009, ASSIGNMENT PROBLEMS; Datta R., 2006, P 14 ANN ACM INT C M, P977; Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Gokalp D., 2007, P IEEE C COMP VIS PA, P1; Gupta A, 2008, LECT NOTES COMPUT SC, V5302, P16, DOI 10.1007/978-3-540-88682-2_3; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Isola P, 2013, IEEE I CONF COMP VIS, P3048, DOI 10.1109/ICCV.2013.457; Jun Zhu, 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P449, DOI 10.1109/WACV.2012.6163023; Kulkarni Girish, 2011, P IEEE C COMP VIS PA; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Li LJ, 2009, PROC CVPR IEEE, P2036, DOI 10.1109/CVPRW.2009.5206718; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Ordonez Vicente, 2011, ADV NEURAL INFORM PR, P1143; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Rubinstein M, 2012, LECT NOTES COMPUT SC, V7574, P85, DOI 10.1007/978-3-642-33712-3_7; Salton G., 1986, INTRO MODERN INFORM; Si ZZ, 2013, IEEE T PATTERN ANAL, V35, P2189, DOI 10.1109/TPAMI.2013.35; Siagian C, 2007, IEEE T PATTERN ANAL, V29, P300, DOI 10.1109/TPAMI.2007.40; Socher R., 2011, P 28 INT C INT C MAC, P129; Su H., 2010, ADV NEURAL PROCESSIN, V1, P1378; Sudderth EB, 2008, INT J COMPUT VISION, V77, P291, DOI 10.1007/s11263-007-0069-5; Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386; Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z; VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010; Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1; Wang G, 2009, IEEE I CONF COMP VIS, P537, DOI 10.1109/ICCV.2009.5459194; Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018; Wang S., 2012, P AS C COMP VIS, P796; Wang S, 2013, PROC CVPR IEEE, P3111, DOI 10.1109/CVPR.2013.400; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yu XD, 2010, LECT NOTES COMPUT SC, V6315, P127; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	41	4	4	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2015	37	12					2478	2491		10.1109/TPAMI.2015.2424880	http://dx.doi.org/10.1109/TPAMI.2015.2424880			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CW2OK	26539852	hybrid			2022-12-18	WOS:000364831700010
J	Xu, Y; Qiu, P; Roysam, B				Xu, Yan; Qiu, Peng; Roysam, Badrinath			Unsupervised Discovery of Subspace Trends	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Trend-relevant feature selection; subspace trend discovery; multivariate data visualization	CLUSTERING-ALGORITHM; EXPRESSION; CLASSIFICATION	This paper presents unsupervised algorithms for discovering previously unknown subspace trends in high-dimensional data sets without the benefit of prior information. A subspace trend is a sustained pattern of gradual/progressive changes within an unknown subset of feature dimensions. A fundamental challenge to subspace trend discovery is the presence of irrelevant data dimensions, noise, outliers, and confusion from multiple subspace trends driven by independent factors that are mixed in with each other. These factors can obscure the trends in conventional dimension reduction & projection based data visualizations. To overcome these limitations, we propose a novel graph-theoretic neighborhood similarity measure for detecting concordant progressive changes across data dimensions. Using this measure, we present an unsupervised algorithm for trend-relevant feature selection, subspace trend discovery, quantification of trend strength, and validation. Our method successfully identified verifiable subspace trends in diverse synthetic and real-world biomedical datasets. Visualizations derived from the selected trend-relevant features revealed biologically meaningful hidden subspace trend(s) that were obscured by irrelevant features and noise. Although our examples are drawn from the biological domain, the proposed algorithm is broadly applicable to exploratory analysis of high-dimensional data including visualization, hypothesis generation, knowledge discovery, and prediction in diverse other applications.	[Xu, Yan; Roysam, Badrinath] Univ Houston, Dept Elect & Comp Engn, Houston, TX 77004 USA; [Qiu, Peng] Georgia Inst Technol, Dept Biomed Engn, Atlanta, GA 30332 USA; [Qiu, Peng] Emory Univ, Atlanta, GA 30322 USA	University of Houston System; University of Houston; University System of Georgia; Georgia Institute of Technology; Emory University	Xu, Y (corresponding author), Univ Houston, Dept Elect & Comp Engn, Houston, TX 77004 USA.	yxu15@uh.edu; peng.qiu@bme.gatech.edu; broysam@central.uh.edu	Xu, Yan/I-9733-2019		DARPA [N66001-11-1-4015]; NIH [R01 CA163481]; NATIONAL CANCER INSTITUTE [R01CA163481] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING [R01EB014955] Funding Source: NIH RePORTER	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NATIONAL CANCER INSTITUTE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI)); NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB))	This work was supported by DARPA Grant N66001-11-1-4015, and NIH Grant R01 CA163481. Badrinath Roysam is the corresponding author.	Achtert Elke, 2007, 2007 International Conference on Scientific and Statistical Database Management, DOI 10.1109/SSDBM.2007.21; Aggarwal CC, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P61, DOI 10.1145/304181.304188; Agrawal R., 1998, SIGMOD Record, V27, P94, DOI 10.1145/276305.276314; [Anonymous], [No title captured]; Belkin M, 2002, ADV NEUR IN, V14, P585; Chang J.-W., 2002, P 2002 ACM S APPL CO, P503; Chen LF, 2012, IEEE T KNOWL DATA EN, V24, P1291, DOI 10.1109/TKDE.2010.256; Cheng C. -H., 1999, P 5 ACM SIGKDD INT C, P84, DOI DOI 10.1145/312129.312199; Chu YH, 2010, IEEE T KNOWL DATA EN, V22, P16, DOI 10.1109/TKDE.2008.224; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102; Cordeiro RLF, 2013, IEEE T KNOWL DATA EN, V25, P387, DOI 10.1109/TKDE.2011.176; Cormen T. H., 2001, INTRO ALGORITHMS, V2nd, P540; Desper R, 2004, J THEOR BIOL, V228, P477, DOI 10.1016/j.jtbi.2004.02.021; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Giesen J, 2000, DISCRETE COMPUT GEOM, V24, P577, DOI 10.1007/s004540010061; Hastie T., 2009, ELEMENTS STAT LEARNI, P523; Hystad ME, 2007, J IMMUNOL, V179, P3662, DOI 10.4049/jimmunol.179.6.3662; Jensen EV, 2003, CLIN CANCER RES, V9, P1980; Kriegel HP, 2009, ACM T KNOWL DISCOV D, V3, DOI 10.1145/1497577.1497578; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Madeira SC, 2004, IEEE ACM T COMPUT BI, V1, P24, DOI 10.1109/TCBB.2004.2; Magwene PM, 2003, BIOINFORMATICS, V19, P842, DOI 10.1093/bioinformatics/btg081; Park Y, 2009, IEEE ACM T COMPUT BI, V6, P200, DOI 10.1109/TCBB.2008.126; Pokharkar S., 2009, P SIAM INT C DAT MIN, P557; Qiu P, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001123; Rao S, 2010, IEEE T PATTERN ANAL, V32, P1832, DOI 10.1109/TPAMI.2009.191; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; SHANBHAG AG, 1994, CVGIP-GRAPH MODEL IM, V56, P414, DOI 10.1006/cgip.1994.1037; Sotiriou C, 2003, P NATL ACAD SCI USA, V100, P10393, DOI 10.1073/pnas.1732912100; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Whitfield ML, 2002, MOL BIOL CELL, V13, P1977, DOI 10.1091/mbc.02-02-0030; Woo KG, 2004, INFORM SOFTWARE TECH, V46, P255, DOI 10.1016/j.infsof.2003.07.003; Zhong Q, 2012, NAT METHODS, V9, P711, DOI [10.1038/NMETH.2046, 10.1038/nmeth.2046]; Zimek A., 2008, P SIAM INT C DAT MIN, P763	41	4	6	1	39	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2015	37	10					2131	2145		10.1109/TPAMI.2015.2394475	http://dx.doi.org/10.1109/TPAMI.2015.2394475			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CQ7VL	26353189				2022-12-18	WOS:000360813400014
J	Bousmalis, K; Zafeiriou, S; Morency, LP; Pantic, M; Ghahramani, Z				Bousmalis, Konstantinos; Zafeiriou, Stefanos; Morency, Louis-Philippe; Pantic, Maja; Ghahramani, Zoubin			Variational Infinite Hidden Conditional Random Fields	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Nonparametric models; discriminative models; hidden conditional random fields; dirichlet processes; variational inference		Hidden conditional random fields (HCRFs) are discriminative latent variable models which have been shown to successfully learn the hidden structure of a given classification problem. An Infinite hidden conditional random field is a hidden conditional random field with a countably infinite number of hidden states, which rids us not only of the necessity to specify a priori a fixed number of hidden states available but also of the problem of overfitting. Markov chain Monte Carlo (MCMC) sampling algorithms are often employed for inference in such models. However, convergence of such algorithms is rather difficult to verify, and as the complexity of the task at hand increases the computational cost of such algorithms often becomes prohibitive. These limitations can be overcome by variational techniques. In this paper, we present a generalized framework for infinite HCRF models, and a novel variational inference approach on a model based on coupled Dirichlet Process Mixtures, the HCRF-DPM. We show that the variational HCRF-DPM is able to converge to a correct number of represented hidden states, and performs as well as the best parametric HCRFs-chosen via cross-validation-for the difficult tasks of recognizing instances of agreement, disagreement, and pain in audiovisual sequences.	[Bousmalis, Konstantinos; Zafeiriou, Stefanos; Pantic, Maja] Univ London Imperial Coll Sci Technol & Med, Dept Comp, London, England; [Bousmalis, Konstantinos] Google Inc, Mountain View, CA USA; [Morency, Louis-Philippe] Univ So Calif, Inst Creat Technol, Los Angeles, CA 90089 USA; [Ghahramani, Zoubin] Univ Cambridge, Cambridge, England	Imperial College London; Google Incorporated; University of Southern California; University of Cambridge	Bousmalis, K (corresponding author), Google Robotics, Mountain View, CA USA.	konstantinos@google.com; s.zafeiriou@imperial.ac.uk; morency@ict.usc.edu; m.pantic@imperial.ac.uk; zoubin@eng.cam.ac.uk			European Community [611153]; EPSRC [EP/J017787/1]; EPSRC [EP/H016988/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/H016988/1, EP/J017787/1] Funding Source: researchfish	European Community(European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The current work of the team is funded in part by the European Community 7th Framework Programme [FP7/2007-2013] under grant agreement no. 611153 (TERESA). The work by Stefanos Zafeiriou is funded in part by the EPSRC project EP/J017787/1 (4DFAB). Konstantinos Bousmalis is the corresponding author.	Beal MJ, 2002, ADV NEUR IN, V14, P577; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; BERTSEKAS DP, 1976, IEEE T AUTOMAT CONTR, V21, P174, DOI 10.1109/TAC.1976.1101194; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Bousmalis K., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P746, DOI 10.1109/FG.2011.5771341; Bousmalis K., 2009, IEEE INT C AFF COMP, P1; Bousmalis K, 2013, IEEE T NEUR NET LEAR, V24, P170, DOI 10.1109/TNNLS.2012.2224882; Chatzis SP, 2010, IEEE T NEURAL NETWOR, V21, P1004, DOI 10.1109/TNN.2010.2046910; Ekman P., 2002, FACIAL ACTION CODING; Eyben F., 2009, 2009 3 INT C AFFECT, P1, DOI DOI 10.1109/ACII.2009.5349350; Fox E. B., 2008, 25 INT C MACHINE LEA, P312; Ghahramani Z, 2001, ADV NEUR IN, V13, P507; Gunawardana A., 2005, P INTERSPEECH, V2, P1; Jiang Y., 2013, P ROB SCI SYST; Lucey P., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Morency L. -P., 2007, P I C COMP VI PATT R, P1, DOI DOI 10.1109/CVPR.2007.383299; Orbanz P, 2008, INT J COMPUT VISION, V77, P25, DOI 10.1007/s11263-007-0061-0; Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Van Gael J., 2008, PROC 25 INT C MACHIN, V25, P1088; Van Gael J., 2009, ADV NEURAL INFORM PR, V21, P1697; Vinciarelli A., 2009, P INT C AFF COMP INT, V2, P96; Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007	25	4	4	1	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2015	37	9					1917	1929		10.1109/TPAMI.2014.2388228	http://dx.doi.org/10.1109/TPAMI.2014.2388228			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CO5RQ	26353136	Green Submitted			2022-12-18	WOS:000359216600014
J	Yamada, M; Sigal, L; Raptis, M; Toyoda, M; Chang, Y; Sugiyama, M				Yamada, Makoto; Sigal, Leonid; Raptis, Michalis; Toyoda, Machiko; Chang, Yi; Sugiyama, Masashi			Cross-Domain Matching with Squared-Loss Mutual Information	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cross-domain object matching; cross-domain temporal alignment; squared-loss mutual information	REDUCTION; ALGORITHM; ALIGNMENT	The goal of cross-domain matching (CDM) is to find correspondences between two sets of objects in different domains in an unsupervised way. CDM has various interesting applications, including photo album summarization where photos are automatically aligned into a designed frame expressed in the Cartesian coordinate system, and temporal alignment which aligns sequences such as videos that are potentially expressed using different features. In this paper, we propose an information-theoretic CDM framework based on squared-loss mutual information (SMI). The proposed approach can directly handle non-linearly related objects/sequences with different dimensions, with the ability that hyper-parameters can be objectively optimized by cross-validation. We apply the proposed method to several real-world problems including image matching, unpaired voice conversion, photo album summarization, cross-feature video and cross-domain video-to-mocap alignment, and Kinect-based action recognition, and experimentally demonstrate that the proposed method is a promising alternative to state-of-the-art CDM methods.	[Yamada, Makoto; Chang, Yi] Yahoo Labs, Sunnyvale, CA 94089 USA; [Sigal, Leonid] Disney Res, Pittsburgh, PA 15213 USA; [Raptis, Michalis] Comcast Labs, Washington, DC USA; [Toyoda, Machiko] NTT CS Labs, Kyoto, Japan; [Sugiyama, Masashi] Univ Tokyo, Grad Sch Frontier Sci, Dept Complex Sci & Engn, Bunkyo Ku, Tokyo 1130033, Japan	University of Tokyo	Yamada, M (corresponding author), Yahoo Labs, Sunnyvale, CA 94089 USA.	makotoy@yahoo-inc.com; lsigal@disneyresearch.com; mraptis@cs.ucla.edu; toyoda.machiko@lab.ntt.co.jp; yichang@yahoo-inc.com; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743; Chang, Yi/0000-0003-2697-8093	JST PRESTO program; AOARD; KAKENHI [25700022]	JST PRESTO program; AOARD; KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	The authors thank Dr. Fernando Villavicencio and Dr. Akisato Kimura for their valuable comments. They also thank Dr. Feng Zhou and Dr. Fernando de la Torre for data and valuable discussions. Makoto Yamada was supported by the JST PRESTO program, and Masashi Sugiyama was supported by AOARD and KAKENHI 25700022.	[Anonymous], 2002, LEARNING KERNELS; Blank M, 2005, IEEE I CONF COMP VIS, P1395; Cover TM, 2006, ELEMENTS INFORM THEO; FINKE G, 1987, ANN DISCRETE MATH, V31, P61; Fukumizu K., 2009, P ADV NEUR INF PROC, P489; Gong D, 2011, IEEE I CONF COMP VIS, P571, DOI 10.1109/ICCV.2011.6126290; Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Hardy G. H., 1952, INEQUALITIES; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315; Jagarlamudi J, 2010, AAAI CONF ARTIF INTE, P1020; Jebara T, 2004, LECT NOTES COMPUT SC, V3120, P609, DOI 10.1007/978-3-540-27819-1_42; Kain A., 1988, P ICASSP SEATTL WA, P285; Keogh E.J., 2001, P 2001 SIAM INT C DA, P1, DOI [10.1137/1.9781611972719.1, DOI 10.1137/1.9781611972719.1]; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1; Listgarten J., 2005, P 18 ANN C NEUR INF, P817; Masood S. Z., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1540, DOI 10.1109/ICCVW.2011.6130433; Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156; Minka T., 2000, OLD NEW MATRIX ALGEB; Pearson K, 1900, PHILOS MAG, V50, P157, DOI 10.1080/14786440009463897; Petersen K. B., 2008, MATRIX COOKBOOK; Quackenbush S.R., 1988, OBJECTIVE MEASURES S; Quadrianto N, 2010, IEEE T PATTERN ANAL, V32, P1809, DOI 10.1109/TPAMI.2009.184; Rabiner L., 1993, FUNDAMENTALS SPEECH; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Sugiyama M, 2012, IEICE T INF SYST, VE95D, P2564, DOI 10.1587/transinf.E95.D.2564; Sugiyama M, 2010, NEURAL NETWORKS, V23, P44, DOI 10.1016/j.neunet.2009.07.007; Suzuki T, 2011, NEURAL COMPUT, V23, P284, DOI 10.1162/NECO_a_00062; Suzuki T, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-S1-S52; Yamada M, 2011, P 14 INT C ART INT S, P807; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zhou F., 2009, ADV NEURAL INFORM PR, V22, P2286; Zhou F, 2013, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2013.376; Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812	43	4	4	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2015	37	9					1764	1776		10.1109/TPAMI.2014.2388235	http://dx.doi.org/10.1109/TPAMI.2014.2388235			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CO5RQ	26353125				2022-12-18	WOS:000359216600003
J	Xu, ZG; MacEachern, S; Xu, XY				Xu, Zhiguang; MacEachern, Steven; Xu, Xinyi			Modeling Non-Gaussian Time Series with Nonparametric Bayesian Model	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Autoregressive process; Copula model; GARCH; probability integral transformation	DISTRIBUTIONS; RETURN	We present a class of Bayesian copula models whose major components are the marginal (limiting) distribution of a stationary time series and the internal dynamics of the series. We argue that these are the two features with which an analyst is typically most familiar, and hence that these are natural components with which to work. For the marginal distribution, we use a nonparametric Bayesian prior distribution along with a cdf-inverse cdf transformation to obtain large support. For the internal dynamics, we rely on the traditionally successful techniques of normal-theory time series. Coupling the two components gives us a family of (Gaussian) copula transformed autoregressive models. The models provide coherent adjustments of time scales and are compatible with many extensions, including changes in volatility of the series. We describe basic properties of the models, show their ability to recover non-Gaussian marginal distributions, and use a GARCH modification of the basic model to analyze stock index return series. The models are found to provide better fit and improved short-range and long-range predictions than Gaussian competitors. The models are extensible to a large variety of fields, including continuous time models, spatial models, models for multiple series, models driven by external covariate streams, and non-stationary models.	[Xu, Zhiguang; MacEachern, Steven; Xu, Xinyi] Ohio State Univ, Dept Stat, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Xu, ZG (corresponding author), Ohio State Univ, Dept Stat, 404 Cockins Hall,1958 Neil Ave, Columbus, OH 43210 USA.	xu.304@osu.edu; snm@stat.osu.edu; xinyi@stat.osu.edu	MacEachern, Steve/C-5299-2017	MacEachern, Steve/0000-0003-4106-1232	US National Science Foundation [DMS-0907070, DMS-10-07682, DMS-12-09194]; Direct For Mathematical & Physical Scien [1209194] Funding Source: National Science Foundation	US National Science Foundation(National Science Foundation (NSF)); Direct For Mathematical & Physical Scien(National Science Foundation (NSF)NSF - Directorate for Mathematical & Physical Sciences (MPS))	The authors would like to thank the associate editor and two anonymous referees for their insightful comments and suggestions. This work was supported in part by the US National Science Foundation under award numbers DMS-0907070, DMS-10-07682 and DMS-12-09194.	Andrews B, 2007, ANN STAT, V35, P844, DOI 10.1214/009053606000001316; Angelidis T., 2004, STAT METHODOL, V1, P105, DOI [DOI 10.1016/J.STAMET.2004.08.004, 10.1016/j.stamet.2004.08.004]; BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372; BOLLERSLEV T, 1986, J ECONOMETRICS, V31, P307, DOI 10.1016/0304-4076(86)90063-1; Box G., 1970, TIME SERIES ANAL; Brockwell P., 1991, TIME SERIES THEORY M; Bush CA, 1996, BIOMETRIKA, V83, P275, DOI 10.1093/biomet/83.2.275; Delatola EI, 2011, BAYESIAN ANAL, V6, P901, DOI 10.1214/11-BA632; DIEBOLT J, 1994, J ROY STAT SOC B MET, V56, P363; Dunson DB, 2008, BIOMETRIKA, V95, P307, DOI 10.1093/biomet/asn012; ESCOBAR MD, 1994, J AM STAT ASSOC, V89, P268, DOI 10.2307/2291223; Fan J, 2003, NONLINEAR TIME SERIE; GLOSTEN LR, 1993, J FINANC, V48, P1779, DOI 10.2307/2329067; Gneiting T, 2011, J AM STAT ASSOC, V106, P746, DOI 10.1198/jasa.2011.r10138; Griffin JE, 2010, BAYESIAN ANAL, V5, P45, DOI 10.1214/10-BA502; Griffin JE, 2006, J AM STAT ASSOC, V101, P179, DOI 10.1198/016214505000000727; Hjort N.L., 2007, BAYESIAN STAT, P193; Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758; Jensen M.J., 2012, 453 U TOR DEP EC; Liu J. S., 2008, MONTE CARLO STRATEGI; MacEachern S., 2000, DEPENDENT DIRI UNPUB; MacEachern S.N., 2007, BAYESIAN STAT, P196; NELSON DB, 1991, ECONOMETRICA, V59, P347, DOI 10.2307/2938260; NELSON DB, 1990, ECONOMET THEOR, V6, P318, DOI 10.1017/S0266466600005296; Nelson R.B., 2006, INTRO COPULAS; Pan JZ, 2004, SCI CHINA SER A, V47, P321, DOI 10.1360/02ys0317; Petrone S, 2009, J R STAT SOC B, V71, P755, DOI 10.1111/j.1467-9868.2009.00708.x; Robert CP., 2010, MONTE CARLO STAT MET, V2nd ed.; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Sklar A., 1959, PUBLICATIONS I STAT, V8, P229, DOI DOI 10.1007/978-3-642-33590-7; West M., 1997, BAYESIAN FORECASTING, V2nd. ed.; Wilson A.G., 2010, P NEUR INF PROC SYST	32	4	5	1	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2015	37	2					372	382		10.1109/TPAMI.2013.222	http://dx.doi.org/10.1109/TPAMI.2013.222			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CB4VD	26353248				2022-12-18	WOS:000349625500013
J	Lim, Y; Jung, K; Kohli, P				Lim, Yongsub; Jung, Kyomin; Kohli, Pushmeet			Efficient Energy Minimization for Enforcing Label Statistics	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computer vision; image segmentation; energy minimization; Markov random fields		Energy minimization algorithms, such as graph cuts, enable the computation of the MAP solution under certain probabilistic models such as Markov random fields. However, for many computer vision problems, the MAP solution under the model is not the ground truth solution. In many problem scenarios, the system has access to certain statistics of the ground truth. For instance, in image segmentation, the area and boundary length of the object may be known. In these cases, we want to estimate the most probable solution that is consistent with such statistics, i.e., satisfies certain equality or inequality constraints. The above constrained energy minimization problem is NP-hard in general, and is usually solved using Linear Programming formulations, which relax the integrality constraints. This paper proposes a novel method that directly finds the discrete approximate solution of such problems by maximizing the corresponding Lagrangian dual. This method can be applied to any constrained energy minimization problem whose unconstrained version is polynomial time solvable, and can handle multiple, equality or inequality, and linear or non-linear constraints. One important advantage of our method is the ability to handle second order constraints with both-side inequalities with a weak restriction, not trivial in the relaxation based methods, and show that the restriction does not affect the accuracy in our cases. We demonstrate the efficacy of our method on the foreground/background image segmentation problem, and show that it produces impressive segmentation results with less error, and runs more than 20 times faster than the state-of-the-art LP relaxation based approaches.	[Lim, Yongsub] Korea Adv Inst Sci & Technol, Dept Comp Sci, Taejon 305701, South Korea; [Jung, Kyomin] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea; [Kohli, Pushmeet] Microsoft Res, Machine Learning & Percept Dept, Cambridge, England	Korea Advanced Institute of Science & Technology (KAIST); Seoul National University (SNU); Microsoft	Lim, Y (corresponding author), Korea Adv Inst Sci & Technol, Dept Comp Sci, Taejon 305701, South Korea.	yongsub@kaist.ac.kr; kjung@snu.ac.kr; pkohli@microsoft.com			Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT & Future Planning [2012R1A1A1014965]	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT & Future Planning	This research was in part funded by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (2012R1A1A1014965).	Blake A., 2004, P 18 EUR C COMP VIS; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y., 2001, P IEEE 18 INT C COMP; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Guignard Monique, 2003, TOP, V11, P151, DOI DOI 10.1007/BF02579036; Klodt A., 2011, P IEEE INT C COMP VI; KOHLI P., 2008, P IEEE C COMP VIS PA; KOHLI P., 2009, P IEEE INT C COMP VI; KOHLI P, 2007, P IEEE C COMP VIS PA; Kohli P, 2010, PROC CVPR IEEE, P1863, DOI 10.1109/CVPR.2010.5539858; Kolev K., 2008, P EUR C COMP VIS ECC; Kolmogorov V., 2002, P EUR C COMP VIS ECC; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kolmogorov Vladimir, 2007, P IEEE INT C COMP VI, P3; KOMODAKIS N, 2007, P IEEE INT C COMP VI; Lim Y., 2010, P 11 EUR C COMP VIS; Lim Y., 2013, ARXIV13077793; Nowozin S., 2009, P IEEE C COMP VIS PA; Pletscher Patrick, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P111, DOI 10.1007/978-3-642-23123-0_12; Ravikumar Pradeep, 2006, P 23 INT C MACH LEAR; ROTH S, 2005, P IEEE C COMP VIS PA; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Sinha S., 2005, P IEEE 10 INT C COMP; SZELISKI R, 2006, P EUR C COMP VIS ECC; Vicente S., 2008, P IEEE C COMP VIS PA; Vogiatzis G., 2005, P IEEE C COMP VIS PA; WERNER T, 2008, P IEEE C COMP VIS PA; Woodford O., 2009, P IEEE 12 INT C COMP; Yedidia J., 2001, P NEUR INF PROC SYST	30	4	4	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2014	36	9					1893	1899		10.1109/TPAMI.2014.2306415	http://dx.doi.org/10.1109/TPAMI.2014.2306415			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AM9OE	26352240				2022-12-18	WOS:000340210100015
J	Tung, T; Matsuyama, T				Tung, Tony; Matsuyama, Takashi			Geodesic Mapping for Dynamic Surface Alignment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Geodesic mapping; surface alignment; dynamic surface; non-rigid deformation; 3D video; MRF	ENERGY MINIMIZATION; DEFORMATION; SIMILARITY; CAPTURE	This paper presents a novel approach that achieves dynamic surface alignment by geodesing mapping. The surfaces are 3D manifold meshes representing non-rigid objects in motion (e.g., humans) which can be obtained by multiview stereo reconstruction. The proposed framework consists of a geodesic mapping (i.e., geodesic diffeomorphism) between surfaces which carry a distance function (namely the global geodesic distance), and a geodesic-based coordinate system (namely the global geodesic coordinates) defined similarly to generalized barycentric coordinates. The coordinates are used to recursively choose correspondence points in non-ambiguous regions using a coarse-to-fine strategy to reliably locate all surface points and define a discrete mapping. Complete point-to-point surface alignment with smooth mapping is then derived by optimizing a piecewise objective function within a probabilistic framework. The proposed technique only relies on surface intrinsic geometrical properties, and does not require prior knowledge on surface appearance (e.g., color or texture), shape (e.g., topology) or parameterization (e.g., mesh connectivity or complexity). The method can be used for numerous applications, such as visual information (e.g., texture) transfer between surface models representing different objects, dense motion flow estimation of 3D dynamic surfaces, wide-timeframe matching, etc. Experiments show compelling results on challenging publicly available real-world datasets.	[Tung, Tony; Matsuyama, Takashi] Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan; [Tung, Tony] Kyoto Univ, Acad Ctr Comp & Media Studies, Kyoto 6068501, Japan	Kyoto University; Kyoto University	Tung, T (corresponding author), Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan.	tm@i.kyoto-u.ac.jp; tung@vision.kuee.kyoto-u.ac.jp			JST-CREST project "Creation of Human-Harmonized Information Technology for Convivial Society"	JST-CREST project "Creation of Human-Harmonized Information Technology for Convivial Society"	This work was supported in part by the JST-CREST project "Creation of Human-Harmonized Information Technology for Convivial Society".	Ahmed N., 2008, P IEEE C CVPR ANCH A; Allard J., 2007, P SIGGRAPH; Bay H., 2006, P 9 ECCV GRAZ AUSTR; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Bronstein AM, 2007, IEEE T VIS COMPUT GR, V13, P902, DOI 10.1109/TVCG.2007.1041; Bronstein AM, 2009, INT J COMPUT VISION, V81, P281, DOI 10.1007/s11263-008-0172-2; Cagniart C., 2009, P IEEE INT WORKSH 3; Cagniart C., 2010, P 11 ECCV HER GREEC; Chazal F, 2009, COMPUT GRAPH FORUM, V28, P1393, DOI 10.1111/j.1467-8659.2009.01516.x; Cheung KM, 2005, INT J COMPUT VISION, V63, P225, DOI 10.1007/s11263-005-6879-4; de Aguiar E., 2007, P IEEE C CVPR MINN M; de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697; Edelsbrunner H., 2004, P SOCG BROOKL NY US; Franco J.-S., 2004, P IEEE COMP SOC C CO, P31; Furukawa Y., 2008, P IEEE C CVPR ANCH A; Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282; Huang P., 2010, P INT S 3DPVT; Huang P, 2010, INT J COMPUT VISION, V89, P362, DOI 10.1007/s11263-010-0319-9; Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x; Jiang H., 2012, P 12 ECCV FLOR IT; Kanade T., 1996, P IEEE C CVPR SAN FR; Kim V. G., 2011, P SIGGRAPH; Klein T., 2005, P TOP BAS METH VIS; Lazarus F, 1998, VISUAL COMPUT, V14, P373, DOI 10.1007/s003710050149; Li H., P SIGGRAPH AS 09 NEW; Li H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077343; Liao M., 2009, P IEEE 12 ICCV KYOT; Lombaert H, 2013, IEEE T PATTERN ANAL, V35, P2143, DOI 10.1109/TPAMI.2012.276; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Matsuyama T, 2004, COMPUT VIS IMAGE UND, V96, P393, DOI 10.1016/j.cviu.2004.03.012; Matsuyama T., 2012, 3D VIDEO ITS APPL; Morse M., 1934, AM MATH SOC; Pascucci V, 2007, P SIGGRAPH; Seitz S., 2006, P IEEE C CVPR WASH D; Shi J., 1994, P IEEE C CVPR WASH D; Sorkine O, 2006, COMPUT GRAPH FORUM, V25, P789, DOI 10.1111/j.1467-8659.2006.00999.x; Starck J., 2007, P 11 IEEE ICCV RIO J; Starck J., 2005, P 10 IEEE ICCV BEIJ; Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68; Stoll C., 2007, P PAC GRAPH TAIP TAI; Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; TEVS A., 2009, P IEEE C CVPR MIAM F; Tung T., 2005, International Journal of Shape Modeling, V11, P91, DOI 10.1142/S0218654305000748; Tung T., 2010, P IEEE C CVPR SAN FR; Tung T, 2012, IEEE T PATTERN ANAL, V34, P1645, DOI 10.1109/TPAMI.2011.258; Varanasi K., 2008, P 10 ECCV MARS FRANC; Vedula S, 2005, IEEE T PATTERN ANAL, V27, P475, DOI 10.1109/TPAMI.2005.63; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Wang Y., 2000, P IEEE C CVPR HILT H; Wardetzky M., 2007, P 5 EUR SGP GEN SWIT; Zaharescu A., 2009, P IEEE C CVPR MIAM F	52	4	6	0	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2014	36	5					901	913		10.1109/TPAMI.2013.179	http://dx.doi.org/10.1109/TPAMI.2013.179			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AH3VN	26353225	Green Submitted			2022-12-18	WOS:000336054200006
J	Martinez-Rego, D; Castillo, E; Fontenla-Romero, O; Alonso-Betanzos, A				Martinez-Rego, David; Castillo, Enrique; Fontenla-Romero, Oscar; Alonso-Betanzos, Amparo			A Minimum Volume Covering Approach with a Set of Ellipsoids	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						One class classification; data clustering; bilevel algorithm; minimum volume covering ellipsoids	LOCAL SENSITIVITY-ANALYSIS; ALGORITHM; QUANTIZATION; COMPUTATION	A technique for adjusting a minimum volume set of covering ellipsoids technique is elaborated. Solutions to this problem have potential application in one-class classification and clustering problems. Its main original features are: 1) It avoids the direct evaluation of determinants by using diagonalization properties of the involved matrices, 2) it identifies and removes outliers from the estimation process, 3) it avoids binary variables resulting from the combinatorial character of the assignment problem that are replaced by continuous variables in the range [0, 1], 4) the problem can be solved by a bilevel algorithm that in its first level determines the ellipsoids and in its second level reassigns the data points to ellipsoids and identifies outliers based on an algorithm that forces the Karush-Kuhn-Tucker conditions to be satisfied. Two theorems provide rigorous bases for the proposed methods. Finally, a set of examples of application in different fields is given to illustrate the power of the method and its practical performance.	[Martinez-Rego, David; Fontenla-Romero, Oscar; Alonso-Betanzos, Amparo] Univ A Coruna, Dept Comp Sci, Fac Informat, La Coruna 15071, Spain; [Castillo, Enrique] Univ Cantabria, Dept Appl Math & Computat Sci, Escuela Tecn Super Ingn Caminos, E-39005 Santander, Cantabria, Spain	Universidade da Coruna; Universidad de Cantabria	Martinez-Rego, D (corresponding author), Univ A Coruna, Dept Comp Sci, Fac Informat, Campus Elvina S-N, La Coruna 15071, Spain.	dmartinez@udc.es; enrique.castillo@unican.es; ofontenla@udc.es; ciamparo@udc.es	Fontenla-Romero, Oscar/A-1142-2015; Alonso-Betanzos, Amparo/Z-6185-2019; Martínez-Rego, David/I-2710-2015; Alonso-Betanzos, Amparo/K-5057-2014; Castillo, Enrique F/A-7858-2008	Fontenla-Romero, Oscar/0000-0003-4203-8720; Alonso-Betanzos, Amparo/0000-0003-0950-0012; Alonso-Betanzos, Amparo/0000-0003-0950-0012; Castillo, Enrique/0000-0002-8570-0844; Martinez Rego, David/0000-0003-1809-1169	Secretaria de Estado de Investigacion of the Spanish Government [TIN 2009-02402, TIN2012-37954]; Xunta de Galicia [CN2011/007, CN2012/211]; European Union ERDF; Spanish Ministry of Education FPU Grant Program	Secretaria de Estado de Investigacion of the Spanish Government; Xunta de Galicia(Xunta de GaliciaEuropean Commission); European Union ERDF(European Commission); Spanish Ministry of Education FPU Grant Program(German Research Foundation (DFG))	This work was supported by Secretaria de Estado de Investigacion of the Spanish Government under projects TIN 2009-02402 and TIN2012-37954, and by the Xunta de Galicia through projects CN2011/007 and CN2012/211, all partially supported by the European Union ERDF. David Martinez Rego would like to thank the support by the Spanish Ministry of Education FPU Grant Program. In addition, the authors would like to thank Masud Moshtaghi for his helpful support in providing the datasets for the clustering experiment.	BARNES ER, 1982, IBM J RES DEV, V26, P759, DOI 10.1147/rd.266.0759; Bishop, 1995, NEURAL NETWORKS PATT; Bishop Christopher M., 2007, PATTERN RECOGNITION, V4, DOI 10.1117/1.2819119; BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918; Boyd S., 1994, LINEAR MATRIX INEQUA, DOI 10.1137/1.9781611970777; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Burkard R., 2009, ASSIGNMENT PROBLEMS; Castillo E, 2007, TOP, V15, P355, DOI 10.1007/s11750-007-0023-2; Castillo E, 2006, ENG OPTIMIZ, V38, P93, DOI 10.1080/03052150500229418; Castillo E, 2006, J OPTIMIZ THEORY APP, V128, P49, DOI 10.1007/s10957-005-7557-y; Castillo E, 2004, TECHNOMETRICS, V46, P430, DOI 10.1198/004017004000000509; Castillo E., 2001, BUILDING SOLVING MAT; Chiu S.L., 1994, J INTELL FUZZY SYST, V2, P267, DOI DOI 10.3233/IFS-1994-2306; Conejo A., 2005, DECOMPOSITION TECHNI; Dolia AN, 2006, LECT NOTES COMPUT SC, V4212, P630; Duda R.O., 2001, PATTERN CLASSIFICATI; Ester M., 1996, P 2 INT C KNOWL DISC, P6226; Franti P, 2006, PATTERN RECOGN, V39, P761, DOI 10.1016/j.patcog.2005.09.012; Gustafson D. E., 1979, Proceedings of the 1978 IEEE Conference on Decision and Control Including the 17th Symposium on Adaptive Processes, P761; Harville D.A., 1997, MATRIX ALGEBRA STATI; Hastie T., 2009, HIERARCHICAL CLUSTER; Hayton P, 2001, ADV NEUR IN, V13, P946; Hinneburg A, 2007, LECT NOTES COMPUT SC, V4723, P70; Hoffmann H, 2007, PATTERN RECOGN, V40, P863, DOI 10.1016/j.patcog.2006.07.009; Japkowicz N, 2001, MACH LEARN, V42, P97, DOI 10.1023/A:1007660820062; Japkowicz N, 1999, CONCEPT LEARNING ABS; John B.S., 1999, NEURAL COMPUT, V13, P2001; John F., 1948, STUDIES ESSAYS PRESE; KHACHIYAN LG, 1993, MATH PROGRAM, V61, P137, DOI 10.1007/BF01582144; Khachiyan LG, 1996, MATH OPER RES, V21, P307, DOI 10.1287/moor.21.2.307; KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209; Kumar P, 2005, J OPTIMIZ THEORY APP, V126, P1, DOI 10.1007/s10957-005-2653-6; Lee J., 2007, NASA AMES PROGNOSTIC, V1, P69; Lewis T., 1994, OUTLIERS STAT DATA; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Martinez-Rego D., 2011, P INT JOINT C NEUR N; Martinus D., 2001, THESIS TU DELFT; MCFADDEN PD, 1984, J SOUND VIB, V96, P69, DOI 10.1016/0022-460X(84)90595-9; Moshtaghi M, 2011, PATTERN RECOGN, V44, P2197, DOI 10.1016/j.patcog.2011.03.007; Nesterov Y., 1994, INTERIOR POINT POLYN, DOI 10.1137/1.9781611970791; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Shivaswamy P.K., 2007, P 11 INT C ART INT S; Somervuo P, 1999, NEURAL PROCESS LETT, V10, P151, DOI 10.1023/A:1018741720065; Sun P, 2004, OPER RES, V52, P690, DOI 10.1287/opre.1040.0115; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Taylor J., 2003, VIBRATION ANAL HDB; Todd MJ, 2007, DISCRETE APPL MATH, V155, P1731, DOI 10.1016/j.dam.2007.02.013; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Vrahatis MN, 2002, J COMPLEXITY, V18, P375, DOI 10.1006/jcom.2001.0633; Wang CJ, 2010, SIAM J OPTIMIZ, V20, P2994, DOI 10.1137/090772514; Xu Rui, 2008, CLUSTERING, V10; Ypma A., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P67, DOI 10.1109/NNSP.1999.788124	53	4	5	0	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2013	35	12					2997	3009		10.1109/TPAMI.2013.94	http://dx.doi.org/10.1109/TPAMI.2013.94			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	245YV	24136436				2022-12-18	WOS:000326502200015
J	Mu, TT; Goulermas, JY				Mu, Tingting; Goulermas, John Yannis			Automatic Generation of Co-Embeddings from Relational Data with Adaptive Shaping	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Relational data; data co-embedding; heterogeneous embedding; data visualization; structural matching		In this paper, we study the co-embedding problem of how to map different types of patterns into one common low-dimensional space, given only the associations (relation values) between samples. We conduct a generic analysis to discover the commonalities between existing co-embedding algorithms and indirectly related approaches and investigate possible factors controlling the shapes and distributions of the co-embeddings. The primary contribution of this work is a novel method for computing co-embeddings, termed the automatic co-embedding with adaptive shaping (ACAS) algorithm, based on an efficient transformation of the co-embedding problem. Its advantages include flexible model adaptation to the given data, an economical set of model variables leading to a parametric co-embedding formulation, and a robust model fitting criterion for model optimization based on a quantization procedure. The secondary contribution of this work is the introduction of a set of generic schemes for the qualitative analysis and quantitative assessment of the output of co-embedding algorithms, using existing labeled benchmark datasets. Experiments with synthetic and real-world datasets show that the proposed algorithm is very competitive compared to existing ones.	[Mu, Tingting; Goulermas, John Yannis] Univ Liverpool, Dept Elect Engn & Elect, Sch Elect Engn Elect & Comp Sci, Liverpool L69 3GJ, Merseyside, England	University of Liverpool	Mu, TT (corresponding author), Univ Liverpool, Dept Elect Engn & Elect, Sch Elect Engn Elect & Comp Sci, Brownlow Hill, Liverpool L69 3GJ, Merseyside, England.	t.mu@liverpool.ac.uk; j.y.goulermas@liverpool.ac.uk	Mu, Tingting/AAV-4795-2020	Mu, Tingting/0000-0001-6315-3432				[Anonymous], 2013, UCI DELVE STATLOG BE; Bellegarda JR, 2005, IEEE SIGNAL PROC MAG, V22, P70, DOI 10.1109/MSP.2005.1511825; Benzecri Jean-Paul, 1973, ANAL DONNEES ANALYSE; Bezdek JC, 2007, IEEE T FUZZY SYST, V15, P890, DOI 10.1109/TFUZZ.2006.889956; Bichot C.-E, 2009, JMMA, V9, P131, DOI [10.1007/s10852-010-9126-0, DOI 10.1007/S10852-010-9126-0]; Cox T., 2000, MULTIDIMENSIONAL SCA; Cristianini N., 2001, NCTR01087 ROYAL HOLL; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Dhillon I.S., 2001, P 7 ACM SIGKDD INT C, P269, DOI DOI 10.1145/502512.502550; Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367; Gao B., 2005, P ACM SIGKDD INT C K, P41, DOI DOI 10.1145/1081870.1081879; Globerson A., 2006, P 21 NAT C ART INT; Globerson A., 2004, P ADV NEUR INF PROC; Globerson A, 2007, J MACH LEARN RES, V8, P2265; Greenacre M., 2009, COMPUTATIONAL STAT D, V53, P3108; Greenacre M, 1983, THEORY APPL CORRES A; Hahsler M, 2008, J STAT SOFTW, V25, P1; Hongyuan Zha, 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P25; Iwata T., 2005, P ADV NEUR INF PROC; Iwata T, 2007, NEURAL COMPUT, V19, P2536, DOI 10.1162/neco.2007.19.9.2536; Long B., 2005, P 11 ACM SIGKDD INT, P635, DOI 10.1145/1081870.1081949; Long Bo, 2006, P 23 INT C MACH LEAR, P585, DOI DOI 10.1145/1143844.1143918; Mu TT, 2012, IEEE T PATTERN ANAL, V34, P2216, DOI 10.1109/TPAMI.2012.20; Rege M, 2008, DATA MIN KNOWL DISC, V16, P276, DOI 10.1007/s10618-008-0091-4; Richardson MW, 1933, PERS J, V12, P36; Sarkar Purnamrita, 2006, Statistical Network Analysis: Models, Issues, and New Directions. ICML 2006 Workshop on Statistical Network Analysis. Revised Selected Papers, P126; Sarkar P., 2007, P 11 INT C ART INT S, P420; Sindhwani V, 2008, IEEE DATA MINING, P1025, DOI 10.1109/ICDM.2008.113; Van Der Maaten L.J.P., 2009, J MACH LEARN RES, V10, P1, DOI [10.1080/ 135062804440 0 0102, DOI 10.1080/13506280444000102]; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wu HM, 2010, COMPUT STAT DATA AN, V54, P767, DOI 10.1016/j.csda.2008.09.029; Xie B, 2011, IEEE T NEURAL NETWOR, V22, P660, DOI 10.1109/TNN.2011.2107919; Yelland P.M., 2010, MATH J, V12; Young F.W., 1996, VISTA VISUAL STAT SY; Zhong H, 2004, PROC CVPR IEEE, P819	36	4	4	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2013	35	10					2340	2356		10.1109/TPAMI.2013.66	http://dx.doi.org/10.1109/TPAMI.2013.66			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	201XB	23969381				2022-12-18	WOS:000323175200003
J	Boddeti, VN; Kumar, BVKV				Boddeti, Vishnu Naresh; Kumar, B. V. K. Vijaya			A Framework for Binding and Retrieving Class-Specific Information to and from Image Patterns Using Correlation Filters	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Biometric security; correlation filters; biometric key-binding; face recognition; palmprint recognition	FUZZY VAULT; BIOMETRIC ENCRYPTION(TM)	We describe a template-based framework to bind class-specific information to a set of image patterns and retrieve that information by matching the template to a query pattern of the same class. This is done by mapping the class-specific information to a set of spatial translations which are applied to the set of image patterns from which a template is designed, taking advantage of the properties of correlation filters. The bound information is retrieved during matching with an authentic query by estimating the spatial translations applied to the images that were used to design the template. In this paper, we focus on the problem of binding information to biometric signatures as an application of our framework. Our framework is flexible enough to allow spreading the information to be bound over multiple pattern classes which, in the context of biometric key-binding, enables multiclass and multimodal biometric key-binding. We demonstrate the effectiveness of the proposed scheme via extensive numerical results on multiple biometric databases.	[Boddeti, Vishnu Naresh; Kumar, B. V. K. Vijaya] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Boddeti, VN (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	naresh@cmu.edu; kumar@ece.cmu.edu		Bhagavatula, Vijayakumar/0000-0001-7126-6381	CyLab at Carnegie Mellon from the US Army Research Office [DAAD19-02-1-0389, W911NF-09-1-0273]	CyLab at Carnegie Mellon from the US Army Research Office	This research was supported by CyLab at Carnegie Mellon under grants DAAD19-02-1-0389 and W911NF-09-1-0273 from the US Army Research Office.	Boddeti VN, 2009, LECT NOTES COMPUT SC, V5558, P919, DOI 10.1007/978-3-642-01793-3_93; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hao F, 2006, IEEE T COMPUT, V55, P1081, DOI 10.1109/TC.2006.138; Hennings-Yeomans PH, 2007, IEEE T INF FOREN SEC, V2, P613, DOI 10.1109/TIFS.2007.902039; Juels A, 2006, DESIGN CODE CRYPTOGR, V38, P237, DOI 10.1007/s10623-005-6343-z; Kholmatov A, 2008, P SPIE, V6819; Kumar BV., 2005, CORRELATION PATTERN, DOI 10.1017/CBO9780511541087; KUMAR BVKV, 1994, OPT LETT, V19, P1556, DOI 10.1364/OL.19.001556; Lee YJ, 2008, IEEE T SYST MAN CY B, V38, P1302, DOI 10.1109/TSMCB.2008.927261; MAHALANOBIS A, 1987, APPL OPTICS, V26, P3633, DOI 10.1364/AO.26.003633; MAHALANOBIS A, 1994, APPL OPTICS, V33, P3751, DOI 10.1364/AO.33.003751; Nandakumar K, 2007, LECT NOTES COMPUT SC, V4642, P927; Phillips PJ, 2005, PROC CVPR IEEE, P947; Pillai JK, 2011, IEEE T PATTERN ANAL, V33, P1877, DOI 10.1109/TPAMI.2011.34; Prabhakar S., 2003, HDB FINGERPRINT RECO; Ratha NK, 2007, IEEE T PATTERN ANAL, V29, P561, DOI 10.1109/TPAMI.2007.1004; Savvides M, 2004, INT C PATT RECOG, P922, DOI 10.1109/ICPR.2004.1334679; Scheirer WJ, 2007, 2007 BIOMETRICS SYMPOSIUM, P30; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; Soutar C, 1998, P SOC PHOTO-OPT INS, V3386, P24, DOI 10.1117/12.304770; Soutar C, 1998, P SOC PHOTO-OPT INS, V3314, P178, DOI 10.1117/12.304705; SOUTAR C, 1999, ICSA GUIDE CRYPTOGRA, P649; Sutcu Yagiz, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563111; Sutcu Y, 2008, IEEE INT SYMP INFO, P2297, DOI 10.1109/ISIT.2008.4595400; Teoh ABJ, 2006, IEEE T PATTERN ANAL, V28, P1892, DOI 10.1109/TPAMI.2006.250; Uludag U, 2005, LECT NOTES COMPUT SC, V3546, P310; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892; Zhang D., 2012, POLYU PALMPR DAT	32	4	4	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2013	35	9					2064	2077		10.1109/TPAMI.2012.244	http://dx.doi.org/10.1109/TPAMI.2012.244			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	186GB	23868770				2022-12-18	WOS:000322029000002
J	Li, RN; Chellappa, R				Li, Ruonan; Chellappa, Rama			Spatiotemporal Alignment of Visual Signals on a Special Manifold	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Spatiotemporal alignment; video matching; stochastic optimization; geometric methods	RECOGNITION; VIEW	We investigate the problem of spatiotemporal alignment of videos, signals, or feature sequences extracted from them. Specifically, we consider the scenario where the spatiotemporal misalignments can be characterized by parametric transformations. Using a nonlinear analytical structure referred to as an alignment manifold, we formulate the alignment problem as an optimization problem on this nonlinear space. We focus our attention on semantically meaningful videos or signals, e.g., those describing or capturing human motion or activities, and propose a new formalism for temporal alignment accounting for executing rate variations among instances of the same video event. The strategy taken in this effort bridges the family of geometric optimization and the family of stochastic algorithms: We regard the search for optimal alignment parameters as a recursive state estimation problem for a particular dynamic System evolving on the alignment manifold. Subsequently, a Sequential Importance Sampling procedure on the alignment manifold is designed for effective alignment. We further extend the basic Sequential Importance Sampling algorithm into a new version called Stochastic Gradient Sequential Importance Sampling, in which we incorporate a steepest descent structure on the alignment manifold and provide a more efficient particle propagation mechanism. We demonstrate the performance of alignment using Manifolds on several types of input data that arise in vision problems.	[Li, Ruonan] Harvard Univ, Zickler Grp, Cambridge, MA 02138 USA; [Chellappa, Rama] Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA	Harvard University; University System of Maryland; University of Maryland College Park	Li, RN (corresponding author), Harvard Univ, Zickler Grp, 33 Oxford St, Cambridge, MA 02138 USA.	ruonanli@seas.harvard.edu; rama@umiacs.umd.edu	Chellappa, Rama/AAJ-1504-2020; Li, Ruonan/J-5126-2014; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012		MURI grant from the US Office of Naval Research [N00014-10-1-0934]	MURI grant from the US Office of Naval Research	This research was supported by a MURI grant from the US Office of Naval Research under N00014-10-1-0934.	Absil P., 2009, UCLINMA2009043; Absil PA, 2004, ACTA APPL MATH, V80, P199, DOI 10.1023/B:ACAP.0000013855.14971.91; Caspi Y, 2002, INT J COMPUT VISION, V48, P39, DOI 10.1023/A:1014803327923; Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148; Caspi Y, 2000, PROC CVPR IEEE, P682, DOI 10.1109/CVPR.2000.854940; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68; Kwon J., 2009, P IEEE C COMP VIS PA; Laptev I, 2005, IEEE I CONF COMP VIS, P816; Lee L, 2000, IEEE T PATTERN ANAL, V22, P758, DOI 10.1109/34.868678; Li R., 2010, P IEEE C COMP VIS PA; Li R, 2009, P IEEE C COMP VIS PA; Li RN, 2010, LECT NOTES COMPUT SC, V6315, P547, DOI 10.1007/978-3-642-15555-0_40; Liu XW, 2004, IEEE T PATTERN ANAL, V26, P662, DOI 10.1109/TPAMI.2004.1273986; Lui Y.M., 2008, P 10 EUR C COMP VIS; Maybank SJ, 2005, INT J COMPUT VISION, V63, P191, DOI 10.1007/s11263-005-6877-6; Padua FLC, 2010, IEEE T PATTERN ANAL, V32, P304, DOI 10.1109/TPAMI.2008.301; Porikli F, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P406, DOI 10.1109/AVSS.2009.95; Rao C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P939; Rossmann W., 2003, LIE GROUPS INTRO LIN; Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Srivastava A, 2004, ADV APPL PROBAB, V36, P43, DOI 10.1239/aap/1077134463; Srivastava A., 2007, P IEEE C COMP VIS PA; Ukrainitz Y, 2006, LECT NOTES COMPUT SC, V3953, P538, DOI 10.1007/11744078_42; Veeraraghavan A, 2009, IEEE T IMAGE PROCESS, V18, P1326, DOI 10.1109/TIP.2009.2017143; Wolf L, 2002, LECT NOTES COMPUT SC, V2351, P370; Wolf L, 2006, INT J COMPUT VISION, V68, P43, DOI 10.1007/s11263-005-4841-0; Wu Y., 2008, P 19 INT C PATT REC; Zhou F., 2009, P NEUR INF PROC SYST, P2286	33	4	4	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2013	35	3					697	715		10.1109/TPAMI.2012.144	http://dx.doi.org/10.1109/TPAMI.2012.144			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	087VS	22778150				2022-12-18	WOS:000314792900014
J	Cohen, M; Shimshoni, I; Rivlin, E; Adam, A				Cohen, Meir; Shimshoni, Ilan; Rivlin, Ehud; Adam, Amit			Detecting Mutual Awareness Events	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Head pose; mutual awareness; social signal processing; sparse 3D structure	VISUAL FOCUS; HEAD POSE; ATTENTION RECOGNITION	It is quite common that multiple human observers attend to a single static interest point. This is known as a mutual awareness event (MAWE). A preferred way to monitor these situations is with a camera that captures the human observers while using existing face detection and head pose estimation algorithms. The current work studies the underlying geometric constraints of MAWEs and reformulates them in terms of image measurements. The constraints are then used in a method that 1) detects whether such an interest point does exist, 2) determines where it is located, 3) identifies who was attending to it, and 4) reports where and when each observer was while attending to it. The method is also applied on another interesting event when a single moving human observer fixates on a single static interest point. The method can deal with the general case of an uncalibrated camera in a general environment. This is in contrast to other work on similar problems that inherently assumes a known environment or a calibrated camera. The method was tested on about 75 images from various scenes and robustly detects MAWEs and estimates their related attributes. Most of the images were found by searching the Internet.	[Cohen, Meir; Rivlin, Ehud] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel; [Shimshoni, Ilan] Univ Haifa, Dept Informat Syst, Fac Social Sci, IL-31905 Haifa, Israel; [Adam, Amit] Microsoft, IL-34405 Haifa, Israel	Technion Israel Institute of Technology; University of Haifa	Cohen, M (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.	meirc@cs.technion.ac.il; ishimshoni@mis.haifa.ac.il; ehudr@cs.technion.ac.il; amita@cs.technion.ac.il						[Anonymous], 2008, 19 BRIT MACH VIS C B, DOI DOI 10.5244/C.22.18; Ba SO, 2006, LECT NOTES COMPUT SC, V4299, P75; Ba SO, 2011, IEEE T PATTERN ANAL, V33, P101, DOI 10.1109/TPAMI.2010.69; BA SO, 2008, P IEEE INT C AC SPEE; Baron-Cohen S., 2005, ORIGINS SOCIAL MIND; BenAbdelkader C, 2002, INT C PATT RECOG, P377, DOI 10.1109/ICPR.2002.1047474; Benfold B., 2009, P 20 BRIT MACH VIS C, V1; Brolly XLC, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P113; Cohen M., 2010, P EUR C COMP VIS WOR; Desolneux A, 2003, ANN STAT, V31, P1822; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999; Dong LG, 2010, LECT NOTES COMPUT SC, V5996, P548; Emery NJ, 2000, NEUROSCI BIOBEHAV R, V24, P581, DOI 10.1016/S0149-7634(00)00025-7; Farenzena M., 2009, P WORKSH PATT REC AR; Frank MC, 2009, COGNITION, V110, P160, DOI 10.1016/j.cognition.2008.11.010; Gallagher AC, 2009, IEEE I CONF COMP VIS, P1187, DOI 10.1109/ICCV.2009.5459340; Gourier N., 2004, P INT WORKSH VIS OBS, P1; HARALICK RM, 1994, INT C PATT RECOG, P493, DOI 10.1109/ICPR.1994.576335; Hartley R., 2004, ROBOTICA; Lienhart R, 2002, IEEE IMAGE PROC, P900; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Osadchy M, 2007, J MACH LEARN RES, V8, P1197; Otsuka  Kazuhiro, 2005, P 7 INT C MULT INT; Smith K, 2008, IEEE T PATTERN ANAL, V30, P1212, DOI 10.1109/TPAMI.2007.70773; Stiefelhagen R., 1999, Visual Information and Information Systems. Third International Conference, VISUAL'99. Proceedings (Lecture Notes in Computer Science Vol.1614), P761; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang Y, 2006, INT C PATT RECOG, P354	30	4	4	0	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2012	34	12					2327	2340		10.1109/TPAMI.2012.49	http://dx.doi.org/10.1109/TPAMI.2012.49			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	021VO	22331857				2022-12-18	WOS:000309913700004
J	Tam, GKL; Lau, RWH				Tam, Gary K. L.; Lau, Rynson W. H.			Embedding Retrieval of Articulated Geometry Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Geometry retrieval; articulated model retrieval; geometry analysis; geometry recognition	DIMENSIONALITY REDUCTION	Due to the popularity of computer games and animation, research on 3D articulated geometry model retrieval has attracted a lot of attention in recent years. However, most existing works extract high-dimensional features to represent models and suffer from practical limitations. First, misalignment in high-dimensional features may produce unreliable euclidean distances and affect retrieval accuracy. Second, the curse of dimensionality also degrades efficiency. In this paper, we propose an embedding retrieval framework to improve the practicability of these methods. It is based on a manifold learning technique, the Diffusion Map (DM). We project all pairwise distances onto a low-dimensional space. This improves retrieval accuracy because intercluster distances are exaggerated. Then we adapt the Density-Weighted Nystrom extension and further propose a novel step to locally align the Nystrom embedding to the eigensolver embedding so as to reduce extension error and preserve retrieval accuracy. Finally, we propose a heuristic to handle disconnected manifolds by augmenting the kernel matrix with multiple similarity measures and shortcut edges, and further discuss the choice of DM parameters. We have incorporated two existing matching algorithms for testing. Our experimental results show improvement in precision at high recalls and in speed. Our work provides a robust retrieval framework for the matching of multimedia data that lie on manifolds.	[Tam, Gary K. L.] Cardiff Univ, Dept Comp Sci & Informat, Cardiff CF24 3AA, S Glam, Wales; [Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China	Cardiff University; City University of Hong Kong	Tam, GKL (corresponding author), Cardiff Univ, Dept Comp Sci & Informat, Queens Bldg,5 Parade, Cardiff CF24 3AA, S Glam, Wales.	kltam327@gmail.com; rynson.lau@cityu.edu.hk	Tam, Gary KL/E-5098-2011	Tam, Gary KL/0000-0001-7387-5180; LAU, Rynson W H/0000-0002-8957-8129	SRG grant from City University of Hong Kong [7002576]; EPSRC	SRG grant from City University of Hong Kong; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors would like to thank the anonymous reviewers for their helpful and constructive comments on this paper. The work described in this paper was partially supported by an SRG grant from City University of Hong Kong (Project Number: 7002576). Gary Tam was supported by an EPSRC research studentship during his study at Durham University.	Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Cox T.F., 1994, MULTIDIMENSIONAL SCA; Demirci MF, 2008, COMPUT VIS IMAGE UND, V110, P312, DOI 10.1016/j.cviu.2007.09.012; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Gershenfeld N., 1999, NATURE MATH MODELING; Ham J., 2005, P INT WORKSH ART INT; He X., 2004, P 12 ANN ACM INT C M, P2; Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282; Ion Adrian, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563032; Jain V, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P118; Kazhdan M., 2003, Symposium on Geometry Processing, P156; LAFON S. S., 2004, THESIS YALE U; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184; Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298; Liu R, 2009, COMPUT GRAPH FORUM, V28, P397, DOI 10.1111/j.1467-8659.2009.01379.x; Ohbuchi R., 2010, P ACM WORKSH 3D OBJ, P63; Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386; Ovsjanikov Maks, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P320, DOI 10.1109/ICCVW.2009.5457682; Platt J. C., 2005, P 10 INT WORKSH ART, P261; Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Ruggeri M., 2008, P EG WORKSH 3D OBJ R; Rustamov R. M., 2007, P 5 EUR S GEOM PROC; Sahbi H., 2008, P IEEE C COMP VIS PA, P1; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8; Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609; Tam GKL, 2007, IEEE T VIS COMPUT GR, V13, P470, DOI 10.1109/TVCG.2007.1011; Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tierny J., 2007, P EUR; Tung T, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P157, DOI 10.1109/SMI.2004.1314503; Wang H., 2006, P ACM MULT, P45, DOI DOI 10.1109/ICASSP.2006.1660275; Zhang H, 2007, P EUR STAT OF THE AR, P1, DOI DOI 10.1109/IPDPS.2007.370248; Zhang K, 2009, NEURAL COMPUT, V21, P121, DOI [10.1162/neco.2009.11-07-651, 10.1162/neco.2008.11-07-651]	42	4	4	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2012	34	11					2134	2146		10.1109/TPAMI.2012.17	http://dx.doi.org/10.1109/TPAMI.2012.17			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	005MR	22231592	Green Accepted, Green Submitted			2022-12-18	WOS:000308755000006
J	Essa, I; Kang, SB; Pollefeys, M				Essa, Irfan; Kang, Sing Bing; Pollefeys, Marc			Guest Editors' Introduction to the Special Section on Award-Winning Papers from the IEEE Conference on Computer Vision and Pattern Recognition 2009 (CVPR 2009)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Essa, Irfan] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA; [Kang, Sing Bing] Microsoft Corp, Redmond, WA 98052 USA; [Pollefeys, Marc] ETH, Dept Comp Sci, CH-8092 Zurich, Switzerland	University System of Georgia; Georgia Institute of Technology; Microsoft; Swiss Federal Institutes of Technology Domain; ETH Zurich	Essa, I (corresponding author), Georgia Inst Technol, Sch Interact Comp, 801 Atlantic Dr, Atlanta, GA 30332 USA.	irfan@gatech.edu; sbkang@microsoft.com; marc.pollefeys@inf.ethz.ch	Pollefeys, Marc/I-7607-2013						0	4	4	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2011	33	12					2339	2340		10.1109/TPAMI.2011.215	http://dx.doi.org/10.1109/TPAMI.2011.215			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	834RE	26807448				2022-12-18	WOS:000295980000002
J	Nejhum, SMS; Chi, YT; Ho, J; Yang, MH				Nejhum, S. M. Shahed; Chi, Yu-Tseh; Ho, Jeffrey; Yang, Ming-Hsuan			Higher-Dimensional Affine Registration and Vision Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Affine registration; point matching; stereo correspondence	OBJECT RECOGNITION; IMAGE REGISTRATION; INVARIANTS	Affine registration has a long and venerable history in computer vision literature, and in particular, extensive work has been done for affine registration in IR2 and IR3. This paper studies affine registration in R-m with m typically ranging from 4 to 12. To justify breaking of this dimension barrier, the first part of the paper describes three novel matching problems that can be formulated and solved as affine point-set registration problems in dimensions greater than three: stereo correspondence under motion, image set matching, and covariant point-set matching, problems that are not only interesting in their own right but also have potential for important vision applications. Unfortunately, most of the existing affine registration algorithms do not generalize easily to higher dimensions due to their inefficiency. Therefore, the second part of this paper develops a novel algorithm for estimating the affine transform between two point sets in R-m. Specifically, the algorithm follows the common approach of iteratively solving the correspondences and transform. The initial correspondences are determined using the novel notion of local spectral features, features constructed from local distance matrices. Unlike many correspondence-based methods, the proposed algorithm is capable of registering point sets of different size, and the use of local features provides some degree of robustness against noise and outliers. The proposed algorithm is validated on a variety of synthetic point sets in different dimensions with varying degrees of deformation and noise, and the paper also shows experimentally that several instances of the aforementioned three matching problems can indeed be solved satisfactorily using the proposed affine registration algorithm.	[Nejhum, S. M. Shahed; Chi, Yu-Tseh; Ho, Jeffrey] Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32607 USA; [Yang, Ming-Hsuan] Univ Calif Merced, Dept Elect Engn & Comp Sci, Merced, CA 95344 USA	State University System of Florida; University of Florida; University of California System; University of California Merced	Nejhum, SMS (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32607 USA.	smshahed@cise.ufl.edu; ychi@cise.ufl.edu; jho@cise.ufl.edu; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019	Yang, Ming-Hsuan/0000-0003-4848-2304; 	US National Science Foundation (NSF) [IIS-0916001]; Google; University of California	US National Science Foundation (NSF)(National Science Foundation (NSF)); Google(Google Incorporated); University of California(University of California System)	The authors thank one of the reviewers for providing many invaluable comments and suggestions. S.M. Shahed Nejhum, Y.-T. Chi, and J. Ho are partially supported by a US National Science Foundation (NSF) grant IIS-0916001. M.-H. Yang is supported in part by a Google faculty award and University of California Merced faculty start-up fund.	Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Brand M, 2001, PROC CVPR IEEE, P456; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Campbell N. A., 1980, Applied Statistics, V29, P231, DOI 10.2307/2346896; Carcassoni M, 2003, PATTERN RECOGN, V36, P193, DOI 10.1016/S0031-3203(02)00054-7; CHI YT, 2008, P EUR C COMP VIS, V4, P256; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; De la Torre F, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P362, DOI 10.1109/ICCV.2001.937541; Du SY, 2010, PATTERN RECOGN LETT, V31, P791, DOI 10.1016/j.patrec.2010.01.020; Duda R.O., 2000, PATTERN CLASSIFICATI; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004; FLUSSER J, 1993, PATTERN RECOGN, V26, P167, DOI 10.1016/0031-3203(93)90098-H; GARLAND M, 1998, P SIGGRAPH 98, P209; Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5; Gope C, 2007, PATTERN RECOGN, V40, P309, DOI 10.1016/j.patcog.2006.04.026; GOSHTASBY A, 1985, IEEE T SYST MAN CYB, V15, P631, DOI 10.1109/TSMC.1985.6313439; GRANGER S, 2002, P EUR C COMP VIS, V3, P418; Hagedoorn M, 1999, INT J COMPUT VISION, V31, P203, DOI 10.1023/A:1008022116857; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; HEATH MT, 2002, SCI COMPUTING INT SU; Heikkila J, 2004, PATTERN RECOGN, V37, P1825, DOI 10.1016/j.patcog.2004.03.005; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; HUTTENLOCHER DP, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P263; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; LAMDAN Y, 1990, IEEE T ROBOTIC AUTOM, V6, P578, DOI 10.1109/70.62047; LI H, 2007, P IEEE INT C COMP VI; Makadia A., 2006, P IEEE C COMP VIS PA, V1, P1297, DOI DOI 10.1109/CVPR.2006.122; PAULY M, 2001, P ACM SIGGRAPH, P379, DOI DOI 10.1145/383259.383301; REISS TH, 1991, IEEE T PATTERN ANAL, V13, P830, DOI 10.1109/34.85675; Rucklidge WJ, 1997, INT J COMPUT VISION, V24, P251, DOI 10.1023/A:1007975324482; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; RUYMGAART FH, 1981, J MULTIVARIATE ANAL, V11, P485, DOI 10.1016/0047-259X(81)90091-9; SARFARI R, 1996, P IEEE S CIRC SYST A, V2, P656; SCLAROFF S, 1995, IEEE T PATTERN ANAL, V17, P545, DOI 10.1109/34.387502; SCOTT GL, 1991, P ROY SOC B-BIOL SCI, V244, P21, DOI 10.1098/rspb.1991.0045; SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3; Sharp GC, 2002, IEEE T PATTERN ANAL, V24, P90, DOI 10.1109/34.982886; SPRINZAK J, 1994, PATTERN RECOGN LETT, V15, P337, DOI 10.1016/0167-8655(94)90081-7; STOCKMAN G, 1987, COMPUT VISION GRAPH, V40, P361, DOI 10.1016/S0734-189X(87)80147-0; Suesse H, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P375; TOAMSI C, 1992, INT J COMPUT VISION, V9, P137; Voss K., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P155; Wakahara T, 2001, IEEE T PATTERN ANAL, V23, P384, DOI 10.1109/34.917573; WANG Z, 2009, P IEEE C COMP VIS PA; Wolfson HJ, 1997, IEEE COMPUT SCI ENG, V4, P10, DOI 10.1109/99.641604; XU L, 1995, IEEE T NEURAL NETWOR, V6, P131, DOI 10.1109/72.363442; Yang ZW, 1999, IEEE T PATTERN ANAL, V21, P804, DOI 10.1109/34.784312; Yang ZW, 1999, IEEE T IMAGE PROCESS, V8, P934, DOI 10.1109/83.772236; ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149	56	4	5	0	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2011	33	7					1324	1338		10.1109/TPAMI.2010.219	http://dx.doi.org/10.1109/TPAMI.2010.219			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	763QE	21135439	Green Submitted			2022-12-18	WOS:000290574000004
J	Agarwal, A; Blake, A				Agarwal, Ankur; Blake, Andrew			Dense Stereo Matching over the Panum Band	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Stereoscopic vision; energy minimization; Panum's fusional area; 3D vision; active vision		Stereo matching algorithms conventionally match over a range of disparities sufficient to encompass all visible 3D scene points. Human vision, however, works over a narrow band of disparities-Panum's fusional band-whose typical range may be as little as 1/20 of the full range of disparities for visible points. Only points inside the band are fused visually; the remainder of points are seen diplopically. A probabilistic approach is presented for dense stereo matching under the Panum band restriction. It is shown that existing dense stereo algorithms are inadequate in this problem setting and the main problem is segmentation, marking the image into the areas that fall inside the band. An approximation is derived that makes up for missing out-of-band information with a "proxy" based on image autocorrelation. It is shown that the Panum Proxy algorithm achieves accuracy close to what can be obtained when the full disparity band is available, and with gains of between one and two orders of magnitude in computation time. There are also substantial gains in computation space. Panum band processing is also demonstrated in an active stereopsis framework.	[Agarwal, Ankur; Blake, Andrew] Microsoft Res UK Ltd, Cambridge CB3 0FB, England		Agarwal, A (corresponding author), Microsoft Res UK Ltd, 7 JJ Thomson Ave, Cambridge CB3 0FB, England.	ankur.agarwal1@gmail.com; Andrew.Blake@microsoft.com						Abbott A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P532, DOI 10.1109/CCV.1988.590034; AGARWAL A, 2006, P IEEE C COMP VIS PA, V2, P2339; BAKER HH, 1981, P 7 INT JOINT C ART, P631; BELHUMEUR PN, 1996, P EUR C COMP VIS, P45; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; BROWN C, 1992, ACTIVE VISION, P123; Cox IJ, 1996, COMPUT VIS IMAGE UND, V63, P542, DOI 10.1006/cviu.1996.0040; Criminisi A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P191; FRISBY J, 1980, VISION RES, V20, P727, DOI 10.1016/0042-6989(80)90099-1; Hirschmuller H., 2007, P IEEE C COMP VIS PA; Julesz B., 1971, FDN CYCLOPEAN PERCEP; KOLMOGOROV V, 2001, P INT C COMP VIS; KOLMOGOROV V, 2002, P EUR C COMP VIS, P82; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1480, DOI 10.1109/TPAMI.2006.193; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; PAHLAVAN K, 1993, FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION : PROCEEDINGS, P412; POGGIO GF, 1984, ANNU REV NEUROSCI, V7, P379, DOI 10.1146/annurev.ne.07.030184.002115; Reid I. D., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P76, DOI 10.1109/ICCV.1993.378233; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977	20	4	5	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2010	32	3					416	430		10.1109/TPAMI.2008.298	http://dx.doi.org/10.1109/TPAMI.2008.298			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	543WG	20075469				2022-12-18	WOS:000273609600003
J	Aizawa, K; Tanaka, S				Aizawa, Kunio; Tanaka, Shojiro			A Constant-Time Algorithm for Finding Neighbors in Quadtrees	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image processing; quadtrees; linear quadtrees; neighbor finding	TRANSFORM; OCTREES	Quadtrees and linear quadtrees are well-known hierarchical data structures to represent square images of size 2(r) x 2(r). Finding the neighbors of a specific leaf node is a fundamental operation for many algorithms that manipulate quadtree data structures. In quadtrees, finding neighbors takes O(r) computational time for the worst case, where r is the resolution (or height) of a given quadtree. Schrack [1] proposed a constant-time algorithm for finding equal-sized neighbors in linear quadtrees. His algorithm calculates the location codes of equal-sized neighbors; it says nothing, however, about their existence. To ensure their existence, additional checking of the location codes is needed, which usually takes O(r) computational time. In this paper, a new algorithm to find the neighbors of a given leaf node in a quadtree is proposed which requires just O(1) (i.e., constant) computational time for the worst case. Moreover, the algorithm takes no notice of the existence or nonexistence of neighbors. Thus, no additional checking is needed. The new algorithm will greatly reduce the computational complexities of almost all algorithms based on quadtrees.	[Aizawa, Kunio; Tanaka, Shojiro] Shimane Univ, Dept Math & Comp Sci, Interdisciplinary Fac Sci & Engn, Matsue, Shimane 6908502, Japan	Shimane University	Aizawa, K (corresponding author), Shimane Univ, Dept Math & Comp Sci, Interdisciplinary Fac Sci & Engn, 1060 Nishi Kawatsu Cho, Matsue, Shimane 6908502, Japan.	aizawa@cis.shimane-u.ac.jp; tanaka@cis.shimane-u.ac.jp						BERG M, 1998, COMPUTATIONAL GEOMET; Finkel R. A., 1974, Acta Informatica, V4, P1, DOI 10.1007/BF00288933; FUHRMANN DR, 1988, IEEE T PATTERN ANAL, V10, P955, DOI 10.1109/34.9118; GARGANTINI I, 1982, COMMUN ACM, V25, P905, DOI 10.1145/358728.358741; SAMET H, 1989, COMPUT VISION GRAPH, V46, P367, DOI 10.1016/0734-189X(89)90038-8; SAMET H, 1982, IEEE T PATTERN ANAL, V4, P298, DOI 10.1109/TPAMI.1982.4767246; SAMET H, 1983, COMMUN ACM, V26, P680, DOI 10.1145/358172.358409; SAMET H, 1985, IEEE T PATTERN ANAL, V7, P94, DOI 10.1109/TPAMI.1985.4767622; Samet H., 1990, DESIGN ANAL SPATIAL, V85; SAMET H, 1982, COMPUTER GRAPHICS IM, V18, P35; SAMET H, 1990, APPL SPACIAL DATA ST; SCHRACK G, 1992, CVGIP-IMAG UNDERSTAN, V55, P221, DOI 10.1016/1049-9660(92)90022-U; Voros J, 1997, PATTERN RECOGN LETT, V18, P955, DOI 10.1016/S0167-8655(97)80001-3; YANG SN, 1993, J INF SCI ENG, V9, P81	14	4	6	0	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2009	31	7					1178	1183		10.1109/TPAMI.2008.145	http://dx.doi.org/10.1109/TPAMI.2008.145			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	447KB	19443917				2022-12-18	WOS:000266188900003
J	Ross, MG; Kaelbling, LP				Ross, Michael G.; Kaelbling, Leslie Pack			Segmentation According to Natural Examples: Learning Static Segmentation from Motion Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Segmentation; machine learning; motion; computer vision; Markov random field		The Segmentation According to Natural Examples (SANE) algorithm learns to segment objects in static images from video training data. SANE uses background subtraction to find the segmentation of moving objects in videos. This provides object segmentation information for each video frame. The collection of frames and segmentations forms a training set that SANE uses to learn the image and shape properties of the observed motion boundaries. When presented with new static images, the trained model infers segmentations similar to the observed motion segmentations. SANE is a general method for learning environment-specific segmentation models. Because it can automatically generate training data from video, it can adapt to a new environment and new objects with relative ease, an advantage over untrained segmentation methods or those that require human-labeled training data. By using the local shape information in the training data, it outperforms a trained local boundary detector. Its performance is competitive with a trained top-down segmentation algorithm that uses global shape. The shape information it learns from one class of objects can assist the segmentation of other classes.	[Ross, Michael G.] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA; [Kaelbling, Leslie Pack] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Ross, MG (corresponding author), MIT, Dept Brain & Cognit Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	mgross@mit.edu; lpk@csail.mit.edu			Defense Advanced Research Projects Agency (DARPA) [NBCHD030010]; Singapore-MIT Alliance agreement	Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Singapore-MIT Alliance agreement	The authors thank John Winn for adapting his LOCUS code and running it on the SANE data, and David Martin, Charless Fowlkes, Chris Stauffer, Joshua Migdal, Timothee Cour, Stella Yu, and Jianbo Shi for providing software and (in many cases) help with adapting it to the data. They also thank Andrew Cohen and Aude Oliva for postdoctoral support of Michael Ross, and Bill Freeman, Tomas Lozano-Perez, Alan Yuille, and many others for their advice and encouragement. Additionally, the authors thank the anonymous reviewers and the TPAMI editors, whose questions, comments, and suggestions greatly improved the paper. This work was funded in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract NBCHD030010, and in part by the Singapore-MIT Alliance agreement dated 11/6/98. This paper contains material previously published and copyrighted by the Massachusetts Institute of Technology [29] and the Association for the Advancement of Artificial Intelligence [32].	BESAG J, 1974, J ROYAL STAT SOC B, V36; Besag J., 1986, J ROYAL STAT SOC B, V48; BORENSTEIN E, 2002, P 7 EUR C COMP VIS; BORENSTEIN E, 2004, P 8 EUR C COMP VIS; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; Cour T., 2004, MATLAB NORMALIZED CU; Fitzpatrick P. M., 2003, THESIS MIT; Freeman W. T., 2000, INT J COMPUTER VISIO, V40; Gelman A., 2003, BAYESIAN DATA ANAL, DOI 10.1201/b16018; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HESKES T, 2004, NEURAL COMPUTATION, V16; JIROUSEK R, 1995, COMPUT STAT DATA AN, V19, P177, DOI 10.1016/0167-9473(93)E0055-9; KASS M, 1987, P INT C COMP VIS; KONISHI SM, 2003, IEEE T PATTERN ANAL, V25; Kumar M. P., 2005, P COMP VIS PATT REC; KUMAR S, 2003, P INT C COMP VIS; Lafferty John, 2001, CONDITIONAL RANDOM F, P282; LEIBE B, 2004, P EUR C COMP VIS WOR; LEVIN A, 2006, P 9 EUR C COMP VIS; Marr D., 1982, VISION; MARTIN D, 2004, MATLAB BOUNDARY DETE; MARTIN D, 2001, P INT C COMP VIS; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Migdal J., 2005, P IEEE WORKSH MOT VI; Murphy K. P., 1999, P 15 C UNC ART INT; Palmer S.E., 1999, VISION SCI PHOTONS P; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; REN X, 2005, P INT C COMP VIS; ROSS MG, 2003, P NIPS WORKSH OP CHA; ROSS MG, 2005, P 20 NAT C ART INT; ROSS MG, 2003, AIM2003022 MIT ART I; ROSS MG, 2005, THESIS MIT; Russell SJ, 1995, ARTIF INTELL, V4th; SHASHUA A, 1988, P INT C COMP VIS; Shi J., 1997, P COMP VIS PATT REC; SPELKE ES, 1994, COGNITIVE NEUROSCIEN, P165; Stauffer C., 1999, P 1999 IEEE COMP SOC, VVolume 2; TU Z, 2003, P INT C COMP VIS; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; Wainwright M. J., 2003, P IEEE 9 INT WORKSH, P1; Wainwright MJ, 2003, IEEE T INFORM THEORY, V49, P1120, DOI 10.1109/TIT.2003.810642; WAINWRIGHT MJ, 2002, 1616 MIT ART INT LAB; WAINWRIGHT MJ, 2002, COMMUNICATION; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Winn J. M., 2005, P INT C COMP VIS; YANOVER C, 2003, P ADV NEUR INF PROC	46	4	4	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2009	31	4					661	676		10.1109/TPAMI.2008.109	http://dx.doi.org/10.1109/TPAMI.2008.109			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	407WX	19229082	Green Published			2022-12-18	WOS:000263396100007
J	Loog, M				Loog, Marco			On distributional assumptions and whitened cosine similarities	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						decision rule; class distributions; distributional assumptions; consistency; whitened cosine similarity		Recently, an interpretation of the whitened cosine similarity measure as a Bayes decision rule was proposed [1]. This communication makes the observation that some of the distributional assumptions made to derive this measure are very restrictive and, considered simultaneously even inconsistent.	Delft Univ Technol, Fac Elect Engn Math & Comp Sci, Pattern Recognit Grp, Delft, Netherlands	Delft University of Technology	Loog, M (corresponding author), Delft Univ Technol, Fac Elect Engn Math & Comp Sci, Pattern Recognit Grp, Delft, Netherlands.	m.loog@tudelft.nl						FUKUNAGA K, 1990, INTRO STAT PATTERN; Liu CJ, 2007, IEEE T PATTERN ANAL, V29, P1086, DOI 10.1109/TPAMI.2007.1063; Loog M, 2001, IEEE T PATTERN ANAL, V23, P762, DOI 10.1109/34.935849; Mclachlan GJ., 2005, DISCRIMINANT ANAL ST	4	4	4	1	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2008	30	6					1114	1115		10.1109/TPAMI.2007.70838	http://dx.doi.org/10.1109/TPAMI.2007.70838			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	286UW	18421115				2022-12-18	WOS:000254872500015
J	Wyatt, C; Bayram, E; Ge, YR				Wyatt, C; Bayram, E; Ge, YR			Minimum reliable scale selection in 3D	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						edge and feature detection; filtering; scale selection; image models	3-DIMENSIONAL EDGE OPERATOR; IMAGES; SEGMENTATION; DERIVATIVES; DETECTOR; DESIGN	Multiscale analysis is often required in image processing applications because image features are optimally detected at different levels of resolution. With the advance of high-resolution 3D imaging, the extension of multiscale analysis to higher dimensions is necessary. This paper extends an existing 2D scale selection method, known as the minimum reliable scale, to 3D volumetric images. The method is applied to 3D boundary detection and is illustrated in examples from biomedical imaging. The experimental results show that the 3D scale selection improves the detection of edges over single scale operators using as few as three different scales.	Virginia Polytech Inst & State Univ, Dept Elect & Comp Engn, Blacksburg, VA 24061 USA; Wake Forest Univ, Dept Biomed Engn, Winston Salem, NC 27157 USA; IDX Syst Corp, Winston Salem, NC 27101 USA	Virginia Polytechnic Institute & State University; Wake Forest University	Wyatt, C (corresponding author), Virginia Polytech Inst & State Univ, Dept Elect & Comp Engn, 340 Whittemore Hall, Blacksburg, VA 24061 USA.	clwyatt@vt.edu; ebayram@wfubmc.edu; gey@idx.com			NCI NIH HHS [1 R01 CA 78485-01A1] Funding Source: Medline; NATIONAL CANCER INSTITUTE [R55CA078485, R01CA078485] Funding Source: NIH RePORTER	NCI NIH HHS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI)); NATIONAL CANCER INSTITUTE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI))		Bentum MJ, 1996, IEEE T VIS COMPUT GR, V2, P242, DOI 10.1109/2945.537307; BERGHOLM F, 1987, IEEE T PATTERN ANAL, V9, P726, DOI 10.1109/TPAMI.1987.4767980; Bhattacharya P, 1996, COMPUT BIOL MED, V26, P315, DOI 10.1016/0010-4825(96)00003-0; Blom J., 1993, Journal of Visual Communication and Image Representation, V4, P1, DOI 10.1006/jvci.1993.1001; BOMANS M, 1990, IEEE T MED IMAGING, V9, P177, DOI 10.1109/42.56342; Bricault I, 1997, COMPUT VIS IMAGE UND, V67, P24, DOI 10.1006/cviu.1996.0501; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Collins DL, 1998, IEEE T MED IMAGING, V17, P463, DOI 10.1109/42.712135; Danielsson PE, 2001, J VIS COMMUN IMAGE R, V12, P255, DOI 10.1006/jvci.2000.0472; Elder JH, 1998, IEEE T PATTERN ANAL, V20, P699, DOI 10.1109/34.689301; Elder JH, 1999, INT J COMPUT VISION, V34, P97, DOI 10.1023/A:1008183703117; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; JEONG H, 1992, IEEE T PATTERN ANAL, V14, P579, DOI 10.1109/34.134062; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; LIU HK, 1977, COMPUT VISION GRAPH, V6, P123, DOI 10.1016/S0146-664X(77)80008-7; LU Y, 1989, IEEE T PATTERN ANAL, V11, P337, DOI 10.1109/34.19032; LUO LM, 1993, IEEE T BIO-MED ENG, V40, P693, DOI 10.1109/10.237699; MONGA O, 1991, CVGIP-IMAG UNDERSTAN, V53, P76, DOI 10.1016/1049-9660(91)90006-B; MONGA O, 1995, COMPUT VIS IMAGE UND, V61, P171, DOI 10.1006/cviu.1995.1014; MORGENTHALER DG, 1981, IEEE T PATTERN ANAL, V3, P482, DOI 10.1109/TPAMI.1981.4767134; Niessen WJ, 1999, INT J COMPUT VISION, V31, P185, DOI 10.1023/A:1008070000018; Papoulis A., 2002, PROBABILITY RANDOM V; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; Sporring Jon, 1997, GAUSSIAN SCALE SPACE; Viniotis Y., 1998, PROBABILITY RANDOM P; WEICKERT J, 1997, P 1 INT C SCAL SPAC, P3; WHITAKER RT, 1993, CVGIP-IMAG UNDERSTAN, V57, P99, DOI 10.1006/ciun.1993.1006; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; ZHAN SM, 1994, CVGIP-IMAG UNDERSTAN, V59, P242, DOI 10.1006/cviu.1994.1018; ZUCKER SW, 1981, IEEE T PATTERN ANAL, V3, P324, DOI 10.1109/TPAMI.1981.4767105	32	4	4	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2006	28	3					481	U1		10.1109/TPAMI.2006.58	http://dx.doi.org/10.1109/TPAMI.2006.58			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	001FB	16526434				2022-12-18	WOS:000234517900014
J	de Campos, TE; Tordoff, BJ; Murray, DW				de Campos, TE; Tordoff, BJ; Murray, DW			Recovering articulated pose: A comparison of two pre and postimposed constraint methods	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						visual tracking; articulated objects; motion constraints	MODEL-BASED TRACKING	We contrast the performance of two methods of imposing constraints during the tracking of articulated objects, the first method preimposing the kinematic constraints during tracking and, thus, using the minimum degrees of freedom, and the second imposing constraints after tracking and, hence, using the maximum. Despite their very different formulations, the methods recover the same pose change. Further comparisons are drawn in terms of computational speed and algorithmic simplicity and robustness, and it is the last area which is the most telling. The results suggest that using built-in constraints is well-suited to tracking individual articulated objects, whereas applying constraints afterward is most suited to problems involving contact and breakage between articulated (or rigid) objects, where the ability to test tracking performance quickly with constraints turned on or off is desirable.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England; Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Oxford; University of Cambridge	de Campos, TE (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	teo@robots.ox.ac.uk; bjt21@eng.cam.ac.uk; dwm@robots.ox.ac.uk	de Campos, Teofilo/ABF-8003-2020	de Campos, Teofilo/0000-0001-6172-0229				Brand M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1237, DOI 10.1109/ICCV.1999.790422; Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581; DECAMPOS TE, 2005, 227905 OUEL; Deutscher J, 2000, PROC CVPR IEEE, P126, DOI 10.1109/CVPR.2000.854758; DRUMMON T, 2001, P 8 INT C COMP VIS; Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620; DRUMMOND T, 2000, P 6 EUR C COMP VIS E, P20; Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056; Harris C., 1992, ACTIVE VISION, P59; Heap T, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P140, DOI 10.1109/AFGR.1996.557255; HelOr Y, 1996, INT J COMPUT VISION, V19, P5, DOI 10.1007/BF00131146; HOGG D, 1983, IMAGE VISION COMPUT, V1, P5, DOI DOI 10.1016/0262-8856(83)90003-3; ISARD M, 1996, P EUR C COMP VIS, P343; Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241; Morris DD, 2003, INT J ROBOT RES, V22, P393, DOI 10.1177/0278364903022006004; Nickels K, 2001, IEEE T ROBOTIC AUTOM, V17, P28, DOI 10.1109/70.917080; REHG JM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P612, DOI 10.1109/ICCV.1995.466882; Sidenbladh H., 2000, LNCS, V2, P702; Sminchisescu C, 2001, PROC CVPR IEEE, P447; Stenger B, 2001, PROC CVPR IEEE, P310; TORDOFF B, 2002, P 7 ECCV, V1, P82; TORDOFF B, 2002, P BRIT MACH VIS C, P807; Wachter S, 1999, COMPUT VIS IMAGE UND, V74, P174, DOI 10.1006/cviu.1999.0758; Wu Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1094, DOI 10.1109/ICCV.2003.1238471; Yacoob Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P446, DOI 10.1109/ICCV.1998.710757	25	4	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2006	28	1					163	168		10.1109/TPAMI.2006.22	http://dx.doi.org/10.1109/TPAMI.2006.22			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	982OR	16402630	Green Submitted			2022-12-18	WOS:000233172000016
J	DeVore, MD				DeVore, MD			Estimates of error probability for complex Gaussian channels with generalized likelihood ratio detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						complex Gaussian channels; probability of error; model inaccuracy; moment estimators; Johnson's systems of distributions	SAR ATR; RECOGNITION; APPROXIMATIONS; CLASSIFICATION	We derive approximate expressions for the probability of error in a two-class hypothesis testing problem in which the two hypotheses are characterized by zero-mean complex Gaussian distributions. These error expressions are given in terms of the moments of the test statistic employed and we derive these moments for both the likelihood ratio test, appropriate when class densities are known, and the generalized likelihood ratio test, appropriate when class densities must be estimated from training data. These moments are functions of class distribution parameters which are generally unknown so we develop unbiased moment estimators in terms of the training data. With these, accurate estimates of probability of error can be calculated quickly for both the optimal and plug-in rules from available training data. We present a detailed example of the behavior of these estimators and demonstrate their application to common pattern recognition problems, which include quantifying the incremental value of larger training data collections, evaluating relative geometry in data fusion from multiple sensors, and selecting a good subset of available features.	Univ Virginia, Dept Syst & Informat Engn, Charlottesville, VA 22904 USA	University of Virginia	DeVore, MD (corresponding author), Univ Virginia, Dept Syst & Informat Engn, 102G Olsson Hall,151 Engineers Way,POB 400747, Charlottesville, VA 22904 USA.	mdevore@virginia.edu						[Anonymous], 1994, CONTINUOUS UNIVARIAT; BARNDORFFNIELSEN O, 1979, J ROY STAT SOC B MET, V41, P279; COVER TM, 1977, IEEE T SYST MAN CYB, V7, P657, DOI 10.1109/TSMC.1977.4309803; DeVore M., 2000, IEEE T AERO ELEC SYS, V38, P632; DeVore MD, 2004, PROC SPIE, V5427, P407, DOI 10.1117/12.548352; DeVore MD, 2004, IEEE T IMAGE PROCESS, V13, P113, DOI 10.1109/TIP.2004.823825; Devore MD, 2003, MULTIDIM SYST SIGN P, V14, P139, DOI 10.1023/A:1022277209974; FRIED DL, 1976, J OPT SOC AM, V66, P1150, DOI 10.1364/JOSA.66.001150; GLICK N, 1972, J AM STAT ASSOC, V67, P116, DOI 10.2307/2284709; GOODMAN JW, 1976, J OPT SOC AM, V66, P1145, DOI 10.1364/JOSA.66.001145; Hill I. D., 1976, Applied Statistics, V25, P190, DOI 10.2307/2346693; HILL JD, 1976, APPL STATIST, V25, P180; *J HOPK U CTR IM S, 2005, U RES IN SYNTH DAT H; Jacobs SP, 2000, IEEE T AERO ELEC SYS, V36, P364, DOI 10.1109/7.845214; Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797; JOHNSON NL, 1949, BIOMETRIKA, V36, P149, DOI 10.2307/2332539; Kay SM, 2001, IEEE T SIGNAL PROCES, V49, P2240, DOI 10.1109/78.950780; Mathai A.M., 1992, QUADRATIC FORMS RAND; O'Sullivan JA, 2001, IEEE T AERO ELEC SYS, V37, P91, DOI 10.1109/7.913670; RAUDYS S, 1980, IEEE T PATTERN ANAL, V2, P243; RAUDYS SJ, 1991, IEEE T PATTERN ANAL, V13, P252, DOI 10.1109/34.75512; ROHATGI VK, 1989, STAT PROBABIL LETT, V8, P297, DOI 10.1016/0167-7152(89)90035-7; Schmid NA, 2001, IEEE T INFORM THEORY, V47, P2903, DOI 10.1109/18.959269; VanTrees H., 1968, DETECTION ESTIMATION, V1; VANTREES HL, 1971, DETECTION ESTIMATION, V3	26	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2005	27	10					1580	1591		10.1109/TPAMI.2005.198	http://dx.doi.org/10.1109/TPAMI.2005.198			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	953OM	16237993				2022-12-18	WOS:000231086700006
J	Baggenstoss, PM				Baggenstoss, PM			Image distortion analysis using polynomial series expansion	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						image processing and computer vision; image registration; pattern recognition; text processing; computer vision; text processing		In this paper, we derive a technique for analysis of local distortions which affect data in real-world applications. In the paper, we focus on image data, specifically handwritten characters. Given a reference image and a distorted copy of it, the method is able to efficiently determine the rotations, translations, scaling, and any other distortions that have been applied. Because the method is robust, it is also able to estimate distortions for two unrelated images, thus determining the distortions that would be required to cause the two images to resemble each other. The approach is based on a polynomial series expansion using matrix powers of linear transformation matrices. The technique has applications in pattern recognition in the presence of distortions.	USN, Undersea Warfare Ctr, Newport, RI 02841 USA	United States Department of Defense; United States Navy	Baggenstoss, PM (corresponding author), USN, Undersea Warfare Ctr, Newport, RI 02841 USA.	p.m.baggenstoss@ieee.org						BROWN L, 1992, ACM COMPUTING SURVEY, V24; DAVIES IOG, 1990, P IEE C APPL MOT COM; Kay S.M, 1988, MODERN SPECTRAL ESTI; LYNCH MR, 1988, P ICASSP 88, V2, P920; OWCZARCZYK J, 1989, P 3 INT C IM PROC IT, P10; Rui W, 2003, ICCIMA 2003: FIFTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND MULTIMEDIA APPLICATIONS, PROCEEDINGS, P385; SMITH C, 1997, P 6 INT C IM PROC IT, P785; Teo PC, 1999, IEEE T PATTERN ANAL, V21, P552, DOI 10.1109/34.771325	8	4	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2004	26	11					1438	1451		10.1109/TPAMI.2004.106	http://dx.doi.org/10.1109/TPAMI.2004.106			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	852EC	15521492				2022-12-18	WOS:000223737000004
J	Sengupta, K; Burman, P				Sengupta, K; Burman, P			A curve fitting problem and its applicationin modeling objects in monocular image sequences	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						curve fitting; splines; regression; face modeling	MOTION	In this paper, we present a solution to a particular curve (surface) fitting problem and demonstrate its application in modeling objects from monocular image sequences. The curve fitting algorithm is based on a modified nonparametric regression method, which forms the core contribution of this work. This method is far more effective compared to standard estimation techniques, such as the maximum likelihood estimation method, and can take into account the discontinuities present in the curve. Next, the theoretical results of this 1 D curve estimation technique are extended significantly for an object modeling problem. Here, the input to the algorithm is a monocular image sequence of an object undergoing rigid motion. By using the affine camera projection geometry and a given choice of an image frame pair in the sequence, we adopt the KvD [12] model to express the depth at each point on the object as a function of the unknown out-of-plane rotation, and some measurable quantities computed directly from the optical flow. This is repeated for multiple image pairs (keeping one fixed image frame which we formally call the base image and choosing another frame from the sequence). The depth map is next estimated from these equations using the modified nonparametric regression analysis. We conducted experiments on various image sequences to verify the effectiveness of the technique. The results obtained using our curve fitting technique can be refined further by hierarchical techniques, as well as by nonlinear optimization techniques in structure from motion.	Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119260, Singapore; Univ Calif Davis, Dept Stat, Davis, CA 95616 USA	National University of Singapore; University of California System; University of California Davis	Sengupta, K (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 10 Kent Ridge Crescent, Singapore 119260, Singapore.	eleks@nus.edu.sg; pburman@ucdavis.gov						AIZAWA K, 1995, P IEEE, V83, P259, DOI 10.1109/5.364463; AKIMOTO T, 1993, IEEE COMPUT GRAPH, V13, P16, DOI 10.1109/38.232096; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Boult T. E., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P68; BOZDAGI G, 1994, IEEE T CIRCUITS SYST, V4, P413; Burman P, 1998, J TIME SER ANAL, V19, P127; FORSEY DR, 1995, COMPUTER GRAPHIC APR; Fua P, 1999, COMPUT VIS IMAGE UND, V75, P247, DOI 10.1006/cviu.1999.0778; HENG CK, NUSKS0101 NAT U SING; HU XP, 1991, IEEE T ROBOTIC AUTOM, V7, P848, DOI 10.1109/70.105394; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P536, DOI 10.1109/34.24786; Jebara TS, 1997, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.1997.609312; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; SENGUPTA K, 2001, P IEEE INT C MULT EX, P361; SENGUPTA K, 2000, P 4 IEEE INT C FAC G, P424; SHAPIRO LS, 1995, INT J COMPUT VISION, V16, P147, DOI 10.1007/BF01539553; Simonoff J.S., 1996, SMOOTING METHODS STA, V2nd; Sinha S. S., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P44, DOI 10.1109/CVPR.1991.139659; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; Vaidya NM, 1998, COMPUT VIS IMAGE UND, V72, P257, DOI 10.1006/cviu.1998.0700; ZHENG JY, 1994, IEEE T PATTERN ANAL, V16, P163, DOI 10.1109/34.273734	23	4	4	1	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2002	24	5					674	686		10.1109/34.1000240	http://dx.doi.org/10.1109/34.1000240			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	544XU					2022-12-18	WOS:000175187800008
J	Oommen, BJ; Loke, RKS				Oommen, BJ; Loke, RKS			On the pattern recognition of noisy subsequence trees	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						syntactic pattern recognition; tree and subtree recognition; noisy subsequence trees	RNA SECONDARY STRUCTURES; UNORDERED LABELED TREES; EDITING DISTANCE	In this paper, we consider the problem of recognizing ordered labeled trees by processing their noisy subsequence-trees which are "patched-up" noisy portions of their fragments. We assume that we are given H, a finite dictionary of ordered labeled trees. X* is an unknown element of H, and U is any arbitrary subsequence-tree of X*. We consider the problem of estimating X* by processing Y, which is a noisy version of U. The solution which we present is, to our knowledge, the first reported solution to the problem. We solve the problem by sequentially comparing Y with every element X of H, the basis of comparison being a new dissimilarity measure between two trees, which implicitly captures the properties of the corrupting mechanism ("channel") which noisily garbles U into Y. The algorithm which incorporates this constraint has been used to test our pattern recognition system yielding a remarkable accuracy. Experimental results which involve manually constructed trees of sizes between 25 and 35 nodes, and which contain an average of 21.8 errors per tree demonstrate that the scheme has about 92.8 percent accuracy. Similar experiments for randomly generated trees yielded an accuracy of 86.4 percent.	Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada; Alcatel Networks, Plano, TX 75075 USA	Carleton University; Alcatel-Lucent	Oommen, BJ (corresponding author), Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.		Oommen, B. John/P-6323-2017	Oommen, B. John/0000-0002-5105-1575				Duda R.O., 1973, J ROYAL STAT SOC SER; KILPELAINEN P, IN PRESS SIAM J COMP; LE SY, 1989, COMPUT APPL BIOSCI, V5, P205; LE SY, 1989, COMPUTERS BIOMEDICAL, V22, P4617; LU SY, 1979, IEEE T PATTERN ANAL, V1, P219, DOI 10.1109/TPAMI.1979.6786615; LU SY, 1984, IEEE T PATTERN ANAL, V6, P249, DOI 10.1109/TPAMI.1984.4767511; OOMMEN BJ, 1986, INFORM SCIENCES, V40, P267, DOI 10.1016/0020-0255(86)90061-7; OOMMEN BJ, 1987, IEEE T PATTERN ANAL, V9, P676, DOI 10.1109/TPAMI.1987.4767962; OOMMEN BJ, 1994, INFORM SCIENCES, V77, P253, DOI 10.1016/0020-0255(94)90004-3; Oommen BJ, 1998, PATTERN RECOGN, V31, P1159, DOI 10.1016/S0031-3203(97)00124-6; Oommen BJ, 1996, IEEE T COMPUT, V45, P1426, DOI 10.1109/12.545972; Sankoff D., 1983, TIME WRAPS STRING ED; SELKOW SM, 1977, INFORM PROCESS LETT, V6, P184, DOI 10.1016/0020-0190(77)90064-3; SHAPIRO B, 1988, COMPUT APPL BIOSCI, P387; SHAPIRO BA, 1990, COMPUT APPL BIOSCI, V6, P309; TAI KC, 1979, J ACM, V26, P422, DOI 10.1145/322139.322143; TAKAHASHI Y, 1987, ANAL SCI, V3, P23; WAGNER RA, 1974, J ACM, V21, P168, DOI 10.1145/321796.321811; ZHANG K, 1992, P 1992 S COMB PATT M, P148; ZHANG K, 1990, P IASTED INT S, P92; ZHANG KZ, 1989, SIAM J COMPUT, V18, P1245, DOI 10.1137/0218082; ZHANG KZ, 1994, INFORM PROCESS LETT, V49, P249, DOI 10.1016/0020-0190(94)90062-0; ZHANG KZ, 1992, INFORM PROCESS LETT, V42, P133, DOI 10.1016/0020-0190(92)90136-J	23	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2001	23	9					929	946		10.1109/34.955108	http://dx.doi.org/10.1109/34.955108			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	470RP					2022-12-18	WOS:000170885200001
J	Shashua, A; Wexler, Y				Shashua, A; Wexler, Y			Q-warping: Direct computation of quadratic reference surfaces	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						direct estimation; quadratic reconstruction; multiview geometry	OPTICAL-FLOW; MOTION; IMAGE	We consider the problem of wrapping around an object, of which two views are available, a reference surface and recovering the resulting parametric flow using direct computations (via spatio-temporal derivatives). The well known examples are affine flow models and eight-parameter flow models-both describing a flow field of a planar reference surface. We extend those classic flow models to deal with a Quadric reference surface and work out the explicit parametric form of the flow field. As a result we derive a simple warping algorithm that maps between two views and leaves a residual flow proportional to the 3D deviation of the surface from a virtual quadric surface. The applications include image morphing, model building, image stabilization, and disparate view correspondence.	Hebrew Univ Jerusalem, Inst Comp Sci, Jerusalem, Israel; Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA	Hebrew University of Jerusalem; University System of Maryland; University of Maryland College Park	Shashua, A (corresponding author), Hebrew Univ Jerusalem, Inst Comp Sci, Jerusalem, Israel.							ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; BERGEN JR, 1992, P EUR C COMP VIS JUN; BERGEN JR, 1987, J OPT SOC AM A, V4, P35; BERGEN JR, 1990, HIERARCHICAL MOTION; CROSS G, 1998, P INT C COMP VIS JAN; HANNA KJ, 1993, P INT C COMP VIS, P357, DOI DOI 10.1109/ICCV.1993.378192; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Lucas B.D., 1981, ITERATIVE IMAGE REGI, P674; ROSENFELD A, 1984, MULTIRESOLUTION IMAG; Shashua A, 1997, INT J COMPUT VISION, V23, P185, DOI 10.1023/A:1007962930529; STEIN GP, 1997, P C COMP VIS PATT RE	15	4	13	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2001	23	8					920	925		10.1109/34.946996	http://dx.doi.org/10.1109/34.946996			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	460AH					2022-12-18	WOS:000170283300013
J	Ferro, A; Gallo, G; Giugno, R; Pulvirenti, A				Ferro, A; Gallo, G; Giugno, R; Pulvirenti, A			Best-match retrieval for structured images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						structured data storage and retrieval; distance-based query processing; triangle inequality	QUERIES; TREES	This paper propose a new methodology for fast best-match retrieval of structured images. A triangle inequality property for the tree-distance introduced by Oflazer [17] is proven. This property is, in turn, applied to obtain a saturation algorithm of the trie used to store the database of the collection of pictures. The new approach can be considered as a substantial optimization of Oflazer's technique and can be applied to the retrieval of homogeneous hierarchically structured objects of any kind. The new technique inscribes itself in the number of distance-based search strategies and it is of interest for the indexing and maintenance of large collections of historical and pictorial data. We demonstrate the proposed approach on an example and report data about the speed-up that it introduces in query processing. Direct comparison with MVP-trees algorithm is also presented.	Univ Catania, Dipartimento Matemat & Informat, I-95125 Catania, Italy	University of Catania	Ferro, A (corresponding author), Univ Catania, Dipartimento Matemat & Informat, Viale A Doria 6, I-95125 Catania, Italy.		Pulvirenti, Alfredo/I-7272-2018	Pulvirenti, Alfredo/0000-0002-9764-0295; Gallo, Giovanni/0000-0002-6701-0620; Giugno, Rosalba/0000-0001-9843-7638				AHO AV, 1989, ACM T PROGR LANG SYS, V11, P491, DOI 10.1145/69558.75700; Baeza-Yates R., 1994, Combinatorial Pattern Matching. 5th Annual Symposium, CPM 94. Proceedings, P198; Bozkaya T, 1999, ACM T DATABASE SYST, V24, P361, DOI 10.1145/328939.328959; BRIN S, 1995, P 21 INT C VER LARG, P574; BRYD R, 1990, LQL USER NOTES INFOR; BURKHARD WA, 1973, COMMUN ACM, V16, P230, DOI 10.1145/362003.362025; CHENG YC, 1985, IEEE T PATTERN ANAL, V7, P299, DOI 10.1109/TPAMI.1985.4767658; CHODOROW M, 1990, UNPUB LOCATING SYNTA; Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426; cker Chiueh T., 1994, P 20 INT C VER LARG, P582; EHRICH RW, 1976, IEEE T COMPUT, V25, P725, DOI 10.1109/TC.1976.1674681; FERRO A, 1999, P VISUAL 99 JUN, P51; HOFFMANN CM, 1982, J ACM, V29, P68, DOI 10.1145/322290.322295; MOAYER B, 1986, IEEE T PATTERN ANAL, V8, P176; NEFF M, 1989, P 27 ANN M ASS COMP; NIRENBURG S, 1994, P INT C NEW METH LAN, P78; OFLAZER K, 1997, IEEE T PATTERN ANAL, V19; SAMET H, 1982, IEEE T PATTERN ANAL, V4, P298, DOI 10.1109/TPAMI.1982.4767246; Sato S., 1990, P 13 INT C COMPUTATI, P247, DOI [10.3115/991146.991190, DOI 10.3115/991146.991190]; SHAPIRO BA, 1990, COMPUT APPL BIOSCI, V6, P309; SHAPIRO M, 1977, COMMUN ACM, V20, P339, DOI 10.1145/359581.359599; SHASHA D, 1990, ACM T INFORM SYST, V8, P140, DOI 10.1145/96105.96111; TAI KC, 1979, J ACM, V26, P422, DOI 10.1145/322139.322143; UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R; WANG JTL, 1994, IEEE T KNOWL DATA EN, V6, P559, DOI 10.1109/69.298173; YIANILOS PN, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P311	26	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2001	23	7					707	718		10.1109/34.935845	http://dx.doi.org/10.1109/34.935845			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	449TV					2022-12-18	WOS:000169704000002
J	Wang, YP; Qu, RB				Wang, YP; Qu, RB			Fast implementation of scale-space by interpolatory subdivision scheme	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						scale-space; interpolatory subdivision scheme; B-splines; edge detection; image representation	EDGE-DETECTION; ALGORITHMS	While the scale-space approach has been widely used in computer vision, there has been a great interest in fast implementation of scale-space filtering. In this paper, we introduce an interpolatory subdivision scheme (ISS) for this purpose. In order to extract the geometric features in a scale-space representation, discrete derivative approximations are usually needed. Hence, a general procedure is also introduced to derive exact formulae for numerical differentiation with respect to this ISS. Then, from ISS, an algorithm is derived for fast approximation of scale-space filtering. Moreover, the relationship between the ISS and the Whittaker-Shannon sampling theorem and the commonly used spline technique is discussed. As an example of the application of ISS technique, we present some examples on fast implementation of lambda tau-spaces as introduced by Gokmen and Jain [12], which encompasses various famous edge detection filters. It is shown that the ISS technique demonstrates high performance in fast implementation of the scale-space filtering and feature extraction.	Natl Univ Singapore, Wavelets Strateg Res Programme, Singapore 119260, Singapore; SDRC Imageware Inc, Ann Arbor, MI 48105 USA	National University of Singapore	Wang, YP (corresponding author), Washington Univ, Sch Med, St Louis, MO 63110 USA.		Wang, Yu-Ping/H-5223-2012					CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CAVARETTA A, 1991, AMS MEMOIRS, V93, P453; DAUBECHIES I, 1988, COMMUN PUR APPL MATH, V41, P909, DOI 10.1002/cpa.3160410705; Deriche R., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P501; DUBUC S, 1986, J MATH ANAL APPL, V114, P185, DOI 10.1016/0022-247X(86)90077-6; Dyn N., 1987, Computer-Aided Geometric Design, V4, P257, DOI 10.1016/0167-8396(87)90001-X; FERRARI LA, 1987, IEEE T PATTERN ANAL, V9, P461, DOI 10.1109/TPAMI.1987.4767929; Gokmen M, 1997, IEEE T PATTERN ANAL, V19, P545, DOI 10.1109/34.601227; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Qu R, 1996, MATH COMPUT MODEL, V24, P55, DOI 10.1016/S0895-7177(96)00164-1; Qu R., 1995, Neural, Parallel & Scientific Computations, V3, P393; Qu R., 1994, COMPUT AIDED DRAFTED, V4, P28; Qu RB, 1996, INT J COMPUT MATH, V60, P279, DOI 10.1080/00207169608804492; UNSER M, 1993, IEEE T SIGNAL PROCES, V41, P821, DOI 10.1109/78.193220; Wang YP, 1998, IEEE T PATTERN ANAL, V20, P1040, DOI 10.1109/34.722612; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729	16	4	4	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1999	21	9					933	939		10.1109/34.790434	http://dx.doi.org/10.1109/34.790434			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	234TZ					2022-12-18	WOS:000082501600010
J	Akra, M; Bazzi, L; Mitter, S				Akra, M; Bazzi, L; Mitter, S			Sampling of images for efficient model-based vision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						sampling; model-based vision; matching under uncertainty; approximate matching; image understanding		The problem of matching two planar sets of points in the presence of geometric uncertainty has important applications in pattern recognition, image understanding, and robotics. The first set of points corresponds to the "template." The other set corresponds to the "image" that-possibly-contains one or more deformed versions of the "template" embedded in a cluttered image. Significant progress has been made on this problem and various polynomial-time algorithms have been proposed. In this article, we show how to sample the "image" in linear time. reducing the number of foreground points n by a factor of two-six (for commonly occurring images) without degrading the quality of the matching results. The direct consequence is a time-saving by a factor of 2(p)-6(p) for an O(n(p)) matching algorithm. Our result applies to a fairly large class of available matching algorithms.	MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Akra, M (corresponding author), MIT, Dept Elect Engn & Comp Sci, 77 Massachusetts Ave,Rm 35-312, Cambridge, MA 02139 USA.							BAIRD H, 1984, MODEL BASED IMAGE MA, P12; BAIRD H, 1986, IEEE T PATTERN ANAL, V8, P334; BREUEL TM, 1992, THESIS MIT; CASS T, 1996, 4 EUR C COMP VIS, V1, P492; CASS T, 1992, THESIS MIT; Cass T. A., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P360, DOI 10.1109/ICCV.1990.139551; Chazelle B., 1983, ADV COMPUTING RES, VI, P1; HUTTENLOCHER D, 1990, 6 ACM S COMP GEOM, V1, P340; Huttenlocher D. P., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P654, DOI 10.1109/CVPR.1992.223209; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; YI X, 1995, P INT S COMP VIS, P79	11	4	5	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1999	21	1					4	11		10.1109/34.745729	http://dx.doi.org/10.1109/34.745729			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	163DZ					2022-12-18	WOS:000078388900002
J	Djouadi, A				Djouadi, A			On the reduction of the nearest-neighbor variation for more accurate classification and error estimates	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						nearest-neighbor risk; nearest-neighbor classifier; Bayes error; asymptotic risk; risk estimation	RULE	In designing the nearest-neighbor (NN) classifier, a method is presented to produce a finite sample size risk close to the asymptotic one. It is based on an attempt to eliminate the first-order effects of the sample size, as well as all higher odd terms. This method uses the 2-NN rule without the rejection option and utilizes a polarization scheme. Simulation results are included as a means of verifying this analysis.	Lucent Technol, Columbus, OH 43213 USA	Alcatel-Lucent; Lucent Technologies	Djouadi, A (corresponding author), Lucent Technol, Room 3T-335B,6200 E Broad St, Columbus, OH 43213 USA.	adjouadi@lucent.com						BROWN TA, 1979, IEEE T INFORM THEORY, V25, P617, DOI 10.1109/TIT.1979.1056092; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; DEVIJVER PA, 1979, IEEE T INFORM THEORY, V25, P749, DOI 10.1109/TIT.1979.1056099; Duda R. O., 1973, PATTERN CLASSIFICATI; FUKUNAGA K, 1984, IEEE T PATTERN ANAL, V6, P314, DOI 10.1109/TPAMI.1984.4767523; FUKUNAGA K, 1985, IEEE T PATTERN ANAL, V7, P107, DOI 10.1109/TPAMI.1985.4767625; Short R. D., 1980, Proceedings of the 5th International Conference on Pattern Recognition, P81; SHORT RD, 1981, IEEE T INFORM THEORY, V27, P622, DOI 10.1109/TIT.1981.1056403	8	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1998	20	5					567	571		10.1109/34.682188	http://dx.doi.org/10.1109/34.682188			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZR253					2022-12-18	WOS:000073955600014
J	Whaite, P; Ferrie, FP				Whaite, P; Ferrie, FP			On the sequential determination of model misfit	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						autonomous exploration; active vision; misfit; lack-of-fit statistics	GLOBAL DEFORMATIONS; RANGE IMAGES; SUPERQUADRICS	Many strategies in computer vision assume the existence of general purpose models that can be used to characterize a scene or environment at various levels of abstraction. The usual assumptions are that a selected model is competent to describe a particular attribute and that the parameters of this model can be estimated by interpreting the input data in an appropriate manner (e.g., location of lines and edges, segmentation into parts or regions, etc.). This paper considers the problem of how to determine when those assumptions break down. The traditional approach is to use statistical misfit measures based on an assumed sensor noise model. The problem is that correct operation often depends critically on the correctness of the noise model. Instead, we show how this can be accomplished with a minimum of a priori knowledge and within the framework of an active approach which builds a description of environment structure and noise over several viewpoints.	MCGILL UNIV, CTR INTELLIGENT MACHINES, ARTIFICIAL PERCEPT LAB, MONTREAL, PQ H3A 2A7, CANADA	McGill University	Whaite, P (corresponding author), MCGILL UNIV, CTR INTELLIGENT MACHINES, DEPT ELECT ENGN, MONTREAL, PQ H3A 2A7, CANADA.							ALOIMONOS J, 1987, 1ST P INT C COMP VIS, P35; BESL PJ, 1988, IEEE T PATTERN ANAL, V10, P167, DOI 10.1109/34.3881; BRETON P, 1992, P COMP VIS ECCV 92 S, P135; FERRIE FP, 1993, IEEE T PATTERN ANAL, V15, P771, DOI 10.1109/34.236252; Hebert M., 1991, Proceedings IROS '91. IEEE/RSJ International Workshop on Intelligent Robots and Systems '91. Intelligence for Mechanical Systems (Cat. No.91TH0375-6), P359, DOI 10.1109/IROS.1991.174476; Iyer H., 1990, TECHNOMETRICS, V32, P448, DOI [10.1080/00401706.1990.10484733, DOI 10.1080/00401706.1990.10484733]; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KEREN D, 1992, 102 BROWN U DIV ENG; Lejeune A, 1996, COMPUT VIS IMAGE UND, V64, P230, DOI 10.1006/cviu.1996.0056; Luenberger D.G, 2016, LINEAR NONLINEAR PRO, DOI 10.1007/978-3-319-18842-3; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; Press WH, 1988, NUMERICAL RECIPES C; SOLINA F, 1990, IEEE T PATTERN ANAL, V12, P131, DOI 10.1109/34.44401; TERZOPOULOS D, 1991, IEEE T PATTERN ANAL, V13, P703, DOI 10.1109/34.85659; Whaite P, 1997, IEEE T PATTERN ANAL, V19, P193, DOI 10.1109/34.584097; WHAITE P, 1991, IEEE T PATTERN ANAL, V13, P1038, DOI 10.1109/34.99237; ZUCKER SW, 1988, P 2 INT C COMP VIS T	17	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1997	19	8					899	905		10.1109/34.608292	http://dx.doi.org/10.1109/34.608292			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XT987					2022-12-18	WOS:A1997XT98700008
J	Hofmann, T; Buhmann, JM				Hofmann, T; Buhmann, JM			Pairwise data clustering by deterministic annealing (vol 19, pg 1, 1997)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction, Addition													Buhmann, Joachim/AAU-4760-2020					HOFFMANN T, 1997, IEEE T PATTERN ANAL, V19, P1	1	4	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1997	19	2					192	192		10.1109/TPAMI.1997.574806	http://dx.doi.org/10.1109/TPAMI.1997.574806			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WK728					2022-12-18	WOS:A1997WK72800016
J	Noble, JA				Noble, JA			The effect of morphological filters on texture boundary localization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						low-level processing; texture analysis; mathematical morphology; image filtering; median filter		We extend the theoretical results in [1] to two distributions and provide a quantitative comparison of 1D morphological filter edge localization with two classical types of kernel-based smoothing filters (the mean and the close relative to morphological filters, the median). Implications in the context of statistical texture segmentation are briefly discussed.			Noble, JA (corresponding author), UNIV OXFORD,DEPT ENGN SCI,PARKS RD,OXFORD OX2 7JS,ENGLAND.			Noble, Alison/0000-0002-3060-3772				BOVIK AC, 1987, IEEE T PATTERN ANAL, V9, P181, DOI 10.1109/TPAMI.1987.4767894; CHU CHH, 1989, IEEE T BIO-MED ENG, V36, P262, DOI 10.1109/10.16474; COYLE EJ, 1989, IEEE T ACOUST SPEECH, V37, P2037, DOI 10.1109/29.45552; FITCH JP, 1985, IEEE T CIRCUITS SYST, V32, P445, DOI 10.1109/TCS.1985.1085740; HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328; HEIJMANS HJAM, 1991, IEEE T PATTERN ANAL, V13, P568, DOI 10.1109/34.87343; HEYGSTER G, 1981, IEEE T ACOUSTICS SPE, V19, P148; HIRANO K, 1985, ENCY STAT SCI, V6, P644; KASSAM SA, 1985, P IEEE, V73, P433, DOI 10.1109/PROC.1985.13167; Lewis B. A., 1983, Advances in Engineering Software, V5, P183, DOI 10.1016/0141-1195(83)90044-X; LIAO GY, 1985, IEEE T ACOUST SPEECH, V33, P1280, DOI 10.1109/TASSP.1985.1164676; MARAGOS P, 1987, IEEE T ACOUST SPEECH, V35, P1153, DOI 10.1109/TASSP.1987.1165259; MARAGOS PA, 1985, DSPL851 GEORG I TECH; NOBLE J, 1989, THESIS OXFORD U; Serra J, 1982, IMAGE ANAL MATH MORP; Serra J, 1988, IMAGE ANAL MATH MORP; SHAH A, 1991, ANS C LIB V0 6; STEVENSON RL, 1987, IEEE T CIRCUITS SYST, V34, P1292, DOI 10.1109/TCS.1987.1086067; WENDT PD, 1986, IEEE T ACOUST SPEECH, V34, P898, DOI 10.1109/TASSP.1986.1164871; YANG GJ, 1981, COMPUT VISION GRAPH, V15, P224, DOI 10.1016/0146-664X(81)90057-5; [No title captured]	21	4	6	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1996	18	5					554	561		10.1109/34.494645	http://dx.doi.org/10.1109/34.494645			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL691					2022-12-18	WOS:A1996UL69100008
J	LI, XP; CHEN, TW				LI, XP; CHEN, TW			OPTIMAL L(1) APPROXIMATION OF THE GAUSSIAN KERNEL WITH APPLICATION TO SCALE-SPACE CONSTRUCTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						GAUSSIAN KERNEL; SCALE-SPACE; TOTAL POSITIVITY; L1 APPROXIMATION		Scale-space construction based on Gaussian filtering requires convolving signals with a large bank of Gaussian filters with different widths, In this paper we propose an efficient way for this purpose by L(1) optimal approximation of the Gaussian kernel in terms of linear combinations of a small number of basis functions, Exploring total positivity of the Gaussian kernel, the method has the following properties: 1) the optimal basis functions are still Gaussian and can be obtained analytically; 2) scale-spaces for a continuum of scales can be computed easily; 3) a significant reduction in computation and storage costs is possible, Moreover, this work sheds light on some issues related to use of Gaussian models for multiscale image processing.			LI, XP (corresponding author), UNIV CALGARY,DEPT ELECT & COMP ENGN,CALGARY,AB T2N 1N4,CANADA.		Chen, Tongwen/F-4553-2011	Chen, Tongwen/0000-0002-1699-7947				BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749; DEVORE RA, 1992, IEEE T INFORM THEORY, V38, P719, DOI 10.1109/18.119733; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; KARLIN S, 1968, TOTAL POSITIVITY, V1; LI XP, 1995, SIGNAL PROCESS, V41, P119, DOI 10.1016/0165-1684(94)00095-H; LINDEBERG T, 1990, IEEE T PATTERN ANAL, V12, P234, DOI 10.1109/34.49051; Marr D., 1982, VISION; MICCHELLI CA, 1978, J APPROX THEORY, V24, P51, DOI 10.1016/0021-9045(78)90036-9; PERONA P, 1991, MITLIDSP2039 TECHN R; PERONA P, 1992, LECTURE NOTES COMPUT, V588; ROMENY BMT, 1992, LECTURE NOTES COMPUT, V511, P239; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; Witkin A.P., 1983, INT JOINT C ART INT, P1019; YUILLE A, 1986, IEEE T PATTERN ANAL, V8	15	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1995	17	10					1015	1019		10.1109/34.464565	http://dx.doi.org/10.1109/34.464565			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RX289					2022-12-18	WOS:A1995RX28900008
J	TRIER, OD; TAXT, T				TRIER, OD; TAXT, T			EVALUATION OF BINARIZATION METHODS FOR DOCUMENT IMAGES (VOL 17, PG 312, 1995)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction, Addition																		TRIER OD, 1995, IEEE T PATTERN ANAL, V17, P312, DOI 10.1109/34.368197	1	4	4	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1995	17	6					640	640						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QZ940					2022-12-18	WOS:A1995QZ94000011
J	KENDER, JR; KJELDSEN, R				KENDER, JR; KJELDSEN, R			ON SEEING SPAGHETTI - SELF-ADJUSTING PIECEWISE TOROIDAL RECOGNITION OF FLEXIBLE EXTRUDED OBJECTS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						TORUS RECOGNITION; FLEXIBLE EXTRUDED OBJECTS; SELF-ADJUSTING DOUGH TRANSFORM; PARAMETER SPACE DECOMPOSITION; RANGE DATA		We present a model for flexible extruded objects, such as wires, tubes, or grommets, and demonstrate a novel, self-adjusting, seven-dimensional Hough transform that derives their diameter and three-space curved axes from position and surface normal information. The method is purely local and is inexpensive to compute. The model considers such objects as piecewise toroidal, and decomposes the seven parameters of a torus into three nested subspaces, the structures of which counteract the errors implicit in the analysis of objects of great size and/or small curvature. We believe it is the first example of a parameter space structure designed to cluster ill-conditioned hypotheses together so that they can be easily detected and ignored. This work complements existing shape-from-contour approaches for analyzing tori: It uses no edge information, and it does not require the solution of high-degree non-linear equations by iterative techniques. Most of the results, including the conditions for the existence of more than one solution (phantom ''anti-tori''), have been verified using a symbolic mathematical analysis system. We present, in the environment of the IBM ConVEx system, robust results on both synthetic CAD-CAM range data (the hasp of a lock), and actual range data (a knotted piece of coaxial cable), and discuss several system tuning issues.	IBM CORP,THOMAS J WATSON RES CTR,EXPLORATORY COMP VIS GRP,YORKTOWN HTS,NY 10598	International Business Machines (IBM)	KENDER, JR (corresponding author), COLUMBIA UNIV,DEPT COMP SCI,NEW YORK,NY 10027, USA.							BALLARD DH, 1981, 7TH P INT JOINT C AR, P1068; BLANFORD RP, 1987, 870308 U WASH DEP CO; BOLLE R, COMMUNICATION; BOULT TE, COMMUNICATION; CALIFANO A, 1988, 7TH P NAT C ART INT, P831; CALIFANO A, 1988, NOV IEEE C COMP VIS; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; FELDMAN JA, 1981, COGNITIVE SCI, V6, P205; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; JENKS RD, 1986, IBM RC12327 TECHN RE; KENDER JR, IBM RC16576 TECHN RE; KJELDSEN R, 1989, 11TH P INT JOINT C A, P1578; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; LEAVERS VF, 1992, CVGIP-IMAG UNDERSTAN, V56, P381, DOI 10.1016/1049-9660(92)90049-9; O'Rourke J., 1981, 7TH P INT JOINT C AR, V2, P737; SABBAH D, 1985, COGNITIVE SCI, V9, P25, DOI 10.1207/s15516709cog0901_3; SABBAH D, 1986, P SPIE C INTELLIGENT, P222; STUIK DJ, 1961, LECTURES CLASSICAL G; 1986, 100X 3D SCANNER USER	19	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1995	17	2					136	157		10.1109/34.368174	http://dx.doi.org/10.1109/34.368174			22	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QE825					2022-12-18	WOS:A1995QE82500004
J	ONURAL, L; ALP, MB; GURELLI, MI				ONURAL, L; ALP, MB; GURELLI, MI			GIBBS RANDOM-FIELD MODEL-BASED WEIGHT SELECTION FOR THE 2-D ADAPTIVE WEIGHTED MEDIAN FILTER	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note						GIBBS RANDOM FIELD MODEL; ADAPTIVE FILTERING; WEIGHTED MEDIAN FILTER; IMAGE NOISE FILTERING	IMAGES; RESTORATION; NOISE	A generalized filtering method based on the minimization of the energy of the Gibbs model is described. The well-known linear and median filters are all special cases of this method. It is shown that, with the selection of appropriate energy functions, the method can be successfully used to adapt the weights of the adaptive weighted median filter to preserve different textures within the image while eliminating the noise. The newly developed adaptive weighted median filter is based on a 3 x 3 square neighborhood structure. The weights of the pixels are adapted according to the clique energies within this neighborhood structure. The assigned energies to 2- or 3-pixel cliques are based on the local statistics within a larger estimation window. It is shown that the proposed filter performance is better compared to some well-known similar filters like the standard, separable, weighted and some adaptive weighted median filters.	UNIV SO CALIF,DEPT ELECT ENGN SYST,INST SIGNAL PROC,LOS ANGELES,CA 90089	University of Southern California	ONURAL, L (corresponding author), BILKENT UNIV,DEPT ELECT & ELECTR ENGN,ANKARA 06533,TURKEY.							ARCE GR, 1987, IEEE T ACOUST SPEECH, V35, P60, DOI 10.1109/TASSP.1987.1165036; ASTOLA J, 1990, P IEEE, V78, P678, DOI 10.1109/5.54807; BROWNRIGG DRK, 1984, COMMUN ACM, V27, P807, DOI 10.1145/358198.358222; DERIN H, 1987, IEEE T PATTERN ANAL, V9, P39, DOI 10.1109/TPAMI.1987.4767871; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GURELLI MI, 1991, 6TH P INT S COMP INF, V2, P973; HEINONEN P, 1987, IEEE T ACOUST SPEECH, V35, P832, DOI 10.1109/TASSP.1987.1165198; KO SJ, 1988, IEEE INT S CIRC SYST, P1495; KUTKA R, 1989, SIGNAL PROCESS, V18, P217, DOI 10.1016/0165-1684(89)90051-0; LIN HM, 1988, IEEE T CIRCUITS SYST, V35, P675, DOI 10.1109/31.1805; LOUPAS T, 1989, IEEE T CIRCUITS SYST, V36, P129, DOI 10.1109/31.16577; NODES TA, 1983, IEEE T ACOUST SPEECH, V31, P1350, DOI 10.1109/TASSP.1983.1164220; PERLMAN SS, 1987, IEEE T COMMUN, V35, P646, DOI 10.1109/TCOM.1987.1096834; SALEMBIER P, 1992, SIGNAL PROCESS, V27, P1, DOI 10.1016/0165-1684(92)90108-9; YIN L, 1993, IEEE T SIGNAL PROCES, V41, P162, DOI 10.1109/TSP.1993.193136; YLIHARIA O, 1991, IEEE T SIGNAL PROCES, V39, P395, DOI 10.1109/78.80823	16	4	6	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1994	16	8					831	837		10.1109/34.308480	http://dx.doi.org/10.1109/34.308480			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PB475		Green Submitted			2022-12-18	WOS:A1994PB47500010
J	RAO, NSV; OBLOW, EM; GLOVER, CW				RAO, NSV; OBLOW, EM; GLOVER, CW			LEARNING SEPARATIONS BY BOOLEAN COMBINATIONS OF HALF-SPACES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						COMPUTATION LEARNABILITY; N-LEARNERS PROBLEM; N-POLYHEDRAL SEPARABILITY; PERCEPTRON		Given two subsets S1 and S2 (not necessarily finite) of R(d) separable by a Boolean combination of learning halfspaces, we consider the problem of (in the sense of Valiant) the separation function from a finite set of examples, i.e., we produce with a high probability a function close to the actual separating function. Our solution consists of a system of N perceptrons and a single consolidator which combines the outputs of the individual perceptrons. We show that an off-line version of this problem, where the examples are given in a batch, can be solved in time polynomial in the number of examples. We also provide an on-line learning algorithm that incrementally solves the problem by suitably training a system of N perceptrons much in the spirit of the classical perceptron learning algorithm.			RAO, NSV (corresponding author), OAK RIDGE NATL LAB,CTR ENGN SYST ADV RES,OAK RIDGE,TN 37831, USA.		Rao, Nageswara/H-8707-2019	Rao, Nageswara/0000-0002-3408-5941				Barnsley M. F., 2012, FRACTALS EVERYWHERE; Baum E. B., 1990, Journal of Complexity, V6, P67, DOI 10.1016/0885-064X(90)90012-3; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137; DASARATHY BV, 1979, P IEEE, V67, P708, DOI 10.1109/PROC.1979.11321; Edelsbrunner H., 1987, ALGORITHMS COMBINATO; FUKUNAGA K, 1990, INTRO STATISTICAL PA; HAUSSLER D, 1989, 3RD P S F COMP SCI, P40; HERMAN GT, 1992, IEEE T PATTERN ANAL, V14, P782, DOI 10.1109/34.142914; KARMARKAR N, 1984, COMBINATORICA, V4, P373, DOI 10.1007/BF02579150; KHACHIIAN LG, 1979, DOKL AKAD NAUK SSSR+, V244, P1093; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; MEGIDDO N, 1987, DISCRETE COMPUTAT GE, V3, P325; Minsky M., 1988, PERCEPTRONS; Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1023/A:1022605311895; Nilsson N., 1965, LEARNING MACHINES; Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; RAO NSV, 1994, IEEE T SYST MAN CYB, V24, P713, DOI 10.1109/21.293485; RAO NSV, 1994, IEEE T SYST MAN CYB, V24, P319, DOI 10.1109/21.281430; SKLANSKY J, 1981, PATTERN CLASSIFIERS; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1982, ESTIMATION DEPENDENC; [No title captured]	24	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1994	16	7					765	768		10.1109/34.297960	http://dx.doi.org/10.1109/34.297960			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NY134					2022-12-18	WOS:A1994NY13400012
J	BHATNAGAR, R; KANAL, LN				BHATNAGAR, R; KANAL, LN			STRUCTURAL AND PROBABILISTIC KNOWLEDGE FOR ABDUCTIVE REASONING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article								We examine different ways of representing probabilistic relationships among the attributes of a domain and show that the nature of domain relationships used in a representation affects the types of reasoning objectives that can be achieved. We review two well-known formalisms for representing the probabilistic relationships among attributes of a domain. These are the dependence tree formalism presented by Chow and Liu and the Bayesian networks methodology presented by Pearl. We use an example to illustrate the nature of the relationships and the difference in the types of reasoning performed by these two representations. We then demonstrate an abductive type of reasoning objective that requires use of the known qualitative relationships of the domain. We demonstrate a suitable way to represent such qualitative relationships along with the probabilistic knowledge, and we discuss how an explanation for a set of observed events may be constituted. We also present an algorithm for learning the qualitative relationships from empirical data using an algorithm based on the minimization of conditional entropy.	UNIV MARYLAND,DEPT COMP SCI,COLL PK,MD 20742	University System of Maryland; University of Maryland College Park	BHATNAGAR, R (corresponding author), UNIV CINCINNATI,DEPT COMP SCI,CINCINNATI,OH 45221, USA.							BHATNAGAR R, 1989, THESIS U MARYLAND CO; BHATNAGAR R, 1991, CISTR914 U CINC TECH; CHARNIAK E, 1990, 8TH P NAT C ART INT, P106; CHOW C, 1968, IEEE T INFORM THEORY, V14; Cooper G.F., 1991, 7 C UNC ART INT, P86; DEKLEER J, 1989, P IJCAI 89 DETROIT, P1324; FUNG RM, 1990, PROCEEDINGS : EIGHTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P762; HERSKOVITS E, 1991, UNCERTAINTY ARTIFICI, P117; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Neapolitan R.E., 1990, PROBABILISTIC REASON; PEARL J, 1987, ARTIF INTELL, V33, P173, DOI 10.1016/0004-3702(87)90034-8; PEARL J, 1986, ARTIF INTELL, V29, P241, DOI 10.1016/0004-3702(86)90072-X; Pearl J., 1991, PRINCIPLES KNOWLEDGE; Peirce CharlesS., 1940, PHILOS PEIRCE SELECT; PENG Y, 1987, INT J INTELL SYST, V2, P265, DOI 10.1002/int.4550020303; POOLE D, 1988, ARTIF INTELL, V36, P27, DOI 10.1016/0004-3702(88)90077-X; POOLE D, 1991, UNCERTAINTY ARTIFICI, V6; POOLE D, 1991, 7TH P C UNC ART INT, P271; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877	19	4	6	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1993	15	3					233	245		10.1109/34.204905	http://dx.doi.org/10.1109/34.204905			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	KT658					2022-12-18	WOS:A1993KT65800005
J	LEE, KH; EOM, KB; KASHYAP, RL				LEE, KH; EOM, KB; KASHYAP, RL			CHARACTER-RECOGNITION BASED ON ATTRIBUTE-DEPENDENT PROGRAMMED GRAMMAR	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						ATTRIBUTED GRAMMAR; CHARACTER RECOGNITION; SYNTACTIC PATTERN RECOGNITION	PARALLEL THINNING ALGORITHMS; PATTERN-RECOGNITION; DESIGN	The recognition ot Korean characters by a syntactic method is considered in this correspondence. Korean characters are composed of phonetic symbols in two dimensions and contain very little redundancy. In addition, the phonetic symbols in each character are different in shape and number depending on how they are composed. Thus, attribute information, such as the relative position and size of strokes, is important for recognizing Korean characters. We developed a Korean character recognition algorithm based on an attribute-dependent programmed grammar, which is a generalization of Rosenkrantz's programmed grammar. The attribute information, such as position, angle, length, and branch points of strokes, is utilized for recognizing Korean characters. The preprocessing and primitive extraction algorithm is also described in this paper. The proposed algorithm is implemented and tested with more than 9600 Korean characters in pages randomly selected from children's story books. Three hundred and seventy seven productions and 78 attribute test functions are derived from a good quality printed text containing 4206 Korean characters. The productions and the attribute test functions obtained from the 4206 characters are applied to a second text containing 2295 Korean characters and a third text containing 3092 Korean characters. The characters found in the second and third texts have some defects caused by a repeated reproduction process. The algorithm based on the attribute-dependent programmed grammar recognizes Korean characters reasonably quickly, and more than 95.1% of the characters are correctly recognized.	GEORGE WASHINGTON UNIV, DEPT ELECT ENGN & COMP SCI, WASHINGTON, DC 20052 USA; PURDUE UNIV, SCH ELECT ENGN, W LAFAYETTE, IN 47907 USA	George Washington University; Purdue University System; Purdue University; Purdue University West Lafayette Campus	LEE, KH (corresponding author), INHA UNIV, DEPT COMP SCI, INCHON, SOUTH KOREA.							BECKER JD, 1985, COMPUTER, V18, P27, DOI 10.1109/MC.1985.1662679; CASEY RG, 1984, IEEE T INFORM THEORY, V30, P93, DOI 10.1109/TIT.1984.1056834; CHOMSKY N, 1956, IRE T INFORM THEOR, V2, P113; ESHERA MA, 1986, IEEE T PATTERN ANAL, V8, P604, DOI 10.1109/TPAMI.1986.4767835; FU KS, 1983, IEEE T PATTERN ANAL, V5, P200; FU KS, 1982, SYNTACTIC PATTERN RE; GUO C, 1986 P INT C PATT RE, P1013; KAHAN S, 1987, IEEE T PATTERN ANAL, V9, P274, DOI 10.1109/TPAMI.1987.4767901; KUROSAWA Y, 1986 P INT C PATT RE, P1063; LEE KH, 1988, 1988 P IEEE C COMP V; LEE KH, 1981, THESIS INHA U INCHON; MANTAS J, 1986, PATTERN RECOGN, V19, P425, DOI 10.1016/0031-3203(86)90040-3; MORI S, 1984, IEEE T PATTERN ANAL, V6, P386, DOI 10.1109/TPAMI.1984.4767545; NAGAHASHI H, 1986, IEEE T PATTERN ANAL, V8, P112, DOI 10.1109/TPAMI.1986.4767759; NAGY G, 1982, HDB STATISTICS, V2; ROSENFELD A, 1975, INFORM CONTROL, V29, P286, DOI 10.1016/S0019-9958(75)90448-9; ROSENFELD A, 1973, IEEE T COMPUT, VC 22, P875, DOI 10.1109/TC.1973.5009188; Rosenfeld A., 1982, DIGITAL PICTURE PROC; ROSENKRANTZ DJ, 1967, THESIS COLUMBIA U NE; SHRIDHAR M, 1984, PATTERN RECOGN, V17, P515, DOI 10.1016/0031-3203(84)90049-9; STEFANELLI R, 1971, J ACM, V18, P255, DOI 10.1145/321637.321646; STENTIFORD FWM, 1985, IEEE T PATTERN ANAL, V7, P349, DOI 10.1109/TPAMI.1985.4767665; SWAIN PH, 1972, PATTERN RECOGN, V4, P83, DOI 10.1016/0031-3203(72)90021-0; TEH CH, 1989, IEEE T PATTERN ANAL, V11, P859, DOI 10.1109/34.31447; TOUSSAINT GT, 1978, PATTERN RECOGN, V10, P189, DOI 10.1016/0031-3203(78)90027-4; TSAI WH, 1980, IEEE T SYST MAN CYB, V10, P873, DOI 10.1109/TSMC.1980.4308414; Tsukumo J., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P162; YOU KC, 1979, IEEE T SYST MAN CYB, V9, P334, DOI 10.1109/TSMC.1979.4310222	28	4	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1992	14	11					1122	1128						7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JX370					2022-12-18	WOS:A1992JX37000009
J	KAMGARPARSI, B; KAMGARPARSI, B				KAMGARPARSI, B; KAMGARPARSI, B			QUANTIZATION-ERROR IN HEXAGONAL SENSORY CONFIGURATIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						AVERAGE ERROR; ERROR DISTRIBUTION; HEXAGONAL PIXEL; QUANTIZATION ERROR; SPATIAL SAMPLING; SQUARE PIXEL		Hexagonal spatial sampling is used widely in image and signal processing. However, no rigorous treatment of the quantization error due to hexagonal sampling has appeared in the literature. In this paper, we develop mathematical tools for estimating quantization error in hexagonal sensory configurations. These include analytic expressions for the average error and the error distribution of a function of an arbitrary number of independently quantized variables. These two quantities are essential for assessing the reliability of a given algorithm. They can also be used to compare the relative sensitivity of a particular algorithm to quantization error for hexagonal and other spatial samplings, e.g., square, and can have an impact on sensor design. Furthermore, we show that the ratio of hexagonal error to square error is bounded between 0.90 and 1.05.			KAMGARPARSI, B (corresponding author), USN, RES LAB, WASHINGTON, DC 20375 USA.							AHUJA N, 1983, COMPUT VISION GRAPH, V24, P200, DOI 10.1016/0734-189X(83)90043-9; Bell S., 1983, IMAGE VISION COMPUT, V1, P211; Bevington PR, 1969, DATA REDUCTION ERROR; BUCKLEW JA, 1984, IEEE T INFORM THEORY, V30, P107, DOI 10.1109/TIT.1984.1056837; BURT PJ, 1980, COMPUT VISION GRAPH, V14, P271, DOI 10.1016/0146-664X(80)90056-8; DAVIS LS, 1977, IEEE T COMPUT, V26, P236, DOI 10.1109/TC.1977.1674812; GERSHO A, 1979, IEEE T INFORM THEORY, V25, P373, DOI 10.1109/TIT.1979.1056067; HARTMAN NP, 1984, IEEE T SYST MAN CYB, V14, P247, DOI 10.1109/TSMC.1984.6313207; Kamgar-Parsi B., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P604, DOI 10.1109/CVPR.1989.37908; KAMGARPARSI B, 1989, IEEE T PATTERN ANAL, V11, P929, DOI 10.1109/34.35496; MERSEREAU RM, 1979, P IEEE, V67, P930, DOI 10.1109/PROC.1979.11356; REINGOLD EM, 1977, COMBINATORIAL ALGORI; RINES KD, 1982, IEEE T INFORM THEORY, V28, P232, DOI 10.1109/TIT.1982.1056485; SERRA J, 1985, SIGNAL PROCESS, V9, P1, DOI 10.1016/0165-1684(85)90060-X; STEVENSON RL, 1985, J OPT SOC AM A, V2, P1009, DOI 10.1364/JOSAA.2.001009; ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288; ZADOR PL, 1982, IEEE T INFORM THEORY, V28, P139, DOI 10.1109/TIT.1982.1056490	17	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1992	14	6					665	671		10.1109/34.141556	http://dx.doi.org/10.1109/34.141556			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HX546					2022-12-18	WOS:A1992HX54600006
J	MALLIKARJUNA, HS; CHAPARRO, LF				MALLIKARJUNA, HS; CHAPARRO, LF			ITERATIVE COMPOSITE FILTERING FOR IMAGE-RESTORATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						ADAPTIVE LEVINSON FILTERING; COMPOSITE MODEL FOR IMAGES; IMAGE RESTORATION; MEDIAN FILTERING	NOISE	In this paper, we propose an algorithmic solution to the noisy image restoration problem under assumptions of nonstationarity for the image and that the noise process is a superposition of white and impulsive noises. We use a composite model for the image in order to consider its nonstationarities in the mean and the autocorrelation. Separating the gross information of the image from its textural information, we then exploit the advantages of median, range, and Levinson filters in restoring the image. Median statistics are used to estimate the image's gross information and to filter the impulsive noise. Range statistics are used to segment the textural image into approximately locally stationary images to be filtered by Levinson filters. The proposed restoration algorithm adapts to the nonstationarity of the image, and thus, it performs well. By means of an example, we compare the performance of our algorithm with others based on either median or linear filtering alone.	UNIV PITTSBURGH,DEPT ELECT ENGN,PITTSBURGH,PA 15260	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	MALLIKARJUNA, HS (corresponding author), ST LOUIS UNIV,PARKS COLL,DEPT ELECT ENGN,CAHOKIA,IL 62206, USA.							Anderson B. D. O., 1979, OPTIMAL FILTERING; [Anonymous], 1976, TIME SERIES ANAL; ARCE GR, 1987, IEEE T ACOUST SPEECH, V35, P60, DOI 10.1109/TASSP.1987.1165036; BOUDAOUD M, 1989, IEEE T SYST MAN CYB, V19, P112, DOI 10.1109/21.24539; BOUDAOUD M, 1986, THESIS U PITTSBURGH; DAVID HA, 1981, ORDER STATISTICS; DRAVIDA S, 1984 P IEEE ICASSP, V2; KUAN DT, 1985, IEEE T PATTERN ANAL, V7, P165, DOI 10.1109/TPAMI.1985.4767641; LEE JS, 1980, IEEE T PATTERN ANAL, V2, P165, DOI 10.1109/TPAMI.1980.4766994; NARENDRA PM, 1981, IEEE T PATTERN ANAL, V3, P20, DOI 10.1109/TPAMI.1981.4767047; NODES T, 1984 P IEEE ICASSP, V2; STRICKLAND RN, 1983, APPL OPTICS, V22, P1462, DOI 10.1364/AO.22.001462; TEKALP AM, 1985, IEEE T ACOUST SPEECH, V35, P469	13	4	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1992	14	6					674	678		10.1109/34.141561	http://dx.doi.org/10.1109/34.141561			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HX546					2022-12-18	WOS:A1992HX54600008
J	SHAKUNAGA, T				SHAKUNAGA, T			3-D CORRIDOR SCENE MODELING FROM A SINGLE VIEW UNDER NATURAL LIGHTING CONDITIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						GENERIC 3-D MODEL; HOUGH TRANSFORM; INDOOR SCENE; MUTUAL ORTHOGONALITY; NATURAL LIGHTING CONDITIONS; PARALLELISM; SCENE MODELING; SINGLE VIEW ANALYSIS	PERSPECTIVE IMAGES; VANISHING POINTS; OBJECTS; TRANSFORM	This paper discusses a modeling 3-D indoor scene from a single view using a generic object model. It is assumed that an image is made by a well-calibrated camera, and the camera height above the floor is known. The image is assumed to be a projection of a natural corridor scene for which a generic model is known, but the specific model is unknown. This system can model any corridor that satisfies the following conditions: 1) the corridor is a rectangular parallelepiped, 2) there are several lines along each axis of the rectangle parallelepiped, 3) there are not many parallel lines in directions other than along the principle axes, and 4) corridor height is within a known range. The system for 3-D modeling from a single view of such a corridor consists of a robust bottom-up image processing part and a top-down, model-based interpretation part. Image processing treats difficult problems, such as shading, reflection, and high lights, which are invoked by natural lighting conditions. Interpretation extracts feasible 3-D interpretations of the image guided by the generic 3-D corridor model. Both parts are robust enough to make specific corridor models from an image. Experimental results show that specific corridor models can be recovered from a sing]e view with an approximately 1% estimation error.			SHAKUNAGA, T (corresponding author), NIPPON TELEGRAPH & TEL PUBL CORP, HUMAN INTERFACE LABS, TOKYO 100, JAPAN.							[Anonymous], 1985, PERCEPTUAL ORG VISUA; BARNARD ST, 1983, ARTIF INTELL, V21, P435, DOI 10.1016/S0004-3702(83)80021-6; BARNARD ST, 1985, COMPUT VISION GRAPH, V29, P87, DOI 10.1016/S0734-189X(85)90152-5; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; CHEN CH, 1989, IEEE T SYST MAN CYB, V19, P1535, DOI 10.1109/21.44070; DHOME M, 1989, IEEE T PATTERN ANAL, V11, P1265, DOI 10.1109/34.41365; DUDA RO, 1968, PATTERN CLASSIFICATI; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; HORAUD R, 1987, IEEE T PATTERN ANAL, V9, P401, DOI 10.1109/TPAMI.1987.4767922; KENDER JR, 1978, NOV P DARPA IM UND W, P79; MAGEE MJ, 1984, COMPUT VISION GRAPH, V26, P256, DOI 10.1016/0734-189X(84)90188-9; Nakatani H., 1980, Proceedings of the 5th International Conference on Pattern Recognition, P370; QUAN L, 1989, PATTERN RECOGN LETT, V9, P279, DOI 10.1016/0167-8655(89)90006-8; SHAKUNAGA T, 1989, INT J COMPUT VISION, V3, P239, DOI 10.1007/BF00133033; WEISS RS, 1990, IEEE T PATTERN ANAL, V12, P1179, DOI 10.1109/34.62606; YACHIDA M, 1983, P INT JOINT C ART IN, V1, P1125	16	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1992	14	2					293	298		10.1109/34.121796	http://dx.doi.org/10.1109/34.121796			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HC029					2022-12-18	WOS:A1992HC02900015
J	PIECH, MA				PIECH, MA			DECOMPOSING THE LAPLACIAN	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											PIECH, MA (corresponding author), SUNY BUFFALO,DEPT MATH,BUFFALO,NY 14214, USA.							CLARK JJ, 1989, IEEE T PATTERN ANAL, V11, P43, DOI 10.1109/34.23112; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; Thomas G.B.J., 1988, CALCULUS ANAL GEOMET	3	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1990	12	8					830	831		10.1109/34.57673	http://dx.doi.org/10.1109/34.57673			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DQ388					2022-12-18	WOS:A1990DQ38800009
J	LEE, LS; TSENG, CY; CHEN, KJ; HUANG, J; HWANG, CH; TING, PY; LIN, LJ; CHEN, CC				LEE, LS; TSENG, CY; CHEN, KJ; HUANG, J; HWANG, CH; TING, PY; LIN, LJ; CHEN, CC			A MANDARIN DICTATION MACHINE BASED UPON A HIERARCHICAL RECOGNITION APPROACH AND CHINESE NATURAL-LANGUAGE ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									ACAD SINICA,INST INFORMAT SCI,TAIPEI 115,TAIWAN; CORNELL UNIV,DEPT MODERN LANGUAGE & LINGUIST,ITHACA,NY 14853	Academia Sinica - Taiwan; Cornell University	LEE, LS (corresponding author), NATL TAIWAN UNIV,DEPT ELECT ENGN,TAIPEI,TAIWAN.		Ting, Pei-Yih/R-3303-2016	Ting, Pei-Yih/0000-0001-6727-5429				[Anonymous], 2014, TONE LINGUISTIC SURV; AVERBUCH A, 1986, 1986 P INT C AC SPEE, V1, P53; BURTON D, 1985, IEEE T ACOUST SPEECH, V33; Chao Y. R., 1968, GRAMMAR SPOKEN CHINE; CHEN CG, 1986, 1986 P INT C CHIN CO, P33; DEROUAULT AM, 1987, 1987 P INT C AC SPEE; GU HY, 1989, J CHINESE I ENG; Huang James, 1982, THESIS MIT; HWANG CW, 1986, 1986 P INT COMP S TA; LEE JC, 1986, THESIS TSING HUA U T; LEE LS, 1987, 1987 P INT JOINT C A; LEI SM, 1983, J CHIN INST ENG, V6, P107; Li C., 1981, MANDARIN CHINESE FUN; LIN LJ, 1986, 5 NAT C ART INT PHIL, P1059; LIN LJ, 1986, 1986 P INT C CHIN CO, P29; MERIALDO B, 1987, 1987 P INT C AC SPEE, P364; PICHENY M, 1986, REAL TIME IBM PC BAS; TING PY, 1988, 1988 P INT C AC SPEE; WU CH, 1987, 1987 NAT COMP S TAIP, P971; YU MS, 1986, 1986 P INT C CHIN CO, P168; 1987, 1986 1987 P INT C CH	21	4	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1990	12	7					695	704		10.1109/34.56213	http://dx.doi.org/10.1109/34.56213			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DK894		Green Submitted			2022-12-18	WOS:A1990DK89400011
J	MILGRAM, M; DESTPIERRE, T				MILGRAM, M; DESTPIERRE, T			BOUNDARY DETECTION AND SKELETONIZATION WITH A MASSIVELY PARALLEL ARCHITECTURE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											MILGRAM, M (corresponding author), UNIV PARIS 06,ROBOT LAB,F-75230 PARIS 05,FRANCE.							BARROW HG, 1977, P IJCAI; BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5; BORGEFORS G, 1983, MATCHING EDGES MAPS; DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4; DEVOS F, 1985, P CAPAIDM IEEE COMPU; DUFF MJB, 1978, P NAT COMPUTER C ANA; FOUNTAIN TJF, 1986, CELLULAR LOGIC IMAGE; HILLIS D, 1985, CONNECTION MACHINE; HUNT DJ, 1981, LANGUAGE ARCHITECTUR; MILGRAM M, 1986, TRAITEMENT SIGNAL, V3; MILGRAM M, 1982, THESIS U COMPIEGNE F; MONTANARI U, 1968, J ACM, V15, P600, DOI 10.1145/321479.321486; POTTER JL, 1981, P IEEE COMPUT SOC WO; Wolfram S, 1983, CELLULAR AUTOMATA; WOOD AM, 1981, LANGUAGES ARCHITECTU	15	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1990	12	1					74	78		10.1109/34.41385	http://dx.doi.org/10.1109/34.41385			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CG247					2022-12-18	WOS:A1990CG24700007
J	SOTAK, GE; BOYER, KL				SOTAK, GE; BOYER, KL			FAST CONVOLUTION WITH LAPLACIAN-OF-GAUSSIAN MASKS - COMMENTS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											SOTAK, GE (corresponding author), OHIO STATE UNIV,DEPT ELECT ENGN,SIGNAL ANAL & MACHINE PERCEPT LAB,COLUMBUS,OH 43210, USA.							CHEN JS, 1987, IEEE T PATTERN ANAL, V9, P584, DOI 10.1109/TPAMI.1987.4767946; GRIMSON WEL, 1986, TECHNIQUES 3D MACHIN, P75; HILDRETH EC, 1980, MIT AITR579 ART INT; KING D, 1982, ISG102 U SO CAL TECH; SOTAK GE, 1988, SAMPL8801 OH STAT U; WIEJAK JS, 1985, COMPUT VISION GRAPH, P279	6	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	1989	11	12					1329	1332		10.1109/34.41372	http://dx.doi.org/10.1109/34.41372			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CB716					2022-12-18	WOS:A1989CB71600010
J	THOMPSON, WB				THOMPSON, WB			SPECIAL ISSUE ON VISUAL-MOTION - INTRODUCTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material											THOMPSON, WB (corresponding author), UNIV MINNESOTA,DEPT COMP SCI,MINNEAPOLIS,MN 55455, USA.							CAFFORIO C, 1976, IEEE T INFORM THEORY, V22, P573, DOI 10.1109/TIT.1976.1055602; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; 1988, 2ND P INT C COMP VIS	3	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1989	11	5					449	450						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	U3604					2022-12-18	WOS:A1989U360400001
J	OOMMEN, BJ				OOMMEN, BJ			CORRECTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction, Addition													Oommen, B. John/P-6323-2017	Oommen, B. John/0000-0002-5105-1575				OOMMEN BJ, 1987, IEEE T PATTERN ANAL, V9, P676, DOI 10.1109/TPAMI.1987.4767962	1	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1988	10	6					983	983						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	Q9971					2022-12-18	WOS:A1988Q997100022
J	CHANG, JH; IBARRA, OH; PONG, TC; SOHN, SM				CHANG, JH; IBARRA, OH; PONG, TC; SOHN, SM			TWO-DIMENSIONAL CONVOLUTION ON A PYRAMID COMPUTER	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV MINNESOTA,DEPT COMP SCI,MINNEAPOLIS,MN 55455	University of Minnesota System; University of Minnesota Twin Cities								AHUJA N, 1984, IEEE T PATTERN ANAL, V6, P463, DOI 10.1109/TPAMI.1984.4767551; CANTONI V, 1986, PYRAMIDAL SYSTEMS CO; FANG Z, 1986, 1986 INT C PAR PROC, P262; KUNG HT, 1981, NOV IEEE COMP SOC WO, P159; LEE SY, 1987, IEEE T PATTERN ANAL, V9, P590, DOI 10.1109/TPAMI.1987.4767947; MARESCA M, 1986, 1986 P IEEE COMP SOC, P299; MILLER R, 1987, SIAM J COMPUT, V16, P38, DOI 10.1137/0216004; REEVES AP, 1984, COMPUT VISION GRAPH, V25, P68, DOI 10.1016/0734-189X(84)90049-5; STOUT QF, 1982, INFORM PROCESS LETT, V15, P233, DOI 10.1016/0020-0190(82)90124-7; TANIMOTO SL, 1984, MULTIRESOLUTION IMAG, P136; UHR L, 1987, MULTICOMPUTER ARCHIT	11	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1988	10	4					590	593		10.1109/34.3920	http://dx.doi.org/10.1109/34.3920			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	P1493					2022-12-18	WOS:A1988P149300013
J	MITICHE, A; SEIDA, S; AGGARWAL, JK				MITICHE, A; SEIDA, S; AGGARWAL, JK			USING CONSTANCY OF DISTANCE TO ESTIMATE POSITION AND DISPLACEMENT IN SPACE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV TEXAS,COMP & VIS RES CTR,AUSTIN,TX 78712	University of Texas System; University of Texas Austin								AGGARWAL JK, 1984, DIGITAL IMAGING ANAL, P29; BADLER N, 1975, THESIS U TORONTO ONT; GANAPATHY S, 1984, P INT C ROBOTICS, P130; Huang T.S., 1981, IMAGE SEQUENCE ANAL, P1; JERIAN C, 1984, IEEE T PATTERN ANAL, V6, P523, DOI 10.1109/TPAMI.1984.4767558; LELONGFERRAND J, 1974, COURS MATH, V3; Longuet-Higgins H. C., 1984, First Conference on Artificial Intelligence Applications (Cat. No. 84CH2107-1), P395; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MARTIN WN, 1979, PATTERN RECOGN, V11, P169, DOI 10.1016/0031-3203(79)90004-9; MARTIN WN, 1978, COMPUT VISION GRAPH, V7, P356, DOI 10.1016/S0146-664X(78)80003-3; Mitiche A., 1984, First Conference on Artificial Intelligence Applications (Cat. No. 84CH2107-1), P156; MITICHE A, 1985, COMPUT VISION GRAPH, V32, P384, DOI 10.1016/0734-189X(85)90058-1; MITICHE A, 1984, P WORKSHOP COMPUTER, P63; NAGEL HH, 1981, COMPUTER, V14, P29, DOI 10.1109/C-M.1981.220560; PINKNEY HFL, 1978, AUG INT SOC PHOT S C; RIVES P, 1982, TR8210 U QUEB I NAT; ROACH JW, 1979, IEEE T PATTERN ANAL, V1, P127, DOI 10.1109/TPAMI.1979.4766898; ROACH JW, 1980, IEEE T PATTERN ANAL, V2, P554, DOI 10.1109/TPAMI.1980.6447703; TASI RY, 1984, IEEE T PATTERN ANAL, V6, P13; ULLMAN S, 1986, MIT AI476 ART INT LA; ULLMAN S, 1984, PERCEPTION, V13, P225; VANWIJK MC, NRC12928 NAT RES COU; WEBB JA, 1982, ARTIF INTELL, V19, P107, DOI 10.1016/0004-3702(82)90023-6; WEBB JA, 1981, IEEE COMPUT      AUG, P40; YAKIMOVSKY Y, 1978, COMPUT VISION GRAPH, V7, P195, DOI 10.1016/0146-664X(78)90112-0; Zhuang X., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P686; 1986, MAY IEEE WORKSH MOT; 1980, LMDER MINPACK SUBROU	28	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1988	10	4					594	599		10.1109/34.3921	http://dx.doi.org/10.1109/34.3921			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	P1493					2022-12-18	WOS:A1988P149300014
J	MOTA, FA; VELASCO, FRD				MOTA, FA; VELASCO, FRD			A METHOD FOR THE ANALYSIS OF AMBIGUOUS SEGMENTATIONS OF IMAGES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									WANG INST GRAD STUDIES,TYNGSBORO,MA 01879		MOTA, FA (corresponding author), INST PESQUISAS ESPACIAIS,CP 515,BR-12200 SAN JOSE DOS CAM,SP,BRAZIL.							Ballard D.H., 1982, COMPUTER VISION; BARROW HG, 1981, P IEEE, V69, P571; FELDMAN JA, 1974, ARTIF INTELL, V5, P349, DOI 10.1016/0004-3702(74)90002-2; MOTA FA, 1984, THESIS I PESQUISAS E; Rosenfeld A, 1976, IEEE T SYST MAN CYBE, VSMC-6; TENENBAUM JM, 1977, COMPUTER METHODS IMA, P435; VELASCO FRD, 1979, IEEE T SYST MAN CYB, V9, P420, DOI 10.1109/TSMC.1979.4310254	7	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1986	8	6					755	760		10.1109/TPAMI.1986.4767857	http://dx.doi.org/10.1109/TPAMI.1986.4767857			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	E4469	21869371				2022-12-18	WOS:A1986E446900007
J	DUCHENE, J				DUCHENE, J			A SIGNIFICANT PLANE FOR 2-CLASS DISCRIMINATION PROBLEMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											DUCHENE, J (corresponding author), COMPIEGNE UNIV,DEPT BIOMED ENGN,BP 233,F-60206 COMPIEGNE,FRANCE.							CHIEN YT, 1978, INTERACTIVE PATTERN, P223; FOLEY DH, 1975, IEEE T COMPUT, VC 24, P281, DOI 10.1109/T-C.1975.224208; MUCCIARDI AN, 1971, IEEE T COMPUT, VC 20, P1023, DOI 10.1109/T-C.1971.223398; SAMMON JW, 1970, IEEE T COMPUT, VC 19, P826, DOI 10.1109/T-C.1970.223047; WILKS SS, 1962, MATH STATISTICS	5	4	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1986	8	4					557	559		10.1109/TPAMI.1986.4767823	http://dx.doi.org/10.1109/TPAMI.1986.4767823			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	C7400					2022-12-18	WOS:A1986C740000018
J	FU, KS				FU, KS			LEARNING CONTROL-SYSTEMS - REVIEW AND OUTLOOK	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Review											FU, KS (corresponding author), PURDUE UNIV, W LAFAYETTE, IN 47907 USA.							ABRAMSON N, 1962, IRE T INFORM THEOR, V8, pS58; AISERMAN MA, 1964, AVTOMAT TELEMEKH, V25, P1307; AISERMAN MA, 1964, AVTOMAT TELEMEKH, V25, P917; AISERMAN MA, 1964, AVTOMAT TELEMEKH, V25, P1705; BARRON R, 1968, CONTROL ENG      FEB; BRAVERMAN EM, 1969, AVTOMAT TELEMEKH, V30, P57; BRAVERMAN EM, 1966, AVTOMAT TELEMEKH, V27, P95; BRUCE GD, 1963, 1ST P ALL C CIRC SYS; Bush R.R., 1955, STOCHASTIC MODELS LE; BUTZ AR, 1968 P HAW INT C SYS; CARLYLE JW, 1961, U CALIFORNIA TECH RE, V60; CHANDRASEKARAN B, 1966, 5TH S DISCR AD PROC; CHANG KH, 1969, GEOL SOC KOREA J, V5, P145; CHIEN YT, 1966, IEEE T INFORM THEORY, V12, P206, DOI 10.1109/TIT.1966.1053863; CHIEN YT, 1967, IEEE T SYST SCI CYB, VSSC3, P28, DOI 10.1109/TSSC.1967.300105; CHIEN YT, 1966, IEEE T SYST SCI CYB, V5, P237; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; CHOW YS, 1963, Z WAHRSCHEINLICHKEIT, V2; FRALICK SC, 1967, IEEE T INFORM THEORY, V13, P57, DOI 10.1109/TIT.1967.1053952; FREEMAN H, 1962 P NEC, V18; Fu K. S., 1968, SEQUENTIAL METHODS P, V240, P241; FU KS, 1967, IEEE TRANS ELECTRON, VEC16, P790, DOI 10.1109/PGEC.1967.264725; FU KS, 1966, IEEE T AUTOMAT CONTR, VAC11, P756, DOI 10.1109/TAC.1966.1098440; FU KS, 1965, TREE6517 PURD U TECH; FU KS, 1969, ADV INFORMATION SYST; FU KS, 1969, SYSTEM THEORY, pCH11; FU KS, 1967, COMPUTER INFORMATION, V2; FU KS, 1965, TREE6517 PURD U SCH; FU KS, 1969, INFORM SCI, V1; FU KS, 1965 P IFAC S THEOR; FU KS, 1968, TREE6818 PURD U SCH; FU KS, 1964, COMPUTER INFORMATION; GARDEN M, 1967 P JACC PHIL, P58; GIBSON JE, 1963, TREE637 PURD U TECH; GILBERT HD, UNPUB IEEE T SYSTEMS; HARAI H, 1968, MEM FAC ENG OSAKA U, V10, P67; HENRICHON EG, 1969, IEEE T SYST SCI CYB, VSSC5, P150, DOI 10.1109/TSSC.1969.300207; HILL JD, 1965 P JACC TROY, P334; HO YC, 1968, IEEE T AUTOMAT CONTR, VAC13, P676, DOI 10.1109/TAC.1968.1099027; HSU JC, 1962, IRE T AUTOM CONTROL, VAC 7, P24, DOI 10.1109/TAC.1962.1105402; IVAKHNENKO AG, 1969, 4TH IFAC C WARS; IVANOV AZ, P MOSCOW POWER I; JACUBOVICH VA, 1969 P IFAC C WARS; JARVIS RA, 1969 IEEE SYST SCI C; JONES LE, 1968, IEEE T AUTOMAT CONTR, VAC13, P613, DOI 10.1109/TAC.1968.1099032; JONES LE, 1969, AUTOMATICA       NOV; KAHNE SJ, 1966, IEEE T AUTOMAT CONTR, VAC11, P611, DOI 10.1109/TAC.1966.1098350; KANAL LN, 1968, PATTERN RECOGNITION; KEEHN DG, 1965, IEEE T INFORM THEORY, V11, P126, DOI 10.1109/TIT.1965.1053726; KRUG GK, 1963 P IFAC C BAS; KRYLOV VY, 1963, AVTOMAT TELEMEKH, V24; LAMBERT JD, 1968, IEEE T AUTOMAT CONTR, VAC13, P741, DOI 10.1109/TAC.1968.1099059; LAMBERT JD, 1970, UNPUB IEEE T AUTOMAT, V15; LEONDES CT, 1967, 4336 MCDONN ASTR CO; MCLAREN RW, 1966, IEEE T SYST SCI CYB, VSSC2, P109, DOI 10.1109/TSSC.1966.6593092; MCLAREN RW, 1967 P JACC PHIL, P267; MCMURTRY GJ, 1966, IEEE T AUTOMAT CONTR, V11, P379; Mendel J. M., 1970, ADAPTIVE LEARNING PA; MENDEL JM, 1966 P JACC SEATTL, P1; MENDEL JM, 1966, DAC59328 DOUGL AIRCR; MENDEL JM, 1968, ADV CONTROL SYSTEMS, V6; MESAROVIC MD, 1962, SELF ORG SYSTEMS 196; MICHIE D, 1968, MACHINE INTELLIGENCE, V2; MOSTELLER HW, 1964, R64ELC37 GE ADV EL C; NAKAMURA K, 1967, COMPUTER INFORMATION, V2; NAPLATANOFF ND, 1968, CYBERNETICA, V11; NARENDRA KS, 1962, 359 HARV U CRUFT LAB; NETUSHIL AV, 1961, IZV VUZOV SSSR MASHI; NIKOLIC ZJ, 1966, IEEE T AUTOMAT CONTR, VAC11, P414, DOI 10.1109/TAC.1966.1098345; NIKOLIC ZJ, 1966 P NEC; Nilsson N., 1965, LEARNING MACHINES; PAZ A, 1966, INFORM CONTROL, V9, P26, DOI 10.1016/S0019-9958(66)90092-1; PUGACHEV VS, 1967, ENG CYBERNETICS, V5; RABIN MO, 1963, INFORM CONTROL, V6, P230, DOI 10.1016/S0019-9958(63)90290-0; RIORDON JS, 1969 P JACC BOULD; ROHAVA SY, 1968, SOVIET AUTOMATIC NOV, P52; SARIDIS GN, 1969, IEEE T SYST SCI CYB, VSSC5, P8, DOI 10.1109/TSSC.1969.300238; SAWARAGI Y, 1967, STATISTICAL DECISION; Sebestyen G., 1962, DECISION MAKING PROC, V227, P413; SKLANSKY J, 1966, IEEE T AUTOMAT CONTR, VAC11, P6, DOI 10.1109/TAC.1966.1098229; SMITH FB, 1964, FDLTDR6489 TECH REPT; SMITH FW, 1964, 67622 STANF U STANF; SPRAGINS J, 1966, IEEE T INFORM THEORY, V12, P223; SPRAGINS J, 1965, IEEE T INFORM THEORY, V11, P544, DOI 10.1109/TIT.1965.1053826; STRATONOVICH RL, 1968, AVTOMAT TELEMEKH, V29, P83; TEICHER H, 1963, ANN MATH STATIST, V34; Tou J.T., 1964, MODERN CONTROL THEOR; TOU JT, 1966 P JACC SEATTL, P12; TSETLIN ML, 1961, AVTOMAT TELEMEKH, V22; TSYPKIN YZ, 1968, IEEE T AUTOMAT CONTR, VAC13, P608, DOI 10.1109/TAC.1968.1099015; TSYPKIN YZ, 1966, 1966 P IFAC C LOND; TSYPKIN YZ, 1966, AVTOMAT TELEMEKH, V27; TSYPKIN YZ, 1968, AVTOMAT TELEMEKH, V29, P93; TSYPKIN YZ, UNPUB ADAPTATION LEA; TSYPKIN YZ, 1969 P IFAC C WARS; ULA N, 1965, J FRANKLIN I, V280, P189, DOI 10.1016/0016-0032(65)90001-3; VARSHAVSKY VI, 1963, AVTOMAT TELEMEKH, V24; VAYSBARD EM, 1965, ENG CYBERNETICS  SEP, P1; WALTZ MD, 1965, IEEE T AUTOMAT CONTR, VAC10, P390, DOI 10.1109/TAC.1965.1098193; WATANABE S, 1969, METHODOLOGIES PATTER; WEE WG, 1969, IEEE T SYST SCI CYB, VSSC5, P215, DOI 10.1109/TSSC.1969.300263; WIDROW B, 1964, COMPUTER INFORMATION; ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X	103	4	4	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1986	8	3					327	342		10.1109/TPAMI.1986.4767795	http://dx.doi.org/10.1109/TPAMI.1986.4767795			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	C0841	21869351	Green Submitted			2022-12-18	WOS:A1986C084100004
J	DEVROYE, L; MACHELL, F				DEVROYE, L; MACHELL, F			DATA-STRUCTURES IN KERNEL DENSITY-ESTIMATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV TEXAS,APPL RES LABS,AUSTIN,TX 78713	University of Texas System; University of Texas Austin	DEVROYE, L (corresponding author), MCGILL UNIV,SCH COMP SCI,MONTREAL H3A 2T5,QUEBEC,CANADA.							Aho A., 1976, DESIGN ANAL COMPUTER; BENNETT G, 1962, J AM STAT ASSOC, V57, P33, DOI 10.2307/2282438; CACOULLOS T, 1965, ANN I STAT MATH, V18, P179; DAVID HA, 1970, ORDER STATISTICS; DEHEUVELS P, 1977, REV STATISTIQUE APPL, V25, P5; DEVROYE L, UNPUB ANN STATIST, V11; DEVROYE L, 1982, DISTRIBUTION FREE LO; DEVROYE LP, 1979, ANN STAT, V7, P1136, DOI 10.1214/aos/1176344796; EPANECHN.VA, 1969, THEOR PROBAB APPL+, V14, P153, DOI 10.1137/1114019; HOARE CAR, 1962, COMPUT J, V5, P10, DOI 10.1093/comjnl/5.1.10; Knuth D., 1973, ART COMPUTER PROGRAM, V3; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; POSTAIRE JG, 1982, IEEE T PATTERN ANAL, V4, P663, DOI 10.1109/TPAMI.1982.4767322; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; SILVERMAN BW, 1982, APPL STAT-J ROY ST C, V31, P93, DOI 10.2307/2347084; SINGLETON RC, 1969, COMMUN ACM, V12, P185, DOI 10.1145/362875.362901; Wheeden R. L., 1977, MEASURE INTEGRAL; [No title captured]; [No title captured]	19	4	4	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	3					360	366		10.1109/TPAMI.1985.4767668	http://dx.doi.org/10.1109/TPAMI.1985.4767668			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AFM44	21869274				2022-12-18	WOS:A1985AFM4400014
J	LAM, KP				LAM, KP			CONTOUR MAP REGISTRATION USING FOURIER DESCRIPTORS OF GRADIENT CODES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											LAM, KP (corresponding author), NATL RES COUNCIL CANADA,DIV ELECT ENGN,OTTAWA K1A 0R8,ONTARIO,CANADA.							Andreas R. D., 1978, Proceedings of the IEEE 1978 National Aerospace and Electronics Conference NAECON 78, P1263; BARNEA DI, 1972, IEEE T COMPUT, V21, P172; BENDAT JS, 1971, RANDOM DATA ANAL MEA, pCH1; CARTER GC, 1981, IEEE T ACOUST SPEECH, V29, P461, DOI 10.1109/TASSP.1981.1163559; FREEMAN H, 1980, COMPUT VISION GRAPH, V12, P203, DOI 10.1016/0146-664X(80)90012-X; FREEMAN H, 1967, J FRANKLIN I, V248, P1; Golden J. P., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V238, P10; GONZALEZ RC, 1977, DIGITAL IMAGE PROCES, pCH3; HALL EL, 1979, COMPUTER IMAGE PROCE, pCH8; Kuglin C. D., 1979, P SPIE DIGITAL PROCE, V186, P21; LAM KP, 1984, COMPUT VISION GRAPH, V28, P228, DOI 10.1016/S0734-189X(84)80024-9; LAM KP, 1977, THESIS CHINESE U HON; MERILL RD, 1973, COMMUN ACM, V16, P69; Montuno D. Y., 1980, Transactions of the Institute of Electronics and Communication Engineers of Japan, Section E (English), VE63, P421; MORSE SP, 1968, 23RD P ASS COMP MACH, P45; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; SAVOL AM, 1978, IEEE C PATTERN RECOG, P91; TANIGUCHI R, 1982, 6TH P INT C PATT REC, P802; TSUI HT, 1978, 4TH P INT C PATT REC, P635	19	4	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	3					332	338		10.1109/TPAMI.1985.4767662	http://dx.doi.org/10.1109/TPAMI.1985.4767662			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AFM44	21869268				2022-12-18	WOS:A1985AFM4400008
J	ZHANG, B; ZHANG, L				ZHANG, B; ZHANG, L			A NEW HEURISTIC-SEARCH TECHNIQUE - ALGORITHM SA	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									ANQING TEACHERS COLL,DEPT MATH,ANQING,PEOPLES R CHINA	Anqing Normal University	ZHANG, B (corresponding author), TSINGHUA UNIV,DEPT COMP ENGN & SCI,BEIJING,PEOPLES R CHINA.							[Anonymous], 1980, PRINCIPLES ARTIFICIA; BAUDET GM, 1978, ARTIF INTELL, V10, P173, DOI 10.1016/S0004-3702(78)80011-3; BERLINER H, 1982, READINGS ARTIFICIAL, P79; DEGROOT MH, 1975, PROBABILITY STATISTI; KANAL LN, 1981, 7TH P INT JOINT C AI, P569; KNOPP K, 1923, THEORIE ANWENDUNG UN; KNUTH DE, 1975, ARTIF INTELL, V6, P293, DOI 10.1016/0004-3702(75)90019-3; PEARL J, 1980, ARTIF INTELL, V14, P113, DOI 10.1016/0004-3702(80)90037-5; PEARL J, 1980, ARTIFICIAL INTELL, V15, P241; PEARL J, 1983, ARTIFICIAL INTELL, V20; PEARL J, 1981, 7TH P INT JOINT C AR, P554; ZACKS S, 1971, THEORY STATISTIC INF; ZHANG L, UNPUB ALGORITHM SA G	13	4	6	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	1					103	107		10.1109/TPAMI.1985.4767624	http://dx.doi.org/10.1109/TPAMI.1985.4767624			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	ABF09	21869246				2022-12-18	WOS:A1985ABF0900011
J	DAY, WHE; WELLS, RS				DAY, WHE; WELLS, RS			EXTREMES IN THE COMPLEXITY OF COMPUTING METRIC DISTANCES BETWEEN PARTITIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									STAT CANADA,DIV STRUCT ANAL,OTTAWA K1A OT6,ONTARIO,CANADA	Statistics Canada	DAY, WHE (corresponding author), MEM UNIV NEWFOUNDLAND,DEPT COMP SCI,ST JOHNS A1C 5S7,NEWFOUNDLAND,CANADA.							ARABIE P, 1973, J MATH PSYCHOL, V10, P148, DOI 10.1016/0022-2496(73)90012-6; DAY WHE, 1981, MATH SOC SCI, V1, P269, DOI 10.1016/0165-4896(81)90042-1; Garey M.R., 1979, COMPUTERS INTRACTABI; GRAHAM RL, 1982, MATH BIOSCI, V60, P133, DOI 10.1016/0025-5564(82)90125-0; Harary F., 1994, GRAPH THEORY; Jardine N., 1971, MATH TAXONOMY; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Rohlf F.J., 1974, Annual Rev Ecol Syst, V5, P101, DOI 10.1146/annurev.es.05.110174.000533; RUBIN J, 1967, J THEOR BIOL, V15, P103, DOI 10.1016/0022-5193(67)90046-X; [No title captured]	10	4	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	1					69	73		10.1109/TPAMI.1984.4767476	http://dx.doi.org/10.1109/TPAMI.1984.4767476			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	SB213	21869166				2022-12-18	WOS:A1984SB21300007
J	PELEG, S; NAOR, J; HARTLEY, R; AVNIR, D				PELEG, S; NAOR, J; HARTLEY, R; AVNIR, D			MULTIPLE RESOLUTION TEXTURE ANALYSIS AND CLASSIFICATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									UNIV MARYLAND,CTR AUTOMAT RES,COLLEGE PK,MD 20742; HEBREW UNIV JERUSALEM,DEPT ORGAN CHEM,IL-91904 JERUSALEM,ISRAEL	University System of Maryland; University of Maryland College Park; Hebrew University of Jerusalem	PELEG, S (corresponding author), HEBREW UNIV JERUSALEM,DEPT COMP SCI,IL-91904 JERUSALEM,ISRAEL.		Peleg, Shmuel/B-7454-2011; Avnir, David/AAK-4189-2021	Peleg, Shmuel/0000-0002-4468-2619; 				AVNIR D, 1983, NOUV J CHIM, V7, P71; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; LARKIN LI, 1983, JUN IEEE P C COMP VI, P519; LAWS K, 1980, USC940 IM PROC I REP; Mandelbrot, 1982, FRACTAL GEOMETRY NAT, P394; Mandelbrot B.B., 1977, FRACTALS FORM CHANCE; NGUYEN PT, 1982, 6TH P INT C PATT REC, P282; PENTLAND AP, 1983, JUN IEEE COMP SOC C, P201; PFEIFER P, 1983, SURF SCI, V126, P569, DOI 10.1016/0039-6028(83)90759-8; PRATT WK, 1981, P IEEE           MAY	11	4	8	2	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	4					518	523		10.1109/TPAMI.1984.4767557	http://dx.doi.org/10.1109/TPAMI.1984.4767557			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SY289		Green Submitted			2022-12-18	WOS:A1984SY28900013
J	PRICE, K				PRICE, K			IMAGE SEGMENTATION STUDIES IN GLOBAL AND LOCAL HISTOGRAM-GUIDED RELAXATION ALGORITHMS - COMMENT	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											PRICE, K (corresponding author), UNIV SO CALIF,DEPT ELECT ENGN,LOS ANGELES,CA 90089, USA.							OHLANDER R, 1978, COMPUT VISION GRAPH, V8, P313, DOI 10.1016/0146-664X(78)90060-6; SHAFER S, 1982, MAY P IEEE INT C AC	2	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	2					247	249		10.1109/TPAMI.1984.4767509	http://dx.doi.org/10.1109/TPAMI.1984.4767509			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SF591	21869189				2022-12-18	WOS:A1984SF59100012
J	SHAPIRA, R				SHAPIRA, R			THE USE OF OBJECTS FACES IN INTERPRETING LINE DRAWINGS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									MIT,ELECTR RES LAB,COGNIT INFORMAT PROC GRP,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)								CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; DRAPER SW, 1981, ARTIF INTELL, V17, P461, DOI 10.1016/0004-3702(81)90032-1; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; HUFFMAN DA, 1977, MACH INTELL, V8, P493; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; SHAPIRA R, 1984, UNPUB IEEE T PATTERN; SHAPIRA R, 1979, COMMUN ACM, V22, P369; SHAPIRA R, 1976, THESIS NEW YORK U; SUGIHARA K, 1984, IEEE T PATTERN ANAL, V6, P578, DOI 10.1109/TPAMI.1984.4767571	9	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	6					789	794		10.1109/TPAMI.1984.4767602	http://dx.doi.org/10.1109/TPAMI.1984.4767602			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	TX361	22499659				2022-12-18	WOS:A1984TX36100012
J	CHEN, TC; DEFIGUEIREDO, RJP				CHEN, TC; DEFIGUEIREDO, RJP			AN IMAGE TRANSFORM CODING SCHEME BASED ON SPATIAL DOMAIN CONSIDERATIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									RICE UNIV,DEPT ELECT ENGN,HOUSTON,TX 77251	Rice University								Andrews H.C., 1970, COMPUTER TECHNIQUES; ANDREWS HC, 1976, IEEE T COMMUN, V24, P425, DOI 10.1109/TCOM.1976.1093309; ANDREWS HC, 1969, P IEEE, V57, P58; ANDREWS HC, 1968, JAN P HAW INT C SYST, P677; ANDREWS HC, 1975, PICTURE PROCESSING D; CHEN TC, UNPUB IMAGE DECIMATI; ENOMOTO H, 1971, IEEE T ELECTROMAGN C, VEM13, P11, DOI 10.1109/TEMC.1971.303101; HABIBI A, 1971, IEEE T COMMUN TECHN, VCO19, P50, DOI 10.1109/TCOM.1971.1090601; JAIN AK, 1976, IEEE T COMMUN, V24, P1023, DOI 10.1109/TCOM.1976.1093409; JAIN AK, 1979, IEEE T PATTERN ANAL, V1, P356, DOI 10.1109/TPAMI.1979.4766944; JAIN AK, 1981, P IEEE, V69, P349, DOI 10.1109/PROC.1981.11971; PRATT WK, 1974, IEEE T COMMUN, VCO22, P1075, DOI 10.1109/TCOM.1974.1092335; RABINER LR, 1975, THEORY APPLICATION D, P59; WINTZ PA, 1972, PR INST ELECTR ELECT, V60, P809, DOI 10.1109/PROC.1972.8780	14	4	5	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	3					332	337		10.1109/TPAMI.1983.4767395	http://dx.doi.org/10.1109/TPAMI.1983.4767395			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QS785	21869116				2022-12-18	WOS:A1983QS78500008
J	KASHYAP, RL; OOMMEN, BJ				KASHYAP, RL; OOMMEN, BJ			SCALE PRESERVING SMOOTHING OF POLYGONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									CARLETON UNIV,SCH COMP SCI,OTTAWA K1S 5B6,ONTARIO,CANADA	Carleton University	KASHYAP, RL (corresponding author), PURDUE UNIV,SCH ELECT ENGN,W LAFAYETTE,IN 47907, USA.		Oommen, B. John/P-6323-2017	Oommen, B. John/0000-0002-5105-1575				Freeman H., 1961, IRE T ELECT COMPUTER, VEC-10, P260, DOI DOI 10.1109/TEC.1961.5219197; KASHYAP RL, 1982, IEEE T PATTERN ANAL, V4, P649, DOI 10.1109/TPAMI.1982.4767320; MONTNARI U, 1970, COMMUN ACM, V13, P41, DOI 10.1145/361953.361967; Pavlidis T., 1977, STRUCTURAL PATTERN R; RAMER A, 1972, COMPUT GRAPHICS IMAG, P244; SKLANSKY J, 1972, IEEE T COMPUT, VC 21, P260, DOI 10.1109/TC.1972.5008948; SKLANSKY J, 1976, IEEE T SYST MAN CYB, V6, P637, DOI 10.1109/TSMC.1976.4309569; 1953, NAT GEOGRAPHIC M DEC	8	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	6					667	671		10.1109/TPAMI.1983.4767460	http://dx.doi.org/10.1109/TPAMI.1983.4767460			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RV488	21869157				2022-12-18	WOS:A1983RV48800017
J	MAIA, MAGM; FAIRHURST, MC				MAIA, MAGM; FAIRHURST, MC			ON THE USE OF I-DIVERGENCE FOR GENERATING DISTRIBUTION APPROXIMATIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											MAIA, MAGM (corresponding author), UNIV KENT,ELECTR LAB,CANTERBURY CT2 7NT,KENT,ENGLAND.							Brown D. T., 1959, INFORM CONTROL, V4, P386, DOI DOI 10.1016/S0019-9958(59)80016-4; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Christofides N., 1975, GRAPH THEORY ALGORIT; ELASHOFF JD, 1967, BIOMETRIKA, V54, P668, DOI 10.1093/biomet/54.3-4.668; KAZAKOS D, 1980, IEEE T PATTERN ANAL, V2, P61, DOI 10.1109/TPAMI.1980.4766971; LAINIOTIS DG, 1971, IEEE T SYST MAN CYB, VSMC1, P175; Lewis PM., 1959, INF CONTROL, V2, P214, DOI 10.1016/S0019-9958(59)90207-4; MEISEL WS, 1972, MATH SCI ENG SERIES, V83; MENDEL JM, 1970, MATH SCI ENG SERIES, V66; TOUSSAINT CT, 1977, MACHINE RECOGNITION; VANNESS JW, 1977, IEEE T SYST MAN CYB, V7, P560	11	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	6					661	664		10.1109/TPAMI.1983.4767458	http://dx.doi.org/10.1109/TPAMI.1983.4767458			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RV488	21869155				2022-12-18	WOS:A1983RV48800015
J	PRESERN, S; GYERGYEK, L				PRESERN, S; GYERGYEK, L			AN INTELLIGENT TACTILE SENSOR - AN ONLINE HIERARCHICAL OBJECT AND SEAM ANALYZER	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											PRESERN, S (corresponding author), JOZEF STEFAN INST, JAMOVA 39, LJUBLJANA, YUGOSLAVIA.							BOLLINGER JG, 1981, SENSOR REV       JUL; HOHN RE, 1982, MAR P ROB, V6; LOWERRE BT, 1977, REPRESENTATION SEARC; MASAKI I, 1981, SENSOR REV       APR; PRESERN S, 1981, 4TH P BRIT ROB ASS A; PRESERN S, 1981, APPLICATION MICROCOM; PRESERN S, 1981, 1ST P INT C ROB VIS; REDDY DR, 1978, REPRESENTATION 3 DIM	8	4	4	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	2					217	220		10.1109/TPAMI.1983.4767375	http://dx.doi.org/10.1109/TPAMI.1983.4767375			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QJ974	21869104				2022-12-18	WOS:A1983QJ97400012
J	LOY, WW; LANDAU, ID				LOY, WW; LANDAU, ID			AN ONLINE PROCEDURE FOR RECOGNITION OF HANDPRINTED ALPHANUMERIC CHARACTERS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											LOY, WW (corresponding author), ECOLE NATL SUPER INGN ELECTR GRENOBLE,AUTOMAT GRENOBLE LAB,ST MARTIN DHERES,FRANCE.							ALI F, 1977, IEEE T SYST MAN CYB, V7, P537, DOI 10.1109/TSMC.1977.4309763; ARAKAWA H, 1976, T I ELECTRON COMMU D, V59, P809; BERTHOD M, 1979, COMPUT VISION GRAPH, V9, P166, DOI 10.1016/0146-664X(79)90055-8; CHINNUSWAMY P, 1980, PATTERN RECOGN, V12, P141, DOI 10.1016/0031-3203(80)90038-2; Duda R.O., 1973, J ROYAL STAT SOC SER; EHRICH RW, 1975, IEEE T COMPUT, VC 24, P182, DOI 10.1109/T-C.1975.224184; Freeman H., 1961, IRE T ELECT COMPUTER, VEC-10, P260, DOI DOI 10.1109/TEC.1961.5219197; GRONER GF, 1966, FAL P JOINT COMP C A, V29, P591; Landau I.D., 1979, ADAPTIVE CONTROL MOD; LOY WW, 1980, PROCEDURE RECONNAISS; Meisel W., 1972, COMPUTER ORIENTED AP; MERMELSTEIN P, 1964, INFORM CONTROL, V7, P255, DOI 10.1016/S0019-9958(64)90142-1; MILLER GM, 1969, FAL P JOINT COMP C A, V32, P399; PAVLIDIS T, 1974, IEEE T COMPUT, VC 23, P860, DOI 10.1109/T-C.1974.224041; PAVLIDIS T, 1975, IEEE T SYST MAN CYB, V5, P610, DOI 10.1109/TSMC.1975.4309402; Ramer U, 1972, COMPUT GRAPH IMAGE P, V1, P244, DOI [DOI 10.1016/S0146-664X(72)80017-0, 10.1016/S0146-664X(72)80017-0]; SPINRAD RJ, 1965, INFORM CONTROL, V8, P124, DOI 10.1016/S0019-9958(65)90024-0; SUEN CY, 1978, 4TH P INT J C PATT R, P30; TEITELMAN W, 1964, FAL P JOINT COMP C, V26, P559; YAMAMOTO K, 1980, PATTERN RECOGN, V12, P229, DOI 10.1016/0031-3203(80)90062-X	20	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	4					422	427		10.1109/TPAMI.1982.4767275	http://dx.doi.org/10.1109/TPAMI.1982.4767275			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	NT735	21869058				2022-12-18	WOS:A1982NT73500010
J	SHORT, RD; FUKUNAGA, K				SHORT, RD; FUKUNAGA, K			FEATURE-EXTRACTION USING PROBLEM LOCALIZATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									PURDUE UNIV,SCH ELECT ENGN,W LAFAYETTE,IN 47907	Purdue University System; Purdue University; Purdue University West Lafayette Campus	SHORT, RD (corresponding author), SPERRY RAND CORP,CTR RES,SUDBURY,MA 01776, USA.							Devijver P. A., 1973, 1st International Joint Conference on Pattern Recognition, P139; FUKUNAGA K, 1978, IEEE T COMPUT, V27, P176, DOI 10.1109/TC.1978.1675056; FUKUNAGA K, 1977, IEEE T INFORM THEORY, V23, P453, DOI 10.1109/TIT.1977.1055755; FUKUNAGA K, 1978, IEEE T INFORM THEORY, V24, P600, DOI 10.1109/TIT.1978.1055942; FUKUNAGA K, 1980, IEEE T INFORM THEORY, V26	5	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	3					323	326		10.1109/TPAMI.1982.4767252	http://dx.doi.org/10.1109/TPAMI.1982.4767252			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NN069	21869042				2022-12-18	WOS:A1982NN06900013
J	JOHNSON, LR; JAIN, AK				JOHNSON, LR; JAIN, AK			AN EFFICIENT TWO-DIMENSIONAL FFT ALGORITHM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									MICHIGAN STATE UNIV,DEPT COMP SCI,E LANSING,MI 48823	Michigan State University								HARRIS DB, 1977 P IEEE INT C AC, P548; NACCARATO DF, 1979 P IEEE COMP SOC, P233; RABINER LR, 1975, THEORY APPLICATION D, P356; RIVARD GE, 1977, IEEE T ACOUST SPEECH, V25, P250, DOI 10.1109/TASSP.1977.1162951; STANLEY WD, 1975, DIGITAL SIGNAL PROCE	5	4	5	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	6					698	701		10.1109/TPAMI.1981.4767174	http://dx.doi.org/10.1109/TPAMI.1981.4767174			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MR996	21868993				2022-12-18	WOS:A1981MR99600013
J	PRESTON, K				PRESTON, K			SOME NOTES ON CELLULAR LOGIC OPERATORS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											PRESTON, K (corresponding author), CARNEGIE MELLON UNIV,DEPT ELECT ENGN,PITTSBURGH,PA 15213, USA.							ABDOU IE, 1979, P IEEE, V67, P753, DOI 10.1109/PROC.1979.11325; DEUSTCH ES, 1969, COMPUTER PROCESSING, P221; GOLAY MJE, 1969, IEEE T COMPUT, VC 18, P733, DOI 10.1109/T-C.1969.222756; HIGHLEYMAN WH, 1962, OPTICAL CHARACTER RE, P249; INGRAM M, 1968, IMAGE PROCESSING BIO, P97; PRESTON K, 1981, PATTERN RECOGN, V13, P17, DOI 10.1016/0031-3203(81)90029-7; PRESTON K, 1979, P IEEE, V67, P826, DOI 10.1109/PROC.1979.11331; PRESTON K, 1973, COMPUTER TECHNIQUES, P295; SIMON JC, 1977, DIGITAL IMAGE PROCES; SKLANSKY J, 1970, PATTERN RECOGN, V2, P3, DOI 10.1016/0031-3203(70)90037-3; STERNBERG SR, 1979, NOV P COMPSAC 79 CHI; Toriwaki J, 1973, COMPUT GRAPH IMAGE P, V2, P252, DOI DOI 10.1016/0146-664X(73)90005-1; USUBUCHI T, 1977, 1977 PICT COD S TOK, P55; 1976, PERKIN ELMER DIFF3 S	14	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	4					476	481		10.1109/TPAMI.1981.4767133	http://dx.doi.org/10.1109/TPAMI.1981.4767133			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MQ357	21868968				2022-12-18	WOS:A1981MQ35700012
J	YAJIMA, S; GOODSELL, JL; ICHIDA, T; HIRAISHI, H				YAJIMA, S; GOODSELL, JL; ICHIDA, T; HIRAISHI, H			DATA-COMPRESSION OF KANJI CHARACTER PATTERNS DIGITIZED ON THE HEXAGONAL MESH	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											YAJIMA, S (corresponding author), KYOTO UNIV, FAC ENGN, DEPT INFORMAT SCI, KYOTO 606, JAPAN.							ARAI K, 1977, J IMAGE ELECTRON JAP, V6, P16; FREEMAN H, 1969, IEEE T SYST SCI CYB, VSSC5, P70, DOI 10.1109/TSSC.1969.300247; GOLAY MJE, 1969, IEEE T COMPUT, VC 18, P733, DOI 10.1109/T-C.1969.222756; HASEGAWA J, 1978, J INFORM PROCESS JAP, V19, P350; HIRAISHI H, 1977, 3RD P EUROMICRO S AM, P66; HIRAISHI H, 1978, IE7817 PAP TECH GROU; KANEDA Y, 1977, KANJI INFORMATION PR; KIDA H, 1978, MAR P NAT CONV I EL; MERSEREAU RM, 1979, P IEEE, V67, P930, DOI 10.1109/PROC.1979.11356; MOSS C, 1978, NEW SCI          FEB; NAKAGAWA T, 1977, EC7684 PAP TECH GROU; NEZU K, 1972, T I ELECTRON COMMUN, V55, P277; PRESTON K, 1971, IEEE T COMPUT, VC 20, P1007, DOI 10.1109/T-C.1971.223396; ROSENFELD A, 1970, J ACM, V17, P146, DOI 10.1145/321556.321570; Sakai K., 1976, 3rd International Joint Conference on Pattern Recognition, P122; SAKAI K, 1974, JUL P NAT CONV I EL; SAKAI K, 1973, PRL7314 PAP TECH GRO; SKLANSKY J, 1976, IEEE T SYST MAN CYB, V6, P637, DOI 10.1109/TSMC.1976.4309569; TANIGUCHI M, 1974, T I ELECTRON COMMUN, V57, P376; YAJIMA S, 1979, FEB P JAP M TECH GRO; ZAHN CT, 1966, SLAC70 STANF U REP; ZAHN CT, 1969, MAY P INT JOINT C AR, P621; ZAHN CT, 1974, 2ND P INT JOINT C PA, P136; 1973, JIPDC47R001 JAP INF, P75; [No title captured]	25	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	2					221	230		10.1109/TPAMI.1981.4767085	http://dx.doi.org/10.1109/TPAMI.1981.4767085			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN968	21868942				2022-12-18	WOS:A1981MN96800017
J	YOUNG, TY; LIU, PS; RONDON, RJ				YOUNG, TY; LIU, PS; RONDON, RJ			STATISTICAL PATTERN-CLASSIFICATION WITH BINARY VARIABLES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											YOUNG, TY (corresponding author), UNIV MIAMI, DEPT ELECT ENGN, CORAL GABLES, FL 33124 USA.							ABEND K, 1965, IEEE T INFORM THEORY, V11, P538, DOI 10.1109/TIT.1965.1053827; Bahadur RR, 1961, STUDIES ITEM ANAL PR, P158; CHIEN YT, 1968, INFORM CONTROL, V12, P394, DOI 10.1016/S0019-9958(68)90420-8; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; CHOW CK, 1966, IEEE T SYST SCI CYB, VSSC2, P101, DOI 10.1109/TSSC.1966.6593091; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; FUKUNAGA K, 1972, INTRO STATISTICAL PA; GOOD IJ, 1963, ANN MATH STAT, V34, P911, DOI 10.1214/aoms/1177704014; KANAL L, 1974, IEEE T INFORM THEORY, V20, P697, DOI 10.1109/TIT.1974.1055306; KU HH, 1969, IEEE T INFORM THEORY, V15, P444, DOI 10.1109/TIT.1969.1054336; KUGUNAGA K, 1971, IEEE T COMPUT, V20, P1521; MOORE DH, 1973, J AM STAT ASSOC, V68, P399, DOI 10.1080/01621459.1973.10482440; NORDYKE RA, 1971, COMPUT BIOMED RES, V4, P374, DOI 10.1016/0010-4809(71)90022-X; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; PATRICK EA, 1974, IEEE T SYST MAN CYB, VSMC4, P1, DOI 10.1109/TSMC.1974.5408512; SCHEINOK PERRY A., 1967, COMPUT BIO MED RES, V1, P221, DOI 10.1016/S0010-4809(67)80010-7; STOFFEL JC, 1974, IEEE T COMPUT, VC 23, P428, DOI 10.1109/T-C.1974.223958; TOU JT, 1967, COMPUTER INFORMATION, V2, P41; WATANABE S, 1965, 4TH T PRAG C INF THE, P635; WONG AKC, 1975, IEEE T COMPUT, VC 24, P158, DOI 10.1109/T-C.1975.224183; Young T. Y., 1974, CLASSIFICATION ESTIM; YOUNG TY, 1971, IEEE T COMPUT, VC 20, P967, DOI 10.1109/T-C.1971.223390; YOUNG TY, 1980, IEEE T SOFTWARE ENG, V6, P340, DOI 10.1109/TSE.1980.234490; YOUNG TY, 1978, IEEE T INFORM THEORY, V24, P152, DOI 10.1109/TIT.1978.1055866	24	4	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	2					155	163		10.1109/TPAMI.1981.4767073	http://dx.doi.org/10.1109/TPAMI.1981.4767073			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN968	21868930				2022-12-18	WOS:A1981MN96800005
J	KAZAKOS, D				KAZAKOS, D			CHOICE OF KERNEL FUNCTION FOR DENSITY-ESTIMATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											KAZAKOS, D (corresponding author), SUNY BUFFALO,DEPT ELECT ENGN,AMHERST,NY 14260, USA.							EPANECHN.VA, 1969, THEOR PROBAB APPL+, V14, P153, DOI 10.1137/1114019; FARRELL RH, 1972, ANN MATH STAT, V43, P170, DOI 10.1214/aoms/1177692711; KAZAKOS D, 1977, NONPARAMETRIC METHOD; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; WAHBA G, 1975, ANN STAT, V3, P30, DOI 10.1214/aos/1176342998; WAHBA G, 1975, ANN STAT, V3, P15, DOI 10.1214/aos/1176342997; WATSON GS, 1969, ANN MATH STAT, V40, P1496, DOI 10.1214/aoms/1177697523	8	4	4	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	3					255	258		10.1109/TPAMI.1980.4767013	http://dx.doi.org/10.1109/TPAMI.1980.4767013			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JR843	21868899				2022-12-18	WOS:A1980JR84300006
J	KITTLER, J; DEVIJVER, PA				KITTLER, J; DEVIJVER, PA			THE PROBABILITY-DISTRIBUTION OF CONDITIONAL CLASSIFICATION ERROR	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									PHILLIPS RES LAB,BRUSSELS,BELGIUM	Philips; Philips Research	KITTLER, J (corresponding author), UNIV OXFORD,NUCL PHYS LAB,IMAGE ANAL GRP,OXFORD,ENGLAND.							FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P285, DOI 10.1109/TIT.1975.1055373; FUKUNAGA K, 1973, IEEE T INFORM THEORY, V19, P434, DOI 10.1109/TIT.1973.1055049; KENDALL MG, 1963, ADV THEORY STATISTIC, V1; Kittler J., 1979, Proceedings of the 1979 IEEE Computer Society Conference on Pattern Recognition and Image Processing, P44; KITTLER J, 1978, 4TH P IJCPR KYOT JAP, P249; KITTLER J, UNPUBLISHED; TOUSSAINT GT, 1974, IEEE T INFORM THEORY, V20, P472, DOI 10.1109/TIT.1974.1055260	7	4	4	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	3					259	261		10.1109/TPAMI.1980.4767014	http://dx.doi.org/10.1109/TPAMI.1980.4767014			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	JR843	21868900				2022-12-18	WOS:A1980JR84300007
J	SCHACHTER, B				SCHACHTER, B			MODEL-BASED TEXTURE MEASURES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											SCHACHTER, B (corresponding author), GE,DAYTONA BEACH,FL 32015, USA.							AHUJA N, 1977, 607 U MAR COMP SCI T; HARALICK RM, 1978, NOV P INT JOINT C PA, V4, P45; LONGUETHIGGINS MS, 1952, J MAR RES, V11, P245; Matern B., 1960, SPATIAL VARIATION ME, P1; Pielou EC., 1977, MATH ECOLOGY; ROSENFELD A, 1977, 547 U MAR COMP SCI T; SCHACHTER BJ, 1978, IEEE T SYST MAN CYB, V8, P694; SWITZER P, 1967, ANN MATH STAT, V38, P138, DOI 10.1214/aoms/1177699064; SWITZER P, 1974, 62 STANF U DEP STAT; WONG E, 1968, SIAM J APPL MATH, V16, P756, DOI 10.1137/0116062	10	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	2					169	171		10.1109/TPAMI.1980.4766995	http://dx.doi.org/10.1109/TPAMI.1980.4766995			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JH803	21868888				2022-12-18	WOS:A1980JH80300009
J	AGUI, T; NAGAHASHI, H				AGUI, T; NAGAHASHI, H			DESCRIPTION METHOD OF HANDPRINTED CHINESE-CHARACTERS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											AGUI, T (corresponding author), TOKYO INST TECHNOL,IMAGING SCI & ENGN LAB,MIDORI KU,YOKOHAMA,JAPAN.							AGUI T, 1977, J I ELECTRON COMM JD, V60, P1109; CASEY R, 1966, IEEE TRANS ELECTRON, VEC15, P91, DOI 10.1109/PGEC.1966.264379; FENG HYF, 1975, IEEE T COMPUT, VC 24, P636, DOI 10.1109/T-C.1975.224276; GRONER GF, 1967, IEEE TRANS ELECTRON, VEC16, P856, DOI 10.1109/PGEC.1967.264750; OGAWA H, 1974, J I ELECTRON COMMUN, V57, P700; RANKIN BK, 1965, THESIS U PENNSYLVANI; STALLINGS W, 1975, COMPUT HUMANITIES, V9, P13, DOI 10.1007/BF02404316	7	4	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	1					20	24		10.1109/TPAMI.1979.4766872	http://dx.doi.org/10.1109/TPAMI.1979.4766872			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HA303	21868827				2022-12-18	WOS:A1979HA30300003
J	HUNG, SHY				HUNG, SHY			GENERALIZATION OF DPCM FOR DIGITAL IMAGE COMPRESSION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											HUNG, SHY (corresponding author), NATL RES COUNCIL CANADA,DIV ELECT ENGN,OTTAWA,ONTARIO,CANADA.							AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784; ANDERSON GB, 1969, P PURDUE CENTENNIAL; CHEN W, 1976 P ICC; CHOW CK, 1974, COMPUT GRAPHICS IMAG, V3, P203; HABIBI A, 1974, IEEE T COMMUN, VCO22, P614, DOI 10.1109/TCOM.1974.1092258; HABIBI A, 1974, COMPUT           MAY, P22; HABIBI OA, 1970, TREE702 PURD U SCH E; HARALICK RM, 1975, SPIE, V66, P144; HUANG TS, 1970, IEEE T INFORM THEORY, V16, P119; HUANG TS, 1969, INT S INFORMATION TH; KRAMER HP, 1956, IRE T INFORM THEOR, V2, P41, DOI 10.1109/TIT.1956.1056808; LANDON HJ, 1970, BELL SYST TECH J; PERSONS JR, 1975, P SPIE           AUG, P196; PRATT WK, 1969, P IEEE, V57, P58, DOI 10.1109/PROC.1969.6869; RADER CM, 1966, P IEEE, V54, P1594; ROSE JA, 1975 P ICC; Schultheiss P., 1963, IEEE T COMMUN SYST, V11, P289; WOODS JW, 1970, PICTURE BANDWIDTH CO	18	4	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	1					100	109		10.1109/TPAMI.1979.4766883	http://dx.doi.org/10.1109/TPAMI.1979.4766883			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HA303	21868838				2022-12-18	WOS:A1979HA30300014
J	TJOSTHEIM, D; SANDVIN, O				TJOSTHEIM, D; SANDVIN, O			MULTIVARIATE AUTOREGRESSIVE FEATURE EXTRACTION AND THE RECOGNITION OF MULTICHANNEL WAVEFORMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									NTNF,NORSAR,KJELLER,NORWAY		TJOSTHEIM, D (corresponding author), NORWEGIAN SCH ECON & BUSINESS ADM,BERGEN,NORWAY.							AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; AKAIKE H, 1971, ANN I STAT MATH, V23, P163, DOI 10.1007/BF02479221; Anderson T.W, 1958, INTRO MULTIVARIATE S; BOLT BA, 1976, NUCLEAR EXPLOSIONS E; BREIMAN L, 1975, VARIABLE KERNEL ESTI; BUNGUM H, 1976, GEOPHYS J ROY ASTR S, V45, P371, DOI 10.1111/j.1365-246X.1976.tb00332.x; DAHLMAN O, 1977, MONITORING UNDERGROU; FUKUNAGA K, 1970, IEEE T COMPUT, VC 19, P311, DOI 10.1109/T-C.1970.222918; LOOFTSGARDEN PO, 1965, ANN MATH STAT, V38, P1049; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; ROBINSON EA, 1967, MULTICHANNEL TIME SE; SANDVIN O, 1978, B SEISMOL SOC AM, V68, P735; TJOSTHEIM D, 1975, GEOPHYS J ROY ASTR S, V43, P269, DOI 10.1111/j.1365-246X.1975.tb00635.x; TJOSTHEIM D, 1977, IEEE T COMPUT, V26, P268, DOI 10.1109/TC.1977.1674816; TJOSTHEIM D, 1978, PHYS EARTH PLANET IN, V16, P85, DOI 10.1016/0031-9201(78)90082-1; Whittle P, 1963, PREDICTION REGULATIO; Young T. Y., 1974, CLASSIFICATION ESTIM	17	4	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	1					80	86		10.1109/TPAMI.1979.4766878	http://dx.doi.org/10.1109/TPAMI.1979.4766878			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HA303	21868833				2022-12-18	WOS:A1979HA30300009
J	Fan, HH; Yu, X; Yang, Y; Kankanhalli, M				Fan, Hehe; Yu, Xin; Yang, Yi; Kankanhalli, Mohan			Deep Hierarchical Representation of Point Cloud Videos via Spatio-Temporal Decomposition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Point cloud compression; Three-dimensional displays; Electron tubes; Convolution; Semantics; Feature extraction; Point cloud; spatio-temporal modeling; video analysis; action recognition; semantic segmentation; scene flow estimation	ACTION RECOGNITION	In point cloud videos, point coordinates are irregular and unordered but point timestamps exhibit regularities and order. Grid-based networks for conventional video processing cannot be directly used to model raw point cloud videos. Therefore, in this work, we propose a point-based network that directly handles raw point cloud videos. First, to preserve the spatio-temporal local structure of point cloud videos, we design a point tube covering a local range along spatial and temporal dimensions. By progressively subsampling frames and points and enlarging the spatial radius as the point features are fed into higher-level layers, the point tube can capture video structure in a spatio-temporally hierarchical manner. Second, to reduce the impact of the spatial irregularity on temporal modeling, we decompose space and time when extracting point tube representations. Specifically, a spatial operation is employed to encode the local structure of each spatial region in a tube and a temporal operation is used to encode the dynamics of the spatial regions along the tube. Empirically, the proposed network shows strong performance on 3D action recognition, 4D semantic segmentation and scene flow estimation. Theoretically, we analyse the necessity to decompose space and time in point cloud video modeling and why the network outperforms existing methods.	[Fan, Hehe; Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore; [Yu, Xin] Univ Technol Sydney, Ctr Artificial Intelligence, Ultimo, NSW 2007, Australia; [Yang, Yi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Zhejiang, Peoples R China	National University of Singapore; University of Technology Sydney; Zhejiang University	Fan, HH (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.	hehe.fan@nus.edu.sg; xin.yu@uts.edu.au; yangyics@zju.edu.cn; mohan@comp.nus.edu.sg	; Yang, Yi/B-9273-2017	Fan, Hehe/0000-0001-9572-2345; Yang, Yi/0000-0002-0512-880X; Yu, Xin/0000-0002-0269-5649; Kankanhalli, Mohan/0000-0002-4846-2015	Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme [A18A2b0046]	Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme(Agency for Science Technology & Research (A*STAR))	This work was supported in part by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme under Grant A18A2b0046.	Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fan H., 2021, PROC INT C LEARN REP; Fan HH, 2021, PROC CVPR IEEE, P14199, DOI 10.1109/CVPR46437.2021.01398; Fan HH, 2022, IEEE T CIRC SYST VID, V32, P275, DOI 10.1109/TCSVT.2021.3058688; Fan HH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P705; Fan HH, 2017, IEEE I CONF COMP VIS, P736, DOI 10.1109/ICCV.2017.86; Gu XY, 2019, PROC CVPR IEEE, P3249, DOI 10.1109/CVPR.2019.00337; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; Hehe Fan, 2019, Arxiv, DOI arXiv:1910.08287; Klaser Alexander, 2008, BMVC; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Li YY, 2018, ADV NEUR IN, V31; Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873; Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127; Liu XY, 2019, IEEE I CONF COMP VIS, P9245, DOI 10.1109/ICCV.2019.00934; Liu XY, 2019, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2019.00062; Niemeyer M, 2019, IEEE I CONF COMP VIS, P5378, DOI 10.1109/ICCV.2019.00548; Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98; Prantl L., 2020, PROC INT C LEARN REP; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389; Rempe Davis, 2020, ARXIV200802792, P2; Ren PZ, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447582; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810; Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230; Simonyan K, 2014, ADV NEUR IN, V27; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Vieira Antonio W., 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P252, DOI 10.1007/978-3-642-33275-3_31; Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329; Wang YC, 2020, PROC CVPR IEEE, P508, DOI 10.1109/CVPR42600.2020.00059; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Xiao Y, 2019, INFORM SCIENCES, V480, P287, DOI 10.1016/j.ins.2018.12.050; Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108; Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295; Zhuo T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P521, DOI 10.1145/3343031.3351040	50	3	3	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9918	9930		10.1109/TPAMI.2021.3135117	http://dx.doi.org/10.1109/TPAMI.2021.3135117			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34905491				2022-12-18	WOS:000880661400097
J	Hirschberger, F; Forster, D; Lucke, J				Hirschberger, Florian; Forster, Dennis; Luecke, Joerg			A Variational EM Acceleration for Efficient Clustering at Very Large Scales	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Big data; clustering; expectation maximization; Gaussian mixture models; variational optimization	K-MEANS; ALGORITHM; INFERENCE	How can we efficiently find very large numbers of clusters C in very large datasets N of potentially high dimensionality D? Here we address the question by using a novel variational approach to optimize Gaussian mixture models (GMMs) with diagonal covariance matrices. The variational method approximates expectation maximization (EM) by applying truncated posteriors as variational distributions and partial E-steps in combination with coresets. Run time complexity to optimize the clustering objective then reduces from O(NCD) per conventional EM iteration to O(N' G(2)D) for a variational EM iteration on coresets (with coreset size N' <= N and truncation parameter G << C- ). Based on the strongly reduced run time complexity per iteration, which scales sublinearly with NC, we then provide a concrete, practically applicable, parallelized and highly efficient clustering algorithm. In numerical experiments on standard large-scale benchmarks we (A) show that also overall clustering times scale sublinearly with NC, and (B) observe substantial wall-clock speedups compared to already highly efficient recently reported results. The algorithm's sublinear scaling allows for applications at scales where alternative methods cease to be applicable. We demonstrate such very large-scale applicability using the YFCC100M benchmark, for which we realize with a GMM of up to 50.000 clusters an optimization of a data density model with up to 150 M parameters.	[Hirschberger, Florian; Forster, Dennis; Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Machine Learning Lab, D-26129 Oldenburg, Lower Saxony, Germany	Carl von Ossietzky Universitat Oldenburg	Forster, D (corresponding author), Carl von Ossietzky Univ Oldenburg, Machine Learning Lab, D-26129 Oldenburg, Lower Saxony, Germany.	florian.hirschberger@uol.de; dennis.forster@uol.de; joerg.luecke@uol.de		Forster, Dennis/0000-0002-0325-2575	German Research Foundation (DFG) [352015383 (SFB 1330), B2, 390895286, EXC 2177/1]; German Ministry of Research and Education (BMBF) [05M20MOA]; HPC Cluster CARL of Oldenburg University under Grant DFG [INST 184/157-1 FUGG]	German Research Foundation (DFG)(German Research Foundation (DFG)); German Ministry of Research and Education (BMBF)(Federal Ministry of Education & Research (BMBF)); HPC Cluster CARL of Oldenburg University under Grant DFG	This work was supported in part by the German Research Foundation (DFG) under Grants 352015383 (SFB 1330, B2) and 390895286 (Cluster of Excellence, EXC 2177/1, H4a 2.0) and in part by the German Ministry of Research and Education (BMBF) under Grant 05M20MOA (SPAplus, TP3). Furthermore, the work was supported by the HPC Cluster CARL of Oldenburg University under Grant DFG, INST 184/157-1 FUGG.	Agustsson E., 2017, PROC JOINT EUR C MAC, P775; [Anonymous], 2004, KDD CUP 2004 RESULTS; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bachem O, 2016, ADV NEUR IN, V29; Bachem O, 2016, AAAI CONF ARTIF INTE, P1459; Bachem O, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1119, DOI 10.1145/3219819.3219973; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Barber D., 2012, BAYESIAN REASONING M; BEI CD, 1985, IEEE T COMMUN, V33, P1132, DOI 10.1109/TCOM.1985.1096214; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Berkhin P, 2006, GROUPING MULTIDIMENSIONAL DATA: RECENT ADVANCES IN CLUSTERING, P25; Bertin-Mahieux Thierry, 2011, P 12 INT C MUS INF R, DOI DOI 10.7916/D8NZ8J07; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Blum M., 1973, Journal of Computer and System Sciences, V7, P448, DOI 10.1016/S0022-0000(73)80033-9; Bouveyron C, 2007, COMPUT STAT DATA AN, V52, P502, DOI 10.1016/j.csda.2007.02.009; Campbell T, 2019, ADV NEUR IN, V32; Campello RJGB, 2020, WIRES DATA MIN KNOWL, V10, DOI 10.1002/widm.1343; Chan JYK, 2017, IEEE IJCNN, P94, DOI 10.1109/IJCNN.2017.7965841; Cheng D.-Y., 1984, PROC IEEE INT C ACOU, P372; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217; Curtin R. R., 2017, P 2017 SIAM INT C DA, P300; Dai ZW, 2014, IEEE T PATTERN ANAL, V36, P1950, DOI 10.1109/TPAMI.2014.2313126; Defazio A, 2014, ADV NEUR IN, V27; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Elkan C., 2003, P 20 INT C MACHINE L, V20, P147, DOI DOI 10.1016/0026-2714(92)90278-S; Erik B. Sudderth, 2016, Arxiv, DOI arXiv:1609.07521; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Feldman D., 2011, ADV NEURAL INFORM PR, V24, P2142; Forster D, 2018, NEURAL COMPUT, V30, P2113, DOI 10.1162/neco_a_01100; Forster D, 2017, IEEE IJCNN, P3769, DOI 10.1109/IJCNN.2017.7966331; Forster Dennis, 2018, INT C ART INT STAT, P124; Ghahramani Z, 2000, ADV NEUR IN, V12, P449; Gong YC, 2015, PROC CVPR IEEE, P19, DOI 10.1109/CVPR.2015.7298596; Hamerly G, 2010, P 2010 SIAM INT C DA, V2010, P130, DOI DOI 10.1137/1.9781611972801.12; Har-Peled Sariel, 2004, P ACM S THEOR COMP, P291; Henderson NC, 2019, J COMPUT GRAPH STAT, V28, P834, DOI 10.1080/10618600.2019.1594835; Hertrich J, 2022, INVERSE PROBL IMAG, V16, P341, DOI 10.3934/ipi.2021053; Hilbert M, 2011, SCIENCE, V332, P60, DOI 10.1126/science.1200970; Hertrich J, 2020, Arxiv, DOI arXiv:2005.02204; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Klami A., 2016, PROC 32 C UNCERTAINT, P329; Kobren A, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P255, DOI 10.1145/3097983.3098079; Lee J, 2017, ADV NEUR IN, V30; Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862; Lucic M, 2018, J MACH LEARN RES, V18; Lucke J, 2019, PATTERN RECOGN LETT, V125, P349, DOI 10.1016/j.patrec.2019.04.001; McLachlan G.J., 2004, FINITE MIXTURE MODEL; Moore AW, 1999, ADV NEUR IN, V11, P543; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Musser DR, 1997, SOFTWARE PRACT EXPER, V27, P983, DOI 10.1002/(SICI)1097-024X(199708)27:8<983::AID-SPE117>3.0.CO;2-#; Nair Vinod, 2014, THE CIFAR 10 DATASET; Nech A, 2017, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2017.363; Newling J., 2017, PROC C ADV NEURAL IN, P5195; Otto C, 2018, IEEE T PATTERN ANAL, V40, P289, DOI 10.1109/TPAMI.2017.2679100; Pelleg D., 1999, PROC 5 ACM SIGKDD IN, P277, DOI [10.1145/312129.312248, DOI 10.1145/312129.312248]; Phillips SJ, 2002, LECT NOTES COMPUT SC, V2409, P166; Richardson E., 2018, PROC C ADV NEURAL IN, P5847; Sheikh A.-S., 2016, PROC C ADV NEURAL IN, P3927; Shelton JA, 2017, NEURAL COMPUT, V29, P2177, DOI [10.1162/NECO_a_00982, 10.1162/neco_a_00982]; Shen XB, 2017, AAAI CONF ARTIF INTE, P2527; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Uzilov AV, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-173; Varadhan R, 2008, SCAND J STAT, V35, P335, DOI 10.1111/j.1467-9469.2007.00585.x; Verbeek JJ, 2006, DATA MIN KNOWL DISC, V13, P291, DOI 10.1007/s10618-005-0033-3; Verbeek JJ, 2003, NEURAL COMPUT, V15, P469, DOI 10.1162/089976603762553004; Wang J, 2012, PROC CVPR IEEE, P3037, DOI 10.1109/CVPR.2012.6248034; Zhang C, 2019, IEEE T PATTERN ANAL, V41, P2008, DOI 10.1109/TPAMI.2018.2889774; Zhang T, 1997, DATA MIN KNOWL DISC, V1, P141, DOI 10.1023/A:1009783824328	72	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9787	9801		10.1109/TPAMI.2021.3133763	http://dx.doi.org/10.1109/TPAMI.2021.3133763			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34882546	Green Submitted, hybrid			2022-12-18	WOS:000880661400088
J	Li, TJ; Zhao, XM; Li, LM				Li, Tianjiao; Zhao, Xing-Ming; Li, Limin			Co-VAE: Drug-Target Binding Affinity Prediction by Co-Regularized Variational Autoencoders	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Drugs; Predictive models; Feature extraction; Computational modeling; Mathematical models; Proteins; Probabilistic logic; Variational autoencoders; co-regularized VAE; drug-target binding affinity	KINASE; NETWORKS	Identifying drug-target interactions has been a key step in drug discovery. Many computational methods have been proposed to directly determine whether drugs and targets can interact or not. Drug-target binding affinity is another type of data which could show the strength of the binding interaction between a drug and a target. However, it is more challenging to predict drug-target binding affinity, and thus a very few studies follow this line. In our work, we propose a novel co-regularized variational autoencoders (Co-VAE) to predict drug-target binding affinity based on drug structures and target sequences. The Co-VAE model consists of two VAEs for generating drug SMILES strings and target sequences, respectively, and a co-regularization part for generating the binding affinities. We theoretically prove that the Co-VAE model is to maximize the lower bound of the joint likelihood of drug, protein and their affinity. The Co-VAE could predict drug-target affinity and generate new drugs which share similar targets with the input drugs. The experimental results on two datasets show that the Co-VAE could predict drug-target affinity better than existing affinity prediction methods such as DeepDTA and DeepAffinity, and could generate more new valid drugs than existing methods such as GAN and VAE.	[Li, Tianjiao; Li, Limin] Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Peoples R China; [Zhao, Xing-Ming] Fudan Univ, Inst Sci & Technol Brain Inspired Intelligence, Shanghai, Peoples R China; [Zhao, Xing-Ming] MOE Key Lab Computat Neurosci & Brain Inspired In, Shanghai 200433, Peoples R China; [Zhao, Xing-Ming] MOE Frontiers Ctr Brain Sci, Shanghai 200433, Peoples R China	Xi'an Jiaotong University; Fudan University	Li, LM (corresponding author), Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Peoples R China.; Zhao, XM (corresponding author), Fudan Univ, Inst Sci & Technol Brain Inspired Intelligence, Shanghai, Peoples R China.	litj468@stu.xjtu.edu.cn; xmzhao@fudan.edu.cn; liminli@mail.xjtu.edu.cn			National Key R&D Program of China [2020YFA0712403]; National Natural Science Foundation of China [11631012, 61932008, 61772368]; Shanghai Municipal Science and Technology Major Project [2018SHZDZX01]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Municipal Science and Technology Major Project	This work was partly supported by National Key R&D Program of China under Grant 2020YFA0712403, National Natural Science Foundation of China under Grants 11631012, 61932008, and 61772368, Shanghai Municipal Science and Technology Major Project under Grant 2018SHZDZX01.	Alex Krizhevsky, 2012, Arxiv, DOI arXiv:1207.0580; Andrew M. Dai, 2016, Arxiv, DOI arXiv:1511.06349; [Anonymous], 2016, RDKIT OPEN SOURCE CH, DOI [10.2307/3592822, DOI 10.2307/3592822]; Bleakley K, 2007, BIOINFORMATICS, V23, pI57, DOI 10.1093/bioinformatics/btm204; Bleakley K, 2009, BIOINFORMATICS, V25, P2397, DOI 10.1093/bioinformatics/btp433; Dauphin YN, 2017, PR MACH LEARN RES, V70; Davis MI, 2011, NAT BIOTECHNOL, V29, P1046, DOI 10.1038/nbt.1990; Diederik P Kingma, 2014, Arxiv, DOI arXiv:1312.6114; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; Gonen M, 2005, BIOMETRIKA, V92, P965, DOI 10.1093/biomet/92.4.965; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guimaraes GL., 2017, ARXIV; He T, 2017, J CHEMINFORMATICS, V9, DOI 10.1186/s13321-017-0209-z; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jacob L, 2008, BIOINFORMATICS, V24, P2149, DOI 10.1093/bioinformatics/btn409; Kadurin A, 2017, MOL PHARMACEUT, V14, P3098, DOI 10.1021/acs.molpharmaceut.7b00346; Karimi M, 2019, BIOINFORMATICS, V35, P3329, DOI 10.1093/bioinformatics/btz111; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Liu MY, 2017, ADV NEUR IN, V30; Manallack DT, 2002, J CHEM INF COMP SCI, V42, P1256, DOI 10.1021/ci020267c; Mordelet F, 2008, BIOINFORMATICS, V24, pI76, DOI 10.1093/bioinformatics/btn273; Nagamine N, 2007, BIOINFORMATICS, V23, P2004, DOI 10.1093/bioinformatics/btm266; Nair V, 2010, P 27 INT C MACHINE L, P807; Olivecrona M, 2017, J CHEMINFORMATICS, V9, DOI 10.1186/s13321-017-0235-x; Ozturk H, 2018, BIOINFORMATICS, V34, P821, DOI 10.1093/bioinformatics/bty593; Pahikkala T, 2015, BRIEF BIOINFORM, V16, P325, DOI 10.1093/bib/bbu010; Sutskever I, 2014, ADV NEUR IN, V27; Tang J, 2014, J CHEM INF MODEL, V54, P735, DOI 10.1021/ci400709d; Nguyen T, 2021, BIOINFORMATICS, V37, P1140, DOI 10.1093/bioinformatics/btaa921; Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9; Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361; Xia Z, 2010, BMC SYST BIOL, V4, DOI 10.1186/1752-0509-4-S2-S6; Xu Z, 2017, ACM-BCB' 2017: PROCEEDINGS OF THE 8TH ACM INTERNATIONAL CONFERENCE ON BIOINFORMATICS, COMPUTATIONAL BIOLOGY,AND HEALTH INFORMATICS, P285, DOI 10.1145/3107411.3107424; Yabuuchi H, 2011, MOL SYST BIOL, V7, DOI 10.1038/msb.2011.5; Yamanishi Y, 2008, BIOINFORMATICS, V24, pI232, DOI 10.1093/bioinformatics/btn162; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	38	3	3	13	34	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8861	8873		10.1109/TPAMI.2021.3120428	http://dx.doi.org/10.1109/TPAMI.2021.3120428			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34652996				2022-12-18	WOS:000880661400025
J	Li, XL; Zhang, HY; Zhang, R				Li, Xuelong; Zhang, Hongyuan; Zhang, Rui			Adaptive Graph Auto-Encoder for General Data Clustering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Neural networks; Convolution; Adaptation models; Decoding; Data models; Task analysis; Clustering methods; General data clustering; graph auto-encoder; scalable methods		Graph-based clustering plays an important role in the clustering area. Recent studies about graph neural networks (GNN) have achieved impressive success on graph-type data. However, in general clustering tasks, the graph structure of data does not exist such that GNN can not be applied to clustering directly and the strategy to construct a graph is crucial for performance. Therefore, how to extend GNN into general clustering tasks is an attractive problem. In this paper, we propose a graph auto-encoder for general data clustering, AdaGAE, which constructs the graph adaptively according to the generative perspective of graphs. The adaptive process is designed to induce the model to exploit the high-level information behind data and utilize the non-euclidean structure sufficiently. Importantly, we find that the simple update of the graph will result in severe degeneration, which can be concluded as better reconstruction means worse update. We provide rigorous analysis theoretically and empirically. Then we further design a novel mechanism to avoid the collapse. Via extending the generative graph models to general type data, a graph auto-encoder with a novel decoder is devised and the weighted graphs can be also applied to GNN. AdaGAE performs well and stably in different scale and type datasets. Besides, it is insensitive to the initialization of parameters and requires no pretraining.	[Li, Xuelong; Zhang, Hongyuan; Zhang, Rui] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China; [Li, Xuelong; Zhang, Hongyuan; Zhang, Rui] Northwestern Polytech Univ, Minist Ind & Informat Technol, Key Lab Intelligent Interact & Applicat, Xian 710072, Peoples R China	Northwestern Polytechnical University; Northwestern Polytechnical University	Li, XL; Zhang, R (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.	li@nwpu.edu.cn; hyzhang98@gmail.com; ruizhang8633@gmail.com			National Natural Science Foundation of China [61871470]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by The National Natural Science Foundation of China under Grant 61871470. The codes can be downloaded from https://github.com/hyzhang98/AdaGAE.	Abu-El-Haifa S, 2019, PR MACH LEARN RES, V97; Arthur Szlam, 2014, Arxiv, DOI arXiv:1312.6203; BEZDEK JC, 1980, IEEE T PATTERN ANAL, V2, P1, DOI 10.1109/TPAMI.1980.4766964; Bo DY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1400, DOI 10.1145/3366423.3380214; Boaz Nadler, 2018, Arxiv, DOI arXiv:1801.01587; Cao SS, 2016, AAAI CONF ARTIF INTE, P1145; Defferrard M, 2016, ADV NEUR IN, V29; Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Dunn J.C., 1973, J CYBERNETICS, V3, P32, DOI [10.1080/ 01969727308546046, DOI 10.1080/01969727308546046]; HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li X., 2020, ARXIV; Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413; Max Welling, 2016, Arxiv, DOI arXiv:1611.07308; Nene, 1996, CUCS00596 COL U DEP; Ng AY, 2002, ADV NEUR IN, V14, P849; Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726; Niepert M, 2016, PR MACH LEARN RES, V48; Pan SR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2609; Park J, 2019, IEEE I CONF COMP VIS, P6518, DOI 10.1109/ICCV.2019.00662; Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670; Wang C, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P889, DOI 10.1145/3132847.3132967; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Wang HW, 2018, AAAI CONF ARTIF INTE, P2508; Wu F, 2019, PR MACH LEARN RES, V97; Xie JY, 2016, PR MACH LEARN RES, V48; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597; Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556; Yang X, 2019, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2019.00419; Zhang R, 2022, IEEE T NEUR NET LEAR, V33, P4300, DOI 10.1109/TNNLS.2021.3056420; Zhang R, 2020, IEEE T FUZZY SYST, V28, P2814, DOI 10.1109/TFUZZ.2019.2945232	38	3	3	4	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9725	9732		10.1109/TPAMI.2021.3125687	http://dx.doi.org/10.1109/TPAMI.2021.3125687			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34748479	Green Submitted			2022-12-18	WOS:000880661400083
J	Lian, DZ; Chen, XN; Li, J; Luo, WX; Gao, SH				Lian, Dongze; Chen, Xianing; Li, Jing; Luo, Weixin; Gao, Shenghua			Locating and Counting Heads in Crowds With a Depth Prior	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Head; Magnetic heads; Location awareness; Feature extraction; Games; Object detection; Annotations; Crowd counting; head localization; detection-based head counting; density map; RGB-D	OBJECT DETECTION; PEOPLE	To simultaneously estimate the number of heads and locate heads with bounding boxes, we resort to detection-based crowd counting by leveraging RGB-D data and design a dual-path guided detection network (DPDNet). Specifically, to improve the performance of detection-based approaches for dense/tiny heads, we propose a density map guided detection module, which leverages density map to improve the head/non-head classification in detection network where the density implies the probability of a pixel being a head, and a depth-adaptive kernel that considers the variances in head sizes is also introduced to generate high-fidelity density map for more robust density map regression. In order to prevent dense heads from being filtered out during post-processing, we utilize such a density map for post-processing of head detection and propose a density map guided NMS strategy. Meanwhile, to improve the ability of detecting small heads, we also propose a depth-guided detection module to generate a dynamic dilated convolution to extract features of heads of different scales, and a depth-aware anchor is further designed for better initialization of anchor sizes in the detection framework. Then we use the bounding boxes whose sizes are generated with depth to train our DPDNet. Considering that existing RGB-D datasets are too small and not suitable for performance evaluation of data-driven based approaches, we collect two large-scale RGB-D crowd counting datasets, which comprise a synthetic dataset and a real-world dataset, respectively. Since the depth value at long-distance positions cannot be obtained in the real-world dataset, we further propose a depth completion method with meta learning, which fully utilizes the synthetic depth data to complete the depth value at long-distance positions. Extensive experiments on our proposed two RGB-D datasets and the MICC RGB-D counting dataset show that our method achieves the best performance for RGB-D crowd counting and localization. Further, our method can be easily extended to RGB image based crowd counting and achieves comparable or even better performance on the RGB datasets for both head counting and localization.	[Lian, Dongze; Chen, Xianing; Li, Jing; Luo, Weixin] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China; [Lian, Dongze] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200050, Peoples R China; [Lian, Dongze] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Gao, Shenghua] ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai Engn Res Ctr Energy Efficient & Custom A, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China	ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute of Microsystem & Information Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; ShanghaiTech University	Gao, SH (corresponding author), ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai Engn Res Ctr Energy Efficient & Custom A, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.	liandz@shanghaitech.edu.cn; chenxn1@shanghaitech.edu.cn; lijing1@shanghaitech.edu.cn; luowx@shanghaitech.edu.cn; gaoshh@shanghaitech.edu.cn		, Weixin/0000-0002-0754-6458	National Key R&D Program of China [2018AAA0100704]; NSFC [61932020]; Science and Technology Commission of Shanghai Municipality [20ZR1436000]; Shuguang Program - Shanghai Education Development Foundation; Shanghai Municipal Education Commission	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Science and Technology Commission of Shanghai Municipality(Science & Technology Commission of Shanghai Municipality (STCSM)); Shuguang Program - Shanghai Education Development Foundation; Shanghai Municipal Education Commission(Shanghai Municipal Education Commission (SHMEC))	This work was supported in part by the National Key R&D Program of China under Grants 2018AAA0100704 and NSFC #61932020, in part by the Science and Technology Commission of Shanghai Municipality under Grant 20ZR1436000, and in part by the Shuguang Program supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission.	Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Amato G, 2019, LECT NOTES COMPUT SC, V11751, P302, DOI 10.1007/978-3-030-30642-7_27; Amos Storkey, 2020, Arxiv, DOI arXiv:2004.05439; Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30; Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; Bondi E, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P337, DOI 10.1109/AVSS.2014.6918691; Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191; Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Daoyuan Jia, 2018, Arxiv, DOI arXiv:1801.01726; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Senushkin D, 2020, Arxiv, DOI arXiv:2005.08607; Fabbri M, 2018, LECT NOTES COMPUT SC, V11208, P450, DOI 10.1007/978-3-030-01225-0_27; Fang YY, 2019, IEEE INT CON MULTI, P814, DOI 10.1109/ICME.2019.00145; Finn C, 2017, PR MACH LEARN RES, V70; Fu HY, 2012, IEEE IMAGE PROC, P2685, DOI 10.1109/ICIP.2012.6467452; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Guangshuai Gao, 2020, Arxiv, DOI arXiv:2003.12783; Guerrero-Gomez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48; Gui LY, 2018, LECT NOTES COMPUT SC, V11212, P441, DOI 10.1007/978-3-030-01237-3_27; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Hu YT, 2019, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2019.00322; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33; Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629; Kingma D.P, P 3 INT C LEARNING R; Lempitsky V., 2010, NIPS, V23, P1324; Li M, 2008, INT C PATT RECOG, P1998; Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615; Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120; Lian D., 2020, PROC INT C LEARN REP; Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192; Lian DZ, 2019, AAAI CONF ARTIF INTE, P2488; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131; Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545; Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186; Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334; Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524; Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799; Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663; Loshchilov I., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1608.03983; Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624; Onoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38; Paszke A., 2017, 31 C NEURAL INFORM P, P1, DOI DOI 10.1017/CB09781107707221.009; Reddy MKK, 2020, IEEE WINT CONF APPL, P2803, DOI 10.1109/WACV45572.2020.9093409; Richter SR, 2017, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2017.243; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830; Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550; Shi ML, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON INDUSTRIAL ARTIFICIAL INTELLIGENCE (IAI 2019); Shi ZL, 2019, IEEE I CONF COMP VIS, P4199, DOI 10.1109/ICCV.2019.00430; Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS); Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969; Sindagi VA, 2019, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2019.00131; Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109; Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206; Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007; Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357; Song DP, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (IEEE ICIA 2017), P416, DOI 10.1109/ICInfA.2017.8078944; Spinello L, 2011, IEEE INT C INT ROBOT, P3838, DOI 10.1109/IROS.2011.6048835; Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255; Tian YK, 2020, IEEE T IMAGE PROCESS, V29, P2714, DOI 10.1109/TIP.2019.2952083; Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734; Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201; Wan J, 2022, IEEE T PATTERN ANAL, V44, P1357, DOI 10.1109/TPAMI.2020.3022878; Wan J, 2019, IEEE I CONF COMP VIS, P1130, DOI 10.1109/ICCV.2019.00122; Wang M, 2011, PROC CVPR IEEE; Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269; Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839; Wang Y, 2021, IEEE T IMAGE PROCESS, V30, P2876, DOI 10.1109/TIP.2021.3055632; Weizhe Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P723, DOI 10.1007/978-3-030-58555-6_43; Wu B, 2005, IEEE I CONF COMP VIS, P90; Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845; Xu ML, 2019, PATTERN RECOGN LETT, V125, P563, DOI 10.1016/j.patrec.2019.02.026; Xu XY, 2017, PATTERN RECOGN, V72, P300, DOI 10.1016/j.patcog.2017.07.026; Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104; Yang SD, 2019, IEEE INT CONF COMP V, P4521, DOI 10.1109/ICCVW.2019.00553; Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684; Zhang Q, 2020, AAAI CONF ARTIF INTE, V34, P12837; Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhang XC, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P215, DOI 10.1109/AVSS.2012.82; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70; Zhao MM, 2019, PROC CVPR IEEE, P12728, DOI 10.1109/CVPR.2019.01302	96	3	3	4	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9056	9072		10.1109/TPAMI.2021.3124956	http://dx.doi.org/10.1109/TPAMI.2021.3124956			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34735337				2022-12-18	WOS:000880661400037
J	Liang, XY; Qian, YH; Guo, Q; Cheng, HH; Liang, JY				Liang, Xinyan; Qian, Yuhua; Guo, Qian; Cheng, Honghong; Liang, Jiye			AF: An Association-Based Fusion Method for Multi-Modal Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Task analysis; Frequency measurement; Videos; Optimization; Deep learning; Data models; Multi-modal fusion; association-based fusion; interpretative fusion; multi-modal classification; association representation	ATTRIBUTE REDUCTION; IMAGE; RECOGNITION	Multi-modal classification (MMC) aims to integrate the complementary information from different modalities to improve classification performance. Existing MMC methods can be grouped into two categories: traditional methods and deep learning-based methods. The traditional methods often implement fusion in a low-level original space. Besides, they mostly focus on the inter-modal fusion and neglect the intra-modal fusion. Thus, the representation capacity of fused features induced by them is insufficient. The deep learning-based methods implement the fusion in a high-level feature space where the associations among features are considered, while the whole process is implicit and the fused space lacks interpretability. Based on these observations, we propose a novel interpretative association-based fusion method for MMC, named AF. In AF, both the association information and the high-order information extracted from feature space are simultaneously encoded into a new feature space to help to train an MMC model in an explicit manner. Moreover, AF is a general fusion framework, and most existing MMC methods can be embedded into it to improve their performance. Finally, the effectiveness and the generality of AF are validated on 22 datasets, four typically traditional MMC methods adopting best modality, early, late and model fusion strategies and a deep learning-based MMC method.	[Liang, Xinyan; Qian, Yuhua; Guo, Qian] Shanxi Univ, Inst Big Data Sci & Ind, Key Lab Computat Intelligence & Chinese Informat, Minist Educ, Taiyuan 030006, Shanxi, Peoples R China; [Cheng, Honghong] Shanxi Univ Finance & Econ, Sch Informat, Taiyuan 030012, Shanxi, Peoples R China; [Cheng, Honghong] Shanxi Univ, Inst Big Data Sci & Ind, Taiyuan 030012, Shanxi, Peoples R China; [Liang, Jiye] Shanxi Univ, Key Lab Computat Intelligence & Chinese Informat, Minist Educ, Taiyuan 030006, Shanxi, Peoples R China	Shanxi University; Shanxi University Finance & Economics; Shanxi University; Shanxi University	Qian, YH (corresponding author), Shanxi Univ, Inst Big Data Sci & Ind, Key Lab Computat Intelligence & Chinese Informat, Minist Educ, Taiyuan 030006, Shanxi, Peoples R China.	liangxinyan48@163.com; czguoqianl@163.com; jinchengqyh@126.com; chhsxdx@163.com; ljy@sxu.edu.cn			Key Program of the National Natural Science Foundation of China [62136005]; National Key Research and Development Program of China [2020AAA0106100]; Key R&D Program (International Science and Technology Cooperation Project) of Shanxi Province, China [201903D421003]; Young Scientists Fund of the National Natural Science Foundation of China [62106132, 61802238, 61906114, 61906115, 62006146]; 1331 Engineering Project of Shanxi Province, China; Scientific and Technological Innovation Programs of Higher Education Institutions in Shanxi [2021L286, 2020L0036]; Program for the Young San Jin Scholars of Shanxi [2016769]	Key Program of the National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; Key R&D Program (International Science and Technology Cooperation Project) of Shanxi Province, China; Young Scientists Fund of the National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); 1331 Engineering Project of Shanxi Province, China; Scientific and Technological Innovation Programs of Higher Education Institutions in Shanxi; Program for the Young San Jin Scholars of Shanxi	This work was supported in part by the Key Program of the National Natural Science Foundation of China under Grant 62136005, in part by National Key Research and Development Program of China under Grant 2020AAA0106100, in part by Key R&D Program (International Science and Technology Cooperation Project) of Shanxi Province, China under Grant 201903D421003, in part by Program for the Young San Jin Scholars of Shanxi under Grant 2016769, in part by Young Scientists Fund of the National Natural Science Foundation of China under Grants 62106132, 61802238, 61906114, 61906115, and 62006146, in part by the 1331 Engineering Project of Shanxi Province, China, and in part by the Scientific and Technological Innovation Programs of Higher Education Institutions in Shanxi under Grants 2021L286 and 2020L0036.	Amir Zadeh, 2016, Arxiv, DOI arXiv:1606.06259; Bai L, 2021, IEEE T PATTERN ANAL, V43, P3247, DOI 10.1109/TPAMI.2020.2979699; Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607; Bird JJ, 2020, IEEE INT C INT ROBOT, P10380, DOI 10.1109/IROS45743.2020.9341557; Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63; [成红红 Cheng Honghong], 2020, [中国科学. 信息科学, Scientia Sinica Informationis], V50, P824; Chua T.-S., 2009, P ACM INT C IM VID R, P1, DOI 10.1145/1646396.1646452; Demsar J, 2006, J MACH LEARN RES, V7, P1; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680; Gat I., 2020, P ADV NEUR INF PROC, V33, P3197; Geng Z., 2021, PROC INT C LEARN REP, P1; Han ZZ, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401777; Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678; Heller R, 2016, J MACH LEARN RES, V17; Hou M, 2019, ADV NEUR IN, V32; Hu QH, 2018, IEEE T FUZZY SYST, V26, P226, DOI 10.1109/TFUZZ.2017.2647966; Hwang SJ, 2012, IEEE T PATTERN ANAL, V34, P1145, DOI 10.1109/TPAMI.2011.190; Jia BB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107423; Khan I., 2009, PROC WORKSHOP AUSTRI, P213; Kludas J., 2011, PROC 14 INT C INF FU, P1; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Li DC, 2012, IEEE T KNOWL DATA EN, V24, P452, DOI 10.1109/TKDE.2010.254; Li FJ, 2019, ARTIF INTELL, V273, P37, DOI 10.1016/j.artint.2018.12.007; Liang XY, 2021, IEEE T EVOLUT COMPUT, V25, P883, DOI 10.1109/TEVC.2021.3064943; Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247; Nguyen D, 2022, IEEE T MULTIMEDIA, V24, P1313, DOI 10.1109/TMM.2021.3063612; Oh S., 2019, REVERSE ENG BLACKBOX; Pereira JC, 2014, COMPUT VIS IMAGE UND, V124, P123, DOI 10.1016/j.cviu.2014.03.003; Pereira JC, 2012, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2012.6248041; Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.0055, 10.1109/ICDM.2016.178]; Qian YH, 2017, INT J APPROX REASON, V82, P119, DOI 10.1016/j.ijar.2016.12.008; Qian YH, 2010, ARTIF INTELL, V174, P597, DOI 10.1016/j.artint.2010.04.018; Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214; Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.1002/ACP.3140; Rasiwasia N, 2010, ACM MM, DOI DOI 10.1145/1873951.1873987; Renyi A., 1959, ACTA MATH ACAD SCI H, V10, DOI [DOI 10.1007/BF02024507, 10.1007/BF02024507]; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Reshef YA, 2016, J MACH LEARN RES, V17, P1; Salim A, 2020, ENG APPL ARTIF INTEL, V90, DOI 10.1016/j.engappai.2020.103534; Song GL, 2017, IEEE T IMAGE PROCESS, V26, P4168, DOI 10.1109/TIP.2017.2713045; Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531; Srivastava N, 2014, J MACH LEARN RES, V15, P2949; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sun SL, 2019, INFORM FUSION, V50, P43, DOI 10.1016/j.inffus.2018.10.004; Szekely GJ, 2009, ANN APPL STAT, V3, P1236, DOI 10.1214/09-AOAS312; Tao H, 2020, IEEE T IMAGE PROCESS, V29, P8083, DOI 10.1109/TIP.2020.3010631; Tao H, 2020, IEEE T CYBERNETICS, V50, P2124, DOI 10.1109/TCYB.2018.2881474; Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656; Tsai Yao-Hung Hubert, 2019, ICLR; van Breukelen M, 1998, KYBERNETIKA, V34, P381; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang C., 2013, PROC INT JOINT C ART, P1736; Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155; Wang YS, 2019, AAAI CONF ARTIF INTE, P7216; Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940; Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931; Xie P., 2013, PROC INT JOINT C ART, P1806; Xu Y, 2015, IEEE T PATTERN ANAL, V37, P2131, DOI 10.1109/TPAMI.2015.2394475; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790; Yu WM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3718; Yu Zheng, 2015, IEEE Transactions on Big Data, V1, P16, DOI 10.1109/TBDATA.2015.2465959; Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977; Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340; Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/d17-1115; Zadeh A., 2019, PROC 57 ANN M ASS CO; Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236; Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634; Zhang Y, 2020, IEEE J BIOMED HEALTH, V24, P171, DOI 10.1109/JBHI.2019.2898471; Zheng Y, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2267, DOI 10.1145/2783258.2788573; Zhou ZH, 2019, NATL SCI REV, V6, P74, DOI 10.1093/nsr/nwy108; Zhou ZH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3553; Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795	74	3	3	7	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9236	9254		10.1109/TPAMI.2021.3125995	http://dx.doi.org/10.1109/TPAMI.2021.3125995			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34752381				2022-12-18	WOS:000880661400050
J	Wang, H; Deng, C; Liu, TL; Tao, DC				Wang, Hao; Deng, Cheng; Liu, Tongliang; Tao, Dacheng			Transferable Coupled Network for Zero-Shot Sketch-Based Image Retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Feature extraction; Semantics; Training; Image retrieval; Testing; Task analysis; Transferable coupled network; semantic metric; sketch-based image retrieval; zero-shot learning		Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) aims at searching corresponding natural images with the given free-hand sketches, under the more realistic and challenging scenario of Zero-Shot Learning (ZSL). Prior works concentrate much on aligning the sketch and image feature representations while ignoring the explicit learning of heterogeneous feature extractors to make themselves capable of aligning multi-modal features, with the expense of deteriorating the transferability from seen categories to unseen ones. To address this issue, we propose a novel Transferable Coupled Network (TCN) to effectively improve network transferability, with the constraint of soft weight-sharing among heterogeneous convolutional layers to capture similar geometric patterns, e.g., contours of sketches and images. Based on this, we further introduce and validate a general criterion to deal with multi-modal zero-shot learning, i.e., utilizing coupled modules for mining modality-common knowledge while independent modules for learning modality-specific information. Moreover, we elaborate a simple but effective semantic metric to integrate local metric learning and global semantic constraint into a unified formula to significantly boost the performance. Extensive experiments on three popular large-scale datasets show that our proposed approach outperforms state-of-the-art methods to a remarkable extent: by more than 12% on Sketchy, 2% on TU-Berlin and 6% on QuickDraw datasets in terms of retrieval accuracy. The project page is available at: https://haowang1992.github.io/publication/TCN.	[Wang, Hao; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China; [Liu, Tongliang] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia; [Tao, Dacheng] JD Explore Acad, Beijing 101100, Peoples R China	Xidian University; University of Sydney	Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.; Liu, TL (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.	haowang.xidian@gmail.com; chdeng.xd@gmail.com; tongliang.liu@sydney.edu.au; dacheng.tao@gmail.com			National Natural Science Foundation of China [62132016, 62171343, 62071361]; Key Research and Development Program of Shaanxi [2021ZDLGY0103]; Fundamental Research Funds for the Central Universities [ZDRC2102]; Australian Research Council [DE-190101473, IC-190100031]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research and Development Program of Shaanxi; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Australian Research Council(Australian Research Council)	This work was supported in part by the National Natural Science Foundation of China under Grants 62132016, 62171343, and 62071361, in part by the Key Research and Development Program of Shaanxi under Grant 2021ZDLGY0103, in part by the Fundamental Research Funds for the Central Universities under Grant ZDRC2102, and in part by the Australian Research Council Projects DE-190101473 and IC-190100031.	Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bucher M, 2016, LECT NOTES COMPUT SC, V9909, P730, DOI 10.1007/978-3-319-46454-1_44; Chen BH, 2019, PROC CVPR IEEE, P2745, DOI 10.1109/CVPR.2019.00286; Chen H, 2006, IEEE T PATTERN ANAL, V28, P1025, DOI 10.1109/TPAMI.2006.131; Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383; Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921; Deng J., 2009, 2009 IEEE C COMP VIS, P248, DOI [DOI 10.1109/CVPR.2009.5206848, 10.1109/CVPR.2009.5206848]; Dey S, 2019, PROC CVPR IEEE, P2174, DOI 10.1109/CVPR.2019.00228; Dutta A, 2019, PROC CVPR IEEE, P5084, DOI 10.1109/CVPR.2019.00523; Dutta T., 2019, PROC 2019 BRIT MACHI, V2, P9; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Greg Corrado, 2013, Arxiv, DOI arXiv:1301.3781; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jiang HJ, 2019, IEEE SIGNAL PROC LET, V26, P1270, DOI 10.1109/LSP.2019.2917148; Kingma D.P, P 3 INT C LEARNING R; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Lei JJ, 2020, IEEE T CIRC SYST VID, V30, P3226, DOI 10.1109/TCSVT.2019.2936710; Li Y, 2017, INT J COMPUT VISION, V122, P169, DOI 10.1007/s11263-016-0963-9; Liu L, 2017, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR.2017.247; Liu Q, 2019, IEEE I CONF COMP VIS, P3661, DOI 10.1109/ICCV.2019.00376; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Pang KY, 2019, PROC CVPR IEEE, P677, DOI 10.1109/CVPR.2019.00077; Paszke A, 2019, ADV NEUR IN, V32; Peng KC, 2018, LECT NOTES COMPUT SC, V11215, P793, DOI 10.1007/978-3-030-01252-6_47; Lu P, 2018, Arxiv, DOI arXiv:1812.04275; Qi YG, 2016, IEEE IMAGE PROC, P2460, DOI 10.1109/ICIP.2016.7532801; Saavedra J. M., 2015, PROC BRIT MACH VIS C, P1; Saavedra JM, 2014, IEEE IMAGE PROC, P2998, DOI 10.1109/ICIP.2014.7025606; Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954; Shen J., 2020, PROC IEEE WINTER C A, P786; Shen YM, 2018, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2018.00379; Socher R., 2013, 13 INT C NEUR INF PR, P935; Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592; Tran L, 2019, INT J COMPUT VISION, V127, P824, DOI 10.1007/s11263-019-01155-7; Wang JH, 2019, IEEE I CONF COMP VIS, P3374, DOI 10.1109/ICCV.2019.00347; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Xu XX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P984; Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319; Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19; Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3; Zeiler M. D., 2014, EUR C COMP VIS, P818; Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125	53	3	3	5	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9181	9194		10.1109/TPAMI.2021.3123315	http://dx.doi.org/10.1109/TPAMI.2021.3123315			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34705637				2022-12-18	WOS:000880661400045
J	Barath, D; Noskova, J; Matas, J				Barath, Daniel; Noskova, Jana; Matas, Jiri			Marginalizing Sample Consensus	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Data models; Estimation; Adaptation models; Optimization; Computational modeling; Upper bound; Testing; Robust model estimation; RANSAC; noise scale; M-estimator; marginalization	GEOMETRY ESTIMATION; EPIPOLAR GEOMETRY; ROBUST ESTIMATOR; RANSAC	A new method for robust estimation, MAGSAC++, is proposed. It introduces a new model quality (scoring) function that does not make inlier-outlier decisions, and a novel marginalization procedure formulated as an M-estimation with a novel class of M-estimators (a robust kernel) solved by an iteratively re-weighted least squares procedure. Instead of the inlier-outlier threshold, it requires only its loose upper bound which can be chosen from a significantly wider range. Also, we propose a new termination criterion and a technique for selecting a set of inliers in a data-driven manner as a post-processing step after the robust estimation finishes. On a number of publicly available real-world datasets for homography, fundamental matrix fitting and relative pose, MAGSAC++ produces results superior to the state-of-the-art robust methods. It is more geometrically accurate, fails fewer times, and it is often faster. It is shown that MAGSAC++ is significantly less sensitive to the setting of the threshold upper bound than the other state-of-the-art algorithms to the inlier-outlier threshold. Therefore, it is easier to be applied to unseen problems and scenes without acquiring information by hand about the setting of the inlier-outlier threshold. The source code and examples both in C++ and Python are available at https://github.com/danini/magsac.	[Barath, Daniel; Noskova, Jana; Matas, Jiri] Czech Tech Univ, Dept Cybernet, Visual Recognit Grp, Prague 16636, Czech Republic; [Barath, Daniel] SZTAKI, Machine Percept Res Lab, Budapest, Hungary; [Barath, Daniel] Swiss Fed Inst Technol, Dept Comp Sci, Comp Vis & Geometry Grp, CH-8092 Zurich, Switzerland	Czech Technical University Prague; Hungarian Academy of Sciences; Hungarian Institute for Computer Science & Control; Swiss Federal Institutes of Technology Domain; ETH Zurich	Barath, D (corresponding author), Czech Tech Univ, Dept Cybernet, Visual Recognit Grp, Prague 16636, Czech Republic.; Barath, D (corresponding author), SZTAKI, Machine Percept Res Lab, Budapest, Hungary.; Barath, D (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Comp Vis & Geometry Grp, CH-8092 Zurich, Switzerland.	dbarath@inf.ethz.ch; Jana.Noskova@cvut.cz; matas@cmp.felk.cvut.cz	Noskova, Jana/GXW-2580-2022		Czech Science Foundation [GA18-05360S]; Ministry of Education OP VVV Project [CZ.02.1.01/0.0/0.0/16 019/0000765]; Ministry of Innovation and Technology NRDI Office; Artificial Intelligence National Laboratory Program	Czech Science Foundation(Grant Agency of the Czech Republic); Ministry of Education OP VVV Project; Ministry of Innovation and Technology NRDI Office(National Research, Development & Innovation Office (NRDIO) - Hungary); Artificial Intelligence National Laboratory Program	This work was supported in part by the Czech Science Foundation under Grant GA18-05360S, in part by the Ministry of Education OP VVV Project CZ.02.1.01/0.0/0.0/16 019/0000765 Research Center for Informatics, in part by the Ministry of Innovation and Technology NRDI Office within the framework of the Autonomous Systems National Laboratory Program, and in part by the Artificial Intelligence National Laboratory Program.	Baker CG, 2012, LINEAR ALGEBRA APPL, V436, P2866, DOI 10.1016/j.laa.2011.07.018; Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410; Barath D, 2020, RANSAC 2020 TUTORIAL; Barath D, 2020, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR42600.2020.00138; Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044; Barath D, 2018, PROC CVPR IEEE, P6733, DOI 10.1109/CVPR.2018.00704; Bilaniuk O, 2014, IEEE COMPUT SOC CONF, P119, DOI 10.1109/CVPRW.2014.23; Capel D. P., 2005, PROC BRIT MACH VIS C; Chum O, 2005, PROC CVPR IEEE, P772; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2004, INT C PATT RECOG, P112, DOI 10.1109/ICPR.2004.1334020; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P623; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Cohen A, 2015, IEEE I CONF COMP VIS, P2282, DOI 10.1109/ICCV.2015.263; Dmytro Mishkin, 2016, Arxiv, DOI arXiv:1503.02619; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Ghosh D, 2016, J VIS COMMUN IMAGE R, V34, P1, DOI 10.1016/j.jvcir.2015.10.014; Haber H., 2011, 3 DIMENSIONAL PROPER; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7; Lebeda K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.95; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Matas J, 2005, IEEE I CONF COMP VIS, P1727; Moisan L, 2012, IMAGE PROCESS ON LIN, V2, P56, DOI 10.5201/ipol.2012.mmm-oh; More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105; Myatt D. R., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P458; Ni K, 2009, IEEE I CONF COMP VIS, P2193, DOI 10.1109/ICCV.2009.5459241; Pritchett P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P754, DOI 10.1109/ICCV.1998.710802; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Raguram R, 2011, IEEE I CONF COMP VIS, P1299, DOI 10.1109/ICCV.2011.6126382; Rais M., 2017, ARXIV; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Sminchisescu C, 2005, IEEE T PATTERN ANAL, V27, P727, DOI 10.1109/TPAMI.2005.104; STEWART CV, 1995, IEEE T PATTERN ANAL, V17, P925, DOI 10.1109/34.464558; Stewenius H, 2008, IMAGE VISION COMPUT, V26, P871, DOI 10.1016/j.imavis.2007.10.003; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Torr PHS, 2002, INT J COMPUT VISION, V50, P35, DOI 10.1023/A:1020224303087; TORR PHS, 1993, P SOC PHOTO-OPT INS, V2059, P432, DOI 10.1117/12.150246; Torr PHS, 1998, COMPUT VIS IMAGE UND, V71, P312, DOI 10.1006/cviu.1997.0559; Trulls E, 2020, IMAGE MATCHING CHALL; Pham TT, 2014, IEEE T IMAGE PROCESS, V23, P4601, DOI 10.1109/TIP.2014.2346025; Wald A., 1947, SEQUENTIAL ANAL; Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282; Jin YH, 2021, Arxiv, DOI arXiv:2003.01587; Zuliani M, 2005, IEEE IMAGE PROC, P2969	51	3	3	4	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8420	8432		10.1109/TPAMI.2021.3103562	http://dx.doi.org/10.1109/TPAMI.2021.3103562			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34375281	Green Published, Green Accepted, Bronze			2022-12-18	WOS:000864325900080
J	Han, K; Xiang, W; Wang, E; Huang, T				Han, Kang; Xiang, Wei; Wang, Eric; Huang, Tao			A Novel Occlusion-Aware Vote Cost for Light Field Depth Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Estimation; Image edge detection; Spatial resolution; Computational modeling; Cameras; Volume measurement; Visualization; Light field; depth estimation; occlusion-aware; vote cost		Capturing the directions of light by light field cameras powers next-generation immersive multimedia applications. A critical problem in taking advantage of the rich visual information in light field images is depth estimation. Conventional light field depth estimation methods build a cost volume that measures the photo-consistency of pixels refocused to a range of depths, and the highest consistency indicates the correct depth. This strategy works well in most regions but usually generates blurry edges in the estimated depth map due to occlusions. Recent work shows that integrating occlusion models to light field depth estimation can largely reduce blurry edges. However, existing occlusion handling methods rely on complex edge-aided processing and post-refinement, and this reliance limits the resultant depth accuracy and impacts on the computational performance. In this paper, we propose a novel occlusion-aware vote cost (OAVC) which is able to accurately preserve edges in the depth map. Instead of using photo-consistency as an indicator of the correct depth, we construct a novel cost from a new perspective that counts the number of refocused pixels whose deviations from the central-view pixel are less than a small threshold, and utilizes that number to select the correct depth. The pixels from occluders are thus excluded in determining the correct depth. Without the use of any explicit occlusion handling methods, the proposed method can inherently preserve edges and produces high-quality depth estimates. Experimental results show that the proposed OAVC outperforms state-of-the-art light field depth estimation methods in terms of depth estimation accuracy and computational complexity.	[Han, Kang; Wang, Eric; Huang, Tao] James Cook Univ, Coll Sci & Engn, Cairns, Qld 4878, Australia; [Xiang, Wei] La Trobe Univ, Sch Engn & Math Sci, Melbourne, Vic 3086, Australia	James Cook University; La Trobe University	Xiang, W (corresponding author), La Trobe Univ, Sch Engn & Math Sci, Melbourne, Vic 3086, Australia.	kang.han@my.jcu.edu.au; w.xiang@latrobe.edu.au; eric.wang@jcu.edu.au; tao.huang1@jcu.edu.au		Xiang, Wei/0000-0002-0608-065X; Han, Kang/0000-0003-3626-7818; Huang, Tao/0000-0002-8098-8906				Alperovich A, 2018, PROC CVPR IEEE, P9145, DOI 10.1109/CVPR.2018.00953; [Anonymous], 2016, HCI 4D LIGHT FIELD B; Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168; Chen J, 2018, IEEE T IMAGE PROCESS, V27, P4889, DOI 10.1109/TIP.2018.2839524; Feng MT, 2018, IEEE T IMAGE PROCESS, V27, P3586, DOI 10.1109/TIP.2018.2814217; Gutsche M., 2017, P IEEE C COMPUTER VI, P82; Heber S, 2016, PROC CVPR IEEE, P3746, DOI 10.1109/CVPR.2016.407; Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2; Huang CT, 2019, IEEE T PATTERN ANAL, V41, P552, DOI 10.1109/TPAMI.2018.2809502; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762; Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926; Lee JY, 2017, IEEE J-STSP, V11, P955, DOI 10.1109/JSTSP.2017.2747154; Ng R., 2005, COMPUT SCI TECH REP, V2, P1; Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323; Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499; Stanford light field archives, 2016, US; Tao MW, 2017, IEEE T PATTERN ANAL, V39, P546, DOI 10.1109/TPAMI.2016.2554121; Tao MW, 2015, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2015.7298804; Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89; Tsai YJ, 2020, AAAI CONF ARTIF INTE, V34, P12095; Wang TC, 2016, IEEE T PATTERN ANAL, V38, P2170, DOI 10.1109/TPAMI.2016.2515615; Wang TC, 2015, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2015.398; Wanner S., 2013, VISION MODELING VISU, P225, DOI DOI 10.2312/PE.VMV.VMV13.225-226; Williem, 2018, IEEE T PATTERN ANAL, V40, P2484, DOI 10.1109/TPAMI.2017.2746858; Williem, 2016, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2016.476; Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126; Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362; Zhang S, 2016, COMPUT VIS IMAGE UND, V145, P148, DOI 10.1016/j.cviu.2015.12.007; Zhang YB, 2017, IEEE T CIRC SYST VID, V27, P739, DOI 10.1109/TCSVT.2016.2555778; Zhao HL, 2016, COMPUT GRAPH-UK, V61, P11, DOI 10.1016/j.cag.2016.09.003; Zhu H, 2017, IEEE J-STSP, V11, P965, DOI 10.1109/JSTSP.2017.2730818	32	3	3	1	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8022	8035		10.1109/TPAMI.2021.3105523	http://dx.doi.org/10.1109/TPAMI.2021.3105523			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34406938				2022-12-18	WOS:000864325900054
J	Liu, NA; Li, L; Zhao, WB; Han, JW; Shao, L				Liu, Nian; Li, Long; Zhao, Wangbo; Han, Junwei; Shao, Ling			Instance-Level Relative Saliency Ranking With Graph Reasoning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Adaptation models; Predictive models; Measurement; Visualization; Object detection; Computational modeling; Saliency detection; graph neural network; global context; local context; instance segmentation; image retargeting	OBJECT DETECTION; VISUAL-ATTENTION; NEURAL-NETWORK; MODEL; PREDICT; VIDEO	Conventional salient object detection models cannot differentiate the importance of different salient objects. Recently, two works have been proposed to detect saliency ranking by assigning different degrees of saliency to different objects. However, one of these models cannot differentiate object instances and the other focuses more on sequential attention shift order inference. In this paper, we investigate a practical problem setting that requires simultaneously segment salient instances and infer their relative saliency rank order. We present a novel unified model as the first end-to-end solution, where an improved Mask R-CNN is first used to segment salient instances and a saliency ranking branch is then added to infer the relative saliency. For relative saliency ranking, we build a new graph reasoning module by combining four graphs to incorporate the instance interaction relation, local contrast, global contrast, and a high-level semantic prior, respectively. A novel loss function is also proposed to effectively train the saliency ranking branch. Besides, a new dataset and an evaluation metric are proposed for this task, aiming at pushing forward this field of research. Finally, experimental results demonstrate that our proposed model is more effective than previous methods. We also show an example of its practical usage on adaptive image retargeting.	[Liu, Nian; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Li, Long; Zhao, Wangbo; Han, Junwei] Northwestern Polytech Univ, Sch Automat, Xian 710060, Peoples R China; [Shao, Ling] Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates	Northwestern Polytechnical University; Mohamed Bin Zayed University of Artificial Intelligence	Han, JW (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710060, Peoples R China.	liunian228@gmail.com; longli.nwpu@gmail.com; wangbo.zhao96@gmail.com; junweihan2010@gmail.com; ling.shao@inceptioniai.org		Liu, Nian/0000-0002-0825-6081	National Key R&D Program of China [2020AAA0105702]; National Science Foundation of China [62136007, 61929104]	National Key R&D Program of China; National Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key R&D Program of China under Grant 2020AAA0105702 and in part by the National Science Foundation of China under Grants 62136007 and 61929104.	Achanta R, 2009, IEEE IMAGE PROC, P1005, DOI 10.1109/ICIP.2009.5413815; Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21; Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461; Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711; Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706; Bylinskii Z, 2016, LECT NOTES COMPUT SC, V9909, P809, DOI 10.1007/978-3-319-46454-1_49; Cerf M, 2007, PREDICTING HUMAN GAZ; Che ZH, 2020, IEEE T IMAGE PROCESS, V29, P2287, DOI 10.1109/TIP.2019.2945857; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15; Chen Weifeng, 2016, NEURIPS, P2, DOI DOI 10.5555/3157096.3157178; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Chen YX, 2015, NEUROCOMPUTING, V151, P645, DOI 10.1016/j.neucom.2014.05.089; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672; Crick F., 1990, Seminars in the Neurosciences, V2, P263; Cristian Canton Ferrer, 2018, Arxiv, DOI arXiv:1701.01081; Droste Richard, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P419, DOI 10.1007/978-3-030-58558-7_25; Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626; Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785; Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; He K., 2017, IEEE INT C COMP VIS, P2961; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563; Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688; Huang HY, 2021, IEEE T AFFECT COMPUT, V12, P832, DOI [10.1109/TAFFC.2019.2901456, 10.1145/3290605.3300851]; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kalash M, 2021, IEEE T PATTERN ANAL, V43, P204, DOI 10.1109/TPAMI.2019.2927203; Kampffmeyer M, 2019, IEEE T IMAGE PROCESS, V28, P2518, DOI 10.1109/TIP.2018.2886997; Kingma D.P., 2015, INT C LEARN REPR ICL; Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004; Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620; Kummerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513; Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399; Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047; Liu NA, 2018, IEEE T NEUR NET LEAR, V29, P392, DOI 10.1109/TNNLS.2016.2628878; Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698; Mastan ID, 2020, IEEE WINT CONF APPL, P2355, DOI 10.1109/WACV45572.2020.9093637; MOTTER BC, 1993, J NEUROPHYSIOL, V70, P909, DOI 10.1152/jn.1993.70.3.909; NIEBUR E, 1993, VISION RES, V33, P2789, DOI 10.1016/0042-6989(93)90236-P; Paszke A, 2019, ADV NEUR IN, V32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren TW, 2009, IEEE INT CON MULTI, P406, DOI 10.1109/ICME.2009.5202520; Ronneberger O., 2015, P MED IM COMP ASS IN, P234, DOI DOI 10.1007/978-3-319-24574-4_28; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133; Siris Avishek, 2020, CVPR; Sun C, 2019, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2019.00036; Tan WM, 2020, IEEE T MULTIMEDIA, V22, P1730, DOI 10.1109/TMM.2019.2959925; Tian X., 2020, PROC BRIT MACH VIS C; Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938; Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50; Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330; Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433; Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099; Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607; Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Xia C, 2021, IEEE T PATTERN ANAL, V43, P4378, DOI 10.1109/TPAMI.2020.3002168; Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943; Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610; Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zhou H., 2020, PROC IEEECVF C COMPU, P9141	91	3	3	4	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8321	8337		10.1109/TPAMI.2021.3107872	http://dx.doi.org/10.1109/TPAMI.2021.3107872			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34437057	Green Submitted			2022-12-18	WOS:000864325900074
J	Shugurov, I; Zakharov, S; Ilic, S				Shugurov, Ivan; Zakharov, Sergey; Ilic, Slobodan			DPODv2: Dense Correspondence-Based 6 DoF Pose Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pose estimation; Detectors; Deep learning; Three-dimensional displays; Cameras; Training data; Solid modeling; 6 DoF pose estimation; dense correspondences; synthetic data		We propose a three-stage 6 DoF object detection method called DPODv2 (Dense Pose Object Detector) that relies on dense correspondences. We combine a 2D object detector with a dense correspondence estimation network and a multi-view pose refinement method to estimate a full 6 DoF pose. Unlike other deep learning methods that are typically restricted to monocular RGB images, we propose a unified deep learning network allowing different imaging modalities to be used (RGB or Depth). Moreover, we propose a novel pose refinement method, that is based on differentiable rendering. The main concept is to compare predicted and rendered correspondences in multiple views to obtain a pose which is consistent with predicted correspondences in all views. Our proposed method is evaluated rigorously on different data modalities and types of training data in a controlled setup. The main conclusions is that RGB excels in correspondence estimation, while depth contributes to the pose accuracy if good 3D-3D correspondences are available. Naturally, their combination achieves the overall best performance. We perform an extensive evaluation and an ablation study to analyze and validate the results on several challenging datasets. DPODv2 achieves excellent results on all of them while still remaining fast and scalable independent of the used data modality and the type of training data.	[Shugurov, Ivan; Zakharov, Sergey; Ilic, Slobodan] Tech Univ Munich, Dept Informat, D-80333 Munich, Germany; [Shugurov, Ivan; Zakharov, Sergey; Ilic, Slobodan] Siemens AG, D-80333 Munich, Germany; [Zakharov, Sergey] Toyota Res Inst, Los Altos, CA 94022 USA	Technical University of Munich; Siemens AG; Siemens Germany; Toyota Motor Corporation	Shugurov, I (corresponding author), Tech Univ Munich, Dept Informat, D-80333 Munich, Germany.; Shugurov, I (corresponding author), Siemens AG, D-80333 Munich, Germany.	ivan.shugurov@tum.de; sergey.zakharov@tri.global; Slobodan.Ilic@siemens.com		Ilic, Slobodan/0000-0002-3413-1936; Shugurov, Ivan/0000-0001-5413-6622				Ahsan Lodhi, 2019, Arxiv, DOI arXiv:1911.01911; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andrea Vedaldi, 2017, Arxiv, DOI arXiv:1607.08022; Barath D, 2018, PROC CVPR IEEE, P6733, DOI 10.1109/CVPR.2018.00704; Barron JT, 2019, PROC CVPR IEEE, P4326, DOI 10.1109/CVPR.2019.00446; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366; Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35; Bradski G, 2000, DR DOBBS J, V25, P120; Bui Mai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P139, DOI 10.1007/978-3-030-58523-5_9; Bui M, 2018, IEEE INT CONF ROBOT, P6140; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Denninger M., BLENDERPROC REDUCING; Dieter Fox, 2018, Arxiv, DOI arXiv:1711.00199; Doumanoglou A, 2016, PROC CVPR IEEE, P3583, DOI 10.1109/CVPR.2016.390; Drost B, 2017, IEEE INT CONF COMP V, P2200, DOI 10.1109/ICCVW.2017.257; Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108; Erkent O, 2016, LECT NOTES COMPUT SC, V9911, P154, DOI 10.1007/978-3-319-46478-7_10; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266; Hinterstoisser S, 2019, LECT NOTES COMPUT SC, V11129, P682, DOI 10.1007/978-3-030-11009-3_42; Hinterstoisser S, 2016, LECT NOTES COMPUT SC, V9907, P834, DOI 10.1007/978-3-319-46487-9_51; Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206; Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326; Hinterstoisser Stefan, 2012, P AS C COMP VIS, P2, DOI DOI 10.1007/978-3-642-37331-2_42; Hodan Tomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11700, DOI 10.1109/CVPR42600.2020.01172; Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103; Hodan T, 2015, IEEE INT C INT ROBOT, P4421, DOI 10.1109/IROS.2015.7354005; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jaesik Park, 2018, Arxiv, DOI arXiv:1801.09847; Jafari O.H., 2018, AS C COMP VIS, P477; Kaskman Roman, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P612, DOI 10.1007/978-3-030-66096-3_41; Kaskman R, 2019, IEEE INT CONF COMP V, P2767, DOI 10.1109/ICCVW.2019.00338; Kehl W., 2015, PROC PROC BRIT MACH; Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169; Kehl W, 2016, LECT NOTES COMPUT SC, V9907, P205, DOI 10.1007/978-3-319-46487-9_13; Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Konishi Y, 2016, LECT NOTES COMPUT SC, V9905, P398, DOI 10.1007/978-3-319-46448-0_24; Labbe Yann, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P574, DOI 10.1007/978-3-030-58520-4_34; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Li C, 2018, LECT NOTES COMPUT SC, V11220, P263, DOI 10.1007/978-3-030-01270-0_16; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780; Manhardt F, 2018, LECT NOTES COMPUT SC, V11218, P833, DOI 10.1007/978-3-030-01264-9_49; Manhardt F, 2019, IEEE I CONF COMP VIS, P6840, DOI 10.1109/ICCV.2019.00694; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469; Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247; Pitteri G, 2019, INT CONF 3D VISION, P614, DOI 10.1109/3DV.2019.00073; Planche B, 2019, IEEE INT C INT ROBOT, P2579, DOI 10.1109/IROS40897.2019.8967829; Qi CR, 2017, ADV NEUR IN, V30; Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Rennie C, 2016, IEEE ROBOT AUTOM LET, V1, P1179, DOI 10.1109/LRA.2016.2532924; Rios-Cabrera R, 2013, IEEE I CONF COMP VIS, P2048, DOI 10.1109/ICCV.2013.256; Ronneberger O., 2015, P INT C MED IM COMP; Shugurov I, 2021, IEEE ROBOT AUTOM LET, V6, P2579, DOI 10.1109/LRA.2021.3062350; Sock J, 2017, IEEE INT CONF COMP V, P2228, DOI 10.1109/ICCVW.2017.260; Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051; Sun, 2020, 2020 IEEE CVF C COMP, P11629, DOI DOI 10.1109/CVPR42600.2020.01165; Sundermeyer M, 2020, INT J COMPUT VISION, V128, P714, DOI 10.1007/s11263-019-01243-8; Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43; Taylor J, 2012, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2012.6247664; Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30; Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038; Vidal J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082678; Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75; Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346; Wang H, 2019, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR.2019.00275; Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930; Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033; Yu J, 2005, PLOS BIOL, V3, P266, DOI 10.1371/journal.pbio.0030038; Zakharov Sergey, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12221, DOI 10.1109/CVPR42600.2020.01224; Zakharov S, 2018, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2018.00012; Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203; Zakharov S, 2017, IEEE INT C INT ROBOT, P552; Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589	86	3	3	4	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7417	7435		10.1109/TPAMI.2021.3118833	http://dx.doi.org/10.1109/TPAMI.2021.3118833			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34623263	Green Submitted			2022-12-18	WOS:000864325900015
J	Wang, XL; Zhang, RF; Shen, CH; Kong, T; Li, L				Wang, Xinlong; Zhang, Rufeng; Shen, Chunhua; Kong, Tao; Li, Lei			SOLO: A Simple Framework for Instance Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Semantics; Object detection; Task analysis; Kernel; Training; Standards; Instance segmentation; object detection; segmenting objects by locations		Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that has made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the "detect-then-segment" strategy (e.g., Mask R-CNN), or predict embedding vectors first then cluster pixels into individual instances. In this paper, we view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location. With this notion, we propose segmenting objects by locations (SOLO), a simple, direct, and fast framework for instance segmentation with strong performance. We derive a few SOLO variants (e.g., Vanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our method directly maps a raw input image to the desired object categories and instance masks, eliminating the need for the grouping post-processing or the bounding box detection. Our approach achieves state-of-the-art results for instance segmentation in terms of both speed and accuracy, while being considerably simpler than the existing methods. Besides instance segmentation, our method yields state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation. We further demonstrate the flexibility and high-quality segmentation of SOLO by extending it to perform one-stage instance-level image matting. Code is available at: https://git.io/AdelaiDet.	[Wang, Xinlong; Shen, Chunhua] Univ Adelaide, Adelaide, SA 5005, Australia; [Zhang, Rufeng] Tongji Univ, Shanghai 200092, Peoples R China; [Kong, Tao; Li, Lei] ByteDance AI Lab, Beijing 100080, Peoples R China	University of Adelaide; Tongji University	Shen, CH (corresponding author), Univ Adelaide, Adelaide, SA 5005, Australia.; Kong, T (corresponding author), ByteDance AI Lab, Beijing 100080, Peoples R China.	wangxinlon@gmail.com; cxrfzhang@tongji.edu.cn; chhshen@gmail.com; taokongcn@gmail.com; lileilab@bytedance.com		Wang, Xinlong/0000-0002-6974-7976				Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925; Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249; Cai LL, 2019, PROC CVPR IEEE, P9348, DOI 10.1109/CVPR.2019.00958; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422; Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610; Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; De Brabandere B, 2016, ADV NEUR IN, V29; De Brabandere B, 2017, IEEE COMPUT SOC CONF, P478, DOI 10.1109/CVPRW.2017.66; Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221; Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; Hao Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8570, DOI 10.1109/CVPR42600.2020.00860; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Islam M. A., 2020, PROC INT C LEARN REP; Jaderberg M, 2015, ADV NEUR IN, V28; Jinlin Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8560, DOI 10.1109/CVPR42600.2020.00859; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719; Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu R, 2018, ADV NEUR IN, V31; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378; Liu ST, 2019, PROC CVPR IEEE, P6452, DOI [10.1109/CVPR.2019.00662, 10.1109/CVPR.2019.01055]; Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI 10.1109/CVPR.2015.7298965; Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079; Newell A, 2017, ADV NEUR IN, V30; Ning Xu, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P311, DOI 10.1109/CVPR.2017.41; Pinheiro Pedro O., 2015, ADV NEURAL INFORM PR, V3, P5; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075; Rufeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10223, DOI 10.1109/CVPR42600.2020.01024; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sofiiuk K, 2019, IEEE I CONF COMP VIS, P7354, DOI 10.1109/ICCV.2019.00745; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Wang X., 2020, PROC INT C NEURAL IN; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38; Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975; Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274; Yuqing Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9310, DOI 10.1109/CVPR42600.2020.00933; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765; Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17; Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094	63	3	3	13	34	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8587	8601		10.1109/TPAMI.2021.3111116	http://dx.doi.org/10.1109/TPAMI.2021.3111116			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34516372	Green Submitted			2022-12-18	WOS:000864325900091
J	Zhao, GM; Feng, QL; Chen, CQ; Zhou, Z; Yu, YZ				Zhao, Gangming; Feng, Quanlong; Chen, Chaoqi; Zhou, Zhen; Yu, Yizhou			Diagnose Like a Radiologist: Hybrid Neuro-Probabilistic Reasoning for Attribute-Based Medical Image Diagnosis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Medical diagnostic imaging; Cognition; Bayes methods; Diseases; Visualization; Task analysis; Probabilistic logic; Bayesian networks; deep neural networks; medical image analysis; neuro-probabilistic reasoning	LUNG NODULES; CLASSIFICATION; TEXTURE; CANCER; LEVEL; SHAPE	During clinical practice, radiologists often use attributes, e.g., morphological and appearance characteristics of a lesion, to aid disease diagnosis. Effectively modeling attributes as well as all relationships involving attributes could boost the generalization ability and verifiability of medical image diagnosis algorithms. In this paper, we introduce a hybrid neuro-probabilistic reasoning algorithm for verifiable attribute-based medical image diagnosis. There are two parallel branches in our hybrid algorithm, a Bayesian network branch performing probabilistic causal relationship reasoning and a graph convolutional network branch performing more generic relational modeling and reasoning using a feature representation. Tight coupling between these two branches is achieved via a cross-network attention mechanism and the fusion of their classification results. We have successfully applied our hybrid reasoning algorithm to two challenging medical image diagnosis tasks. On the LIDC-IDRI benchmark dataset for benign-malignant classification of pulmonary nodules in CT images, our method achieves a new state-of-the-art accuracy of 95.36% and an AUC of 96.54%. Our method also achieves a 3.24% accuracy improvement on an in-house chest X-ray image dataset for tuberculosis diagnosis. Our ablation study indicates that our hybrid algorithm achieves a much better generalization performance than a pure neural network architecture under very limited training data.	[Zhao, Gangming; Chen, Chaoqi; Yu, Yizhou] Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China; [Feng, Quanlong] China Agr Univ, Dept Geog Informat Engn, Beijing 100107, Peoples R China; [Zhou, Zhen] Deepwise Healthcare, AI Lab, Beijing 100080, Peoples R China	University of Hong Kong; China Agricultural University	Yu, YZ (corresponding author), Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China.	gmzhao@connect.hku.hk; fengql@cau.edu.cn; cqchen1994@gmail.com; zhouzhen@deepwise.com; yizhouy@acm.org	/F-3345-2010	/0000-0002-0470-5548; Feng, Quanlong/0000-0002-0569-4131	Beijing Municipal Science and Technology Planning Project [Z201100005620008]; National Natural Science Foundation of China [81971616]	Beijing Municipal Science and Technology Planning Project; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by Beijing Municipal Science and Technology Planning Project under Grant Z201100005620008 and in part by the National Natural Science Foundation of China underGrant 81971616.	Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204; Barik A., 2019, PROC INT C NEURAL IN, P8964; Bray F, 2018, CA-CANCER J CLIN, V68, P394, DOI 10.3322/caac.21492; Buty Mario, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9900, P662, DOI 10.1007/978-3-319-46720-7_77; Dhara AK, 2016, J DIGIT IMAGING, V29, P466, DOI 10.1007/s10278-015-9857-6; Eaton D., 2007, PROC 23 C UNCERTAINT, P101; Elidan G., 2010, PROC INT C NEURAL IN, P559; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Ferrari V, 2008, P INT C NEUR INF PRO, P433; Frey B. J., 1997, PROC ANN ALLERTON C, P666; Gal Y, 2016, PR MACH LEARN RES, V48; Guan QJ, 2020, PATTERN RECOGN LETT, V131, P38, DOI 10.1016/j.patrec.2019.11.040; Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216; Han FF, 2015, J DIGIT IMAGING, V28, P99, DOI 10.1007/s10278-014-9718-8; Han FF, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON MEDICAL IMAGING PHYSICS AND ENGINEERING (ICMIPE), P14, DOI 10.1109/ICMIPE.2013.6864494; HART PD, 1977, BRIT MED J, V2, P293, DOI 10.1136/bmj.2.6082.293; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hussein S, 2017, LECT NOTES COMPUT SC, V10265, P249, DOI 10.1007/978-3-319-59050-9_20; Hussein S, 2017, I S BIOMED IMAGING, P1007, DOI 10.1109/ISBI.2017.7950686; Jiang HJ, 2017, IEEE I CONF COMP VIS, P4233, DOI 10.1109/ICCV.2017.453; Kingma DP, 2015, ADV NEUR IN, V28; Kingma DP, 2015, INT C LEARN REPR ICL; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Kuo WC, 2019, P NATL ACAD SCI USA, V116, P22737, DOI 10.1073/pnas.1908021116; KYBURG HE, 1991, J PHILOS, V88, P434, DOI 10.2307/2026705; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936; Li Q, 2020, PR MACH LEARN RES, V119; Liang KM, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2252; Liang KM, 2019, IEEE T PATTERN ANAL, V41, P1747, DOI 10.1109/TPAMI.2018.2836461; Lin CH, 2014, BMC INFECT DIS, V14, DOI 10.1186/1471-2334-14-5; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; McKinney SM, 2020, NATURE, V577, P89, DOI 10.1038/s41586-019-1799-6; Meng QE, 2020, PROC CVPR IEEE, P4032, DOI 10.1109/CVPR42600.2020.00409; Meng Z., 2018, PROC EUR C COMPUT VI, P552; Min WQ, 2020, IEEE T IMAGE PROCESS, V29, P657, DOI 10.1109/TIP.2019.2932502; Ost DE, 2012, AM J RESP CRIT CARE, V185, P363, DOI 10.1164/rccm.201104-0679CI; Paszke A, 2019, ADV NEUR IN, V32; Pearl J., 1998, HDB BRAIN THEORY NEU, P149; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rohekar RY, 2018, ADV NEUR IN, V31; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santoro A, 2017, ADV NEUR IN, V30; Setio AAA, 2016, IEEE T MED IMAGING, V35, P1160, DOI 10.1109/TMI.2016.2536809; Shavlik J. W., 1994, MACH LEARN, V14, P321; SHAVLIK JW, 1991, MACH LEARN, V6, P111, DOI 10.1023/A:1022602303196; Shen Wei, 2015, Inf Process Med Imaging, V24, P588, DOI 10.1007/978-3-319-19992-4_46; Shi ZL, 2018, LECT NOTES COMPUT SC, V11073, P569, DOI 10.1007/978-3-030-00937-3_65; Tan MX, 2019, PR MACH LEARN RES, V97; Wit E, 2012, STAT NEERL, V66, P217, DOI 10.1111/j.1467-9574.2012.00530.x; Wu BT, 2018, I S BIOMED IMAGING, P1109; Xie Y., 2017, INT C MEDICAL IMAGE, P656; Xie YT, 2019, MED IMAGE ANAL, V57, P237, DOI 10.1016/j.media.2019.07.004; Xie YT, 2019, IEEE T MED IMAGING, V38, P991, DOI 10.1109/TMI.2018.2876510; Xie YT, 2018, INFORM FUSION, V42, P102, DOI 10.1016/j.inffus.2017.10.005; Xu XY, 2020, MED IMAGE ANAL, V65, DOI 10.1016/j.media.2020.101772; Yi KX, 2018, ADV NEUR IN, V31; Yin PC, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P754	66	3	3	8	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7400	7416		10.1109/TPAMI.2021.3130759	http://dx.doi.org/10.1109/TPAMI.2021.3130759			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34822325	Green Submitted, hybrid			2022-12-18	WOS:000864325900014
J	Akhtar, N; Jalwana, MAAK; Bennamoun, M; Mian, A				Akhtar, Naveed; Jalwana, Mohammad A. A. K.; Bennamoun, Mohammed; Mian, Ajmal			Attack to Fool and Explain Deep Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Perturbation methods; Computational modeling; Visualization; Predictive models; Data models; Tools; Task analysis; Adversarial examples; perturbations; targeted attack; model interpretation; explainable AI		Deep visual models are susceptible to adversarial perturbations to inputs. Although these signals are carefully crafted, they still appear noise-like patterns to humans. This observation has led to the argument that deep visual representation is misaligned with human perception. We counter-argue by providing evidence of human-meaningful patterns in adversarial perturbations. We first propose an attack that fools a network to confuse a whole category of objects (source class) with a target label. Our attack also limits the unintended fooling by samples from non-sources classes, thereby circumscribing human-defined semantic notions for network fooling. We show that the proposed attack not only leads to the emergence of regular geometric patterns in the perturbations, but also reveals insightful information about the decision boundaries of deep models. Exploring this phenomenon further, we alter the 'adversarial' objective of our attack to use it as a tool to 'explain' deep visual representation. We show that by careful channeling and projection of the perturbations computed by our method, we can visualize a model's understanding of human-defined semantic notions. Finally, we exploit the explanability properties of our perturbations to perform image generation, inpainting and interactive image manipulation by attacking adversarialy robust 'classifiers'. In all, our major contribution is a novel pragmatic adversarial attack that is subsequently transformed into a tool to interpret the visual models. The article also makes secondary contributions in terms of establishing the utility of our attack beyond the adversarial objective with multiple interesting applications.	[Akhtar, Naveed; Jalwana, Mohammad A. A. K.; Bennamoun, Mohammed; Mian, Ajmal] Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia	University of Western Australia	Akhtar, N (corresponding author), Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia.	naveed.akhtar@uwa.edu.au; mohammad.jalwana@research.uwa.edu.au; mohammed.bennamoun@uwa.edu.au; ajmal.mian@uwa.edu.au	AKHTAR, NAVEED/AAT-1283-2020; Bennamoun, Mohammed/C-2789-2013	AKHTAR, NAVEED/0000-0003-3406-673X; Bennamoun, Mohammed/0000-0002-6603-3257; Mian, Ajmal/0000-0002-5206-3842	ARC [DP190102443]	ARC(Australian Research Council)	This work was supported by ARC DP190102443 and the GPUs were donated by NVIDIA Corporation. Naveed Akhtar and Mohammad A. A. K. Jalwana contributed equally to this work.	Adrian Vladu, 2019, Arxiv, DOI arXiv:1706.06083; Akhtar N, 2018, PROC CVPR IEEE, P3389, DOI 10.1109/CVPR.2018.00357; Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385; Alcorn MA, 2019, PROC CVPR IEEE, P4840, DOI 10.1109/CVPR.2019.00498; Aleksander Madry, 2019, Arxiv, DOI arXiv:1905.02175; Aleksander Madry, 2019, Arxiv, DOI arXiv:1906.00945; Aleksander Madry, 2019, Arxiv, DOI arXiv:1902.06705; Elliott A, 2019, Arxiv, DOI arXiv:1912.09405; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anh Nguyen, 2015, Arxiv, DOI arXiv:1506.06579; Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Anish Athalye, 2018, Arxiv, DOI arXiv:1804.03286; Antonio Torralba, 2018, Arxiv, DOI arXiv:1811.10597; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Cheng Tai, 2018, Arxiv, DOI arXiv:1802.09707; Christian Szegedy, 2015, Arxiv, DOI arXiv:1412.6572; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Croce F, 2019, IEEE I CONF COMP VIS, P4723, DOI 10.1109/ICCV.2019.00482; David Wagner, 2017, Arxiv, DOI arXiv:1711.08478; David Wagner, 2016, Arxiv, DOI arXiv:1607.04311; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong YP, 2019, PROC CVPR IEEE, P7706, DOI 10.1109/CVPR.2019.00790; Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Fong R, 2019, IEEE I CONF COMP VIS, P2950, DOI 10.1109/ICCV.2019.00304; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Hayes J, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P43, DOI 10.1109/SPW.2018.00015; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Inkawhich N, 2019, PROC CVPR IEEE, P7059, DOI 10.1109/CVPR.2019.00723; Jalwana M. A. A. K., 2020, PROC IEEECVF C COMPU, P9543; Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624; Kingma D.P, P 3 INT C LEARNING R; Konda Reddy Mopuri, 2017, Arxiv, DOI arXiv:1707.05572; Li J, 2019, IEEE I CONF COMP VIS, P4898, DOI 10.1109/ICCV.2019.00500; Liu JY, 2019, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2019.00496; Lunz S., 2019, ARXIV; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Prakash A, 2018, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR.2018.00894; Qiu YX, 2019, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2019.00491; Rony J, 2019, PROC CVPR IEEE, P4317, DOI 10.1109/CVPR.2019.00445; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shi YC, 2019, PROC CVPR IEEE, P6512, DOI 10.1109/CVPR.2019.00668; Sun B, 2019, PROC CVPR IEEE, P11439, DOI 10.1109/CVPR.2019.01171; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2; Wiyatno RR, 2019, IEEE I CONF COMP VIS, P4821, DOI 10.1109/ICCV.2019.00492; Woods W., 2019, ARXIV; Xiang C, 2019, PROC CVPR IEEE, P9128, DOI 10.1109/CVPR.2019.00935; Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Xu K., 2018, ARXIV; Yao ZW, 2019, PROC CVPR IEEE, P11342, DOI 10.1109/CVPR.2019.01161; Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017; Zeng XH, 2019, PROC CVPR IEEE, P4297, DOI 10.1109/CVPR.2019.00443; Zhang TY, 2019, PR MACH LEARN RES, V97; Zhao Z., 2017, ARXIV; Zhou BL, 2018, LECT NOTES COMPUT SC, V11212, P122, DOI 10.1007/978-3-030-01237-3_8	73	3	3	19	27	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					5980	5995		10.1109/TPAMI.2021.3083769	http://dx.doi.org/10.1109/TPAMI.2021.3083769			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34038356	Green Submitted			2022-12-18	WOS:000853875300013
J	Guo, YW; Zhang, CS				Guo, Yiwen; Zhang, Changshui			Recent Advances in Large Margin Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Robustness; Training; Perturbation methods; Neural networks; Support vector machines; Stochastic processes; Kernel; Large margin classifier; adversarial perturbation; generalization ability; deep neural networks	VECTOR; ROBUSTNESS	This paper serves as a survey of recent advances in large margin training and its theoretical foundations, mostly for (nonlinear) deep neural networks (DNNs) that are probably the most prominent machine learning models for large-scale data in the community over the past decade. We generalize the formulation of classification margins from classical research to latest DNNs, summarize theoretical connections between the margin, network generalization, and robustness, and introduce recent efforts in enlarging the margins for DNNs comprehensively. Since the viewpoint of different methods is discrepant, we categorize them into groups for ease of comparison and discussion in the paper. Hopefully, our discussions and overview inspire new research work in the community that aim to improve the performance of DNNs, and we also point to directions where the large margin principle can be verified to provide theoretical evidence why certain regularizations for DNNs function well in practice. We managed to shorten the paper such that the crucial spirit of large margin learning and related methods are better emphasized.	[Guo, Yiwen] ByteDance AI Lab, Beijing 100000, Peoples R China; [Zhang, Changshui] Tsinghua Univ THUAI, Inst Artificial Intelligence, Beijing 100084, Peoples R China; [Zhang, Changshui] Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China	Tsinghua University	Guo, YW (corresponding author), ByteDance AI Lab, Beijing 100000, Peoples R China.	guoyiwen.ai@bytedance.com; zcs@mail.tsinghua.edu.cn			National Key Research and Development Program of China [2018AAA0100701]; Guoqiang Institute, Tsinghua University	National Key Research and Development Program of China; Guoqiang Institute, Tsinghua University	This work was supported in part by the National Key Research and Development Program of China under Grant 2018AAA0100701 and in part by a grant from the Guoqiang Institute, Tsinghua University.	An SJ, 2015, IEEE I CONF COMP VIS, P2515, DOI 10.1109/ICCV.2015.289; Arora S, 2019, ADV NEUR IN, V32; Bansal Y., 2018, ARXIV; Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43; Boopathy A, 2019, AAAI CONF ARTIF INTE, P3240; Cao KD, 2019, ADV NEUR IN, V32; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Charles Z., 2019, ARXIV; Chen BH, 2018, ADV NEUR IN, V31; Cohen J, 2019, PR MACH LEARN RES, V97; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cotter A., 2013, PROC INT C MACH LEAR, P266; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Croce F, 2019, PR MACH LEARN RES, V89; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Ding G. W., 2019, ARXIV; Dvijotham K, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P550; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Elkan Charles, 2008, P 14 ACM SIGKDD INT, P213, DOI DOI 10.1145/1401890.1401920; Elsayed G. F., 2018, P 32 INT C NEUR INF, P850; Galloway A., 2017, PROC INT C LEARN REP; Gong TL, 2018, AAAI CONF ARTIF INTE, P3037; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I. J., 2015, PROC INT C LEARN REP; Goodfellow I.J., 2013, ARXIV; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Gunasekar S, 2018, ADV NEUR IN, V31; Guo YW, 2021, IEEE T PATTERN ANAL, V43, P4469, DOI 10.1109/TPAMI.2020.3006917; Guo YW, 2016, ADV NEUR IN, V29; Han S, 2015, ADV NEUR IN, V28; Han SY, 2018, INT J DIGIT EARTH, V11, P451, DOI 10.1080/17538947.2017.1330366; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Huang JJ, 2015, ADV NEUR IN, V28; Hubara I, 2018, J MACH LEARN RES, V18; Jakubovitz D, 2018, LECT NOTES COMPUT SC, V11216, P525, DOI 10.1007/978-3-030-01258-8_32; Ji Z., 2019, C LEARNING THEORY, P1772; Ji Z., 2018, PROC INT C LEARN REP; Jiang Yiding, 2019, INT C LEARN REPR; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Kawaguchi K., 2017, ARXIV; Kemker R, 2018, AAAI CONF ARTIF INTE, P3390; Kobayashi T., 2019, P BMVC, P139; KROGH A, 1992, ADV NEUR IN, V4, P950; Lee G, 2019, ADV INTELL SYST, V797, P1, DOI 10.1007/978-981-13-1165-9_1; Li Y., 2018, ARXIV; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Liu W, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3023; Loh P. -L., 2019, PROC INT C MACH LEAR, P5321; Madry A., 2018, P ICLR VANC BC CAN; Moosavi-Dezfooli SM, 2019, PROC CVPR IEEE, P9070, DOI 10.1109/CVPR.2019.00929; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Nacson MS, 2019, PR MACH LEARN RES, V89; Neyshabur B., 2017, ARXIV; Novak R., 2018, INT C LEARN REPR; Parno B., 2021, PROC INT C LEARN REP, P1; Paszke A., 2017, AUTOMATIC DIFFERENTI; Ros AS, 2018, AAAI CONF ARTIF INTE, P1660; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Singh G, 2018, ADV NEUR IN, V31; Smola A. J., 2000, ADV LARGE MARGIN CLA; Sokolic J, 2017, PR MACH LEARN RES, V54, P1094; Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039; Soudry D, 2018, J MACH LEARN RES, V19; Stutz D., 2019, ARXIV; Sun SZ, 2016, AAAI CONF ARTIF INTE, P2066; Szegedy C., 2014, ICLR 2014; Tang Y., 2013, ARXIV PREPRINT ARXIV, P2; Tjeng V., 2017, ARXIV; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tsuzuku Y, 2018, ADV NEUR IN, V31; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang PD, 2019, INTERSPEECH, P246, DOI 10.21437/Interspeech.2019-1680; Wang XB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P992; Wang X, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P899; Wang YS, 2018, J BIOMED INFORM, V87, P12, DOI 10.1016/j.jbi.2018.09.008; Wei C., 2020, PROC INT C LEARN REP; Wei C., 2019, P ADV NEUR INF PROC; Wei CL, 2019, ADV NEUR IN, V32; Weihong Deng, 2019, Arxiv, DOI arXiv:1909.09481; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wen W, 2016, ADV NEUR IN, V29; Weng T. -W., 2018, PROC INT C LEARN REP; Wong E, 2018, PR MACH LEARN RES, V80; Wu K., 2019, ARXIV; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; Xu H, 2009, J MACH LEARN RES, V10, P1485; Yan Z, 2018, ADV NEUR IN, V31; Yan Z, 2021, IEEE T PATTERN ANAL, V43, P1129, DOI 10.1109/TPAMI.2019.2948348; Yoshida Y., 2017, ARXIV; Yu C, 2017, INT CONF E BUS ENG, P1, DOI 10.1109/ICEBE.2017.11; Zhang H., 2017, ARXIV; Zhang H, 2018, ADV NEUR IN, V31; Zhang SX, 2015, INT CONF ACOUST SPEE, P4275, DOI 10.1109/ICASSP.2015.7178777; Zhang SX, 2016, INT CONF ACOUST SPEE, P5885, DOI 10.1109/ICASSP.2016.7472806; Zhang T, 2020, IEEE T KNOWL DATA EN, V32, P1143, DOI 10.1109/TKDE.2019.2897662; Zhu J, 2004, ADV NEUR IN, V16, P49	116	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7167	7174		10.1109/TPAMI.2021.3091717	http://dx.doi.org/10.1109/TPAMI.2021.3091717			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34161238	Green Submitted			2022-12-18	WOS:000853875300091
J	Lai, SX; Jin, LW; Zhu, YC; Li, Z; Lin, LJ				Lai, Songxuan; Jin, Lianwen; Zhu, Yecheng; Li, Zhe; Lin, Luojun			SynSig2Vec: Forgery-Free Learning of Dynamic Signature Representations by Sigma Lognormal-Based Synthesis and 1D CNN	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Forgery; Databases; Training; Perturbation methods; Distortion; Deep learning; Data models; Dynamic signature verification and synthesis; Sigma Lognormal; average precision optimization; Sig2Vec	ONLINE; VERIFICATION; RECOGNITION; NETWORKS; STATE	Handwritten signature verification is a challenging task because signatures of a writer may be skillfully imitated by a forger. As skilled forgeries are generally difficult to acquire for training, in this paper, we propose a deep learning-based dynamic signature verification framework, SynSig2Vec, to address the skilled forgery attack without training with any skilled forgeries. Specifically, SynSig2Vec consists of a novel learning-by-synthesis method for training and a 1D convolutional neural network model, called Sig2Vec, for signature representation extraction. The learning-by-synthesis method first applies the Sigma Lognormal model to synthesize signatures with different distortion levels for genuine template signatures, and then learns to rank these synthesized samples in a learnable representation space based on average precision optimization. The representation space is achieved by the proposed Sig2Vec model, which is designed to extract fixed-length representations from dynamic signatures of arbitrary lengths. Through this training method, the Sig2Vec model can extract extremely effective signature representations for verification. Our SynSig2Vec framework requires only genuine signatures for training, yet achieves state-of-the-art performance on the largest dynamic signature database to date, DeepSignDB, in both skilled forgery and random forgery scenarios. Source codes of SynSig2Vec will be available at https://github.com/LaiSongxuan/SynSig2Vec.	[Lai, Songxuan; Jin, Lianwen; Zhu, Yecheng; Li, Zhe] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China; [Jin, Lianwen] Pazhou Lab, Guangdong Artificial Intelligence & Digital Econ, Guangzhou 510335, Peoples R China; [Lin, Luojun] Fuzhou Univ, Sch Math & Comp Sci, Fuzhou 350108, Peoples R China	South China University of Technology; Pazhou Lab; Fuzhou University	Jin, LW (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.	lai.sx@mail.scut.edu.cn; eelwjin@scut.edu.cn; 461438818@qq.com; eelizhe@qq.com; linluojun2009@126.com		Li, Zhe/0000-0002-0372-8895; Jin, Lianwen/0000-0002-5456-0957; lin, luojun/0000-0002-1141-2487	National Natural Science Foundation of China [61936003, 61771199]; Natural Science Foundation of Guangdong Province (GD-NSF) [2017A030312006]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong Province (GD-NSF)(National Natural Science Foundation of Guangdong Province)	This work was supported in part by the National Natural Science Foundation of China (NSFC under Grants 61936003 and 61771199), and the Natural Science Foundation of Guangdong Province (GD-NSF under Grant 2017A030312006).	Ahrabian K, 2019, NEURAL COMPUT APPL, V31, P9321, DOI 10.1007/s00521-018-3844-z; Bhattacharya U, 2017, INT J DOC ANAL RECOG, V20, P155, DOI 10.1007/s10032-017-0287-5; Bhowal P, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03491-4; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Chuang Li, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P526, DOI 10.1109/ICDAR.2019.00090; Diaz M, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3274658; Diaz M, 2018, IEEE T CYBERNETICS, V48, P228, DOI 10.1109/TCYB.2016.2630419; Ferrer M. A., 2019, PROC INT CARNAHAN C, P1; Ferrer MA, 2018, INT CARN CONF SECU, P26; Ferrer MA, 2020, IEEE T PATTERN ANAL, V42, P114, DOI 10.1109/TPAMI.2018.2879312; Ferrer MA, 2018, IEEE T CYBERNETICS, V48, P2896, DOI 10.1109/TCYB.2017.2751740; Ferrer MA, 2017, IEEE T PATTERN ANAL, V39, P1041, DOI 10.1109/TPAMI.2016.2582167; Fierrez J, 2010, PATTERN ANAL APPL, V13, P235, DOI 10.1007/s10044-009-0151-4; Galbally J, 2012, PATTERN RECOGN, V45, P2610, DOI 10.1016/j.patcog.2011.12.011; Guru DS, 2017, EXPERT SYST APPL, V80, P232, DOI 10.1016/j.eswa.2017.03.024; Guru DS, 2009, IEEE T PATTERN ANAL, V31, P1059, DOI 10.1109/TPAMI.2008.302; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Impedovo D, 2008, IEEE T SYST MAN CY C, V38, P609, DOI 10.1109/TSMCC.2008.923866; Klambauer G, 2017, ADV NEUR IN, V30; Lai SX, 2020, AAAI CONF ARTIF INTE, V34, P735; Lai SX, 2019, IEEE T INF FOREN SEC, V14, P1624, DOI 10.1109/TIFS.2018.2883152; Lai SX, 2018, INT CONF FRONT HAND, P175, DOI 10.1109/ICFHR-2018.2018.00039; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Martinez-Diaz M, 2014, IET BIOMETRICS, V3, P267, DOI 10.1049/iet-bmt.2013.0081; O'Reilly C, 2009, PATTERN RECOGN, V42, P3324, DOI 10.1016/j.patcog.2008.10.017; Okawa M, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107699; Okawa M, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107227; Ortega-Garcia J, 2003, IEE P-VIS IMAGE SIGN, V150, P395, DOI 10.1049/ip-vis:20031078; Ortega-Garcia J, 2010, IEEE T PATTERN ANAL, V32, P1097, DOI 10.1109/TPAMI.2009.76; Park C.-Y., 2019, PROC IEEE INT C CONS, P1; PLAMONDON R, 1995, BIOL CYBERN, V72, P295, DOI 10.1007/BF00202785; PLAMONDON R, 1989, PATTERN RECOGN, V22, P107, DOI 10.1016/0031-3203(89)90059-9; Plamondon R, 2000, IEEE T PATTERN ANAL, V22, P63, DOI 10.1109/34.824821; Sae-Bae N, 2018, PATTERN RECOGN, V84, P332, DOI 10.1016/j.patcog.2018.07.024; Sae-Bae N, 2014, IEEE T INF FOREN SEC, V9, P933, DOI 10.1109/TIFS.2014.2316472; Sekhar V Chandra, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1470, DOI 10.1109/ICDAR.2019.00236; Sharma A, 2018, IEEE T CYBERNETICS, V48, P611, DOI 10.1109/TCYB.2017.2647826; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Song Y, 2016, PR MACH LEARN RES, V48; Sundararajan K, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3190618; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tolosana Ruben, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P229, DOI 10.1109/TBIOM.2021.3054533; Tolosana Ruben, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1143, DOI 10.1109/ICDAR.2019.00185; Tolosana R, 2018, IEEE ACCESS, V6, P5128, DOI 10.1109/ACCESS.2018.2793966; Tolosana R, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176792; Triantafillou E, 2017, ADV NEUR IN, V30; Vaswani A, 2017, ADV NEUR IN, V30; Vera-Rodriguez Ruben, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1326, DOI 10.1109/ICDAR.2019.00214; Wu XM, 2019, INT CONF ACOUST SPEE, P2467, DOI 10.1109/ICASSP.2019.8683036; Xia XH, 2017, PATTERN RECOGN, V65, P188, DOI 10.1016/j.patcog.2016.12.019; Xiao W, 2019, 2018 INTERNATIONAL WORKSHOP ON ADVANCES IN SOCIAL SCIENCES (IWASS 2018), P1103, DOI [10.1109/ICDAR.2019.00179, 10.25236/iwass.2018.239]; Yeung DY, 2004, LECT NOTES COMPUT SC, V3072, P16; Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271	54	3	3	3	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6472	6485		10.1109/TPAMI.2021.3087619	http://dx.doi.org/10.1109/TPAMI.2021.3087619			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34101587				2022-12-18	WOS:000853875300045
J	Xu, T; Li, ZN; Yu, Y				Xu, Tian; Li, Ziniu; Yu, Yang			Error Bounds of Imitating Policies and Environments for Reinforcement Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Planning; Reinforcement learning; Cloning; Complexity theory; Supervised learning; Decision making; Upper bound; Imitation learning; behavioral cloning; generative adversarial imitation; model-based reinforcement learning	NEURAL-NETWORKS; GO	In sequential decision-making, imitation learning (IL) trains a policy efficiently by mimicking expert demonstrations. Various imitation methods were proposed and empirically evaluated, meanwhile, their theoretical understandings need further studies, among which the compounding error in long-horizon decisions is a major issue. In this paper, we first analyze the value gap between the expert policy and imitated policies by two imitation methods, behavioral cloning (BC) and generative adversarial imitation. The results support that generative adversarial imitation can reduce the compounding error compared to BC. Furthermore, we establish the lower bounds of IL under two settings, suggesting the significance of environment interactions in IL. By considering the environment transition model as a dual agent, IL can also be used to learn the environment model. Therefore, based on the bounds of imitating policies, we further analyze the performance of imitating environments. The results show that environment models can be more effectively imitated by generative adversarial imitation than BC. Particularly, we obtain a policy evaluation error that is linear with the effective planning horizon w.r.t. the model bias, suggesting a novel application of adversarial imitation for model-based reinforcement learning (MBRL). We hope these results could inspire future advances in IL and MBRL.	[Xu, Tian; Yu, Yang] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China; [Li, Ziniu] Chinese Univ Hong Kong, Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China; [Yu, Yang] Pazhou Lab, Guangzhou 510330, Peoples R China	Nanjing University; Chinese University of Hong Kong, Shenzhen; Pazhou Lab	Yu, Y (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.	xut@lamda.nju.edu.cn; ziniuli@link.cuhk.edu.cn; yuy@nju.edu.cn			National Key R&D Program of China [2020AAA0107200, NSFC 61876077]; Collaborative Innovation Center of Novel Software Technology and Industrialization	National Key R&D Program of China; Collaborative Innovation Center of Novel Software Technology and Industrialization	The authors would like to thank Dr. Weinan Zhang and Zongzhang Zhang for their helpful comments. This work was supported by National Key R&D Program of China under Grants 2020AAA0107200, NSFC 61876077, and Collaborative Innovation Center of Novel Software Technology and Industrialization.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Agarwal A., 2020, PROC 34 INT C NEURAL, P13399; Alemi A. A., 2017, PROC INT C LEARN REP; Arjovsky M, 2017, PR MACH LEARN RES, V70; Asadi K, 2018, PR MACH LEARN RES, V80; Azar MG, 2017, PR MACH LEARN RES, V70; Bagnell J. A., 2021, PROC 38 INT C MACH L, P10022; Cai X.-Q., 2021, PROC 20 INT C AUTON, P279; Chen M., 2020, PROC INT C LEARN REP; Codevilla F, 2018, IEEE INT CONF ROBOT, P4693; Csiszar Imre, 2011, INFORM THEORY CODING, VSecond; Eric Langlois, 2019, Arxiv, DOI arXiv:1907.02057; Fu J., 2018, PROC INT C LEARN REP; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greg Brockman, 2016, Arxiv, DOI arXiv:1606.01540; Gu S., 2019, C ROB LEARN CORL, P1259; Haarnoja T, 2018, PR MACH LEARN RES, V80; Ho J, 2016, ADV NEUR IN, V29; Ho J, 2016, PR MACH LEARN RES, V48; Gulrajani I, 2017, ADV NEUR IN, V30; Janner M, 2019, ADV NEUR IN, V32; Jiang N., 2018, C LEARN THEOR, P3395; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kostrikov I., 2020, PROC INT C LEARN REP; Kostrikov I., 2019, P INT C LEARN REPR; Kurutach T., 2018, P INT C LEARN REPR; Liese F, 2006, IEEE T INFORM THEORY, V52, P4394, DOI 10.1109/TIT.2006.881731; Luo Y., 2019, PROC INT C LEARN REP; McAllester D, 2004, J MACH LEARN RES, V4, P895, DOI 10.1162/1532443041424292; Mohri M., 2018, FDN MACHINE LEARNING; Nowozin S, 2016, ADV NEUR IN, V29; Peng Xingchao, 2019, ARXIV191102054; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rajaraman N., 2020, PROC 34 INT C NEURAL, P2914; Ross S., 2010, PROC 13 INT C ARTIF, V9, P661; Ross St<prime>ephane, 2011, AISTATS; Schrittwieser J, 2020, NATURE, V588, P604, DOI 10.1038/s41586-020-03051-4; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shang WJ, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P566, DOI 10.1145/3292500.3330933; Shi JC, 2019, AAAI CONF ARTIF INTE, P4902; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Syed U., 2010, ADV NEURAL INFORM PR, V23, P2253; Syed U., 2007, ADV NEURAL INFORM PR; Torabi F, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4950; Venkatraman A, 2015, AAAI CONF ARTIF INTE, P3024; Wang R., 2020, ADV NEURAL INFORM PR, P9075; Yang YH, 1999, ANN STAT, V27, P1564; Yu B., 1997, FESTSCHRIFT L LECAM, P423; Yu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5739; Zhang C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3061; Zhang P., 2018, PROC INT C LEARN REP; Zhang Y., 2020, PROC 37 INT C MACH L, P11044	63	3	3	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6968	6980		10.1109/TPAMI.2021.3096966	http://dx.doi.org/10.1109/TPAMI.2021.3096966			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34260348				2022-12-18	WOS:000853875300079
J	Ye, F; Bors, AG				Ye, Fei; Bors, Adrian G.			Lifelong Teacher-Student Network Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Training; Data models; Generative adversarial networks; Probabilistic logic; Neural networks; Linear programming; Lifelong representation learning; variational autoencoders; generative adversarial nets; teacher -student framework		A unique cognitive capability of humans consists in their ability to acquire new knowledge and skills from a sequence of experiences. Meanwhile, artificial intelligence systems are good at learning only the last given task without being able to remember the databases learnt in the past. We propose a novel lifelong learning methodology by employing a Teacher-Student network framework. While the Student module is trained with a new given database, the Teacher module would remind the Student about the information learnt in the past. The Teacher, implemented by a Generative Adversarial Network (GAN), is trained to preserve and replay past knowledge corresponding to the probabilistic representations of previously learnt databases. Meanwhile, the Student module is implemented by a Variational Autoencoder (VAE) which infers its latent variable representation from both the output of the Teacher module as well as from the newly available database. Moreover, the Student module is trained to capture both continuous and discrete underlying data representations across different domains. The proposed lifelong learning framework is applied in supervised, semi-supervised and unsupervised training.	[Ye, Fei; Bors, Adrian G.] Univ York, Dept Comp Sci, York YO10 5GH, N Yorkshire, England	University of York - UK	Bors, AG (corresponding author), Univ York, Dept Comp Sci, York YO10 5GH, N Yorkshire, England.	fy689@york.ac.uk; adrian.borsi@york.ac.uk						Achille A, 2018, ADV NEUR IN, V31; Adriana Romero, 2015, Arxiv, DOI arXiv:1412.6550; Afshin Rostamizadeh, 2009, Arxiv, DOI arXiv:0902.3430; Akagunduz E, 2020, IEEE T PATTERN ANAL, V42, P2165, DOI 10.1109/TPAMI.2019.2914392; Alexander Lerchner, 2018, Arxiv, DOI arXiv:1804.03599; Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753; Andreas S. Tolias, 2019, Arxiv, DOI arXiv:1809.10635; Rusu AA, 2016, Arxiv, DOI arXiv:1606.04671; Anil A Bharath, 2018, Arxiv, DOI arXiv:1711.05175; Arjovsky M, 2017, PR MACH LEARN RES, V70; Ben Poole, 2017, Arxiv, DOI arXiv:1611.01144; Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49; Chen RTQ, 2018, ADV NEUR IN, V31; Chen X, 2016, ADV NEUR IN, V29; Christos Louizos, 2019, Arxiv, DOI arXiv:1905.10427; Cortes C, 2017, PR MACH LEARN RES, V70; Dai W., 2007, PROC INT C MACH LEAR, P193, DOI [10.1145/1273496.1273521, DOI 10.1145/1273496.1273521]; Diederik P Kingma, 2014, Arxiv, DOI arXiv:1312.6114; Fagott J, 2006, P NATL ACAD SCI USA, V103, P17564, DOI 10.1073/pnas.0605184103; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Gan Z., 2017, P ADV NEUR INF P SYS, P4333; Gao SY, 2019, PR MACH LEARN RES, V89; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han Xiao, 2017, Arxiv, DOI arXiv:1708.07747; Heechul Jung, 2016, Arxiv, DOI arXiv:1607.00122; Higgins Irina, 2017, 5 INT C LEARN REPR I; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2014, ADV NEUR IN, V27; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Klys J, 2018, ADV NEUR IN, V31; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Maddison CJ, 2014, ADV NEUR IN, V27; Marco Loog, 2019, Arxiv, DOI arXiv:1812.11806; Mescheder L, 2017, PR MACH LEARN RES, V70; Netzer Yuval, 2011, NEURIPS WORKSH, V2, P6; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012; Part JL, 2017, J IEEE I C DEVELOP L, P304; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Ramapuram J, 2020, NEUROCOMPUTING, V404, P381, DOI 10.1016/j.neucom.2020.02.115; Rao D, 2019, ADV NEUR IN, V32; Redko I., 2019, ADV DOMAIN ADAPTATIO; Redko I., 2020, ARXIV; Ren BY, 2017, APPL SOFT COMPUT, V56, P398, DOI 10.1016/j.asoc.2017.03.005; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T, 2016, ADV NEUR IN, V29; Seff A., 2017, ARXIV; Shin H, 2017, ADV NEUR IN, V30; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Thanh-Tung Hoang, 2020, 2020 INT JOINT C NEU, P1; Wu Chenshen, 2018, ADV NEURAL INFORM PR, P5962; Xiao TJ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P177, DOI 10.1145/2647868.2654926; Zenke F, 2017, PR MACH LEARN RES, V70; Zhou G., 2012, 15 INT C ARTIFICIAL, P1453; Zhou GR, 2018, AAAI CONF ARTIF INTE, P4580	63	3	3	3	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6280	6296		10.1109/TPAMI.2021.3092677	http://dx.doi.org/10.1109/TPAMI.2021.3092677			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34170822	Green Accepted, Green Submitted			2022-12-18	WOS:000853875300033
J	Zhuang, BH; Tan, MK; Liu, J; Liu, LQ; Reid, I; Shen, CH				Zhuang, Bohan; Tan, Mingkui; Liu, Jing; Liu, Lingqiao; Reid, Ian; Shen, Chunhua			Effective Training of Convolutional Neural Networks With Low-Bitwidth Weights and Activations	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Quantization (signal); Neural networks; Stochastic processes; Numerical models; Knowledge engineering; Task analysis; Quantized neural network; progressive quantization; stochastic precision; knowledge distillation; image classification	CLASSIFICATION	This paper tackles the problem of training a deep convolutional neural network of both low-bitwidth weights and activations. Optimizing a low-precision network is very challenging due to the non-differentiability of the quantizer, which may result in substantial accuracy loss. To address this, we propose three practical approaches, including (i) progressive quantization; (ii) stochastic precision; and (iii) joint knowledge distillation to improve the network training. First, for progressive quantization, we propose two schemes to progressively find good local minima. Specifically, we propose to first optimize a network with quantized weights and subsequently quantize activations. This is in contrast to the traditional methods which optimize them simultaneously. Furthermore, we propose a second progressive quantization scheme which gradually decreases the bitwidth from high-precision to low-precision during training. Second, to alleviate the excessive training burden due to the multi-round training stages, we further propose a one-stage stochastic precision strategy to randomly sample and quantize sub-networks while keeping other parts in full-precision. Finally, we adopt a novel learning scheme to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training and significantly improves the performance of the low-precision network. Extensive experiments on various datasets (e.g., CIFAR-100, ImageNet) show the effectiveness of the proposed methods.	[Zhuang, Bohan; Shen, Chunhua] Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia; [Tan, Mingkui; Liu, Jing] South China Univ Technol, Guangzhou 510641, Peoples R China; [Liu, Lingqiao; Reid, Ian; Shen, Chunhua] Univ Adelaide, Adelaide, SA 5005, Australia	Monash University; South China University of Technology; University of Adelaide	Shen, CH (corresponding author), Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia.	bohan.zhuang@monash.edu; mingkuitan@scut.edu.cn; seliujing@mail.scut.edu.cn; lingqiao.liu@adelaide.edu.au; ian.reid@adelaide.edu.au; chunhua.shen@adelaide.edu.au		liu, lingqiao/0000-0003-3584-795X	Key Area Research and Development Program of Guangdong Province [2018B010107001]; Australian Research Council through the Centre of Excellence for Robotic Vision [CE140100016]; Australian Research Council through Laureate Fellowship [FL130100102]	Key Area Research and Development Program of Guangdong Province; Australian Research Council through the Centre of Excellence for Robotic Vision(Australian Research Council); Australian Research Council through Laureate Fellowship(Australian Research Council)	The work of Mingkui Tan was supported in part by the KeyArea Research and Development Program of Guangdong Province underGrant 2018B010107001. We gratefully acknowledge the support of the Australian Research Council through the Centre of Excellence for Robotic Vision CE140100016 and Laureate Fellowship FL130100102 to Ian Reid, Bohan Zhuang, Mingkui Tan, and Jing Liu contributed equally to this work.	Adrian Loy, 2018, Arxiv, DOI arXiv:1812.01965; Adriana Romero, 2015, Arxiv, DOI arXiv:1412.6550; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anubhav Ashok, 2017, Arxiv, DOI arXiv:1709.06030; ARISOTTO E, 2016, 4 INT C LEARN REPR I, P1; Asit Mishra, 2017, Arxiv, DOI arXiv:1711.05852; Barret Zoph, 2017, Arxiv, DOI arXiv:1611.01578; Bengio Y., 2013, ARXIV; Cai Han, 2019, INT C LEARN REPR; Cai ZW, 2017, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR.2017.574; Chen GB, 2017, ADV NEUR IN, V30; Choi Y., 2020, EARLY ACCESS, DOI [10.1109/ACCESS.2020.2996936, DOI 10.1109/ACCESS.2020.2996936]; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Christian Bartz, 2018, Arxiv, DOI arXiv:1809.10463; Courbariaux M, 2015, ADV NEUR IN, V28; Deepika Bablani, 2020, Arxiv, DOI arXiv:1902.08153; Ding RZ, 2019, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR.2019.01167; Dong XY, 2019, ADV NEUR IN, V32; Dong Y., 2017, PROC BRIT MACH VIS C; Dongjun Shin, 2016, Arxiv, DOI arXiv:1511.06530; Dongyoung Kim, 2018, Arxiv, DOI arXiv:1812.09818; Edo Liberty, 2019, Arxiv, DOI arXiv:1810.00861; Forrest N. Iandola, 2016, Arxiv, DOI arXiv:1602.07360; Goodfellow Ian J., 2013, ICML; Guo YW, 2017, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2017.430; Hanxiao Liu, 2019, Arxiv, DOI arXiv:1806.09055; Haoyuan Mu, 2020, Arxiv, DOI arXiv:1904.00420; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He T, 2019, PROC CVPR IEEE, P578, DOI 10.1109/CVPR.2019.00067; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hinton G., 2015, NIPS WORKSH, P1; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Hubara I, 2016, ADV NEUR IN, V29; Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P288, DOI 10.1007/978-3-030-11021-5_19; James T. Kwok, 2018, Arxiv, DOI arXiv:1802.08635; Jiahui Yu, 2018, Arxiv, DOI arXiv:1812.08928; Jung S, 2019, PROC CVPR IEEE, P4345, DOI 10.1109/CVPR.2019.00448; Jungwook Choi, 2018, Arxiv, DOI arXiv:1805.06085; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li H., 2017, P INT C LEARN REPR I, P1; Lin XF, 2017, ADV NEUR IN, V30; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Louizos Christos, 2019, INT C LEARN REPR; Nikos Komodakis, 2017, Arxiv, DOI arXiv:1612.03928; Park E, 2017, PROC CVPR IEEE, P7197, DOI 10.1109/CVPR.2017.761; Pham H, 2018, PR MACH LEARN RES, V80; Polino A., 2018, ARXIV180205668; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Smith LN, 2016, PROC CVPR IEEE, P4763, DOI 10.1109/CVPR.2016.515; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tang W, 2017, AAAI CONF ARTIF INTE, P2625; Tung F, 2018, PROC CVPR IEEE, P7873, DOI 10.1109/CVPR.2018.00821; Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wei Y, 2018, LECT NOTES COMPUT SC, V11212, P274, DOI 10.1007/978-3-030-01237-3_17; Yang HJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1209, DOI 10.1145/3123266.3129393; Yu JH, 2019, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2019.00189; Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958; Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579; Zhou A., 2017, PROC INT C LEARN REP; Zhou S., 2016, ARXIV; Zhu Chenzhuo, 2017, 5 INT C LEARN REPR I; Zhuang BH, 2019, PROC CVPR IEEE, P413, DOI 10.1109/CVPR.2019.00050; Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826; Zhuang ZW, 2018, ADV NEUR IN, V31; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	78	3	3	5	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6140	6152		10.1109/TPAMI.2021.3088904	http://dx.doi.org/10.1109/TPAMI.2021.3088904			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34125669	Green Submitted			2022-12-18	WOS:000853875300023
J	Bucci, S; D'Innocente, A; Liao, YJ; Carlucci, FM; Caputo, B; Tommasi, T				Bucci, Silvia; D'Innocente, Antonio; Liao, Yujun; Carlucci, Fabio Maria; Caputo, Barbara; Tommasi, Tatiana			Self-Supervised Learning Across Domains	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Visualization; Indexes; Adaptation models; Data models; Training; Image recognition; Self-supervision; domain generalization; domain adaptation; multi-task learning		Human adaptability relies crucially on learning and merging knowledge from both supervised and unsupervised tasks: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the problem of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals on the same images. This secondary task helps the network to focus on object shapes, learning concepts like spatial orientation and part correlation, while acting as a regularizer for the classification task over multiple visual domains. Extensive experiments confirm our intuition and show that our multi-task method, combining supervised and self-supervised knowledge, provides competitive results with respect to more complex domain generalization and adaptation solutions. It also proves its potential in the novel and challenging predictive and partial domain adaptation scenarios.	[Bucci, Silvia; Liao, Yujun; Caputo, Barbara; Tommasi, Tatiana] Politecn Torino, I-10129 Turin, Italy; [Bucci, Silvia; D'Innocente, Antonio; Caputo, Barbara; Tommasi, Tatiana] Italian Inst Technol, I-16132 Genoa, Italy; [D'Innocente, Antonio] Univ Rome Sapienza, I-00185 Rome, Italy; [Carlucci, Fabio Maria] Huawei Noahs Ark Labs, London N1C 4AG, England	Polytechnic University of Turin; Istituto Italiano di Tecnologia - IIT; Sapienza University Rome	Tommasi, T (corresponding author), Politecn Torino, I-10129 Turin, Italy.	silvia.bucci@polito.it; dinnocente@diag.uniroma1.it; s274673@studenti.polito.it; fabio.maria.carlucci@huawei.com; barbara.caputo@polito.it; tatiana.tommasi@polito.it			ERC [637076]	ERC(European Research Council (ERC)European Commission)	work was supported in part by the ERC Grant 637076 RoboExNovo (BC, SB, AD) and took advantage of the GPU donated by NVIDIA (Academic Hardware Grant, TT).	Antonio Alliegro, 2020, Arxiv, DOI arXiv:2004.07392; Antonio D'Innocente, 2020, Arxiv, DOI arXiv:2005.11610; Asano Yuki Markus, 2020, P ICLR; Balaji Y, 2018, ADV NEUR IN, V31; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bisanz Jeffrey, 1983, LEARNING CHILDREN PR; Bousmalis K, 2016, ADV NEUR IN, V29; Bucci S, 2019, LECT NOTES COMPUT SC, V11752, P70, DOI 10.1007/978-3-030-30645-8_7; Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9; Cao ZJ, 2019, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR.2019.00310; Cao ZJ, 2018, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR.2018.00288; Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233; Carlucci FM, 2017, LECT NOTES COMPUT SC, V10484, P357, DOI 10.1007/978-3-319-68560-1_32; Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Csurka G, 2017, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-58347-1; D'Innocente Antonio, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P187, DOI 10.1007/978-3-030-12939-2_14; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding ZM, 2018, IEEE T IMAGE PROCESS, V27, P304, DOI 10.1109/TIP.2017.2758199; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Dou Q, 2019, ADV NEUR IN, V32; Ferguson B, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190185; Ganin Y, 2016, J MACH LEARN RES, V17; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Gidaris S, 2020, PROC CVPR IEEE, P6926, DOI 10.1109/CVPR42600.2020.00696; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Gulrajani Ishaan, 2021, ICLR; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50; Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521; Jang E., 2018, PROC 2 C ROBOT LEARN; Jenni S, 2020, PROC CVPR IEEE, P6407, DOI 10.1109/CVPR42600.2020.00644; Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393; Kuniaki Saito, 2018, Arxiv, DOI arXiv:1812.07405; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee MA, 2019, IEEE INT CONF ROBOT, P8943, DOI 10.1109/ICRA.2019.8793485; Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153; Li D, 2018, AAAI CONF ARTIF INTE, P3490; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li S, 2021, IEEE T PATTERN ANAL, V43, P2329, DOI 10.1109/TPAMI.2020.2964173; Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Mancini M, 2019, PROC CVPR IEEE, P6561, DOI 10.1109/CVPR.2019.00673; Mancini M, 2018, PROC CVPR IEEE, P3771, DOI 10.1109/CVPR.2018.00397; Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Muandet K., 2013, INT C MACH LEARN, P10; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Noroozi M, 2018, PROC CVPR IEEE, P9359, DOI 10.1109/CVPR.2018.00975; Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Ren ZZ, 2018, PROC CVPR IEEE, P762, DOI [10.1109/CVPR.2018.00086, 10.1109/CVPR.2018.00104]; Rosenstein Michael T, 2005, NIPS 2005 WORKSH TRA, V898, P1; Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10; Santa Cruz R, 2019, IEEE T PATTERN ANAL, V41, P3100, DOI 10.1109/TPAMI.2018.2873701; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Sermanet P, 2018, IEEE INT CONF ROBOT, P1134; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Volpi Riccardo, 2018, ARXIV180512018; Wang HH, 2019, ADV NEUR IN, V32; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wikipedia, INT WIK FREE ENCY; Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151; Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417; Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023; Yang YX, 2016, PROC CVPR IEEE, P5071, DOI 10.1109/CVPR.2016.548; Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28; Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229; Zhang J, 2018, PROC CVPR IEEE, P8156, DOI 10.1109/CVPR.2018.00851; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang YX, 2020, IEEE ACCESS, V8, P63748, DOI 10.1109/ACCESS.2020.2984279; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025	87	3	3	16	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5516	5528		10.1109/TPAMI.2021.3070791	http://dx.doi.org/10.1109/TPAMI.2021.3070791			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33798074	Green Submitted			2022-12-18	WOS:000836666600072
J	Chen, LH; Sun, JM; Xie, YM; Zhang, SY; Shuai, Q; Jiang, QH; Zhang, GF; Bao, HJ; Zhou, XW				Chen, Linghao; Sun, Jiaming; Xie, Yiming; Zhang, Siyu; Shuai, Qing; Jiang, Qinhong; Zhang, Guofeng; Bao, Hujun; Zhou, Xiaowei			Shape Prior Guided Instance Disparity Estimation for 3D Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Estimation; Object detection; Shape; Solid modeling; Laser radar; Image reconstruction; Autonomous driving; 3D detection; stereo matching		In this paper, we propose a novel system named Disp R-CNN for 3D object detection from stereo images. Many recent works solve this problem by first recovering point clouds with disparity estimation and then apply a 3D detector. The disparity map is computed for the entire image, which is costly and fails to leverage category-specific prior. In contrast, we design an instance disparity estimation network (iDispNet) that predicts disparity only for pixels on objects of interest and learns a category-specific shape prior for more accurate disparity estimation. To address the challenge from scarcity of disparity annotation in training, we propose to use a statistical shape model to generate dense disparity pseudo-ground-truth without the need of LiDAR point clouds, which makes our system more widely applicable. Experiments on the KITTI dataset show that, when LiDAR ground-truth is not used at training time, Disp R-CNN outperforms previous state-of-the-art methods based on stereo input by 20 percent in terms of average precision for all categories. The code and pseudo-ground-truth data are available at the project page: https://github.com/zju3dv/disprcnn.	[Chen, Linghao; Xie, Yiming; Shuai, Qing; Zhang, Guofeng; Bao, Hujun; Zhou, Xiaowei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China; [Sun, Jiaming; Zhang, Siyu; Jiang, Qinhong] SenseTime, Hangzhou 311215, Peoples R China	Zhejiang University	Zhou, XW (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.	chenlinghao@zju.edu.cn; suenjiaming@gmail.com; yimingxie@zju.edu.cn; zhangsiyu1@sensetime.com; s_q@zju.edu.cn; jiangqinhong@sensetime.com; zhangguofeng@cad.zju.edu.cn; bao@cad.zju.edu.cn; xwzhou@zju.edu.cn			National Key Research and Development Program of China [2020AAA0108901]; NSFC [61806176]; ZJU-SenseTime Joint Lab of 3D Vision	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); ZJU-SenseTime Joint Lab of 3D Vision	This work was supported in part by the National Key Research and Development Program of China under Grant 2020AAA0108901, in part by the NSFC under Grant 61806176, and in part by the ZJU-SenseTime Joint Lab of 3D Vision. Linghao Chen and Jiaming Sun are contributed equally.	Alex H. Lang, 2020, Arxiv, DOI arXiv:1903.11027; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Chen XZ, 2015, ADV NEUR IN, V28; Engelmann F, 2016, LECT NOTES COMPUT SC, V9796, P219, DOI 10.1007/978-3-319-45886-1_18; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Jiaming Sun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10545, DOI 10.1109/CVPR42600.2020.01056; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim W, 2019, IEEE ROBOT AUTOM LET, V4, P1940, DOI 10.1109/LRA.2019.2896705; Konigshofe H, 2019, IEEE INT C INTELL TR, P1405, DOI 10.1109/ITSC.2019.8917330; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Leventon M. E., 2002, P 5 IEEE EMBS INT SU; Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Manhardt F, 2019, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2019.00217; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Mierle Keir, 2012, CERES SOLVER; Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597; Murthy JK, 2017, IEEE INT C INT ROBOT, P1768; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Peng W, 2020, P IEEE CVF C COMP VI; Pon AD, 2020, IEEE INT CONF ROBOT, P8383, DOI 10.1109/ICRA40945.2020.9196660; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Qi L, 2019, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2019.00313; Qian R, 2020, PROC CVPR IEEE, P5880, DOI 10.1109/CVPR42600.2020.00592; Qin ZY, 2019, PROC CVPR IEEE, P7607, DOI 10.1109/CVPR.2019.00780; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rui Wang, 2020, 2020 IEEE International Conference on Robotics and Automation (ICRA), P11067, DOI 10.1109/ICRA40945.2020.9197095; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249; Xu ZB, 2020, AAAI CONF ARTIF INTE, V34, P12557; Yang GR, 2019, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2019.00099; Yilun Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12533, DOI 10.1109/CVPR42600.2020.01255; You Y., 2020, PROC INT C LEARN REP; Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027	45	3	3	10	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5529	5540		10.1109/TPAMI.2021.3076678	http://dx.doi.org/10.1109/TPAMI.2021.3076678			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33914683				2022-12-18	WOS:000836666600073
J	Wang, LN; Xie, SN; Li, T; Fonseca, R; Tian, YD				Wang, Linnan; Xie, Saining; Li, Teng; Fonseca, Rodrigo; Tian, Yuandong			Sample-Efficient Neural Architecture Search by Learning Actions for Monte Carlo Tree Search	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Neural architecture search; Monte Carlo tree search	GLOBAL OPTIMIZATION	Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0 percent accuracy on CIFAR-10 and 80.8 percent top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33 x fewer samples. Our code is publicly available at https://github.com/facebookresearch/LaMCTS.	[Wang, Linnan; Fonseca, Rodrigo] Brown Univ, Dept Comp Sci, Providence, RI 02905 USA; [Xie, Saining; Li, Teng; Tian, Yuandong] Facebook AI Res, Menlo Pk, CA 94025 USA	Brown University; Facebook Inc	Wang, LN (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02905 USA.	wangnan318@gmail.com; s9xie@fb.com; tengli@fb.com; rfonseca@cs.brown.edu; yuandong.tian@gmail.com						Aaron Klein, 2018, Arxiv, DOI arXiv:1807.01774; Alexander Kirillov, 2019, Arxiv, DOI arXiv:1904.01569; Alok Aggarwal, 2019, Arxiv, DOI arXiv:1802.01548; Ameet Talwalkar, 2019, Arxiv, DOI arXiv:1902.07638; Ankur Bapna, 2019, Arxiv, DOI arXiv:1811.06965; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barret Zoph, 2017, Arxiv, DOI arXiv:1611.01578; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Bichen Wu, 2019, Arxiv, DOI arXiv:1812.03443; Bowen Baker, 2017, Arxiv, DOI arXiv:1611.02167; Busoniu L, 2013, IEEE SYMP ADAPT DYNA, P69, DOI 10.1109/ADPRL.2013.6614991; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Christian Sciuto, 2019, Arxiv, DOI arXiv:1902.08142; Chuang Gan, 2020, Arxiv, DOI arXiv:1908.09791; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; GILKS WR, 1995, J R STAT SOC C-APPL, V44, P455, DOI 10.2307/2986138; Gorur D, 2011, J COMPUT GRAPH STAT, V20, P670, DOI 10.1198/jcgs.2011.09058; Guo Y., 2020, INTERNA TIONAL C MAC, P3822; Guo Zichao, 2019, ARXIV190400420; Guo-Jun Qi, 2020, Arxiv, DOI arXiv:1907.05737; Hanxiao Liu, 2019, Arxiv, DOI arXiv:1806.09055; HORMANN W, 1995, ACM T MATH SOFTWARE, V21, P182, DOI 10.1145/203082.203089; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Hutter F., 2013, P 15 ANN C COMP GEN, P1209; Hutter F., 2009, THESIS U BRIT COLUMB; Jiahui Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P702, DOI 10.1007/978-3-030-58571-6_41; Kento Uchida, 2019, Arxiv, DOI arXiv:1905.08537; Li L., 2016, ARXIV; Linnan Wang, 2019, Arxiv, DOI arXiv:1903.11059; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Luo RQ, 2018, ADV NEUR IN, V31; MALLOWS C, 1991, AM STAT, V45, P257; Mansley C., 2011, P INT C AUT PLAN SCH; Sazanovich M, 2020, Arxiv, DOI arXiv:2012.10335; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Nayman N, 2019, ADV NEUR IN, V32; Pham H, 2018, PR MACH LEARN RES, V80; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Tan M., 2019, ARXIV; Tian Y., ARXIV; Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2; Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298; Wang L., 2020, NEURIPS; Wang Z., 2013, INT JOINT C ART INT; Wang ZY, 2014, JMLR WORKSH CONF PRO, V33, P1005; Weinstein Ari, 2012, P 22 INT C AUT PLANN; Xiangxiang Chu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P465, DOI 10.1007/978-3-030-58555-6_28; Dai XL, 2021, Arxiv, DOI arXiv:2006.02049; Zhao YY, 2020, Arxiv, DOI arXiv:2006.06863; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	58	3	3	4	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5503	5515		10.1109/TPAMI.2021.3071343	http://dx.doi.org/10.1109/TPAMI.2021.3071343			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33826511				2022-12-18	WOS:000836666600071
J	Zheng, YJ; Sui, XD; Jiang, YY; Che, OT; Zhang, ST; Yang, J; Li, HS				Zheng, Yuanjie; Sui, Xiaodan; Jiang, Yanyun; Che, Tontong; Zhang, Shaoting; Yang, Jie; Li, Hongsheng			SymReg-GAN: Symmetric Image Registration With Generative Adversarial Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image registration; Generative adversarial networks; Generators; Estimation; Training; Magnetic resonance imaging; Image resolution; Symmetric registration; generative adversarial networks; multimodal image registration	LEARNING FRAMEWORK; AFFINE	Symmetric image registration estimates bi-directional spatial transformations between images while enforcing an inverse-consistency. Its capability of eliminating bias introduced inevitably by generic single-directional image registration allows more precise analysis in different interdisciplinary applications of image registration, e.g., computational anatomy and shape analysis. However, most existing symmetric registration techniques especially for multimodal images are limited by low speed from the commonly-used iterative optimization, hardship in exploring inter-modality relations or high labor cost for labeling data. We propose SymReg-GAN to shatter these limits, which is a novel generative adversarial networks (GAN) based approach to symmetric image registration. We formulate symmetric registration of unimodal/multimodal images as a conditional GAN and train it with a semi-supervised strategy. The registration symmetry is realized by introducing a loss for encouraging that the cycle composed of the geometric transformation from one image to another and its reverse should bring an image back. The semi-supervised learning enables both the precious labeled data and large amounts of unlabeled data to be fully exploited. Experimental results from six public brain magnetic resonance imaging (MRI) datasets and 1 our own computed tomography (CT) and MRI dataset demonstrate the superiority of SymReg-GAN to several existing state-of-the-art methods.	[Zheng, Yuanjie; Sui, Xiaodan; Jiang, Yanyun] Shandong Normal Univ, Shandong Prov Key Lab Novel Distributed Comp Soft, Key Lab Intelligent Comp & Informat Secur Univ Sh, Shandong Key Lab Med Phys & Image Proc,Sch Inform, Jinan 250014, Peoples R China; [Zheng, Yuanjie; Sui, Xiaodan; Jiang, Yanyun] Shandong Normal Univ, Inst Biomed Sci, Jinan 250014, Peoples R China; [Che, Tontong] BUAA, Sch Biol Sci & Med Engn, Beijing 100191, Peoples R China; [Zhang, Shaoting] SenseTime Res, Beijing 100080, Peoples R China; [Yang, Jie] Shanghai Jiao Tong Univ, Sch Elect & Informat, Shanghai 200240, Peoples R China; [Li, Hongsheng] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China	Shandong Normal University; Shandong Normal University; Beihang University; Shanghai Jiao Tong University; Chinese University of Hong Kong	Zheng, YJ (corresponding author), Shandong Normal Univ, Shandong Prov Key Lab Novel Distributed Comp Soft, Key Lab Intelligent Comp & Informat Secur Univ Sh, Shandong Key Lab Med Phys & Image Proc,Sch Inform, Jinan 250014, Peoples R China.; Zheng, YJ (corresponding author), Shandong Normal Univ, Inst Biomed Sci, Jinan 250014, Peoples R China.	yjzheng@sdnu.edu.cn; xiaodan.sui@gq.com; yanyun.jiang@gq.com; ctt_0624@sina.com; zhangshaoting@sensetime.com; jieyang@sjtu.edu.cn; hsli@ee.cuhk.edu.hk			National Natural Science Foundation of China [81871508, 61773246]; Major Program of Shandong Province Natural Science Foundation [ZR2019ZD04, ZR2018ZB0419]; Taishan Scholar Program of Shandong Province of China [TSHW201502038]; theNational Key R&DProgram of China [2019YFB1311503]; Committee of Science and Technology, Shanghai, China [19510711200]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Major Program of Shandong Province Natural Science Foundation; Taishan Scholar Program of Shandong Province of China; theNational Key R&DProgram of China; Committee of Science and Technology, Shanghai, China	This work was supported by the National Natural Science Foundation of China under Grants 81871508 and No. 61773246, Major Program of Shandong Province Natural Science Foundation under Grants ZR2019ZD04 and No. ZR2018ZB0419, Taishan Scholar Program of Shandong Province of China under Grant TSHW201502038 and its second round support, and in part by theNational Key R&DProgram of China under Grant 2019YFB1311503, and Committee of Science and Technology, Shanghai, China under Grant 19510711200.	ANUTA PE, 1969, SOC PHOTO-OPT INSTRU, V7, P168; Avants BB, 2008, MED IMAGE ANAL, V12, P26, DOI 10.1016/j.media.2007.06.004; Ba J., 2017, P 3 INT C LEARN REPR; Bakas S., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1811.02629; Balakrishnan G, 2019, IEEE T MED IMAGING, V38, P1788, DOI 10.1109/TMI.2019.2897538; Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964; Christensen GE, 2001, IEEE T MED IMAGING, V20, P568, DOI 10.1109/42.932742; Christine Tanner, 2018, Arxiv, DOI arXiv:1807.07349; Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49; de Vos BD, 2019, MED IMAGE ANAL, V52, P128, DOI 10.1016/j.media.2018.11.010; Duan LW, 2019, BIOMED SIGNAL PROCES, V53, DOI 10.1016/j.bspc.2019.101562; Fan JF, 2019, MED IMAGE ANAL, V54, P193, DOI 10.1016/j.media.2019.03.006; Fan JF, 2018, LECT NOTES COMPUT SC, V11070, P739, DOI 10.1007/978-3-030-00928-1_83; Ferrante E, 2018, INT J COMPUT VISION, V126, P36, DOI 10.1007/s11263-017-1040-8; Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021; Glocker B, 2008, MED IMAGE ANAL, V12, P731, DOI 10.1016/j.media.2008.03.006; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gousias IS, 2012, NEUROIMAGE, V62, P1499, DOI 10.1016/j.neuroimage.2012.05.083; Haber E, 2007, INT J COMPUT VISION, V71, P361, DOI 10.1007/s11263-006-8984-4; Hu YP, 2018, LECT NOTES COMPUT SC, V11070, P774, DOI 10.1007/978-3-030-00928-1_87; Hu YP, 2018, I S BIOMED IMAGING, P1070; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaderberg M, 2015, ADV NEUR IN, V28; Jenkinson M, 2001, MED IMAGE ANAL, V5, P143, DOI 10.1016/S1361-8415(01)00036-6; Jenkinson M, 2012, NEUROIMAGE, V62, P782, DOI 10.1016/j.neuroimage.2011.09.015; Jiang P, 2018, PROC CVPR IEEE, P9281, DOI 10.1109/CVPR.2018.00967; Johnson HJ, 2002, IEEE T MED IMAGING, V21, P450, DOI 10.1109/TMI.2002.1009381; Jun Zhang, 2018, Arxiv, DOI arXiv:1809.03443; Kim B, 2019, LECT NOTES COMPUT SC, V11769, P166, DOI 10.1007/978-3-030-32226-7_19; Krebs J., 2017, LNCS, P344, DOI DOI 10.1007/978-3-319-66182-7_40; Lai WS, 2017, ADV NEUR IN, V30; Lee D, 2009, PROC CVPR IEEE, P186, DOI 10.1109/CVPRW.2009.5206840; LeMoigne J, 2011, IMAGE REGISTRATION FOR REMOTE SENSING, P1; Li HM, 2018, I S BIOMED IMAGING, P1075, DOI 10.1109/ISBI.2018.8363757; Liu RSN, 2003, NEUROIMAGE, V20, P22, DOI 10.1016/S1053-8119(03)00219-2; Lombaert H, 2014, INT J COMPUT VISION, V107, P254, DOI 10.1007/s11263-013-0681-5; Mathieu M., 2016, INT C LEARN REPR ICL; Niethammer M, 2019, PROC CVPR IEEE, P8455, DOI [10.1109/CVPR.2019.00866, 10.1109/cvpr.2019.00866]; Rengarajan V, 2017, IEEE T PATTERN ANAL, V39, P1959, DOI 10.1109/TPAMI.2016.2630687; Rogelj P, 2006, MED IMAGE ANAL, V10, P484, DOI 10.1016/j.media.2005.03.003; Roh M.-M., 2017, INT C MEDICAL IMAGE, P266, DOI DOI 10.1007/978-3-319-66182-7_31; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Sedghi A, 2019, PROC MED IMAG IMAGE; Shen DG, 2001, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P29, DOI 10.1109/MMBIA.2001.991696; Shen ZY, 2019, PROC CVPR IEEE, P4219, DOI [10.1109/CVPR.2019.00435, 10.1109/cvpr.2019.00435]; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sokooti H, 2017, LECT NOTES COMPUTER, V10433, P232, DOI 10.1007/978-3-319-66182-7_27; Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603; Thompson WK, 2011, NEUROIMAGE, V57, P1, DOI 10.1016/j.neuroimage.2010.11.092; Vercauteren T, 2008, LECT NOTES COMPUT SC, V5241, P754, DOI 10.1007/978-3-540-85988-8_90; Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040; Woo J, 2015, IEEE T IMAGE PROCESS, V24, P757, DOI 10.1109/TIP.2014.2387019; Yan PK, 2018, LECT NOTES COMPUT SC, V11046, P197, DOI 10.1007/978-3-030-00919-9_23; Yang X, 2017, NEUROIMAGE, V158, P378, DOI 10.1016/j.neuroimage.2017.07.008; Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1; Zhang ZZ, 2018, PROC CVPR IEEE, P9242, DOI 10.1109/CVPR.2018.00963; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	58	3	3	19	33	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5631	5646		10.1109/TPAMI.2021.3083543	http://dx.doi.org/10.1109/TPAMI.2021.3083543			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	34033536	Bronze			2022-12-18	WOS:000836666600080
J	Zhou, SL				Zhou, Shenglong			Sparse SVM for Sufficient Data Reduction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Support vector machines; Training; Optimization; Kernel; Fasteners; Convergence; Computational efficiency; Data reduction; sparsity constrained kernel SVM; Newton method; one-step convergence property	SUPPORT VECTOR MACHINE; OPTIMALITY CONDITIONS; CLASSIFIER; REGRESSION; SPEED; OPTIMIZATION	Kernel-based methods for support vector machines (SVM) have shown highly advantageous performance in various applications. However, they may incur prohibitive computational costs for large-scale sample datasets. Therefore, data reduction (reducing the number of support vectors) appears to be necessary, which gives rise to the topic of the sparse SVM. Motivated by this problem, the sparsity constrained kernel SVM optimization has been considered in this paper in order to control the number of support vectors. Based on the established optimality conditions associated with the stationary equations, a Newton-type method is developed to handle the sparsity constrained optimization. This method is found to enjoy the one-step convergence property if the starting point is chosen to be close to a local region of a stationary point, thereby leading to a super-high computational speed. Numerical comparisons with several powerful solvers demonstrate that the proposed method performs exceptionally well, particularly for large-scale datasets in terms of a much lower number of support vectors and shorter computational time.	[Zhou, Shenglong] Imperial Coll London, Dept Elect & Elect Engn, London SW7 2BX, England	Imperial College London	Zhou, SL (corresponding author), Imperial Coll London, Dept Elect & Elect Engn, London SW7 2BX, England.	slzhou2021@163.com			National Science Foundation of China [11971052]	National Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the the National Science Foundation of China (11971052). The author would like to thank the two Referees and Professor Naihua Xiu from Beijing Jiaotong University for their valuable suggestions to improve this paper.	Beck A, 2013, SIAM J OPTIMIZ, V23, P1480, DOI 10.1137/120869778; Bi J., 2003, Journal of Machine Learning Research, V3, P1229, DOI 10.1162/153244303322753643; Burges C. J. C., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P71; Burges CJC, 1997, ADV NEUR IN, V9, P375; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Collobert R, 2002, NEURAL COMPUT, V14, P1105, DOI 10.1162/089976602753633402; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cotter A., 2013, PROC INT C MACH LEAR, P266; Dekel O., 2006, ADV NEURAL INFORM PR, P259; Dung DN, 2010, IEEE T NEURAL NETWOR, V21, P1903, DOI 10.1109/TNN.2010.2079947; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Graf H., 2004, ADV NEURAL INFORM PR, V17, P521; Huang XL, 2017, IEEE T NEUR NET LEAR, V28, P1584, DOI 10.1109/TNNLS.2016.2547324; Huang XL, 2014, IEEE T PATTERN ANAL, V36, P984, DOI 10.1109/TPAMI.2013.178; Jumutc V, 2013, IEEE IJCNN; Keerthi SS, 2006, J MACH LEARN RES, V7, P1493; Koggalage R., 2004, NEURAL INF PROCESS L, V2, P57; Lee Y.-J., 2001, P 2001 SIAM INT C DA, P1; Lin KM, 2003, IEEE T NEURAL NETWOR, V14, P1449, DOI 10.1109/TNN.2003.820828; Liu ZQ, 2019, ARTIF INTELL MED, V96, P134, DOI 10.1016/j.artmed.2019.04.004; Lopez J., 2011, P EUR S ART NEUR NET, P189; Mason L, 2000, MACH LEARN, V38, P243, DOI 10.1023/A:1007697429651; Mason L, 2000, ADV NEUR IN, V12, P512; Osuna EE, 1999, ADVANCES IN KERNEL METHODS, P271; Pan LL, 2017, SCI CHINA MATH, V60, P759, DOI 10.1007/s11425-016-9010-x; Panda N., 2006, 23 INT C MACHINE LEA, P681; Pelckmans K., 2002, LSSVM LAB MATLAB C T; Perez-Cruz F, 2003, IEEE T NEURAL NETWOR, V14, P296, DOI 10.1109/TNN.2003.809399; Perez-Cruz F, 2000, INT CONF ACOUST SPEE, P3458, DOI 10.1109/ICASSP.2000.860145; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370; Shen X, 2017, PATTERN RECOGN, V68, P199, DOI 10.1016/j.patcog.2017.03.011; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Tan M, 2010, P 27 INT C MACH LEAR, P1047; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang L, 2008, BIOINFORMATICS, V24, P412, DOI 10.1093/bioinformatics/btm579; Wang SZ, 2014, APPL INTELL, V41, P405, DOI 10.1007/s10489-014-0524-2; Williams CKI, 2001, ADV NEUR IN, V13, P682; Wu M, 2005, P 22 INT C MACH LEAR, P996; Wu YC, 2007, J AM STAT ASSOC, V102, P974, DOI 10.1198/016214507000000617; Xu YY, 2016, PATTERN ANAL APPL, V19, P989, DOI 10.1007/s10044-015-0485-z; Yang LM, 2018, CHEMOMETR INTELL LAB, V177, P89, DOI 10.1016/j.chemolab.2018.04.003; Yang XW, 2014, NEUROCOMPUTING, V140, P41, DOI 10.1016/j.neucom.2014.03.037; Yin J, 2019, COMPUT OPTIM APPL, V73, P477, DOI 10.1007/s10589-019-00075-z; Zhan YQ, 2005, PATTERN RECOGN, V38, P157, DOI 10.1016/j.patcog.2004.06.001; Zhanial S, 2020, CONTEMP CINEMA, P1	50	3	3	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5560	5571		10.1109/TPAMI.2021.3075339	http://dx.doi.org/10.1109/TPAMI.2021.3075339			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33891547	Green Submitted			2022-12-18	WOS:000836666600075
J	Wang, JF; Hu, XL				Wang, Jianfeng; Hu, Xiaolin			Convolutional Neural Networks With Gated Recurrent Connections	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Radio frequency; Neurons; Logic gates; Convolution; Task analysis; Computational modeling; Optical character recognition software; Gated recurrent convolution neural network (GRCNN); gated recurrent convolution layer (GRCL); object recognition; object detection; scene text recognition	RECEPTIVE FIELDS; RECOGNITION	The convolutional neural network (CNN) has become a basic model for solving many computer vision problems. In recent years, a new class of CNNs, recurrent convolution neural network (RCNN), inspired by abundant recurrent connections in the visual systems of animals, was proposed. The critical element of RCNN is the recurrent convolutional layer (RCL), which incorporates recurrent connections between neurons in the standard convolutional layer. With increasing number of recurrent computations, the receptive fields (RFs) of neurons in RCL expand unboundedly, which is inconsistent with biological facts. We propose to modulate the RFs of neurons by introducing gates to the recurrent connections. The gates control the amount of context information inputting to the neurons and the neurons' RFs therefore become adaptive. The resulting layer is called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a deep model called gated RCNN (GRCNN). The GRCNN was evaluated on several computer vision tasks including object recognition, scene text recognition and object detection, and obtained much better results than the RCNN. In addition, when combined with other adaptive RF techniques, the GRCNN demonstrated competitive performance to the state-of-the-art models on benchmark datasets for these tasks.	[Wang, Jianfeng] Univ Oxford, Comp Sci Dept, Oxford OX1 2JD, England; [Hu, Xiaolin] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence, Beijing 100084, Peoples R China; [Hu, Xiaolin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China	University of Oxford; Tsinghua University; Tsinghua University	Hu, XL (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence, Beijing 100084, Peoples R China.; Hu, XL (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.	jianfeng.wang@cs.ox.ac.uk; xlhu@tsinghua.edu.cn			National Key Research and Development Program of China [2017YFA0700904]; National Natural Science Foundation of China [61836014, U19B2034, 62061136001]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904) and the National Natural Science Foundation of China (Nos. 61836014, U19B2034, and 62061136001).	Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814; Alsharif Ouais, 2013, ARXIV13101811 CORR; Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102; Cavanaugh JR, 2002, J NEUROPHYSIOL, V88, P2530, DOI 10.1152/jn.00692.2001; Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584; Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543; Cho K., 2014, P 2014 C EMP METH NA, P1724; Cordonnier Jean Baptiste, 2020, INT C LEARN REPR ICL; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dayan P, 2001, THEORETICAL NEUROSCI; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Fu C. -Y., 2017, ARXIV17010665; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Goel V, 2013, PROC INT CONF DOC, P398, DOI 10.1109/ICDAR.2013.87; Gordo A, 2015, PROC CVPR IEEE, P2956, DOI 10.1109/CVPR.2015.7298914; Graves A., 2006, P INT C MACH LEARN I; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu JS, 2018, IEEE ICC; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308; Ioffe S., 2015, P 32 INT C MACH LEAR, P448; Jaderberg M, 2014, ARXIV; Jaderberg M., 3 INT C LEARN REPR S, P7; Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Jones HE, 2002, J NEUROPHYSIOL, V88, P2796, DOI 10.1152/jn.00403.2001; Kandel E.R., 2000, PRINCIPLES NEURAL SC; Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Larsson G., 2017, INT C LEARN REPR ICL; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Lee CY, 2014, PROC CVPR IEEE, P4050, DOI 10.1109/CVPR.2014.516; Lee H., 2009, P ANN INT C MACH LEA, P609; Li BY, 2019, AAAI CONF ARTIF INTE, P8577; Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060; Liang M, 2015, ADV NEURAL INFORM PR, P937; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Lucas SM, 2003, PROC INT CONF DOC, P682; Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127; NELSON JI, 1978, BRAIN RES, V139, P359, DOI 10.1016/0006-8993(78)90937-X; Netzer Y., 2011, P NIPS WORKSH DEEP L; Novikova T, 2012, LECT NOTES COMPUT SC, V7577, P752, DOI 10.1007/978-3-642-33783-3_54; Paszke A, 2019, ADV NEUR IN, V32; Pinheiro PO, 2014, PR MACH LEARN RES, V32; Recasens A, 2018, LECT NOTES COMPUT SC, V11213, P52, DOI 10.1007/978-3-030-01240-3_4; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Rodriguez-Serrano JA, 2015, INT J COMPUT VISION, V113, P193, DOI 10.1007/s11263-014-0793-6; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santurkar S, 2018, ADV NEUR IN, V31; Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shi CZ, 2013, PROC CVPR IEEE, P2961, DOI 10.1109/CVPR.2013.381; Simonyan K., 2015, ICLR; Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551; Springenberg J.T., 2014, ARXIV14126806; Srivastava RK, 2015, ADV NEUR IN, V28; Su BL, 2015, LECT NOTES COMPUT SC, V9003, P35, DOI 10.1007/978-3-319-16865-4_3; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Wang J., 2016, P INT C NEUR INF PRO, P811; Wang JF, 2017, ADV NEUR IN, V30; Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402; Wang T, 2012, INT C PATT RECOG, P3304; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zeiler Matthew D, 2012, ARXIV12125701; Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhang XS, 2019, ADV NEUR IN, V32; Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259; Zhao Y, 2017, INT CONF ACOUST SPEE, P5300, DOI 10.1109/ICASSP.2017.7953168; Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354; Zhou K, 2016, DESTECH TRANS COMP; Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062; Zhou X., 2019, ARXIV190407850V2; Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	98	3	3	15	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3421	3435		10.1109/TPAMI.2021.3054614	http://dx.doi.org/10.1109/TPAMI.2021.3054614			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33497326	Green Submitted			2022-12-18	WOS:000805820500008
J	Chakraborty, S; Das, S				Chakraborty, Saptarshi; Das, Swagatam			Detecting Meaningful Clusters From High-Dimensional Data: A Strongly Consistent Sparse Center-Based Clustering Approach	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Clustering; sparse clustering; feature selection; feature weighting; strong consistency	K-MEANS; FEATURE-SELECTION; VARIABLE SELECTION; DATA SET; NUMBER; CLASSIFICATION; ALGORITHM; PREDICTION; CANCER	In context to high-dimensional clustering, the concept of feature weighting has gained considerable importance over the years to capture the relative degrees of importance of different features in revealing the cluster structure of the dataset. However, the popular techniques in this area either fail to perform feature selection or do not preserve the simplicity of Lloyd's heuristic to solve the k-means problem and the like. In this paper, we propose a Lasso Weighted k-means (LW-k-means) algorithm, as a simple yet efficient sparse clustering procedure for high-dimensional data where the number of features (p) can be much higher than the number of observations (n). The LW-k-means method imposes an e l regularization term involving the feature weights directly to induce feature selection in a sparse clustering framework. We develop a simple block-coordinate descent type algorithm with time-complexity resembling that of Lloyd's method, to optimize the proposed objective. In addition, we establish the strong consistency of the LW-k-means procedure. Such an analysis of the large sample properties is not available for the conventional sparse k-means algorithms, in general. LW-k-means is tested on a number of synthetic and real-life datasets and through a detailed experimental analysis, we find that the performance of the method is highly competitive against the baselines as well as the state-of-the-art procedures for center-based high-dimensional clustering, not only in terms of clustering accuracy but also with respect to computational time.	[Chakraborty, Saptarshi] Univ Calif Berkeley, Deapt Stat, Berkeley, CA 94704 USA; [Das, Swagatam] Indian Stat Inst, Elect & Commun Sci Unit, Kolkata 700108, India	University of California System; University of California Berkeley; Indian Statistical Institute; Indian Statistical Institute Kolkata	Chakraborty, S (corresponding author), Univ Calif Berkeley, Deapt Stat, Berkeley, CA 94704 USA.	saptarshic@berkeley.edu; swagatam.das@isical.ac.in	Das, Swagatam/AAG-6753-2019	Das, Swagatam/0000-0001-6843-4508; Chakraborty, Saptarshi/0000-0002-3668-637X				Alcala-Fdez J, 2011, J MULT-VALUED LOG S, V17, P255; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Azizyan M., 2013, ADV NEURAL INFORM PR, V26, P2139; BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34; Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217; Bhattacharjee A, 2001, P NATL ACAD SCI USA, V98, P13790, DOI 10.1073/pnas.191502998; Billingsley P., 2008, PROBABILITY MEASURE; Boyd S, 2004, CONVEX OPTIMIZATION; Chakraborty S, 2020, PR MACH LEARN RES, V108, P691; Chan EY, 2004, PATTERN RECOGN, V37, P943, DOI 10.1016/j.patcog.2003.11.003; Chang XY, 2018, STAT SINICA, V28, P1265, DOI 10.5705/ss.202015.0261; Chen C., 2014, P 27 INT C NEUR INF, P532; Chen XJ, 2012, PATTERN RECOGN, V45, P434, DOI 10.1016/j.patcog.2011.06.004; Chi EC, 2017, BIOMETRICS, V73, P10, DOI 10.1111/biom.12540; de Amorim RC, 2016, J CLASSIF, V33, P210, DOI 10.1007/s00357-016-9208-4; de Amorim RC, 2012, PATTERN RECOGN, V45, P1061, DOI 10.1016/j.patcog.2011.08.012; DESARBO WS, 1984, PSYCHOMETRIKA, V49, P57, DOI 10.1007/BF02294206; Donoho D, 2008, P NATL ACAD SCI USA, V105, P14790, DOI 10.1073/pnas.0807471105; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Elhamifar E, 2010, INT CONF ACOUST SPEE, P1926, DOI 10.1109/ICASSP.2010.5495317; Fang YX, 2012, COMPUT STAT DATA AN, V56, P468, DOI 10.1016/j.csda.2011.09.003; Fop M, 2018, STAT SURV, V12, P18, DOI 10.1214/18-SS119; Fraley C, 1998, COMPUT J, V41, P578, DOI 10.1093/comjnl/41.8.578; Friedman JH, 2004, J R STAT SOC B, V66, P815, DOI 10.1111/j.1467-9868.2004.02059.x; Gallegos MT, 2013, J MULTIVARIATE ANAL, V117, P14, DOI 10.1016/j.jmva.2013.01.013; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Gordon GJ, 2002, CANCER RES, V62, P4963; Hastie TJ, 2001, ELEMENTS STAT LEARNI; Huang JZ, 2008, CH CRC DATA MIN KNOW, P193; Huang JZX, 2005, IEEE T PATTERN ANAL, V27, P657, DOI 10.1109/TPAMI.2005.95; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Jin JS, 2017, ANN STAT, V45, P2151, DOI 10.1214/16-AOS1522; Jin JS, 2016, ANN STAT, V44, P2323, DOI 10.1214/15-AOS1423; Jing L, 2007, IEEE T KNOWL DATA EN, V19, P1026, DOI 10.1109/TKDE.2007.1048; Kulis B., 2012, P 29 INT C MACH LEAR, P1131; Law MHC, 2004, IEEE T PATTERN ANAL, V26, P1154, DOI 10.1109/TPAMI.2004.71; Li CG, 2018, IEEE J-STSP, V12, P1520, DOI 10.1109/JSTSP.2018.2867446; Li CX, 2006, LECT NOTES ARTIF INT, V4062, P510; Li JD, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3136625; Lichman M., 2013, UCI MACHINE LEARNING; Liu H, 2005, IEEE T KNOWL DATA EN, V17, P491, DOI 10.1109/TKDE.2005.66; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9; Maugis C, 2009, BIOMETRICS, V65, P701, DOI 10.1111/j.1541-0420.2008.01160.x; McLachlan GJ, 2014, WIRES DATA MIN KNOWL, V4, P341, DOI 10.1002/widm.1135; McNicholas PD, 2016, J CLASSIF, V33, P331, DOI 10.1007/s00357-016-9211-9; Modha DS, 2003, MACH LEARN, V52, P217, DOI 10.1023/A:1024016609528; Nikulin V, 2015, J MACH LEARN RES, V16, P775; Pan W, 2007, J MACH LEARN RES, V8, P1145; Pelckmans K., 2005, PASCAL WORKSH STAT O, P1; POLLARD D, 1981, ANN STAT, V9, P135, DOI 10.1214/aos/1176345339; Pomeroy SL, 2002, NATURE, V415, P436, DOI 10.1038/415436a; Raftery AE, 2006, J AM STAT ASSOC, V101, P168, DOI 10.1198/016214506000000113; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Sarkar S, 2020, IEEE T PATTERN ANAL, V42, P2257, DOI 10.1109/TPAMI.2019.2912599; Sun W, 2012, ELECTRON J STAT, V6, P148, DOI 10.1214/12-EJS668; Telgarsky M. J., 2013, P 26 INT C NEUR INF, P2940; Terada Y, 2015, ANN I STAT MATH, V67, P335, DOI 10.1007/s10463-014-0454-0; Terada Y, 2014, SCAND J STAT, V41, P913, DOI 10.1111/sjos.12074; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2001, J ROY STAT SOC B, V63, P411, DOI 10.1111/1467-9868.00293; Tsai CY, 2008, COMPUT STAT DATA AN, V52, P4658, DOI 10.1016/j.csda.2008.03.002; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Valizadegan H, 2007, ADV NEURAL INF PROCE, P1417; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Verzelen N, 2017, ANN STAT, V45, P1920, DOI 10.1214/16-AOS1513; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; Wang JH, 2010, BIOMETRIKA, V97, P893, DOI 10.1093/biomet/asq061; Wang YN, 2015, PR MACH LEARN RES, V37, P862; Wang Z, 2015, IEEE T NEUR NET LEAR, V26, P2583, DOI 10.1109/TNNLS.2014.2379930; Wettschereck D, 1997, ARTIF INTELL REV, V11, P273, DOI 10.1023/A:1006593614256; Witten DM, 2010, J AM STAT ASSOC, V105, P713, DOI 10.1198/jasa.2010.tm09415; Wong KC, 2015, INT CONF SOFT COMP, P64, DOI 10.1109/ISCMI.2015.10; Xu L., 2005, NEURAL INFORM PROCES, P1537; Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141; Ye QL, 2018, IEEE T NEUR NET LEAR, V29, P4494, DOI 10.1109/TNNLS.2017.2749428; You C, 2016, PROC CVPR IEEE, P3928, DOI 10.1109/CVPR.2016.426; Zhang K, 2009, IEEE T NEURAL NETWOR, V20, P583, DOI 10.1109/TNN.2008.2010620; Zhao B., 2008, P SIAM INT C DAT MIN, P751; Zhu YH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3278	83	3	3	8	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					2894	2908		10.1109/TPAMI.2020.3047489	http://dx.doi.org/10.1109/TPAMI.2020.3047489			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33360985				2022-12-18	WOS:000803117500010
J	Paolillo, G; Astarita, T				Paolillo, Gerardo; Astarita, Tommaso			Perspective Camera Model With Refraction Correction for Optical Velocimetry Measurements in Complex Geometries	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Optical distortion; Distortion; Calibration; Solid modeling; Ray tracing; Geometry; Camera calibration; flow visualization; computer vision; perspective camera model; refractive geometry	CALIBRATION; ACCURACY; INDEX	Camera calibration is among the most challenging aspects of the investigation of fluid flows around complex transparent geometries, due to the optical distortions caused by the refraction of the lines-of-sight at the solid/fluid interfaces. This work presents a camera model which exploits the pinhole-camera approximation and represents the refraction of the lines-of-sight directly via Snell's law. The model is based on the computation of the optical ray distortion in the 3D scene and dewarping of the object points to be projected. The present procedure is shown to offer a faster convergence rate and greater robustness than other similar methods available in the literature. Issues inherent to estimation of the refractive extrinsic and intrinsic parameters are discussed and feasible calibration approaches are proposed. The effects of image noise, volume size of the control point grid and number of cameras on the calibration procedure are analyzed. Finally, an application of the camera model to the 3D optical velocimetry measurements of thermal convection inside a polymethylmethacrylate (PMMA) cylinder immersed in water is presented. A specific calibration procedure is designed for such a challenging experiment where the cylinder interior is not physically accessible and its effectiveness is demonstrated by providing velocity field reconstructions.	[Paolillo, Gerardo; Astarita, Tommaso] Univ Napoli Federico II, Dept Ind Engn, I-80125 Naples, Italy	University of Naples Federico II	Paolillo, G (corresponding author), Univ Napoli Federico II, Dept Ind Engn, I-80125 Naples, Italy.	gerardo.paolillo@unina.it; tommaso.astarita@unina.it	Paolillo, Gerardo/AAO-1369-2020; Astarita, Tommaso/B-7771-2008	Astarita, Tommaso/0000-0002-4749-0575; PAOLILLO, GERARDO/0000-0003-1656-1323				Agrawal A, 2012, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2012.6248073; Amini N, 2012, EXP FLUIDS, V53, P2011, DOI 10.1007/s00348-012-1398-x; Belden J, 2013, EXP FLUIDS, V54, DOI 10.1007/s00348-013-1463-0; Castrillo G, 2016, MEAS SCI TECHNOL, V27, DOI 10.1088/0957-0233/27/9/094011; Discetti S, 2014, MEAS SCI TECHNOL, V25, DOI 10.1088/0957-0233/25/8/084001; Discetti S, 2012, EXP FLUIDS, V52, P765, DOI 10.1007/s00348-011-1119-x; Feng MC, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112494; Hartley R., 2003, MULTIPLE VIEW GEOMET; Harvey ES, 1998, MAR TECHNOL SOC J, V32, P3; Hassan YA, 2008, NUCL ENG DES, V238, P3080, DOI 10.1016/j.nucengdes.2008.01.027; Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468; Kotowski R., 1988, INT ARCH PHOTOGRAMM, V27, P324; Lynch KP, 2015, EXP FLUIDS, V56, DOI 10.1007/s00348-015-1934-6; Maas HG, 2015, SENSORS-BASEL, V15, P18140, DOI 10.3390/s150818140; MAAS HG, 1995, OPT ENG, V34, P1970, DOI 10.1117/12.204687; Melen T., 1996, THESIS NORWEGIAN U S; Miller P, 2006, EXP FLUIDS, V41, P375, DOI 10.1007/s0034S-006-0146-5; More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105; Mulsow C, 2010, INT ARCH PHOTOGRAMM, V38, P472; Paolillo G, 2018, THESIS U STUDI NAPOL; Schanz D, 2016, EXP FLUIDS, V57, DOI 10.1007/s00348-016-2157-1; Sedlazeck Anne, 2012, Outdoor and Large-Scale Real-World Scene Analysis. 15th International Workshop on Theoretical Foundations of Computer Vision. Revised Selected Papers, P212, DOI 10.1007/978-3-642-34091-8_10; Treibitz T, 2012, IEEE T PATTERN ANAL, V34, P51, DOI 10.1109/TPAMI.2011.105; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; Usenko V, 2018, INT CONF 3D VISION, P552, DOI 10.1109/3DV.2018.00069; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P965, DOI 10.1109/34.159901; Westfeld P, 2010, INT ARCH PHOTOGRAMM, V38, P597; Wiederseiner S, 2011, EXP FLUIDS, V50, P1183, DOI 10.1007/s00348-010-0996-8; Wieneke B, 2008, EXP FLUIDS, V45, P549, DOI 10.1007/s00348-008-0521-5; Wieneke B, 2013, MEAS SCI TECHNOL, V24, DOI 10.1088/0957-0233/24/2/024008	30	3	3	7	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3185	3196		10.1109/TPAMI.2020.3046467	http://dx.doi.org/10.1109/TPAMI.2020.3046467			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33351748	hybrid			2022-12-18	WOS:000803117500029
J	Zheng, ZR; Yu, T; Liu, YB; Dai, QH				Zheng, Zerong; Yu, Tao; Liu, Yebin; Dai, Qionghai			PaMIR: Parametric Model-Conditioned Implicit Representation for Image-Based Human Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Three-dimensional displays; Surface reconstruction; Solid modeling; Estimation; Training; Shape; Body pose; human reconstruction; surface representation; parametric body model; implicit surface function	MOTION CAPTURE; TRACKING	Modeling 3D humans accurately and robustly from a single image is very challenging, and the key for such an ill-posed problem is the 3D representation of the human models. To overcome the limitations of regular 3D representations, we propose Parametric Model-Conditioned Implicit Representation (PaMIR), which combines the parametric body model with the free-form deep implicit function. In our PaMIR-based reconstruction framework, a novel deep neural network is proposed to regularize the free-form deep implicit function using the semantic features of the parametric model, which improves the generalization ability under the scenarios of challenging poses and various clothing topologies. Moreover, a novel depth-ambiguity-aware training loss is further integrated to resolve depth ambiguities and enable successful surface detail reconstruction with imperfect body reference. Finally, we propose a body reference optimization method to improve the parametric model estimation accuracy and to enhance the consistency between the parametric model and the implicit function. With the PaMIR representation, our framework can be easily extended to multi-image input scenarios without the need of multi-camera calibration and pose synchronization. Experimental results demonstrate that our method achieves state-of-the-art performance for image-based 3D human reconstruction in the cases of challenging poses and clothing types.	[Zheng, Zerong; Yu, Tao; Liu, Yebin; Dai, Qionghai] Tsinghua Univ, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China; [Dai, Qionghai] Shenzhen Inst Future Media Technol, Shenzhen 518071, Peoples R China	Tsinghua University	Liu, YB; Dai, QH (corresponding author), Tsinghua Univ, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China.; Dai, QH (corresponding author), Shenzhen Inst Future Media Technol, Shenzhen 518071, Peoples R China.	zzr18@mails.tsinghua.edu.cn; ytrock@mail.tsinghua.edu.cn; liuyebin@mail.tsinghua.edu.cn; qhdai@mail.tsinghua.edu.cn	Dai, Qionghai/ABD-5298-2021	Dai, Qionghai/0000-0001-7043-3061; Zheng, Zerong/0000-0003-1339-2480	NSFC [61827805]; Tsinghua-Kuaishou Technology Joint Reseach Center, China Postdoctoral Science Foundation [2020M670340]; Shenzhen Science and Technology Project [GGFW2017040714161462 andA JCYJ20180226181021364]	NSFC(National Natural Science Foundation of China (NSFC)); Tsinghua-Kuaishou Technology Joint Reseach Center, China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Shenzhen Science and Technology Project	This work was supported by the NSFC No.61827805, Tsinghua-Kuaishou Technology Joint Reseach Center, China Postdoctoral Science Foundation NO.2020M670340, and Shenzhen Science and Technology Project under Grants GGFW2017040714161462 andA JCYJ20180226181021364.	Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238; Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Bhatnagar Bharat Lal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P311, DOI 10.1007/978-3-030-58536-5_19; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700; Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697; Dou MS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130801; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Gabeur V, 2019, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2019.00232; Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755; Gilbert A, 2018, LECT NOTES COMPUT SC, V11215, P591, DOI 10.1007/978-3-030-01252-6_35; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Guler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Huang YH, 2017, INT CONF 3D VISION, P421, DOI 10.1109/3DV.2017.00055; Huang Z, 2020, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR42600.2020.00316; Huang Z, 2018, LECT NOTES COMPUT SC, V11220, P351, DOI 10.1007/978-3-030-01270-0_21; Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604; Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394; Kanamori Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275104; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500; Li Ruilong, 2020, ARXIV200713988, P2; Liang JB, 2019, IEEE I CONF COMP VIS, P4351, DOI 10.1109/ICCV.2019.00445; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Lin CH, 2018, AAAI CONF ARTIF INTE, P7114; Liu YB, 2013, IEEE T PATTERN ANAL, V35, P2720, DOI 10.1109/TPAMI.2013.47; Liu YB, 2010, IEEE T VIS COMPUT GR, V16, P407, DOI 10.1109/TVCG.2009.88; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Moon G., 2020, ARXIV 201111534; Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Oechsle M, 2019, IEEE I CONF COMP VIS, P4530, DOI 10.1109/ICCV.2019.00463; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016; Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239; Smith D, 2019, IEEE I CONF COMP VIS, P5329, DOI 10.1109/ICCV.2019.00543; Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Vlasic D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618520; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Wald I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601199; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Waschbusch M, 2005, VISUAL COMPUT, V21, P629, DOI 10.1007/s00371-005-0346-7; Wu CL, 2011, IEEE I CONF COMP VIS, P1108, DOI 10.1109/ICCV.2011.6126358; Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785; Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582; Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783; Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	70	3	3	11	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3170	3184		10.1109/TPAMI.2021.3050505	http://dx.doi.org/10.1109/TPAMI.2021.3050505			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33434121	Green Submitted			2022-12-18	WOS:000803117500028
J	Smith-Miles, K; Geng, X				Smith-Miles, Kate; Geng, Xin			Revisiting Facial Age Estimation With New Insights From Instance Space Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Faces; Prediction algorithms; Shape; Estimation; Machine learning algorithms; Clustering algorithms; Feature extraction; Algorithm testing; instance space analysis; facial age estimation; machine learning; no-free-lunch theorem	ALGORITHM PERFORMANCE	When demonstrating the effectiveness of a new algorithm, researchers are traditionally encouraged to compare their algorithm's performance against existing algorithms on well-studied benchmark test suites. In the absence of more nuanced methodologies, algorithm performance is typically summarized on average across the test suite examples. This paper highlights the potential bias of conclusions drawn by analyzing "on average" performance, and the opportunities offered by a recent testing methodology known as instance space analysis. To illustrate, we revisit our 2007 comparative study of algorithms for facial age estimation, and rigorously stress-test to challenge the original conclusions. The case study demonstrates how powerful visualizations offered by instance space analysis enable greater insights into unique strengths and weaknesses, and which algorithm should be used when and why. Inspired by such insights, a new algorithm is proposed, and its unique advantage is demonstrated. The bias often hidden in well-studied datasets, and the ramifications for drawing biased conclusions, are also illustrated in this case study. While focused on facial age estimation, the methodology and lessons learned from the case study are broadly applicable to any study seeking to draw conclusions about algorithm performance based on empirical results.	[Smith-Miles, Kate] Univ Melbourne, Sch Math & Stat, Melbourne, Vic 3010, Australia; [Geng, Xin] Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China	University of Melbourne; Southeast University - China	Smith-Miles, K (corresponding author), Univ Melbourne, Sch Math & Stat, Melbourne, Vic 3010, Australia.; Geng, X (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.	smith-miles@unimelb.edu.au; xgeng@seu.edu.cn	Smith-Miles, Kate/B-7493-2008	Smith-Miles, Kate/0000-0003-2718-7680	Australian Research Council [FL140100012]; National Natural Science Foundation of China [62076063]	Australian Research Council(Australian Research Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The authors are grateful to Dr. Mario Andres Munoz-Acosta and Dr. Neelofar (University of Melbourne) for their contributions to the development of the instance space analysis MATLAB code and the online tool MATILDA. They also thank Xi Qian (Southeast University) for providing research assistance to re-run the feature calculations and algorithms from the 2007 study. This work was funded by the Australian Research Council under Laureate Fellowship scheme (FL140100012) and the National Natural Science Foundation of China (62076063).	Edwards GJ, 1998, IMAGE VISION COMPUT, V16, P203, DOI 10.1016/S0262-8856(97)00069-3; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733; Greenberg H. J., 1990, ORSA Journal on Computing, V2, P94, DOI 10.1287/ijoc.2.1.94; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hooker J. N., 1995, Journal of Heuristics, V1, P33, DOI 10.1007/BF02430364; Huang Gary B., 2007, 0749 U MASS, P7; Kang YF, 2017, INT J FORECASTING, V33, P345, DOI 10.1016/j.ijforecast.2016.09.004; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091; Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553; Munoz MA, 2018, MACH LEARN, V107, P109, DOI 10.1007/s10994-017-5629-5; Munoz MA, 2017, EVOL COMPUT, V25, P529, DOI [10.1162/evco_a_00194, 10.1162/EVCO_a_00194]; Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068; Oliveira C, 2018, IEEE T RELIAB, V67, P771, DOI 10.1109/TR.2018.2832072; Parkhi Omkar M., 2015, BRIT MACH VIS C; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Smith-Miles K, 2015, COMPUT OPER RES, V63, P102, DOI 10.1016/j.cor.2015.04.022; Smith-Miles K, 2014, COMPUT OPER RES, V45, P12, DOI 10.1016/j.cor.2013.11.015; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893	20	3	3	2	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2689	2697		10.1109/TPAMI.2020.3038760	http://dx.doi.org/10.1109/TPAMI.2020.3038760			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33201807				2022-12-18	WOS:000792921400034
J	Xie, JW; Zheng, ZL; Gao, RQ; Wang, WG; Zhu, SC; Wu, YN				Xie, Jianwen; Zheng, Zilong; Gao, Ruiqi; Wang, Wenguan; Zhu, Song-Chun; Wu, Ying Nian			Generative VoxelNet: Learning Energy-Based Models for 3D Shape Synthesis and Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Solid modeling; Shape; Data models; Training; Analytical models; Feature extraction; Deep generative models; energy-based models; Langevin dynamics; volumetric shape synthesis; generative VoxelNet; cooperative learning; multi-grid sampling	FRAME MODELS; NETWORKS	3D data that contains rich geometry information of objects and scenes is valuable for understanding 3D physical world. With the recent emergence of large-scale 3D datasets, it becomes increasingly crucial to have a powerful 3D generative model for 3D shape synthesis and analysis. This paper proposes a deep 3D energy-based model to represent volumetric shapes. The maximum likelihood training of the model follows an "analysis by synthesis" scheme. The benefits of the proposed model are six-fold: first, unlike GANs and VAEs, the model training does not rely on any auxiliary models; second, the model can synthesize realistic 3D shapes by Markov chain Monte Carlo (MCMC); third, the conditional model can be applied to 3D object recovery and super resolution; fourth, the model can serve as a building block in a multi-grid modeling and sampling framework for high resolution 3D shape synthesis; fifth, the model can be used to train a 3D generator via MCMC teaching; sixth, the unsupervisedly trained model provides a powerful feature extractor for 3D data, which is useful for 3D object classification. Experiments demonstrate that the proposed model can generate high-quality 3D shape patterns and can be useful for a wide variety of 3D shape analysis.	[Xie, Jianwen] Baidu Res, Cognit Comp Lab, Bellevue, WA 98004 USA; [Zheng, Zilong] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA; [Gao, Ruiqi; Wu, Ying Nian] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Wang, Wenguan] Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland; [Zhu, Song-Chun] Tsinghua Univ, Beijing, Peoples R China; [Zhu, Song-Chun] Peking Univ, Beijing, Peoples R China	Baidu; University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; Swiss Federal Institutes of Technology Domain; ETH Zurich; Tsinghua University; Peking University	Xie, JW (corresponding author), Baidu Res, Cognit Comp Lab, Bellevue, WA 98004 USA.	jianwen@ucla.edu; z.zheng@ucla.edu; ruiqigao@ucla.edu; wenguanwang.ai@gmail.com; sczhu@stat.ucla.edu; ywu@stat.ucla.edu			NSF [DMS-2015577]; DARPA SIMPLEX [N66001-15-C-4035]; ONR MURI [N00014-16-12007]; DARPA ARO [W911NF-16-1-0579]; DARPA [N66001-172-4029]; XSEDE grant [ASC180018]; NVIDIA Corporation	NSF(National Science Foundation (NSF)); DARPA SIMPLEX; ONR MURI(MURIOffice of Naval Research); DARPA ARO(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); XSEDE grant; NVIDIA Corporation	The work was supported by the NSF DMS-2015577, DARPA SIMPLEX N66001-15-C-4035, ONR MURI N00014-16-12007, DARPA ARO W911NF-16-1-0579, DARPA N66001-172-4029, and XSEDE grant ASC180018. The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. They thank Erik Nijkamp for his help with coding. They would also like to thank Siyuan Huang for helpful discussions. Jianwen Xie and Zilong Zheng contributed equally to this work.	Achlioptas P, 2018, PR MACH LEARN RES, V80; Ahmed E, 2018, ARXIV 180801462; Alain G, 2014, J MACH LEARN RES, V15, P3563; Arjovsky M, 2017, PR MACH LEARN RES, V70; Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459; Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Brock Andrew, 2016, ARXIV160804236; Carlson W. E., 1982, Computer Graphics, V16, P255, DOI 10.1145/965145.801288; Chang Angel X., 2015, ARXIV151203012CSGR P; Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930; Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Du Y., 2019, PROC INT C NEURAL IN, P3608; Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488; Gao RQ, 2018, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2018.00954; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Geng J, 2011, ADV OPT PHOTONICS, V3, P128, DOI 10.1364/AOP.3.000128; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grenander U., 2007, PATTERN THEORY REPRE; Han T, 2017, AAAI CONF ARTIF INTE, P1976; Hansard Miles, 2012, TIME FLIGHT CAMERAS, P2; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Huang WL, 2019, AAAI CONF ARTIF INTE, P8481; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, IEEE T NEURAL NETWOR, V18, P1529, DOI 10.1109/TNN.2007.895819; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jin L, 2017, ADV NEUR IN, V30; Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]; Kazhdan M., 2003, Symposium on Geometry Processing, P156; Khan SH, 2019, PROC CVPR IEEE, P9731, DOI 10.1109/CVPR.2019.00997; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lazarow J, 2017, IEEE I CONF COMP VIS, P2793, DOI 10.1109/ICCV.2017.302; LeCun Yann, 2006, PREDICTING STRUCTURE, P2; Lee K, 2018, PROC CVPR IEEE, P3702, DOI 10.1109/CVPR.2018.00390; Lu Y, 2016, AAAI CONF ARTIF INTE, P1902; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2; Nijkamp E, 2019, ADV NEUR IN, V32; Nijkamp E, 2020, AAAI CONF ARTIF INTE, V34, P5272; Oh YJ, 2002, SICE 2002: PROCEEDINGS OF THE 41ST SICE ANNUAL CONFERENCE, VOLS 1-5, P3222; Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Sfikas K., 2017, P 3DOR, V8, P1, DOI DOI 10.2312/3DOR.20171045; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11; Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Swersky K., 2011, PROC 28 INT C MACH L, P1201; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Tu Z, 2007, PROC CVPR IEEE, P500; Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142; Warde-Farley D., 2017, INT C LEARN REPR; Welling Max, 2009, P 26 ANN INT C MACH, P1121, DOI DOI 10.1145/1553374.1553517; Wu YN, 2018, ANN MATH SCI APPL, V3, P211; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xie JW, 2018, AAAI CONF ARTIF INTE, P4292; Xie JW, 2021, IEEE T PATTERN ANAL, V43, P516, DOI 10.1109/TPAMI.2019.2934852; Xie JW, 2020, IEEE T PATTERN ANAL, V42, P27, DOI 10.1109/TPAMI.2018.2879081; Xie JW, 2018, PROC CVPR IEEE, P8629, DOI 10.1109/CVPR.2018.00900; Xie JW, 2016, PR MACH LEARN RES, V48; Xie JW, 2017, PROC CVPR IEEE, P1061, DOI 10.1109/CVPR.2017.119; Xie JW, 2016, APPL COMPUT HARMON A, V41, P4, DOI 10.1016/j.acha.2015.08.004; Xie JW, 2015, INT J COMPUT VISION, V114, P91, DOI 10.1007/s11263-014-0757-x; Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420	79	3	3	2	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2468	2484		10.1109/TPAMI.2020.3045010	http://dx.doi.org/10.1109/TPAMI.2020.3045010			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33320811	Green Submitted			2022-12-18	WOS:000792921400020
J	Xu, D; Alameda-Pineda, X; Ouyang, WL; Ricci, E; Wang, XG; Sebe, N				Xu, Dan; Alameda-Pineda, Xavier; Ouyang, Wanli; Ricci, Elisa; Wang, Xiaogang; Sebe, Nicu			Probabilistic Graph Attention Network With Conditional Kernels for Pixel-Wise Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Predictive models; Semantics; Task analysis; Estimation; Probabilistic logic; Kernel; Electronic mail; Structured representation learning; attention model; conditional random fields; conditional kernels; pixel-wise prediction	IMAGE SEGMENTATION	Multi-scale representations deeply learned via convolutional neural networks have shown tremendous importance for various pixel-level prediction problems. In this paper we present a novel approach that advances the state of the art on pixel-level prediction in a fundamental aspect, i.e. structured multi-scale features learning and fusion. In contrast to previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture, and simply fusing the features with weighted averaging or concatenation, we propose a probabilistic graph attention network structure based on a novel Attention-Gated Conditional Random Fields (AG-CRFs) model for learning and fusing multi-scale representations in a principled manner. In order to further improve the learning capacity of the network structure, we propose to exploit feature dependant conditional kernels within the deep probabilistic framework. Extensive experiments are conducted on four publicly available datasets (i.e. BSDS500, NYUD-V2, KITTI and Pascal-Context) and on three challenging pixel-wise prediction problems involving both discrete and continuous labels (i.e. monocular depth estimation, object contour prediction and semantic segmentation). Quantitative and qualitative results demonstrate the effectiveness of the proposed latent AG-CRF model and the overall probabilistic graph attention network with feature conditional kernels for structured feature learning and pixel-wise prediction.	[Xu, Dan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Alameda-Pineda, Xavier] INRIA, Percept Grp, F-38000 Isere, France; [Ouyang, Wanli] Univ Sydney, Dept Elect & Informat Engn, Camperdown, NSW 2006, Australia; [Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China; [Ricci, Elisa; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, Trento, TN 38122 USA	Hong Kong University of Science & Technology; Inria; University of Sydney; Chinese University of Hong Kong	Xu, D (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	danxu@cse.ust.hk; xavier.alameda-pineda@inria.fr; wanli.ouyang@sydney.edu.au; elisa.ricci@unitn.it; xgwang@ee.cuhk.edu.hk; niculae.sebe@unitn.it		Sebe, Niculae/0000-0002-6597-7248				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, P 4 INT C LEARN REPR; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Arnab A., 2016, PROC EUR C COMPUT VI; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Chu Xiao, 2016, ADV NEURAL INFORM PR, P316; Chung J., 2014, ARXIV14123555; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eigen David, 2014, NEURIPS; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gan YK, 2018, LECT NOTES COMPUT SC, V11207, P232, DOI 10.1007/978-3-030-01219-9_14; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Hallman S, 2015, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2015.7298782; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He XM, 2004, PROC CVPR IEEE, P695; Huang Gao, 2018, ICLR; Kr_ahenb_uhl P., 2011, P 27 INT C NEUR INF; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Kundu JN, 2018, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2018.00281; Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lee J. H., 2019, ARXIV 190710326; Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849; Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594; Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35; Mnih V, 2014, ADV NEUR IN, V27; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Pilzer A, 2018, INT CONF 3D VISION, P587, DOI 10.1109/3DV.2018.00073; Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481406; Puscas MM, 2019, INT CONF 3D VISION, P18, DOI 10.1109/3DV.2019.00012; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Quattoni A., 2005, P 27 INT C NEUR INF; Ramanan D., 2017, ARXIV 170206506; Ren XF, 2008, LECT NOTES COMPUT SC, V5304, P533; Ren ZL, 2013, PROC CVPR IEEE, P2011, DOI 10.1109/CVPR.2013.262; Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594; Sarawagi S., 2005, P 27 INT C NEUR INF; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Sun K., ARXIV190404514, V2019; Tang Y, 2010, P ANN C NEUR INF PRO; Vandenhende Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P527, DOI 10.1007/978-3-030-58548-8_31; Vaswani A, 2017, ADV NEUR IN, V30; Veli ~ckovi ~c P., 2017, P INT C LEARN REPR; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang Jingdong, 2020, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2020.2983686, 10.1109/tpami.2020.2983686]; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Wang YP, 2019, IEEE T IMAGE PROCESS, V28, P1285, DOI 10.1109/TIP.2018.2874279; Winn J., 2012, P 15 INT C ART INT S, P1314; Winn J, 2009, P 27 INT C NEUR INF; Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685; Xie SN, 2016, LECT NOTES COMPUT SC, V9908, P302, DOI 10.1007/978-3-319-46493-0_19; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xu D., 2019, P BRIT MACH VIS C; Xu D., 2017, P 27 INT C NEUR INF; Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077; Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412; Xu D, 2019, IEEE T PATTERN ANAL, V41, P1426, DOI 10.1109/TPAMI.2018.2839602; Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451; Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu YY, 2019, IEEE I CONF COMP VIS, P3788, DOI 10.1109/ICCV.2019.00389; Yang JM, 2016, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2016.28; Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144; Yu F., MULTISCALE CONTEXT A; Yuan Y., 2018, ARXIV 180900916; Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11; Zeng XY, 2018, IEEE T PATTERN ANAL, V40, P2109, DOI 10.1109/TPAMI.2017.2745563; Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423; Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15; Zhang ZZ, 2016, PROC CVPR IEEE, P251, DOI 10.1109/CVPR.2016.34; Zhao Q, 2015, P BRIT MACH VIS C; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zou YL, 2018, LECT NOTES COMPUT SC, V11209, P38, DOI 10.1007/978-3-030-01228-1_3	107	3	3	5	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2673	2688		10.1109/TPAMI.2020.3043781	http://dx.doi.org/10.1109/TPAMI.2020.3043781			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33301402	Green Submitted			2022-12-18	WOS:000792921400033
J	Liu, YZ; Chen, YL; Lasang, P; Sun, QS				Liu, Yazhou; Chen, Yuliang; Lasang, Pongsak; Sun, Quansen			Covariance Attention for Semantic Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Covariance matrices; Feature extraction; Image segmentation; Task analysis; Neural networks; Image edge detection; Image segmentation; deep neural networks; covariance matrices; attention module	LINEAR DISCRIMINANT-ANALYSIS; FACE REPRESENTATION; 2-DIMENSIONAL PCA; NETWORK; RECOGNITION; MODEL	The dependency between global and local information can provide important contextual cues for semantic segmentation. Existing attention methods capture this dependency by calculating the pixel wise correlation between the learnt feature maps, which is of high space and time complexity. In this article, a new attention module, covariance attention, is presented, and which is interesting in the following aspects: 1) covariance matrix is used as a new attention module to model the global and local dependency for the feature maps and the local-global dependency is formulated as a simple matrix projection process; 2) since covariance matrix can encode the joint distribution information for the heterogeneous yet complementary statistics, the hand-engineered features are combined with the learnt features effectively using covariance matrix to boost the segmentation performance; 3) a covariance attention mechanism based semantic segmentation framework, CANet, is proposed and very competitive performance has been obtained. Comparisons with the state-of-the-art methods reveal the superiority of the proposed method.	[Liu, Yazhou; Chen, Yuliang; Sun, Quansen] Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China; [Lasang, Pongsak] Panasonic R&D Ctr Singapore, Singapore 469332, Singapore	Nanjing University of Science & Technology; Panasonic	Sun, QS (corresponding author), Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China.	yazhouliu@njust.edu.cn; yuliangchen@njust.edu.cn; pongsak.lasang@sg.panasonic.com; sunquansen@njust.edu.cn			National Natural Science Foundation of China [61672286, 61673220]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Natural Science Foundation of China under Grants 61672286 and 61673220.	Arnab A, 2016, LECT NOTES COMPUT SC, V9906, P524, DOI 10.1007/978-3-319-46475-6_33; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Berg A.C., 2015, ARXIV150604579; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254; Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Hamsici OC, 2008, IEEE T PATTERN ANAL, V30, P647, DOI 10.1109/TPAMI.2007.70717; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu XW, 2019, IEEE T INTELL TRANSP, V20, P1010, DOI 10.1109/TITS.2018.2838132; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069; Hung WC, 2017, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2017.287; Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819; Jegou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156; Jin XJ, 2017, IEEE I CONF COMP VIS, P5581, DOI 10.1109/ICCV.2017.595; Kim TK, 2005, IEEE T PATTERN ANAL, V27, P318, DOI 10.1109/TPAMI.2005.58; KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Kwak S, 2017, AAAI CONF ARTIF INTE, P4111; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li H., 2018, ARXIV180510180, V1805, P10180; Li HY, 2019, INT J COMPUT VISION, V127, P225, DOI 10.1007/s11263-018-1101-7; Lin D, 2018, LECT NOTES COMPUT SC, V11207, P622, DOI 10.1007/978-3-030-01219-9_37; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197; Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maninis KK, 2019, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2019.00195; Mikolajczyk K, 2004, LECT NOTES COMPUT SC, V3021, P69; Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189; Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roy A, 2016, LECT NOTES COMPUT SC, V9908, P186, DOI 10.1007/978-3-319-46493-0_12; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926; Teichmann M, 2018, IEEE INT VEH SYM, P1013; Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang Y, 2018, LECT NOTES COMPUT SC, V11073, P523, DOI 10.1007/978-3-030-00937-3_60; Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI 10.1109/ICIP.2019.8803154; Wei J, 2020, IEEE T INTELL TRANSP, V21, P1572, DOI 10.1109/TITS.2019.2910643; Wu Zifeng, 2016, ARXIV160506885; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097; Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Ye JP, 2005, IEEE T PATTERN ANAL, V27, P929, DOI 10.1109/TPAMI.2005.110; Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199; Yu F., 2016, P ICLR 2016; Yu YH, 2018, ADV SOC SCI EDUC HUM, V182, P1; Yuan Y., 2018, ARXIV180900916; Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004; Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544; Zhou YZ, 2019, PROC CVPR IEEE, P4041, DOI 10.1109/CVPR.2019.00417; Zhu LY, 2019, IEEE INT CONF COMP V, P1920, DOI 10.1109/ICCVW.2019.00241	74	3	3	6	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					1805	1818		10.1109/TPAMI.2020.3026069	http://dx.doi.org/10.1109/TPAMI.2020.3026069			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	32976093				2022-12-18	WOS:000764815300013
J	Wu, ZX; Li, HD; Xiong, CM; Jiang, YG; Davis, LS				Wu, Zuxuan; Li, Hengduo; Xiong, Caiming; Jiang, Yu-Gang; Davis, Larry Steven			A Dynamic Frame Selection Framework for Fast Video Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computational modeling; Three-dimensional displays; Video sequences; Two dimensional displays; Computational efficiency; Standards; Electronic mail; Video classification; conditional computation; deep neural networks; reinforcement learning		We introduce AdaFrame, a conditional computation framework that adaptively selects relevant frames on a per-input basis for fast video recognition. AdaFrame, which contains a Long Short-Term Memory augmented with a global memory to provide context information, operates as an agent to interact with video sequences aiming to search over time which frames to use. Trained with policy search methods, at each time step, AdaFrame computes a prediction, decides where to observe next, and estimates a utility, i.e., expected future rewards, of viewing more frames in the future. Exploring predicted utilities at testing time, AdaFrame is able to achieve adaptive lookahead inference so as to minimize the overall computational cost without incurring a degradation in accuracy. We conduct extensive experiments on two large-scale video benchmarks, FCVID and ActivityNet. With a vanilla ResNet-101 model, AdaFrame achieves similar performance of using all frames while only requiring, on average, 8.21 and 8.65 frames on FCVID and ActivityNet, respectively. We also demonstrate AdaFrame is compatible with modern 2D and 3D networks for video recognition. Furthermore, we show, among other things, learned frame usage can reflect the difficulty of making prediction decisions both at instance-level within the same class and at class-level among different categories.	[Wu, Zuxuan; Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China; [Li, Hengduo; Davis, Larry Steven] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Xiong, Caiming] Salesforce Res, Palo Alto, CA 94301 USA	Fudan University; University System of Maryland; University of Maryland College Park; Salesforce	Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.	zxwu@fudan.edu.cn; hdli@umiacs.umd.edu; cxiong@salesforce.com; ygj@fudan.edu.cn; lsd@umiacs.umd.edu	Li, Hengduo/ADM-1901-2022		Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) [D17PC00345]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC)	LSD is partially supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00345. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied of IARPA, DOI/IBC or the U.S. Government.	Bhardwaj S, 2019, PROC CVPR IEEE, P354, DOI 10.1109/CVPR.2019.00044; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Collins J., 2017, P 5 INT C LEARN REPR; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fan HH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P705; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Figurnov M, 2017, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2017.194; Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047; Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Howard A. G., 2017, MOBILENETS EFFICIENT; Huang Gao, 2018, ICLR; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560; Kay W., 2017, ARXIV PREPRINT ARXIV; Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669; McGill M, 2017, PR MACH LEARN RES, V70; Narang S., 2017, P 5 INT C LEARN REPR; Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684; Pan BW, 2018, PROC CVPR IEEE, P1536, DOI 10.1109/CVPR.2018.00166; Qiu ZF, 2017, PROC CVPR IEEE, P4085, DOI 10.1109/CVPR.2017.435; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Soomro K., 2012, ARXIV PREPRINT ARXIV; Su YC, 2016, LECT NOTES COMPUT SC, V9911, P783, DOI 10.1007/978-3-319-46478-7_48; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Teerapittayanon S, 2016, INT C PATT RECOG, P2464, DOI 10.1109/ICPR.2016.7900006; Vaswani A, 2017, ADV NEUR IN, V30; Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang X, 2018, LECT NOTES COMPUT SC, V11217, P420, DOI 10.1007/978-3-030-01261-8_25; Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631; Wu WH, 2019, IEEE I CONF COMP VIS, P6231, DOI 10.1109/ICCV.2019.00632; Wu ZX, 2019, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2019.00137; Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293; Yogatama D., 2018, P 6 INT C LEARN REPR; Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297; Zhang S., 2020, P 8 INT C LEARN REPR; Zhu C, 2018, LECT NOTES COMPUT SC, V11209, P139, DOI 10.1007/978-3-030-01228-1_9; Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43	53	3	3	4	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					1699	1711		10.1109/TPAMI.2020.3029425	http://dx.doi.org/10.1109/TPAMI.2020.3029425			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	33026981				2022-12-18	WOS:000764815300006
J	Xu, XY; Ma, YR; Sun, WX; Yang, MH				Xu, Xiangyu; Ma, Yongrui; Sun, Wenxiu; Yang, Ming-Hsuan			Exploiting Raw Images for Real-Scene Super-Resolution	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image color analysis; Cameras; Image restoration; Color; Pipelines; Data models; Super-resolution; raw image; training data generation; two-branch structure; convolutional neural network (CNN)	QUALITY ASSESSMENT; NETWORK	Super-resolution is a fundamental problem in computer vision which aims to overcome the spatial limitation of camera sensors. While significant progress has been made in single image super-resolution, most algorithms only perform well on synthetic data, which limits their applications in real scenarios. In this paper, we study the problem of real-scene single image super-resolution to bridge the gap between synthetic data and real captured images. We focus on two issues of existing super-resolution algorithms: lack of realistic training data and insufficient utilization of visual information obtained from cameras. To address the first issue, we propose a method to generate more realistic training data by mimicking the imaging process of digital cameras. For the second issue, we develop a two-branch convolutional neural network to exploit the radiance information originally-recorded in raw images. In addition, we propose a dense channel-attention block for better image restoration as well as a learning-based guided filter network for effective color correction. Our model is able to generalize to different cameras without deliberately training on images from specific camera types. Extensive experiments demonstrate that the proposed algorithm can recover fine details and clear structures, and achieve high-quality results for single image super-resolution in real scenes.	[Xu, Xiangyu] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Ma, Yongrui] SenseTime Res, Beijing 100084, Peoples R China; [Sun, Wenxiu] SenseTime Res, Hong Kong 999077, Peoples R China; [Yang, Ming-Hsuan] Univ Calif, Sch Engn, Merced, CA 95343 USA	Carnegie Mellon University; University of California System; University of California Merced	Sun, WX (corresponding author), SenseTime Res, Hong Kong 999077, Peoples R China.	xuxiangyu2014@gmail.com; yongrayma@gmail.com; irene.wenxiu.sun@gmail.com; mhyang@ucmerced.edu			NSF CAREER [1149783]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	Ming-Hsuan Yang was supported in part by the NSF CAREER Grant #1149783.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129; Bychkovsky V, 2011, PROC CVPR IEEE, P97; Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347; Coffin D, 2018, DCRAW DECODING RAW D; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Farsiu S, 2006, IEEE T IMAGE PROCESS, V15, P141, DOI 10.1109/TIP.2005.860336; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greivenkamp J. E., 2004, FIELD GUIDE GEOMETRI, V1; Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115; Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Hirakawa K, 2005, IEEE T IMAGE PROCESS, V14, P360, DOI 10.1109/TIP.2004.838691; Hradi M., 2015, BRIT MACH VIS C; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jiang HM, 2017, IEEE T IMAGE PROCESS, V26, P5032, DOI 10.1109/TIP.2017.2713942; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Khashabi D, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2359774; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951; Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511; Li YJ, 2019, IEEE T PATTERN ANAL, V41, P1909, DOI 10.1109/TPAMI.2018.2890623; Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325; Nguyen RMH, 2016, PROC CVPR IEEE, P1655, DOI 10.1109/CVPR.2016.183; Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Pennebaker William B, 1992, JPEG STILL IMAGE DAT; Plotz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294; Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481; Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142; Schwartz E, 2019, IEEE T IMAGE PROCESS, V28, P912, DOI 10.1109/TIP.2018.2872858; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40; Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514; Vandewalle P, 2007, PROC SPIE, V6502, DOI 10.1117/12.703980; Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197; Xu XY, 2019, PROC CVPR IEEE, P1723, DOI 10.1109/CVPR.2019.00182; Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36; Xu XY, 2018, IEEE T IMAGE PROCESS, V27, P194, DOI 10.1109/TIP.2017.2753658; Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328; Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhou R, 2018, COL IM C, V2018, P75	76	3	3	10	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					1905	1921		10.1109/TPAMI.2020.3032476	http://dx.doi.org/10.1109/TPAMI.2020.3032476			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	33079657	Green Submitted, hybrid			2022-12-18	WOS:000764815300019
J	Li, HA; Zhao, J; Bazin, JC; Liu, YH				Li, Haoang; Zhao, Ji; Bazin, Jean-Charles; Liu, Yun-Hui			Quasi-Globally Optimal and Near/True Real-Time Vanishing Point Estimation in Manhattan World	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Estimation; Real-time systems; Three-dimensional displays; Cameras; Cost function; Acceleration; Machine learning; Vanishing point; Manhattan world; sampling; branch and bound; global optimality; real-time	CONSENSUS; SPACE	Image lines projected from parallel 3D lines intersect at a common point called the vanishing point (VP). Manhattan world holds for the scenes with three orthogonal VPs. In Manhattan world, given several lines in a calibrated image, we aim to cluster them by three unknown-but-sought VPs. The VP estimation can be reformulated as computing the rotation between the Manhattan frame and camera frame. To estimate three degrees of freedom (DOF) of this rotation, state-of-the-art methods are based on either data sampling or parameter search. However, they fail to guarantee high accuracy and efficiency simultaneously. In contrast, we propose a set of approaches that hybridize these two strategies. We first constrain two or one DOF of the rotation by two or one sampled image line. Then we search for the remaining one or two DOF based on branch and bound. Our sampling accelerates our search by reducing the search space and simplifying the bound computation. Our search achieves quasi-global optimality. Specifically, it guarantees to retrieve the maximum number of inliers on the condition that two or one DOF is constrained. Our hybridization of two-line sampling and one-DOF search can estimate VPs in real time. Our hybridization of one-line sampling and two-DOF search can estimate VPs in near real time. Experiments on both synthetic and real-world datasets demonstrated that our approaches outperform state-of-the-art methods in terms of accuracy and/or efficiency.	[Li, Haoang; Liu, Yun-Hui] Chinese Univ Hong Kong, Dept Mech & Automat Engn, Hong Kong, Peoples R China; [Li, Haoang; Liu, Yun-Hui] Chinese Univ Hong Kong, T Stone Robot Inst, Hong Kong, Peoples R China; [Zhao, Ji] TuSimple, Beijing 100020, Peoples R China; [Bazin, Jean-Charles] Korea Adv Inst Sci & Technol KAIST, Grad Sch Culture Technol, Daejeon 34051, South Korea; [Bazin, Jean-Charles] Korea Adv Inst Sci & Technol KAIST, Sch Elect Engn, Daejeon 34051, South Korea	Chinese University of Hong Kong; Chinese University of Hong Kong; Korea Advanced Institute of Science & Technology (KAIST); Korea Advanced Institute of Science & Technology (KAIST)	Li, HA (corresponding author), Chinese Univ Hong Kong, Dept Mech & Automat Engn, Hong Kong, Peoples R China.; Li, HA (corresponding author), Chinese Univ Hong Kong, T Stone Robot Inst, Hong Kong, Peoples R China.	hali@mae.cuhk.edu.hk; zhaoji84@gmail.com; bazinjc@kaist.ac.kr; yhliu@mae.cuhk.edu.hk		Reis, AlessanRSS/0000-0001-8486-7469	Hong Kong Centre for Logistics Robotics; CUHK VC Discretionary Fund; Shenzhen Municipal Government via the Shenzhen-HK Collaboration Zone Project; RGC [14207119]	Hong Kong Centre for Logistics Robotics; CUHK VC Discretionary Fund; Shenzhen Municipal Government via the Shenzhen-HK Collaboration Zone Project; RGC(Hong Kong Research Grants Council)	This work was supported in part by the Hong Kong Centre for Logistics Robotics, RGC via Grant 14207119, the CUHK VC Discretionary Fund, and Shenzhen Municipal Government via the Shenzhen-HK Collaboration Zone Project.	Antone ME, 2000, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.2000.854809; Antunes M, 2017, PROC CVPR IEEE, P6691, DOI 10.1109/CVPR.2017.708; BARNARD ST, 1983, ARTIF INTELL, V21, P435, DOI 10.1016/S0004-3702(83)80021-6; Barreto JP, 2005, IEEE I CONF COMP VIS, P625; Bazin JC, 2012, IEEE INT C INT ROBOT, P4282, DOI 10.1109/IROS.2012.6385802; Bazin JC, 2012, PROC CVPR IEEE, P638, DOI 10.1109/CVPR.2012.6247731; Bazin JC, 2012, INT J ROBOT RES, V31, P63, DOI 10.1177/0278364911421954; Bazin Jean-Charles, 2012, P AS C COMP VIS, P2; Bishop C.M, 2006, PATTERN RECOGN; Campbell D, 2017, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2017.10; Campbell D, 2016, PROC CVPR IEEE, P5685, DOI 10.1109/CVPR.2016.613; Coughlan J.M., 1999, P ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349; Denis P, 2008, LECT NOTES COMPUT SC, V5303, P197, DOI 10.1007/978-3-540-88688-4_15; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Flint A, 2011, IEEE I CONF COMP VIS, P2228, DOI 10.1109/ICCV.2011.6126501; Gao Y, 2017, PROC CVPR IEEE, P6718, DOI 10.1109/CVPR.2017.711; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Joo K, 2019, IEEE T PATTERN ANAL, V41, P682, DOI 10.1109/TPAMI.2018.2799944; Jung R, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1, DOI 10.1109/VR.2019.8798326; Kim P, 2018, IEEE INT CONF ROBOT, P7247; Li HA, 2019, IEEE INT CONF ROBOT, P2412, DOI 10.1109/ICRA.2019.8793716; Li HA, 2019, IEEE I CONF COMP VIS, P1646, DOI 10.1109/ICCV.2019.00173; Li H, 2018, IEEE INT CONF ROBOT, P2518; Li HD, 2009, IEEE I CONF COMP VIS, P1074, DOI 10.1109/ICCV.2009.5459398; Mirzaei FM, 2011, IEEE I CONF COMP VIS, P2454, DOI 10.1109/ICCV.2011.6126530; Moore RE., 2009, INTRO INTERVAL ANAL, DOI [10.1137/1.9780898717716, DOI 10.1137/1.9780898717716]; Olsson C, 2009, IEEE T PATTERN ANAL, V31, P783, DOI 10.1109/TPAMI.2008.131; QUAN L, 1989, PATTERN RECOGN LETT, V9, P279, DOI 10.1016/0167-8655(89)90006-8; Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112; Straub J, 2018, IEEE T PATTERN ANAL, V40, P235, DOI 10.1109/TPAMI.2017.2662686; Tardif JP, 2009, IEEE I CONF COMP VIS, P1250, DOI 10.1109/ICCV.2009.5459328; Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41; Tretyak E, 2012, INT J COMPUT VISION, V97, P305, DOI 10.1007/s11263-011-0488-1; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405; Zhai MH, 2016, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2016.610; Zhang LL, 2016, INT J COMPUT VISION, V117, P111, DOI 10.1007/s11263-015-0854-5; Zhou Y., 2019, PROC ADV NEURAL INF, P864	39	3	3	2	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1503	1518		10.1109/TPAMI.2020.3023183	http://dx.doi.org/10.1109/TPAMI.2020.3023183			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32915727				2022-12-18	WOS:000752018000031
J	Park, SW; Kwon, J				Park, Sung Woo; Kwon, Junseok			SphereGAN: Sphere Generative Adversarial Network Based on Geometric Moment Matching and its Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Gallium nitride; Training; Three-dimensional displays; Linear programming; Manifolds; Generative adversarial networks; Measurement; Generative adversarial network; integral probability metric; riemannian manifolds; geometric moment matching		We propose a novel integral probability metric-based generative adversarial network (GAN), called SphereGAN. In the proposed scheme, the distance between two probability distributions (i.e., true and fake distributions) is measured on a hypersphere. Given that its hypersphere-based objective function computes the upper bound of the distance as a half arc, SphereGAN can be stably trained and can achieve a high convergence rate. In sphereGAN, higher-order information of data is processed using multiple geometric moments, thus improving the accuracy of the distance measurement and producing more realistic outcomes. Several properties of the proposed distance metric on the hypersphere are mathematically derived. The effectiveness of the proposed SphereGAN is demonstrated through quantitative and qualitative experiments for unsupervised image generation and 3D point cloud generation, demonstrating its superiority over state-of-the-art GANs with respect to accuracy and convergence on the CIFAR-10, STL-10, LSUN bedroom, and ShapeNet datasets.	[Park, Sung Woo; Kwon, Junseok] Chung Ang Univ, Sch Comp Sci & Engn, Seoul 06974, South Korea	Chung Ang University	Kwon, J (corresponding author), Chung Ang Univ, Sch Comp Sci & Engn, Seoul 06974, South Korea.	pswkiki@gmail.com; jskwon@cau.ac.kr		kwon, junseok/0000-0001-9526-7549	National Research Foundation of Korea [2020R1C1C1004907]; Chung-Ang University	National Research Foundation of Korea(National Research Foundation of Korea); Chung-Ang University	This work was supported by the National Research Foundation of Korea (No. 2020R1C1C1004907) and the Chung-Ang University Research Scholarship Grants in 2020.	Achlioptas P, 2018, PR MACH LEARN RES, V80; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbel M., 2018, P 32 INT C NEUR INF, P6701; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2017, ARXIV; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986; Cho K., 2014, P 2014 C EMP METH NA, P1724; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Coxeter H., 1969, INTRO GEOMETRY, P77; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Ehsani K, 2018, PROC CVPR IEEE, P6144, DOI 10.1109/CVPR.2018.00643; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Groueix T., 2018, ABS180605228 CORR; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Gulrajani I, 2017, P NIPS 2017; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Henderson P, 2018, AAAI CONF ARTIF INTE, P3199; Hensel M, 2017, ADV NEUR IN, V30; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jolicoeur-Martineau A., 2019, 7 INT C LEARN REPR I; Karras T, 2017, ARXIV171010196; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li C.-L., 2018, ABS181005795 CORR; Li CL, 2019, PR MACH LEARN RES, V89; Li CL, 2019, PROC CVPR IEEE, P11959, DOI 10.1109/CVPR.2019.01224; Li Q., 2018, ABS181110427 CORR; Limand J. H., 2017, ABS170502894 CORR; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin JX, 2018, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2018.00579; Lukovnikov D., 2018, P INT C LEARN REPR; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mathieu Michael, 2015, ARXIV151105440; Mroueh Y., 2017, ABS170208398 CORR; Mroueh Y., 2017, P ADV NEURAL INFORM, P2510, DOI DOI 10.5555/3294996.3295012; Mroueh Y., 2018, P INT C LEARN REPR; Park N., 2017, ABS170708273 CORR; Park SW, 2019, PROC CVPR IEEE, P4287, DOI 10.1109/CVPR.2019.00442; Qi Charles Ruizhongtai, 2017, P ADV NEUR INF PROC, P5099; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Reed S, 2016, PR MACH LEARN RES, V48; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Salimans T, 2016, ADV NEUR IN, V29; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396; Sutherland Danica J, 2018, ICLR; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018; Tao C., 2018, P MACHINE LEARNING R, P4887; Unterthiner T., 2018, P INT C LEARN REPR; Valsesia D., 2019, P INT C LEARN REPR; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Virosztek D., 2018, ABS180203305 CORR; Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Warde-Farley D., 2017, INT C LEARN REPR; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei X., 2018, P INT C LEARN REPR F; Wu JQ, 2018, LECT NOTES COMPUT SC, V11209, P673, DOI 10.1007/978-3-030-01228-1_40; Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhang ZZ, 2018, PROC CVPR IEEE, P9242, DOI 10.1109/CVPR.2018.00963; Zhao J., 2017, ICLR	77	3	3	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1566	1580		10.1109/TPAMI.2020.3015948	http://dx.doi.org/10.1109/TPAMI.2020.3015948			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32784130				2022-12-18	WOS:000752018000035
J	Xue, J; Zhang, H; Nishino, K; Dana, KJ				Xue, Jia; Zhang, Hang; Nishino, Ko; Dana, Kristin J.			Differential Viewpoints for Ground Terrain Material Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Databases; Image recognition; Cameras; Robots; Lighting; Image capture; Material recognition; deep convolutional neural networks; texture reflectance; robot navigation	TEXTURE; REFLECTANCE; REPRESENTATION; CLASSIFICATION	Computational surface modeling that underlies material recognition has transitioned from reflectance modeling using in-lab controlled radiometric measurements to image-based representations based on internet-mined single-view images captured in the scene. We take a middle-ground approach for material recognition that takes advantage of both rich radiometric cues and flexible image capture. A key concept is differential angular imaging, where small angular variations in image capture enables angular-gradient features for an enhanced appearance representation that improves recognition. We build a large-scale material database, Ground Terrain in Outdoor Scenes (GTOS) database, to support ground terrain recognition for applications such as autonomous driving and robot navigation. The database consists of over 30,000 images covering 40 classes of outdoor ground terrain under varying weather and lighting conditions. We develop a novel approach for material recognition called texture-encoded angular network (TEAN) that combines deep encoding pooling of RGB information and differential angular images for angular-gradient features to fully leverage this large dataset. With this novel network architecture, we extract characteristics of materials encoded in the angular and spatial gradients of their appearance. Our results show that TEAN achieves recognition performance that surpasses single view performance and standard (non-differential/large-angle sampling) multiview performance.	[Xue, Jia; Dana, Kristin J.] Rutgers Univ New Brunswick, Dept Elect & Comp Engn, New Brunswick, NJ 08901 USA; [Zhang, Hang] Amazon Web Serv Inc, Amazon AI, East Palto Alto, CA 94025 USA; [Nishino, Ko] Kyoto Univ, Grad Sch Informat, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan	Rutgers State University New Brunswick; Kyoto University	Xue, J (corresponding author), Rutgers Univ New Brunswick, Dept Elect & Comp Engn, New Brunswick, NJ 08901 USA.	jia.xue@rutgers.edu; hzaws@amazon.com; kon@i.kyoto-u.ac.jp; kristin.dana@rutgers.edu		Xue, Jia/0000-0001-7153-0568; Nishino, Ko/0000-0002-3534-3447	US National Science Foundation [IIS-1421134, IIS-1715195]	US National Science Foundation(National Science Foundation (NSF))	This work was supported by US National Science Foundation Award IIS-1421134 and IIS-1715195. A GPU used for this research was donated by the NVIDIA Corporation.	Andrearczyk V, 2016, PATTERN RECOGN LETT, V84, P63, DOI 10.1016/j.patrec.2016.08.016; Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970; Bu XY, 2019, PATTERN RECOGN, V91, P34, DOI 10.1016/j.patcog.2019.02.003; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Choe GM, 2016, PROC CVPR IEEE, P2452, DOI 10.1109/CVPR.2016.269; Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007; Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Cula OG, 2001, PROC CVPR IEEE, P1041; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; Dana KJ, 2004, J OPT SOC AM A, V21, P1, DOI 10.1364/JOSAA.21.000001; Dana KJ, 2016, IEEE SIGNAL PROC MAG, V33, P70, DOI 10.1109/MSP.2016.2580179; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Degol J, 2016, PROC CVPR IEEE, P1554, DOI 10.1109/CVPR.2016.172; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Deschaintre V, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13765; Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Filip J, 2014, COMPUT GRAPH FORUM, V33, P91, DOI 10.1111/cgf.12477; Filip J, 2009, IEEE T PATTERN ANAL, V31, P1921, DOI 10.1109/TPAMI.2008.246; Freeman WT, 1997, PROC CVPR IEEE, P554, DOI 10.1109/CVPR.1997.609380; Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042; Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kampouris C, 2016, LECT NOTES COMPUT SC, V9909, P778, DOI 10.1007/978-3-319-46454-1_47; Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5; Li Zi-xin, 2018, Advanced Technology of Electrical Engineering and Energy, V37, P1, DOI 10.12067/ATEEE1712020; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu C, 2014, IEEE T PATTERN ANAL, V36, P86, DOI 10.1109/TPAMI.2013.110; Liu L., 2018, ARXIV180110324, V3; Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; Nicodemus F. E., 1977, GEOMETRICAL CONSIDER, V160; Oxholm G, 2012, LECT NOTES COMPUT SC, V7572, P58, DOI 10.1007/978-3-642-33718-5_5; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Riviere J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130894; Salamati N, 2009, SEVENTEENTH COLOR IMAGING CONFERENCE - COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, P216; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Schwartz G, 2015, PROC CVPR IEEE, P3565, DOI 10.1109/CVPR.2015.7298979; Schwartz G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P883, DOI 10.1109/ICCVW.2013.121; Sharan L, 2014, J VISION, V14, DOI 10.1167/14.9.12; Simonyan K, 2014, ADV NEUR IN, V27; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tenenbaum JB, 1997, ADV NEUR IN, V9, P662; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wang Oliver, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2805, DOI 10.1109/CVPRW.2009.5206558; Wang TC, 2016, LECT NOTES COMPUT SC, V9907, P121, DOI 10.1007/978-3-319-46487-9_8; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; Weinmann M, 2014, LECT NOTES COMPUT SC, V8691, P156, DOI 10.1007/978-3-319-10578-9_11; Xue J, 2018, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2018.00065; Xue J, 2017, PROC CVPR IEEE, P6940, DOI 10.1109/CVPR.2017.734; Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309; Zhang H, 2015, PROC CVPR IEEE, P3071, DOI 10.1109/CVPR.2015.7298926	59	3	3	4	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1205	1218		10.1109/TPAMI.2020.3025121	http://dx.doi.org/10.1109/TPAMI.2020.3025121			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32946386	Green Submitted			2022-12-18	WOS:000752018000011
J	Ciano, G; Rossi, A; Bianchini, M; Scarselli, F				Ciano, Giorgio; Rossi, Alberto; Bianchini, Monica; Scarselli, Franco			On Inductive-Transductive Learning With Graph Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Neural networks; Computational modeling; Training; Encoding; Graph neural networks; Topology; Diffusion processes; Graph neural networks; transductive learning; inductive learning	BIOLOGY	Many real-world domains involve information naturally represented by graphs, where nodes denote basic patterns while edges stand for relationships among them. The graph neural network (GNN) is a machine learning model capable of directly managing graph-structured data. In the original framework, GNNs are inductively trained, adapting their parameters based on a supervised learning environment. However, GNNs can also take advantage of transductive learning, thanks to the natural way they make information flow and spread across the graph, using relationships among patterns. In this paper, we propose a mixed inductive-transductive GNN model, study its properties and introduce an experimental strategy that allows us to understand and distinguish the role of inductive and transductive learning. The preliminary experimental results show interesting properties for the mixed model, highlighting how the peculiarities of the problems and the data can impact on the two learning strategies.	[Ciano, Giorgio; Rossi, Alberto] Univ Florence, I-50121 Florence, Italy; [Ciano, Giorgio; Rossi, Alberto] Univ Siena, I-53100 Siena, Italy	University of Florence; University of Siena	Rossi, A (corresponding author), Univ Florence, I-50121 Florence, Italy.	giorgio.ciano@unifi.it; alberto.rossi@unifi.it; monica@diism.unisi.it; franco@diism.unisi.it		Ciano, Giorgio/0000-0003-2863-4315; Bianchini, Monica/0000-0002-8206-8142; rossi, alberto/0000-0003-1688-6961				Arnold A., 2007, PROC 7 IEEE INT C DA, P77, DOI DOI 10.1109/ICDMW.2007.109; Arrell DK, 2010, CLIN PHARMACOL THER, V88, P120, DOI 10.1038/clpt.2010.91; Barabasi AL, 2004, NAT REV GENET, V5, P101, DOI 10.1038/nrg1272; Belahcen A, 2015, SMART INNOV SYST TEC, V37, P83, DOI 10.1007/978-3-319-18164-6_9; Bianchini M., 2013, HDB NEURAL INFORM PR, P67; Bollob ~as B, 2001, RANDOM GRAPHS, V2nd; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J, 2013, PROC INT C LEARN REP; Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575; Castillo C., 2006, SIGIR Forum, V40, P11, DOI 10.1145/1189702.1189703; Castillo Carlos, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P423, DOI 10.1145/1277741.1277814; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Coppi D, 2016, IEEE T CIRC SYST VID, V26, P762, DOI 10.1109/TCSVT.2015.2416555; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Costa Fabrizio, 2010, INT C MACHINE LEARNI, P255, DOI DOI 10.1016/J.NEUROPHARM.2007.07.003; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Frasconi P, 1998, IEEE T NEURAL NETWOR, V9, P768, DOI 10.1109/72.712151; Gartner T, 2004, MACH LEARN, V57, P205, DOI 10.1023/B:MACH.0000039777.23772.30; Getoor Lise, 2007, INTRO STAT RELATIONA; Gilmer J, 2017, PR MACH LEARN RES, V70; Goldberg Y., 2017, NEURAL NETWORK METHO; Hagenbuchner M, 2003, IEEE T NEURAL NETWOR, V14, P491, DOI 10.1109/TNN.2003.810735; Hagenbuchner M, 2009, NEUROCOMPUTING, V72, P1419, DOI 10.1016/j.neucom.2008.12.021; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Henaff M, 2015, ARXIV150605163; Hu W., 2020, ADV NEURAL INFORM PR, V33, P22118; Joachims T., 2003, P 20 INT C MACH LEAR, P290, DOI DOI 10.1145/2612669.2612699; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Kondor R.I., 2002, P 19 INT C MACHINE L, P315; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Li K., 2012, P ACM BCB, P210; Li Yujia, 2015, ARXIV151105493; Liu W, 2009, PROC CVPR IEEE, P381, DOI 10.1109/CVPRW.2009.5206871; Niepert M, 2016, PR MACH LEARN RES, V48; Orsini F, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3756; Ramon J., 2003, PROC 1 INT WORKSHOP, P65; Rossi A, 2018, LECT NOTES ARTIF INT, V11081, P201, DOI 10.1007/978-3-319-99978-4_16; Scarselli F, 2018, NEURAL NETWORKS, V108, P248, DOI 10.1016/j.neunet.2018.08.010; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P81, DOI 10.1109/TNN.2008.2005141; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Sperduti A, 1997, IEEE T NEURAL NETWOR, V8, P714, DOI 10.1109/72.572108; Vapnik V.N, 1998, STAT LEARNING THEORY; Velickovic P., 2018, P INT C LEARN REPR; Wang KS, 2020, QUANT SCI STUD, V1, P396, DOI 10.1162/qss_a_00021; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Xu K., 2018, INT C LEARN REPR; Yang Z, 2016, PR MACH LEARN RES, V48; Zhou D., 2007, P 24 INT C MACHINE L, P1159, DOI DOI 10.1145/1273496.1273642	49	3	3	3	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					758	769		10.1109/TPAMI.2021.3054304	http://dx.doi.org/10.1109/TPAMI.2021.3054304			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	33493112				2022-12-18	WOS:000740006100017
J	Agudo, A				Agudo, Antonio			Unsupervised 3D Reconstruction and Grouping of Rigid and Non-Rigid Categories	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Category reconstruction; multiple unions of subspaces; class clustering; augmented lagrange multipliers	STRUCTURE-FROM-MOTION; SHAPE; FACTORIZATION; ALGORITHM; GEOMETRY	In this paper we present an approach to jointly recover camera pose, 3D shape, and object and deformation type grouping, from incomplete 2D annotations in a multi-instance collection of RGB images. Our approach is able to handle indistinctly both rigid and non-rigid categories. This advances existing work, which only addresses the problem for one single object or, they assume the groups to be known a priori when multiple instances are handled. In order to address this broader version of the problem, we encode object deformation by means of multiple unions of subspaces, that is able to span from small rigid motion to complex deformations. The model parameters are learned via Augmented Lagrange Multipliers, in a completely unsupervised manner that does not require any training data at all. Extensive experimental evaluation is provided in a wide variety of synthetic and real scenarios, including rigid and non-rigid categories with small and large deformations. We obtain state-of-the-art solutions in terms of 3D reconstruction accuracy, while also providing grouping results that allow splitting the input images into object instances and their associated type of deformation.	[Agudo, Antonio] CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain	Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i Informatica Industrial (IRII); Universitat Politecnica de Catalunya	Agudo, A (corresponding author), CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain.	aagudo@iri.upc.edu	Agudo, Antonio/C-5147-2017	Agudo, Antonio/0000-0001-6845-4998	CSIC project [R3OBJ 201850I099]; Spanish Ministry of Science and Innovation [HuMoUR TIN2017-90086-R]; Salvador de Madariaga [PRX19/00626]	CSIC project; Spanish Ministry of Science and Innovation(Ministry of Science and Innovation, Spain (MICINN)Spanish Government); Salvador de Madariaga	This work has been partially supported by the CSIC project R3OBJ 201850I099, the Spanish Ministry of Science and Innovation under project HuMoUR TIN2017-90086-R as well as the Salvador de Madariaga grant PRX19/00626.	Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148; Agudo A, 2018, PROC CVPR IEEE, P2607, DOI 10.1109/CVPR.2018.00276; Agudo A, 2018, IEEE IMAGE PROC, P2930, DOI 10.1109/ICIP.2018.8451235; Agudo A, 2018, IEEE T PATTERN ANAL, V40, P2137, DOI 10.1109/TPAMI.2017.2752710; Agudo A, 2019, IEEE T PATTERN ANAL, V41, P971, DOI 10.1109/TPAMI.2018.2823717; Agudo A, 2017, INT J COMPUT VISION, V122, P371, DOI 10.1007/s11263-016-0972-8; Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Bach F., 2008, ARXIV08121869V1; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Cabral R, 2013, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2013.309; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Cha G, 2019, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2019.00395; Chen WY, 2011, IEEE T PATTERN ANAL, V33, P568, DOI 10.1109/TPAMI.2010.88; Chen Y., 2011, PROC 28 INT C MACHIN, P873; Chhatkuli A, 2018, IEEE T PATTERN ANAL, V40, P2428, DOI 10.1109/TPAMI.2017.2762669; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905; Del Bue A, 2012, IEEE T PATTERN ANAL, V34, P1496, DOI 10.1109/TPAMI.2011.238; Del Pero L, 2015, PROC CVPR IEEE, P2151, DOI 10.1109/CVPR.2015.7298827; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fragkiadaki Katerina, 2014, ADV NEURAL INFORM PR, P55; Gao Y, 2016, LECT NOTES COMPUT SC, V9906, P408, DOI 10.1007/978-3-319-46475-6_26; Garg R, 2013, PROC CVPR IEEE, P1272, DOI 10.1109/CVPR.2013.168; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Golyanik V, 2017, IEEE WINT CONF APPL, P254, DOI 10.1109/WACV.2017.35; Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319; Gotardo PFU, 2011, IEEE T PATTERN ANAL, V33, P2051, DOI 10.1109/TPAMI.2011.50; Jongwoo Lim, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3489, DOI 10.1109/CVPR.2011.5995511; Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Kong C, 2019, IEEE I CONF COMP VIS, P1558, DOI 10.1109/ICCV.2019.00164; Kong C, 2016, INT CONF 3D VISION, P296, DOI 10.1109/3DV.2016.38; Kumar S, 2017, PATTERN RECOGN, V71, P428, DOI 10.1016/j.patcog.2017.05.014; Lee M, 2016, PROC CVPR IEEE, P4670, DOI 10.1109/CVPR.2016.505; Lee M, 2013, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR.2013.169; Lin Z., 2009, UILUENG092215; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Marques M, 2008, 2008 IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING, P127; Milborrow S., 2010, PATTERN RECOGN, P1; Moreno-Noguer F., 2016, P AS C COMP VIS, P291; Newcombe RA, 2010, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2010.5539794; Novotny D, 2019, IEEE I CONF COMP VIS, P7687, DOI 10.1109/ICCV.2019.00778; Paladini M, 2009, PROC CVPR IEEE, P2890; Parashar S, 2018, IEEE T PATTERN ANAL, V40, P2442, DOI 10.1109/TPAMI.2017.2760301; Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Sayd P., 2008, P IEEE C COMP VIS PA, P1; Shaji Appu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563071; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Wang QQ, 2018, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2018.00078; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Xu XY, 2019, IEEE I CONF COMP VIS, P1548, DOI 10.1109/ICCV.2019.00163; Yang JF, 2009, SIAM J IMAGING SCI, V2, P569, DOI 10.1137/080730421; Yao QM, 2019, IEEE T PATTERN ANAL, V41, P2628, DOI 10.1109/TPAMI.2018.2858249; Zass R., 2006, P ADV NEUR INF PROC, P1569; Zhang Z, 2019, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2019.00519; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459; Zhu YY, 2014, PROC CVPR IEEE, P1542, DOI 10.1109/CVPR.2014.200	64	3	3	4	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					519	532		10.1109/TPAMI.2020.3008276	http://dx.doi.org/10.1109/TPAMI.2020.3008276			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750810	Green Submitted			2022-12-18	WOS:000728561300037
J	Cao, JZ; Guo, Y; Wu, QY; Shen, CH; Huang, JZ; Tan, MK				Cao, Jiezhang; Guo, Yong; Wu, Qingyao; Shen, Chunhua; Huang, Junzhou; Tan, Mingkui			Improving Generative Adversarial Networks With Local Coordinate Coding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generative adversarial networks; local coordinate coding; latent distribution; generalization bound	DIMENSIONALITY	Generative adversarial networks (GANs) have shown remarkable success in generating realistic data from some predefined prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data. However, such latent distribution may incur difficulties in data sampling for GAN methods. In this paper, rather than sampling from the predefined prior distribution, we propose a GAN model with local coordinate coding (LCC), termed LCCGAN, to improve the performance of the image generation. First, we propose an LCC sampling method in LCCGAN to sample meaningful points from the latent manifold. With the LCC sampling method, we can explicitly exploit the local information on the latent manifold and thus produce new data with promising quality. Second, we propose an improved version, namely LCCGAN++, by introducing a higher-order term in the generator approximation. This term is able to achieve better approximation and thus further improve the performance. More critically, we derive the generalization bound for both LCCGAN and LCCGAN++ and prove that a low-dimensional input is sufficient to achieve good generalization performance. Extensive experiments on several benchmark datasets demonstrate the superiority of the proposed method over existing GAN methods.	[Cao, Jiezhang; Guo, Yong; Wu, Qingyao; Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510640, Guangdong, Peoples R China; [Cao, Jiezhang; Guo, Yong] Pazhou Lab, Guangzhou 510335, Peoples R China; [Shen, Chunhua] Univ Adelaide, Adelaide, SA 5005, Australia; [Huang, Junzhou] Tencent AI Lab, Shenzhen 518000, Peoples R China; [Huang, Junzhou] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA	South China University of Technology; Pazhou Lab; University of Adelaide; Tencent; University of Texas System; University of Texas Arlington	Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510640, Guangdong, Peoples R China.	secaojiezhang@mail.scut.edu.cn; guo.yong@mail.scut.edu.cn; qyw@scut.edu.cn; chunhua.shen@adelaide.edu.au; jzhuang@uta.edu; mingkuitan@scut.edu.cn		Guo, Yong/0000-0002-3444-4588; Shen, Chunhua/0000-0002-8648-8718	Key-Area Research and Development Program of Guangdong Province [2018B010107001]; National Natural Science Foundation of China (NSFC) [61836003]; Guangdong Project [2017ZT07X183]; Fundamental Research Funds for the Central Universities [D2191240]	Key-Area Research and Development Program of Guangdong Province; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Guangdong Project; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported in part by the Key-Area Research and Development Program of Guangdong Province (2018B010107001), National Natural Science Foundation of China (NSFC) 61836003 (key project), Guangdong Project 2017ZT07X183, Fundamental Research Funds for the Central Universities D2191240. Jiezhang Cao and Yong Guo contributed equally to this work.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Cao J., 2019, ADV NEURAL INF PROCE, P1776; Cao JZ, 2018, PR MACH LEARN RES, V80; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2017, ICLR 2017, P4; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352; Guo Yong, 2018, ARXIV180907099; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hensel M, 2017, ADV NEUR IN, V30; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; JIANG H, 2019, P INT C LEARN REPR; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin JX, 2021, IEEE T PATTERN ANAL, V43, P1254, DOI 10.1109/TPAMI.2019.2950198; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Makhzani A., 2015, ARXIV151105644; Mao XD, 2019, IEEE T PATTERN ANAL, V41, P2947, DOI 10.1109/TPAMI.2018.2872043; Mathieu Michael, 2016, ICLR; Miyato Takeru, 2018, ARXIV180205637; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Otberdout N, 2022, IEEE T PATTERN ANAL, V44, P848, DOI 10.1109/TPAMI.2020.3002500; Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348; Qi GJ, 2018, PROC CVPR IEEE, P1517, DOI 10.1109/CVPR.2018.00164; Radford A., 2015, ARXIV PREPR ARXIV151; Ranzato MarcAurelio, 2014, ARXIV14126604; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Salimans T, 2016, ADV NEUR IN, V29; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Thanh-Tung H, 2019, ARXIV190203984; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; ULYANOV D, 2018, P AAAI C ART INT; Xie SA, 2018, PR MACH LEARN RES, V80; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Yu Kai, 2009, ADV NEURAL INFORM PR, P2223; Yu Kai, 2010, ICML, P1215; Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang Pengchuan, 2018, P INT C LEARN REPR; Zhu FD, 2019, PROC CVPR IEEE, P11380, DOI 10.1109/CVPR.2019.01165; Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595	50	3	3	6	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					211	227		10.1109/TPAMI.2020.3012096	http://dx.doi.org/10.1109/TPAMI.2020.3012096			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750833	Green Submitted			2022-12-18	WOS:000728561300016
J	Gundogdu, E; Constantin, V; Parashar, S; Seifoddini, A; Dang, M; Salzmann, M; Fua, P				Gundogdu, Erhan; Constantin, Victor; Parashar, Shaifali; Seifoddini, Amrollah; Minh Dang; Salzmann, Mathieu; Fua, Pascal			GarNet plus plus : Improving Fast and Accurate Static 3D Cloth Draping by Curvature Loss	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						3D point cloud; garment draping; mesh convolution; physics-based simulation		In this paper, we tackle the problem of static 3D cloth draping on virtual human bodies. We introduce a two-stream deep network model that produces a visually plausible draping of a template cloth on virtual 3D bodies by extracting features from both the body and garment shapes. Our network learns to mimic a physics-based simulation (PBS) method while requiring two orders of magnitude less computation time. To train the network, we introduce loss terms inspired by PBS to produce plausible results and make the model collision aware. To increase the details of the draped garment, we introduce two loss functions that penalize the difference between the curvature of the predicted cloth and PBS. Particularly, we study the impact of mean curvature normal and a novel detail-preserving loss both qualitatively and quantitatively. Our new curvature loss computes the local covariance matrices of the 3D points, and compares the Rayleigh quotients of the prediction and PBS. This leads to more details while performing favorably or comparably against the loss that considers mean curvature normal vectors in the 3D triangulated meshes. We validate our framework on four garment types for various body shapes and poses. Finally, we achieve superior performance against a recently proposed data-driven method.	[Gundogdu, Erhan; Constantin, Victor; Parashar, Shaifali; Salzmann, Mathieu; Fua, Pascal] Ecole Polytech Fed Lausanne, Comp Vis Lab CVLab, CH-1015 Lausanne, Switzerland; [Seifoddini, Amrollah; Minh Dang] Fis Technol, CH-8006 Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Gundogdu, E (corresponding author), Ecole Polytech Fed Lausanne, Comp Vis Lab CVLab, CH-1015 Lausanne, Switzerland.	erhanguendogdu@gmail.com; victor.constantin@epfl.ch; shaifali.parashar@gmail.com; as@fision-technologies.com; minh.dang@fision-technologies.com; mathieu.salzmann@epfl.ch; pascal.fua@epfl.ch		Salzmann, Mathieu/0000-0002-8347-8637	Swiss Innovation Agency	Swiss Innovation Agency	This work was supported in part by a grant from Innosuisse, the Swiss Innovation Agency.	Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238; Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Bednarik J, 2018, INT CONF 3D VISION, P606, DOI 10.1109/3DV.2018.00075; Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552; Blackwell S., 2002, DESCRIPTIONS, V2, P192; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Brouet R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185532; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Danerek R, 2017, COMPUT GRAPH FORUM, V36, P269, DOI 10.1111/cgf.13125; de Aguiar E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778843; Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028; Gabeur V, 2019, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2019.00232; Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531; Guan P, 2010, LECT NOTES COMPUT SC, V6311, P285, DOI 10.1007/978-3-642-15549-9_21; Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883; Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970; Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Horn R.A., 2013, MATRIX ANAL, VSecond; Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39; Kim DY, 2013, ACM T GRAPHIC, V32, DOI [10.1145/2461912.2462020, 10.1145/2451236.2451241]; Kim T.-Y., 2008, P EUR SIGGRAPH S COM; King DB, 2015, ACS SYM SER, V1214, P1; Lahner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41; Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112; Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35; Miguel E, 2012, COMPUT GRAPH FORUM, V31, P519, DOI 10.1111/j.1467-8659.2012.03031.x; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Muller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005; Nvidia, 2018, NVCLOTH; Nvidia, 2018, NVIDIA FLEX; Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711; Popa T, 2009, COMPUT GRAPH FORUM, V28, P427, DOI 10.1111/j.1467-8659.2009.01382.x; Qi CR, 2017, ADV NEUR IN, V30; Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239; Santesteban Igor, 2019, Computer Graphics Forum, V38, P355, DOI 10.1111/cgf.13643; Sorkine Olga., 2004, P EUR ACM SIGGRAPH S, P175; Strang G., 2006, LINEAR ALGEBRA, V4th; Tang M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275005; Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275; Wang HM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778844; Wang Tuanfeng Y., 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275074; Wang TY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356512; Wang Wei, 2019, ADV NEURAL INFORM PR, P3162; Xu WP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181973; Yang JL, 2018, LECT NOTES COMPUT SC, V11211, P245, DOI 10.1007/978-3-030-01234-2_15; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282; Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295; Yu T, 2019, PROC CVPR IEEE, P5499, DOI 10.1109/CVPR.2019.00565; Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761; Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582; Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783	60	3	3	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					181	195		10.1109/TPAMI.2020.3010886	http://dx.doi.org/10.1109/TPAMI.2020.3010886			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750825	Green Submitted			2022-12-18	WOS:000728561300014
J	Milbich, T; Roth, K; Brattoli, B; Ommer, B				Milbich, Timo; Roth, Karsten; Brattoli, Biagio; Ommer, Bjoern			Sharing Matters for Generalization in Deep Metric Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deep metric learning; generalization; shared features; image retrieval; similarity learning; deep learning		Learning the similarity between images constitutes the foundation for numerous vision tasks. The common paradigm is discriminative metric learning, which seeks an embedding that separates different training classes. However, the main challenge is to learn a metric that not only generalizes from training to novel, but related, test samples. It should also transfer to different object classes. So what complementary information is missed by the discriminative paradigm? Besides finding characteristics that separate between classes, we also need them to likely occur in novel categories, which is indicated if they are shared across training classes. Thiswork investigates how to learn such characteristics without the need for extra annotations or training data. By formulating our approach as a novel triplet sampling strategy, it can be easily applied on top of recent ranking loss frameworks. Experiments show that, independent of the underlying network architecture and the specific ranking loss, our approach significantly improves performance in deep metric learning, leading to new the state-of-the-art results on various standard benchmark datasets.	[Milbich, Timo; Roth, Karsten; Brattoli, Biagio; Ommer, Bjoern] Heidelberg Univ, Mathematikon INF 205, Heidelberg Collaboratory Image Proc HCI & IWR, D-69120 Heidelberg, Germany	Ruprecht Karls University Heidelberg	Milbich, T (corresponding author), Heidelberg Univ, Mathematikon INF 205, Heidelberg Collaboratory Image Proc HCI & IWR, D-69120 Heidelberg, Germany.	timo_milbich@gmx.de; Karsten.Rh1@gmail.com; biagio.brattoli@gmail.com; ommer@uni-heidelberg.de		Milbich, Timo/0000-0002-5012-0874	Bayer AG; German federal ministry BMWi; German Research Foundation DFG [EXC 2181/1 -390900948]	Bayer AG(Bayer AG); German federal ministry BMWi; German Research Foundation DFG(German Research Foundation (DFG))	This work has been supported in part by Bayer AG, the German federal ministry BMWi within the project "KI Absicherung", the German Research Foundation DFG within EXC 2181/1 -390900948, and a hardware donation from NVIDIA corporation. Timo Milbich and Karsten Roth contributed equally to this work.	[Anonymous], 2017, SPHERE GAME N DIMENS; Bautista M. A., 2016, ADV NEURAL INFORM PR, V29, P3846; Bhattarai B, 2016, PROC CVPR IEEE, P4226, DOI 10.1109/CVPR.2016.458; Brattoli B, 2017, PROC CVPR IEEE, P3747, DOI 10.1109/CVPR.2017.399; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Chawla NV, 2004, ACM SIGKDD EXPLOR NE, V6, P1, DOI [DOI 10.1145/1007730.1007733, 10.1145/1007730.1007733]; Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294; Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Lin XD, 2018, LECT NOTES COMPUT SC, V11219, P714, DOI 10.1007/978-3-030-01267-0_42; Lorenz D, 2019, PROC CVPR IEEE, P10947, DOI 10.1109/CVPR.2019.01121; Milbich T, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107107; Milbich T, 2017, IEEE I CONF COMP VIS, P4404, DOI 10.1109/ICCV.2017.471; Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47; Noh H, 2017, ADV NEUR IN, V30; Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pu J, 2014, LECT NOTES COMPUT SC, V8691, P425, DOI 10.1007/978-3-319-10578-9_28; Roth K, 2019, IEEE I CONF COMP VIS, P7999, DOI 10.1109/ICCV.2019.00809; Roth Karsten, 2020, REVISITING TRAINING; Sanakoyeu A, 2019, PROC CVPR IEEE, P471, DOI 10.1109/CVPR.2019.00056; Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sohn K, 2016, ADV NEUR IN, V29; Song HO, 2017, PROC CVPR IEEE, P2206, DOI 10.1109/CVPR.2017.237; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Ustinova E., 2016, ADV NEURAL INFORM PR, V29, P4170; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wah C., 2011, CALTECHUCSD BIRDS 20; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Wang XS, 2022, IEEE T PATTERN ANAL, V44, P5414, DOI [10.1109/TPAMI.2021.3068449, 10.1109/CVPR.2019.00535]; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Xuan H, 2018, LECT NOTES COMPUT SC, V11220, P751, DOI 10.1007/978-3-030-01270-0_44; Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94; Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31; Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016	43	3	3	3	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					416	427		10.1109/TPAMI.2020.3009620	http://dx.doi.org/10.1109/TPAMI.2020.3009620			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750817	Green Submitted			2022-12-18	WOS:000728561300030
J	Pan, XD; Zhang, M; Ding, DZ; Yang, M				Pan, Xudong; Zhang, Mi; Ding, Daizong; Yang, Min			A Geometrical Perspective on Image Style Transfer With Adversarial Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generative adversarial learning; unsupervised learning theory; generalization theory; machine learning		Recent years witness the booming trend of applying generative adversarial nets (GAN) and its variants to image style transfer. Although many reported results strongly demonstrate the power of GAN on this task, there is still little known about neither the interpretations of several fundamental phenomenons of image style transfer by generative adversarial learning, nor its underlying mechanism. To bridge this gap, this paper presents a general framework for analyzing style transfer with adversarial learning through the lens of differential geometry. To demonstrate the utility of our proposed framework, we provide an in-depth analysis of Isola et al.'s pioneering style transfer model pix2pix [1] and reach a comprehensive interpretation on their major experimental phenomena. Furthermore, we extend the notion of generalization to conditional GAN and derive a condition to control the generalization capability of the pix2pix model. From a higher viewpoint, we further prove a learning-free condition to guarantee the existence of infinitely many perfect style transfer mappings. Besides, we also provide a number of practical suggestions on model design and dataset construction based on these derived theoretical results to facilitate further researches.	[Pan, Xudong; Zhang, Mi; Ding, Daizong; Yang, Min] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China	Fudan University	Zhang, M; Yang, M (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.	xdpan18@fudan.edu.cn; mi_zhang@fudan.edu.cn; dzding17@fudan.edu.cn; m_yang@fudan.edu.cn			National Natural Science Foundation of China [61972099, U1636204, U1836213, U1836210, U1736208]; Natural Science Foundation of Shanghai [19ZR1404800]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Shanghai(Natural Science Foundation of Shanghai)	The authors would like to thank the anonymous reviewers and the editors for their constructive comments and input to improve our manuscript. This work was supported in part by the National Natural Science Foundation of China (61972099, U1636204, U1836213, U1836210, and U1736208), and Natural Science Foundation of Shanghai (19ZR1404800). Min Yang is a faculty of Shanghai Institute of Intelligent Electronics& Systems, Shanghai Institute for Advanced Communication and Data Science, and Engineering Research Center of CyberSecurity Auditing and Monitoring, Ministry of Education, China.	Amari S.-I., 2007, METHODS INFORM GEOME; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Aumann R.J., 1989, GAME THEORY; Azencott, 1999, EUROCOLT, P230; Balduzzi D, 2018, PR MACH LEARN RES, V80; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Boyd S, 2004, CONVEX OPTIMIZATION; Cameron P., 1999, PERMUTATION GROUPS, V45; Cao JZ, 2018, PR MACH LEARN RES, V80; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Chung K. L., 2001, COURSE PROBABILITY T; Dai W., 2009, ADV NEURAL INFORM PR, P353; Edelsbrunner H, 2008, CONTEMP MATH, V453, P257; Fukaya K., 1990, ADV STUD PURE MATH, V18, P143; GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Harer Jacob, 2018, P ADV NEUR INF PROC, P7933; Hinton Geoffrey, 2002, ADV NEURAL INFORM PR, V15, P833, DOI DOI 10.1109/TSMCB.2011.2106208; Huang ZW, 2019, AAAI CONF ARTIF INTE, P3886; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jost J, 2008, UNIVERSITEXT, P1; Khrulkov V, 2018, PR MACH LEARN RES, V80; Lee J, 2010, INTRO TOPOLOGICAL MA, V940; Lei N, 2019, COMPUT AIDED GEOM D, V68, P1, DOI 10.1016/j.cagd.2018.10.005; Li YZ, 2017, ADV NEUR IN, V30; Lin Z., 2018, NIPS, P1498; Lu HM, 1998, P SOC PHOTO-OPT INS, V3307, P52, DOI 10.1117/12.304659; Lukaszyk S, 2004, COMPUT MECH, V33, P299, DOI 10.1007/s00466-003-0532-2; Madry Aleksander, 2017, ARXIV; Mescheder L, 2018, PR MACH LEARN RES, V80; Miculescu, REAL ANALYS EXCHANGE, V26, P449; Mirza M., 2014, ARXIV PREPRINT ARXIV; Nakahara M., 2018, GEOMETRY TOPOLOGY PH; PAN X, 2018, PROC INT C MACH LEAR, P4006; Qi GJ, 2018, PROC CVPR IEEE, P1517, DOI 10.1109/CVPR.2018.00164; Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rudin W, 2010, REAL COMPLEX ANAL; Saito Y, 2018, IEEE-ACM T AUDIO SPE, V26, P84, DOI 10.1109/TASLP.2017.2761547; Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175; Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359; Sontag E. D., 1998, Neural Networks and Machine Learning. Proceedings, P69; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Vapnik V.N, 1998, STAT LEARNING THEORY; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Yang C, 2019, IEEE T IMAGE PROCESS, V28, P4845, DOI 10.1109/TIP.2019.2914583; Yoon J, 2018, PR MACH LEARN RES, V80; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zeiler M. D., 2011, ADV NEURAL INFORM PR, P1629; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	54	3	3	5	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					63	75		10.1109/TPAMI.2020.3011143	http://dx.doi.org/10.1109/TPAMI.2020.3011143			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750829				2022-12-18	WOS:000728561300006
J	Chen, JL; Xie, YJ; Wang, K; Zhang, C; Vannan, MA; Wang, B; Qian, Z				Chen, Jialei; Xie, Yujia; Wang, Kan; Zhang, Chuck; Vannan, Mani A.; Wang, Ben; Qian, Zhen			Active Image Synthesis for Efficient Labeling	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Gallium nitride; Labeling; Manufacturing; Feature extraction; Generative adversarial networks; Medical services; Active learning; computer-aided diagnosis; data augmentation; generative adversarial networks; small-data learning		The great success achieved by deep neural networks attracts increasing attention from the manufacturing and healthcare communities. However, the limited availability of data and high costs of data collection are the major challenges for the applications in those fields. We propose in this work AISEL, an active image synthesis method for efficient labeling, to improve the performance of the small-data learning tasks. Specifically, a complementary AISEL dataset is generated, with labels actively acquired via a physics-based method to incorporate underlining physical knowledge at hand. An important component of our AISEL method is the bidirectional generative invertible network (GIN), which can extract interpretable features from the training images and generate physically meaningful virtual images. Our AISEL method then efficiently samples virtual images not only further exploits the uncertain regions but also explores the entire image space. We then discuss the interpretability of GIN both theoretically and experimentally, demonstrating clear visual improvements over the benchmarks. Finally, we demonstrate the effectiveness of our AISEL framework on aortic stenosis application, in which our method lowers the labeling cost by 90 percent while achieving a 15 percent improvement in prediction accuracy.	[Chen, Jialei; Zhang, Chuck] Georgia Inst Technol, Georgia Tech Mfg Inst, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA; [Xie, Yujia] Georgia Inst Technol, Ger Tech Mfg Inst, Atlanta, GA 30332 USA; [Wang, Kan] Georgia Inst Technol, Sch Computat Sci & Engn, Atlanta, GA 30332 USA; [Wang, Ben] Georgia Inst Technol, Sch Mat Sci & Engn, H Milton Stewart Sch Ind & Syst Engn, Ger Tech Mfg Inst, Atlanta, GA 30309 USA; [Vannan, Mani A.] Piedmont Heart Inst, Marcus Heart Valve Ctr, Atlanta, GA 30309 USA; [Qian, Zhen] Tencent Amer, Hippocrates Res Lab, Palo Alto, CA 94306 USA	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology; Piedmont Heart Institute	Chen, JL (corresponding author), Georgia Inst Technol, Georgia Tech Mfg Inst, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	jialei.chen@gatech.edu; xieyujia@gatech.edu; kwang34@mail.gatech.edu; chuck.zhang@gatech.edu; Vannan@piedmont.org; ben.wang@gatech.edu; qianzhen@tencent.com		Chen, Jialei/0000-0002-6053-9302	U.S. National Science Foundation [CMMI-1921646]	U.S. National Science Foundation(National Science Foundation (NSF))	This work was supported by a U.S. National Science Foundation under Grant CMMI-1921646. The authors would like to thank Dr. Shizhen Liu at Piedmont Heart Institute in Atlanta, GA, for providing medical knowledge. The authors also want to thank Dr. Zih Huei Wang at Feng Chia University in Taiwan and Dr. Geet Lahoti at Kabbage, Inc. for the insightful discussions.	Anderson J.D, 1995, COMPUTATIONAL FLUID, V206; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Burges, 1998, MNIST DATABASE HANDW; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen J., 2019, ARXIV191005452; Chen JL, 2018, ADDIT MANUF, V24, P341, DOI 10.1016/j.addma.2018.10.007; Chen JL, 2018, LECT NOTES COMPUT SC, V11070, P537, DOI 10.1007/978-3-030-00928-1_61; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J., 2016, ARXIV160509782; Du S. S., 2018, STAT-US, V1050, P21; Ducoffe Melanie, 2018, ARXIV180209841; Dumoulin Vincent, 2016, ARXIV E PRINTS; Frid-Adar M, 2018, I S BIOMED IMAGING, P289; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Joseph VR, 2015, TECHNOMETRICS, V57, P64, DOI 10.1080/00401706.2014.881749; Kingma D. P, 2014, ARXIV13126114; Kingma DP, 2018, ADV NEUR IN, V31; LeCun Y., 2015, NATURE, V521, P436, DOI DOI 10.1038/NATURE14539; LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Mak S., 2019, ARXIV191001754; Mak S, 2018, ANN STAT, V46, P2562, DOI 10.1214/17-AOS1629; Matheron G., 1963, EC GEOLOGY, V58, P1246, DOI [10.2113/gsecongeo.58.8.1246, DOI 10.2113/GSECONGEO.58.8.1246]; MCCULLOCH WS, 1990, B MATH BIOL, V52, P99, DOI 10.1016/S0092-8240(05)80006-0; Mirza M., 2014, ARXIV; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Odena A, 2017, PR MACH LEARN RES, V70; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Qian Z, 2017, JACC-CARDIOVASC IMAG, V10, P719, DOI 10.1016/j.jcmg.2017.04.005; Raissi M, 2019, J COMPUT PHYS, V378, P686, DOI 10.1016/j.jcp.2018.10.045; Resnick S., 2013, PROBABILITY PATH; Settles B., 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI [10.2200/s00429ed1v01y201207aim018, DOI 10.2200/S00429ED1V01Y201207AIM018]; Settles Burr, 2009, TR1648 U WISC MAD DE, P2; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900; Stewart BF, 1997, J AM COLL CARDIOL, V29, P630, DOI 10.1016/S0735-1097(96)00563-3; Thompson PA, 1997, NATURE, V389, P360, DOI 10.1038/38686; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P1, DOI 10.1007/978-3-540-71050-9; Wang J., 2017, CONVOLUTIONAL NEURAL; Wang JJ, 2018, J MANUF SYST, V48, P144, DOI 10.1016/j.jmsy.2018.01.003; Winer B.J., 1962, STAT PRINCIPLES EXPT; Xiao H., 2017, ARXIV 170807747; YaqingWang Quanming Yao, 2019, ACM COMPUTING SURVEY, P1; Yosinski J, 2014, ADV NEUR IN, V27; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zalando research, 2017, FASH MNIST MNIST LIK; Zhao S., 2017, ARXIV170208658; Zhu J., 2017, ARXIV170207956; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zienkiewicz O. C., 1977, FINITE ELEMENT METHO, V36	54	3	3	4	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					3770	3781		10.1109/TPAMI.2020.2993221	http://dx.doi.org/10.1109/TPAMI.2020.2993221			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32406823	Green Submitted			2022-12-18	WOS:000702649700007
J	Ji, MQ; Zhang, JZ; Dai, QH; Fang, L				Ji, Mengqi; Zhang, Jinzhi; Dai, Qionghai; Fang, Lu			SurfaceNet plus : An End-to-end 3D Neural Network for Very Sparse Multi-View Stereopsis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Cameras; Surface reconstruction; Two dimensional displays; Solid modeling; Geometry; Image reconstruction; Multi-view stereopsis; volumetric MVS; sparse views; occlusion aware; view selection	RECONSTRUCTION	Multi-view stereopsis (MVS) tries to recover the 3D model from 2D images. As the observations become sparser, the significant 3D information loss makes the MVS problem more challenging. Instead of only focusing on densely sampled conditions, we investigate sparse-MVS with large baseline angles since the sparser sensation is more practical and more cost-efficient. By investigating various observation sparsities, we show that the classical depth-fusion pipeline becomes powerless for the case with a larger baseline angle that worsens the photo-consistency check. As another line of the solution, we present SurfaceNet+, a volumetric method to handle the 'incompleteness' and the 'inaccuracy' problems induced by a very sparse MVS setup. Specifically, the former problem is handled by a novel volume-wise view selection approach. It owns superiority in selecting valid views while discarding invalid occluded views by considering the geometric prior. Furthermore, the latter problem is handled via a multi-scale strategy that consequently refines the recovered geometry around the region with the repeating pattern. The experiments demonstrate the tremendous performance gap between SurfaceNet+ and state-of-the-art methods in terms of precision and recall. Under the extreme sparse-MVS settings in two datasets, where existing methods can only return very few points, SurfaceNet+ still works as well as in the dense MVS setting.	[Ji, Mengqi] Tsinghua Univ, Beijing 100084, Peoples R China; [Zhang, Jinzhi] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst TBSI, Beijing 100084, Peoples R China; [Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China; [Dai, Qionghai] Tsinghua Univ, Sch Life Sci, Beijing 100084, Peoples R China; [Fang, Lu] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China	Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University	Fang, L (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.	mji@tsinghua.edu.cn; zhang-jz19@tsinghua.org.cn; daiqionghai@tsinghua.edu.cn; fanglu@tsinghua.edu.cn	Ji, Mengqi/AAA-1232-2020; Dai, Qionghai/ABD-5298-2021	Ji, Mengqi/0000-0002-9210-6851; Dai, Qionghai/0000-0001-7043-3061	Natural Science Foundation of China (NSFC) [61722209, 61860206003]; Shenzhen Science and Technology Research and Development Funds [JCYJ20180507183706645, ZDYBH201900000002]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Shenzhen Science and Technology Research and Development Funds	This work was supported in part by the Natural Science Foundation of China (NSFC) under contract No. 61722209 and 61860206003, in part by the Shenzhen Science and Technology Research and Development Funds (JCYJ20180507183706645 and ZDYBH201900000002). Mengqi Ji and Jinzhi Zhang contributed equally to this work.	Aanaes H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chen R, 2021, IEEE T PATTERN ANAL, V43, P3695, DOI 10.1109/TPAMI.2020.2988729; Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162; Chen Weifeng, 2016, NEURIPS, P2, DOI DOI 10.5555/3157096.3157178; Cheng S, 2019, ARXIV191112012; Chung J, 2019, OPTICA, V6, P647, DOI [10.1364/OPTICA.6.000647, 10.1364/optica.6.000647]; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Galliani S, 2016, PROC CVPR IEEE, P5479, DOI 10.1109/CVPR.2016.591; Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933; Gu X., 2019, ARXIV191206378; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Huang Z, 2018, LECT NOTES COMPUT SC, V11220, P351, DOI 10.1007/978-3-030-01270-0_21; Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693; Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Kar A., 2017, ADV NEURAL INFORM PR; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Knapitsch Arno, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073599; Knobelreiter P, 2017, PROC CVPR IEEE, P1456, DOI 10.1109/CVPR.2017.159; Kutulakos K. N., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P307, DOI 10.1109/ICCV.1999.791235; Lempitsky V., 2007, PROC IEEE C COMPUT V, P1, DOI [10.1109/CVPR.2007.383293, DOI 10.1109/CVPR.2007.383293]; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; Liao J, 2019, COMPUT GRAPH FORUM, V38, P335, DOI 10.1111/cgf.13841; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Mateos J, 2009, IEEE IMAGE PROC, P129, DOI 10.1109/ICIP.2009.5414169; Merrell P, 2007, IEEE I CONF COMP VIS, P3012, DOI 10.1109/iccv.2007.4408984; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Pang JH, 2014, IEEE IMAGE PROC, P4687, DOI 10.1109/ICIP.2014.7025950; Paschalidou D, 2018, PROC CVPR IEEE, P3897, DOI 10.1109/CVPR.2018.00410; Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393; Xu LF, 2013, 3D RES, V4, DOI 10.1007/3DRes.02(2013)1; Xu QS, 2019, PROC CVPR IEEE, P5478, DOI 10.1109/CVPR.2019.00563; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Yucer K, 2016, INT CONF 3D VISION, P249, DOI 10.1109/3DV.2016.33; Zach C, 2008, P INT S 3D DAT PROC, V1; Zheng AM, 2015, IEEE IMAGE PROC, P4371, DOI 10.1109/ICIP.2015.7351632	52	3	3	5	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					4078	4093		10.1109/TPAMI.2020.2996798	http://dx.doi.org/10.1109/TPAMI.2020.2996798			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32750770	Green Submitted			2022-12-18	WOS:000702649700027
J	Won, C; Ryu, J; Lim, J				Won, Changhee; Ryu, Jongbin; Lim, Jongwoo			End-to-End Learning for Omnidirectional Stereo Matching With Uncertainty Prior	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Estimation; Three-dimensional displays; Neural networks; Uncertainty; Lenses; Computational modeling; Deep neural network; stereo matching; omnidirectional 3D estimation		In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra-wide field-of-view cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce an omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. For more accurate depth estimation we also propose an uncertainty prior guidance in two ways: depth map filtering and guiding regularization. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 13K ground-truth depth maps and 53K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms.	[Won, Changhee; Lim, Jongwoo] MultiplEYE Co Ltd, Seoul, South Korea; [Won, Changhee; Ryu, Jongbin; Lim, Jongwoo] Hanyang Univ, Dept Comp Sci, Seoul 04763, South Korea	Hanyang University	Won, C (corresponding author), MultiplEYE Co Ltd, Seoul, South Korea.	chwon@buffalo.edu; jongbinryu@hanyang.ac.kr; jlim@hanyang.ac.kr		Won, Changhee/0000-0002-7307-2799	National Research Foundation of Korea (NRF) - Ministry of Science, ICT [NRF-2017M3C4A7069369]; NRF - Korea government (MSIT) [NRF-2019R1A4A1029800]	National Research Foundation of Korea (NRF) - Ministry of Science, ICT; NRF - Korea government (MSIT)	This work was supported in part by Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT (NRF-2017M3C4A7069369), and the NRF Grant funded by the Korea government (MSIT) (NRF-2019R1A4A1029800).	Agarwal S., 2010, CERES SOLVER; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chang Angel X., 2015, ARXIV151203012CSGR P; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen ZY, 2015, IEEE I CONF COMP VIS, P972, DOI 10.1109/ICCV.2015.117; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Cui ZP, 2019, IEEE INT CONF ROBOT, P6087; de La Garanderie G. Payen, 2018, P EUR C COMP VIS, P789; Gao WL, 2017, IEEE INT C INT ROBOT, P6715; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Guney F, 2015, PROC CVPR IEEE, P4165, DOI 10.1109/CVPR.2015.7299044; Hane C, 2014, INT C 3D VIS 3DV, P57, DOI DOI 10.1109/3DV.2014.77; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hirschmuller Heiko, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383248; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38; Im S, 2016, LECT NOTES COMPUT SC, V9907, P156, DOI 10.1007/978-3-319-46487-9_10; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim S, 2019, PROC CVPR IEEE, P205, DOI 10.1109/CVPR.2019.00029; Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614; MARSAGLIA G, 1972, ANN MATH STAT, V43, P645, DOI 10.1214/aoms/1177692644; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108; Park MG, 2019, IEEE T PATTERN ANAL, V41, P1397, DOI 10.1109/TPAMI.2018.2837760; Richter SR, 2017, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2017.243; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Scaramuzza Davide, 2006, IEEE INT C COMP VIS; Scharstein D, 1998, INT J COMPUT VISION, V28, P155, DOI 10.1023/A:1008015117424; Scharstein D, 2003, PROC CVPR IEEE, P195; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Schonbein M, 2014, IEEE INT C INT ROBOT, P716, DOI 10.1109/IROS.2014.6942637; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Urban S, 2015, ISPRS J PHOTOGRAMM, V108, P72, DOI 10.1016/j.isprsjprs.2015.06.005; Wang YC, 2012, INT J ADV ROBOT SYST, V9, DOI 10.5772/50541; Won C, 2019, IEEE I CONF COMP VIS, P8986, DOI 10.1109/ICCV.2019.00908; Won C, 2019, IEEE INT CONF ROBOT, P6073, DOI 10.1109/ICRA.2019.8793823; Xu L, 2008, LECT NOTES COMPUT SC, V5305, P775; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zbontar J, 2016, J MACH LEARN RES, V17; Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027; Zhang ZC, 2016, IEEE INT CONF ROBOT, P801, DOI 10.1109/ICRA.2016.7487210; Zioulis N, 2018, LECT NOTES COMPUT SC, V11210, P453, DOI 10.1007/978-3-030-01231-1_28	45	3	3	7	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					3850	3862		10.1109/TPAMI.2020.2992497	http://dx.doi.org/10.1109/TPAMI.2020.2992497			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32386142				2022-12-18	WOS:000702649700012
J	Ye, HJ; Zhan, DC; Jiang, Y; Zhou, ZH				Ye, Han-Jia; Zhan, De-Chuan; Jiang, Yuan; Zhou, Zhi-Hua			Heterogeneous Few-Shot Model Rectification With Semantic Mapping	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Adaptation models; Predictive models; Data models; Training; Semantics; Robustness; Model reuse; heterogeneous model reuse; few-shot learning; transfer learning; meta representation; learnware		There still involve lots of challenges when applying machine learning algorithms in unknown environments, especially those with limited training data. To handle the data insufficiency and make a further step towards robust learning, we adopt the learnware notion Z.-H. Zhou, "Learnware: On the future of machine learning," Front. Comput. Sci., vol. 10, no. 4 pp. 589-590, 2016 which equips a model with an essential reusable property-the model learned in a related task could be easily adapted to the current data-scarce environment without data sharing. To this end, we propose the REctiFy via heterOgeneous pRedictor Mapping (ReForm) framework enabling the current model to take advantage of a related model from two kinds of heterogeneous environment, i.e., either with different sets of features or labels. By Encoding Meta InformaTion (Emit) of features and labels as the model specification, we utilize an optimal transported semantic mapping to characterize and bridge the environment changes. After fine-tuning over a few labeled examples through a biased regularization objective, the transformed heterogeneous model adapts to the current task efficiently. We apply ReForm over both synthetic and real-world tasks such as few-shot image classification with either learned or pre-defined specifications. Experimental results validate the effectiveness and practical utility of the proposed ReForm framework.	[Ye, Han-Jia; Zhan, De-Chuan; Jiang, Yuan; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China	Nanjing University	Ye, HJ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.	yehj@lamda.nju.edu.cn; zhandc@lamda.nju.edu.cn; jiangy@lamda.nju.edu.cn; zhouz@lamda.nju.edu.cn	jiang, anyi/GPT-0379-2022		National Science Foundation of China [61673201, 61773198, 61921006]; Collaborative Innovation Center of Novel Software Technology and Industrialization	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Center of Novel Software Technology and Industrialization	This research was supported by the National Science Foundation of China (61673201, 61773198, 61921006), and the Collaborative Innovation Center of Novel Software Technology and Industrialization.	Aljundi R, 2015, PROC CVPR IEEE, P56, DOI 10.1109/CVPR.2015.7298600; [Anonymous], 2007, P 15 ACM INT C MULTI; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Chen Wei-Yu, 2019, INT C LEARN REPR, P12; Chen Z, 2018, PR MACH LEARN RES, V80; Courty N, 2017, ADV NEUR IN, V30; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Darrell Trevor, 2013, ABS13013224 CORR; Dietterich TG, 2019, FRONT COMPUT SCI-CHI, V13, P1, DOI 10.1007/s11704-018-8900-4; Dietterich TG, 2017, AI MAG, V38, P3, DOI 10.1609/aimag.v38i3.2756; Finn C, 2017, PR MACH LEARN RES, V70; Gong B., 2013, P INT C MACH LEARN J, V711, P712; Guo XY, 2019, FRONT COMPUT SCI-CHI, V13, P99, DOI 10.1007/s11704-018-7138-5; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Hou CP, 2018, IEEE T PATTERN ANAL, V40, P2776, DOI 10.1109/TPAMI.2017.2769047; Hou RB, 2019, ADV NEUR IN, V32; Huang G., 2016, PROC NEURAL INF PROC, P4869; Huang ZP, 2018, ACM T KNOWL DISCOV D, V12, DOI 10.1145/3233186; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Koch G., 2015, ICML DEEP LEARNING W; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Kuzborskij I, 2017, MACH LEARN, V106, P171, DOI 10.1007/s10994-016-5594-4; Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431; Lake Brenden, 2011, C COGN SCI SOC, P6; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lin X, 2019, ADV NEUR IN, V32; Lin ZQ, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-018-9915-8; Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1805, DOI 10.1109/TKDE.2013.97; Lopez-Paz D, 2017, ADV NEUR IN, V30; Luise G, 2019, PR MACH LEARN RES, V97; Mahadevan, 2011, IJCAI, V22, P1541, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-259; Maurer A, 2016, J MACH LEARN RES, V17; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755; Mikolov T., 2013, EFFICIENT ESTIMATION, P1, DOI DOI 10.48550/ARXIV.1301.3781; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188; Perrot M., 2016, ADV NEURAL INFORM PR, V29, P4197; Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755; Ravi S., 2017, P INT C LEARN REPR, P1; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Ren M., 2018, ICLR; Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701; Santambrogio F, 2015, PROG NONLINEAR DIFFE, V87, P1, DOI 10.1007/978-3-319-20828-2; Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126; Snell J, 2017, ADV NEUR IN, V30; Tommasi T, 2014, IEEE T PATTERN ANAL, V36, P928, DOI 10.1109/TPAMI.2013.197; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang H., 2014, P 30 INT C MACH LEAR, P352; Wang H, 2014, ADV NEURAL INFORM PR, P2816; Wang H, 2013, IEEE I CONF COMP VIS, P1145, DOI 10.1109/ICCV.2013.146; Wang Y., 2019, ABS191104623 ARXIV; Wang YC, 2017, ADV NEUR IN, V30; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Xiao H., 2017, FASHION MNIST NOVEL; Xiaoxiao Shi, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1049, DOI 10.1109/ICDM.2010.65; Yang Y, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1033; Ye H.-J., 2018, ARXIV PREPRINT ARXIV; Ye HJ, 2020, MACH LEARN, V109, P643, DOI 10.1007/s10994-019-05838-7; Ye J., 2007, P INT C ART INT STAT, P644; Yoon SW, 2019, PR MACH LEARN RES, V97; Zhao PL, 2014, ARTIF INTELL, V216, P76, DOI 10.1016/j.artint.2014.06.003; Zhou Z.-H, 2012, ENSEMBLE METHODS FDN, DOI DOI 10.1201/B12207; Zhou ZH, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-018-9801-4; Zhou ZH, 2016, FRONT COMPUT SCI-CHI, V10, P589, DOI 10.1007/s11704-016-6906-3; Zhu Y., 2011, P 25 AAAI C ART INT	80	3	3	1	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					3878	3891		10.1109/TPAMI.2020.2994749	http://dx.doi.org/10.1109/TPAMI.2020.2994749			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32750764				2022-12-18	WOS:000702649700014
J	Zhang, QS; Wang, X; Cao, RM; Wu, YN; Shi, F; Zhu, SC				Zhang, Quanshi; Wang, Xin; Cao, Ruiming; Wu, Ying Nian; Shi, Feng; Zhu, Song-Chun			Extraction of an Explanatory Graph to Interpret a CNN	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Visualization; Neural networks; Semantics; Annotations; Task analysis; Training; Convolutional neural networks; graphical model; interpretable deep learning		This paper introduces an explanatory graph representation to reveal object parts encoded inside convolutional layers of a CNN. Given a pre-trained CNN, each filter(1) in a conv-layer usually represents a mixture of object parts. We develop a simple yet effective method to learn an explanatory graph, which automatically disentangles object parts from each filter without any part annotations. Specifically, given the feature map of a filter, we mine neural activations from the feature map, which correspond to different object parts. The explanatory graph is constructed to organize each mined part as a graph node. Each edge connects two nodes, whose corresponding object parts usually co-activate and keep a stable spatial relationship. Experiments show that each graph node consistently represented the same object part through different images, which boosted the transferability of CNN features. The explanatory graph transferred features of object parts to the task of part localization, and our method significantly outperformed other approaches.	[Zhang, Quanshi; Wang, Xin] Shanghai Jiao Tong Univ, John Hopcroft Ctr, Shanghai 200240, Peoples R China; [Zhang, Quanshi; Wang, Xin] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence AI Inst, Shanghai 200240, Peoples R China; [Cao, Ruiming; Wu, Ying Nian; Shi, Feng; Zhu, Song-Chun] Univ Calif Los Angeles, Los Angeles, CA 90095 USA	Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of California System; University of California Los Angeles	Zhang, QS (corresponding author), Shanghai Jiao Tong Univ, John Hopcroft Ctr, Shanghai 200240, Peoples R China.; Zhang, QS (corresponding author), Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence AI Inst, Shanghai 200240, Peoples R China.	zqs1022@sjtu.edu.cn; xin.wang@sjtu.edu.cn; rcao@berkeley.edu; ywu@stat.ucla.edu; shi.feng@cs.ucla.edu; sczhu@stat.ucla.edu		Cao, Ruiming/0000-0002-0936-114X	National Natural Science Foundation of China [U19B2043, 61906120]; DARPA XAI Award [N66001-17-2-4029]; NSF [IIS 1423305]; ARO Project [W911NF1810296]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); DARPA XAI Award; NSF(National Science Foundation (NSF)); ARO Project	This work was supported by the National Natural Science Foundation of China (U19B2043 and 61906120), DARPA XAI Award N66001-17-2-4029, NSF IIS 1423305, and ARO Project W911NF1810296.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2016, ADV NEUR IN, V29; Aubry M, 2015, IEEE I CONF COMP VIS, P2875, DOI 10.1109/ICCV.2015.329; Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; Koh PW, 2017, PR MACH LEARN RES, V70; Lakkaraju H, 2017, AAAI CONF ARTIF INTE, P2124; Lapuschkin S, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08987-4; Larsen ABL, 2016, PR MACH LEARN RES, V48; Li B, 2013, IEEE I CONF COMP VIS, P2560, DOI 10.1109/ICCV.2013.318; Lu Y., 2016, IJCAI; Lundberg SM, 2017, ADV NEUR IN, V30; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Mordvintsev A, 2015, INCEPTIONISM GOING D; Nguyen A., 2016, P VIS DEEP LEARN WOR; Olah Chris, 2017, DISTILL, P4, DOI DOI 10.23915/DISTILL.00007; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sabour Sara, 2017, PROC 31 INT C NEURAL; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Si ZZ, 2013, IEEE T PATTERN ANAL, V35, P2189, DOI 10.1109/TPAMI.2013.35; Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136; Simon M, 2015, LECT NOTES COMPUT SC, V9004, P162, DOI 10.1007/978-3-319-16808-1_12; Simonyan K., 2014, WORKSH INT C LEARN R, P1; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858; Szegedy Christian, 2014, P 2 INT C LEARNING R; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Vaughan J., 2018, ARXIV180601933; Wah C., 2011, TECH REP; Wen BR, 2018, P ASME INT C OCEAN; Wu T.-F., 2007, P IEEE C COMP VIS PA, P1; Wu TF, 2011, INT J COMPUT VISION, V93, P226, DOI 10.1007/s11263-010-0346-6; Yang X, 2009, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2009.5459386; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang Q., 2017, ARXIV170801783; Zhang Q.S., 2017, ARXIV PREPRINT ARXIV; Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808; Zhang QS, 2019, PROC CVPR IEEE, P6254, DOI 10.1109/CVPR.2019.00642; Zhang QS, 2017, AAAI CONF ARTIF INTE, P2898; Zhang QS, 2018, AAAI CONF ARTIF INTE, P4464; Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920; Zhang QS, 2017, PROC CVPR IEEE, P3890, DOI 10.1109/CVPR.2017.414; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	54	3	3	7	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					3863	3877		10.1109/TPAMI.2020.2992207	http://dx.doi.org/10.1109/TPAMI.2020.2992207			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32386138	hybrid			2022-12-18	WOS:000702649700013
J	Zhang, QS; Ren, J; Huang, G; Cao, RM; Wu, YN; Zhu, SC				Zhang, Quanshi; Ren, Jie; Huang, Ge; Cao, Ruiming; Wu, Ying Nian; Zhu, Song-Chun			Mining Interpretable AOG Representations From Convolutional Networks via Active Question Answering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolutional neural networks; hierarchical graphical model; part localization		In this paper, we present a method to mine object-part patterns from cony-layers of a pre-trained convolutional neural network (CNN). The mined object-part patterns are organized by an And-Or graph (AOG). This interpretable AOG representation consists of a four-layer semantic hierarchy, i.e., semantic parts, part templates, latent patterns, and neural units. The AOG associates each object part with certain neural units in feature maps of cony-layers. The AOG is constructed with very few annotations (e.g., 3-20) of object parts. We develop a question-answering (QA) method that uses active human-computer communications to mine patterns from a pre-trained CNN, in order to explain features in cony-layers incrementally. During the learning process, our QA method uses the current AOG for part localization. The OA method actively identifies objects, whose feature maps cannot be explained by the AOG. Then, our method asks people to annotate parts on the unexplained objects, and uses answers to discover CNN patterns corresponding to newly labeled parts. In this way, our method gradually grows new branches and refines existing branches on the AOG to semanticize CNN representations. In experiments, our method exhibited a high learning efficiency. Our method used about 1/6-1/3 of the part annotations for training, but achieved similar or better part-localization performance than fast-RCNN methods.	[Zhang, Quanshi; Ren, Jie; Huang, Ge] Shanghai Jiao Tong Univ, John Hopcroft Ctr, Shanghai 200240, Peoples R China; [Zhang, Quanshi; Ren, Jie; Huang, Ge] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence AI Inst, Shanghai 200240, Peoples R China; [Cao, Ruiming; Wu, Ying Nian; Zhu, Song-Chun] Univ Calif Los Angeles, Los Angeles, CA 90095 USA	Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of California System; University of California Los Angeles	Zhang, QS (corresponding author), Shanghai Jiao Tong Univ, John Hopcroft Ctr, Shanghai 200240, Peoples R China.; Zhang, QS (corresponding author), Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence AI Inst, Shanghai 200240, Peoples R China.	zqs1022@sjtu.edu.cn; ariesrj@sjtu.edu.cn; josiecookie@sjtu.edu.cn; ruimingcao@ucla.edu; ywu@stat.ucla.edu; sczhu@stat.ucla.edu		Cao, Ruiming/0000-0002-0936-114X; Ren, Jie/0000-0001-9918-3000	National Natural Science Foundation of China [U19B2043, 61906120]; DARPA XAI Award [N66001-17-2-4029]; NSF [IIS 1423305]; ARO Project [W911NF1810296]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); DARPA XAI Award; NSF(National Science Foundation (NSF)); ARO Project	This work was supported in part by the National Natural Science Foundation of China (U19B2043 and 61906120), DARPA XAI Award N66001-17-2-4029, NSF IIS 1423305, and ARO Project W911NF1810296.	Abe N., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P1; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aubry M, 2015, IEEE I CONF COMP VIS, P2875, DOI 10.1109/ICCV.2015.329; Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60; Batra D, 2011, INT J COMPUT VISION, V93, P273, DOI 10.1007/s11263-010-0415-x; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Branson S, 2011, IEEE I CONF COMP VIS, P1832, DOI 10.1109/ICCV.2011.6126450; Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; Cho M, 2015, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2015.7298724; Cong Y, 2013, IEEE T IMAGE PROCESS, V22, P3179, DOI 10.1109/TIP.2013.2260168; Dasgupta S., 2008, P 25 INT C MACH LEAR, P208, DOI DOI 10.1145/1390156.1390183; Deng J, 2014, P SIGCHI, P3099; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Fu YF, 2013, KNOWL INF SYST, V35, P249, DOI 10.1007/s10115-012-0507-8; Gallagher P. W., 2015, WHAT HAPPENED TO MY; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gonzalez-Garcia A., 2018, INT J COMPUT VISION, V126, P476, DOI DOI 10.1007/s11263-017-1048-0; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Holub A, 2008, PROC CVPR IEEE, P885; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kingma D. P, 2014, ARXIV13126114; Koh PW, 2017, PR MACH LEARN RES, V70; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lakkaraju H, 2017, AAAI CONF ARTIF INTE, P2124; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3; Li B, 2013, IEEE I CONF COMP VIS, P2560, DOI 10.1109/ICCV.2013.318; Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long CJ, 2015, IEEE I CONF COMP VIS, P2839, DOI 10.1109/ICCV.2015.325; Lu Y., 2016, IJCAI; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Nguyen H., 2004, INT C MACHINE LEARNI, P623; Olah Chris, 2017, DISTILL, P4, DOI DOI 10.23915/DISTILL.00007; Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668; Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Russakovsky O, 2015, PROC CVPR IEEE, P2121, DOI 10.1109/CVPR.2015.7298824; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417; Shih K.J., 2015, BRIT MACH VIS C, DOI DOI 10.5244/C.29.128; Si ZZ, 2013, IEEE T PATTERN ANAL, V35, P2189, DOI 10.1109/TPAMI.2013.35; Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136; Simon M, 2015, LECT NOTES COMPUT SC, V9004, P162, DOI 10.1007/978-3-319-16808-1_12; Song HO, 2014, PR MACH LEARN RES, V32, P1611; Song Y. C., 2016, INT JOINT C ART INT, P2025; Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858; Sun Q, 2015, PROC CVPR IEEE, P3612, DOI 10.1109/CVPR.2015.7298984; Szegedy Christian, 2014, P 2 INT C LEARNING R; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Tong S., 2001, PROC ACM INT C MULTI, V9, P107; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Vijayanarasimhan S, 2011, PROC CVPR IEEE, P1449, DOI 10.1109/CVPR.2011.5995430; Wah C., 2011, TECH REP; Wah C, 2011, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2011.6126539; Xie JW, 2020, IEEE T PATTERN ANAL, V42, P27, DOI 10.1109/TPAMI.2018.2879081; Yosinski J, 2014, ADV NEUR IN, V27; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang QS, 2017, AAAI CONF ARTIF INTE, P2898; Zhang QS, 2018, AAAI CONF ARTIF INTE, P4464; Zhang QS, 2017, PROC CVPR IEEE, P3890, DOI 10.1109/CVPR.2017.414; Zhang QS, 2015, IEEE I CONF COMP VIS, P55, DOI 10.1109/ICCV.2015.15; Zhang QS, 2016, IEEE T PATTERN ANAL, V38, P532, DOI 10.1109/TPAMI.2015.2456892; Zhang QS, 2014, PROC CVPR IEEE, P1394, DOI 10.1109/CVPR.2014.181; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2; Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3; Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106; Zhu L., 2008, IEEE C COMP VIS PATT, P1; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	82	3	3	2	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					3949	3963		10.1109/TPAMI.2020.2993147	http://dx.doi.org/10.1109/TPAMI.2020.2993147			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32396071	Green Submitted			2022-12-18	WOS:000702649700019
J	Kontar, R; Raskutti, G; Zhou, SY				Kontar, Raed; Raskutti, Garvesh; Zhou, Shiyu			Minimizing Negative Transfer of Knowledge in Multivariate Gaussian Processes: A Scalable and Regularized Approach	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolution; Gaussian processes; Covariance matrices; Computational modeling; Estimation; Numerical models; Kernel; Negative transfer; multivariate Gaussian process; convolution process; pairwise models; regularization	COMPUTER EXPERIMENTS; PROCESS MODELS; CALIBRATION; PREDICTION; LIKELIHOOD; EMULATION; PROFILE	Recently there has been an increasing interest in the multivariate Gaussian process (MGP) which extends the Gaussian process (GP) to deal with multiple outputs. One approach to construct the MGP and account for non-trivial commonalities amongst outputs employs a convolution process (CP). The CP is based on the idea of sharing latent functions across several convolutions. Despite the elegance of the CP construction, it provides new challenges that need yet to be tackled. First, even with a moderate number of outputs, model building is extremely prohibitive due to the huge increase in computational demands and number of parameters to be estimated. Second, the negative transfer of knowledge may occur when some outputs do not share commonalities. In this paper we address these issues. We propose a regularized pairwise modeling approach for the MGP established using CP. The key feature of our approach is to distribute the estimation of the full multivariate model into a group of bivariate GPs which are individually built. Interestingly pairwise modeling turns out to possess unique characteristics, which allows us to tackle the challenge of negative transfer through penalizing the latent function that facilitates information sharing in each bivariate model. Predictions are then made through combining predictions from the bivariate models within a Bayesian framework. The proposed method has excellent scalability when the number of outputs is large and minimizes the negative transfer of knowledge between uncorrelated outputs. Statistical guarantees for the proposed method are studied and its advantageous features are demonstrated through numerical studies.	[Kontar, Raed] Univ Michigan, Dept Ind & Operat Engn Engn, Ann Arbor, MI 48109 USA; [Raskutti, Garvesh] Univ Wisconsin, Stat, Madison, WI 53706 USA; [Zhou, Shiyu] Univ Wisconsin, Dept Ind & Syst Engn, Madison, WI 53706 USA	University of Michigan System; University of Michigan; University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison	Kontar, R (corresponding author), Univ Michigan, Dept Ind & Operat Engn Engn, Ann Arbor, MI 48109 USA.	alkontar@umich.edu; raskutti@stat.wisc.edu; shiyuzhou@wisc.edu		Al Kontar, Raed/0000-0002-4546-324X	National Science Foundation (NSF) [1561512, 1811767]	National Science Foundation (NSF)(National Science Foundation (NSF))	This work was supported by the National Science Foundation (NSF Grant #1561512 and NSF Grant #1811767).	Alvarez M. A., 2009, ADV NEURAL INFORM PR, P57; Alvarez M. A., 2010, P 13 INT C ART INT S, P25; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Alvarez MA, 2011, J MACH LEARN RES, V12, P1459; Barry R. P., 1996, Journal of Agricultural, Biological, and Environmental Statistics, V1, P297, DOI 10.2307/1400521; Basawa IV, 1980, STAT INFERENCES STOC; BASAWA IV, 1976, SANKHYA A, V38, P259; Boyle P, 2007, GAUSSIAN PROCESSES R; Boyle P., 2005, ADV NEURAL INFORM PR, P217; Calder C. A., 2007, P 65 SESS INT STAT I, P22; Cao Y., 2014, ARXIV14107827; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Colosimo BM, 2014, J QUAL TECHNOL, V46, P95, DOI 10.1080/00224065.2014.11917956; Conti S, 2009, BIOMETRIKA, V96, P663, DOI 10.1093/biomet/asp028; Conti S, 2010, J STAT PLAN INFER, V140, P640, DOI 10.1016/j.jspi.2009.08.006; Deisenroth MP, 2015, PR MACH LEARN RES, V37, P1481; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Fieuws S, 2006, BIOMETRICS, V62, P424, DOI 10.1111/j.1541-0420.2006.00507.x; Fricker TE, 2013, TECHNOMETRICS, V55, P47, DOI 10.1080/00401706.2012.715835; GOULARD M, 1992, MATH GEOL, V24, P269, DOI 10.1007/BF00893750; Han G, 2009, TECHNOMETRICS, V51, P278, DOI 10.1198/TECH.2009.07132; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Haykin S., 2008, COMMUNICATION SYSTEM; HELTERBRAND JD, 1994, MATH GEOL, V26, P205, DOI 10.1007/BF02082764; Higdon D, 2002, QUANTITATIVE METHODS, P37, DOI DOI 10.1007/978-1-4471-0657-9_2; Higdon D, 2008, J AM STAT ASSOC, V103, P570, DOI 10.1198/016214507000000888; Kleinrock L., 1976, QUEUEING SYSTEMS, V66; Kontar R, 2018, TECHNOMETRICS, V60, P484, DOI 10.1080/00401706.2017.1383310; Kontar R, 2018, IEEE T RELIAB, V67, P41, DOI 10.1109/TR.2017.2717190; Li YX, 2018, TECHNOMETRICS, V60, P70, DOI 10.1080/00401706.2017.1305298; Li YX, 2016, TECHNOMETRICS, V58, P483, DOI 10.1080/00401706.2015.1079244; Majumdar A, 2007, MATH GEOL, V39, P225, DOI 10.1007/s11004-006-9072-6; MARDIA KV, 1989, BIOMETRIKA, V76, P289; McFarland J, 2008, AIAA J, V46, P1253, DOI 10.2514/1.35288; Melkumyan A., 2011, INT JOINT C ART INT, P1408, DOI 10.5591/978-1-57735-516-8/IJCAI11-238; Ming Chai K. A., 2009, ADV NEURAL INF PROCE, V21, P265; Ng Jun Wei, 2014, ARXIV14123078; Nguyen TV, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P643; Osborne MA, 2008, 2008 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, PROCEEDINGS, P109, DOI 10.1109/IPSN.2008.25; Paciorek CJ, 2004, ADV NEUR IN, V16, P273; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Parra G., 2017, P NIPS, P6681; Qian PZG, 2008, TECHNOMETRICS, V50, P383, DOI 10.1198/004017008000000262; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Ramsay J. O., 2006, FUNCTIONAL DATA ANAL; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Schwaighofer A., 2003, ADV NEURAL INFO PROC, P977; Shi J.Q., 2011, GAUSSIAN PROCESS REG; STEIN A, 1991, BIOMETRICS, V47, P575, DOI 10.2307/2532147; Tajbakhsh S. D, 2014, ARXIV14055576; Thi~ebaux H. J, 1987, SPATIAL OBJETIVE ANA; Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908; Ver Hoef JM, 1998, J STAT PLAN INFER, V69, P275, DOI 10.1016/S0378-3758(97)00162-6; Whittaker J., 2009, GRAPHICAL MODELS APP; Wikle CK, 2002, STAT MODEL, V2, P299, DOI 10.1191/1471082x02st036oa; Wonnacott ThomasH., 1990, INTRO STAT, V5; Yuan XT, 2012, IEEE T IMAGE PROCESS, V21, P4349, DOI 10.1109/TIP.2012.2205006; Zhou Q, 2011, TECHNOMETRICS, V53, P266, DOI 10.1198/TECH.2011.10025	60	3	3	2	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2021	43	10					3508	3522		10.1109/TPAMI.2020.2987482	http://dx.doi.org/10.1109/TPAMI.2020.2987482			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UK8RG	32305903	Green Submitted			2022-12-18	WOS:000692232400019
J	Wang, ZW; Lu, JW; Tao, CX; Zhou, J; Tian, Q				Wang, Ziwei; Lu, Jiwen; Tao, Chenxin; Zhou, Jie; Tian, Qi			Learning Channel-Wise Interactions for Binary Convolutional Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolutional neural networks; Quantization (signal); Learning (artificial intelligence); Noise reduction; Machine learning; Convolution; Binary convolutional neural networks; channel-wise interactions; deep reinforcement learning; hierarchical reinforcement learning; feature denoising		In this paper, we propose a channel-wise interaction based binary convolutional neural networks (CI-BCNN) approach for efficient inference. Conventional binary convolutional neural networks usually apply the xnor and bitcount operations in the binary convolution with notable quantization errors, which obtain opposite signs of pixels in binary feature maps compared to their full-precision counterparts and lead to significant information loss. In our proposed CI-BCNN method, we exploit the channel-wise interactions with the prior knowledge which aims to alleviate inconsistency of signs in binary feature maps and preserves the information of input samples during inference. Specifically, we mine the channel-wise interactions by using a reinforcement learning model, and impose channel-wise priors on the intermediate feature maps to correct inconsistent signs through the interacted bitcount. Since CI-BCNN mines the channel-wise interactions in a large search space where each channel may correlate with others, the search deficiency caused by sparse interactions obstacles the agent to obtain the optimal policy. To address this, we further present a hierarchical channel-wise interaction based binary convolutional neural networks (HCI-BCNN) method to shrink the search space via hierarchical reinforcement learning. Moreover, we propose a denoising interacted bitcount operation in binary convolution by smoothing the channel-wise interactions, so that noise in channel-wise priors can be alleviated. Extensive experimental results on the CIFAR-10 and ImageNet datasets demonstrate the effectiveness of the proposed CI-BCNN and HCI-BCNN.	[Wang, Ziwei; Lu, Jiwen; Zhou, Jie] Tsinghua Univ, Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China; [Zhou, Jie] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China	Tsinghua University; Tsinghua University	Lu, JW (corresponding author), Tsinghua Univ, Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.	wang-zw18@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn; jzhou@tsinghua.edu.cn	Lu, Jiwen/C-5291-2009	Lu, Jiwen/0000-0002-6121-5529; Wang, Ziwei/0000-0001-9225-8495	National Key Research and Development Program of China [2017YFA0700802]; National Natural Science Foundation of China [61822603, U1813218, U1713214, 61672306]; Beijing Academy of Artificial Intelligence (BAAI) [BAAI2020ZJ0202]; Institute for Guo Qiang, Tsinghua University; Shenzhen Fundamental Research Fund [JCYJ20170412170602564]; Tsinghua University Initiative Scientific Research Program	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI); Institute for Guo Qiang, Tsinghua University; Shenzhen Fundamental Research Fund; Tsinghua University Initiative Scientific Research Program	This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 61822603, Grant U1813218, Grant U1713214, and Grant 61672306, in part by the Beijing Academy of Artificial Intelligence (BAAI) under Grant BAAI2020ZJ0202, in part by a grant from the Institute for Guo Qiang, Tsinghua University, in part by the Shenzhen Fundamental Research Fund (SubjectArrangement) under Grant JCYJ20170412170602564, and in part by Tsinghua University Initiative Scientific Research Program. Parital of this work was presented in [60]. For more information, please visit: https://github.com/ZiweiWangTHU/CI-BCNN.git	Ashok Anubhav, 2018, INT C LEARN REPR; Belagiannis V., 2018, P EUR C COMP VIS; Bello I, 2017, PR MACH LEARN RES, V70; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Cai ZW, 2017, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR.2017.574; Cao LL, 2016, NEUROCOMPUTING, V174, P60, DOI 10.1016/j.neucom.2015.02.096; Courbariaux M, 2015, ADV NEUR IN, V28; Dan Zhang, 2018, Advances in Polymer Technology, V37, P1878, DOI 10.1002/adv.21846; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Denton E, 2014, ADV NEUR IN, V27; Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390; Du B, 2017, IEEE T CYBERNETICS, V47, P1017, DOI 10.1109/TCYB.2016.2536638; Faraone J, 2018, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR.2018.00452; Florensa C., 2017, PROC INT C MACH LEAR, P1; Han SY, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511104; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hou L, 2017, IEEE GLOBE WORK; Howard A.G, 2017, ARXIV170404861; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang B, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2464; Huang C, 2017, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2017.21; Hubara I, 2016, ADV NEUR IN, V29; Iandola F.N., 2016, ARXIV; KARP RM, 1961, IRE T INFORM THEOR, V7, P27, DOI 10.1109/TIT.1961.1057615; King DB, 2015, ACS SYM SER, V1214, P1; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kulkarni TD, 2016, ADV NEUR IN, V29; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li H., 2017, P INT C LEARNING REP; Lin J, 2017, ADV NEUR IN, V30; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Lin XF, 2017, ADV NEUR IN, V30; Liu C, 2019, PROC CVPR IEEE, P4445, DOI 10.1109/CVPR.2019.00458; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Louizos C, 2017, ADV NEUR IN, V30; Lu XQ, 2016, IEEE T VIS COMPUT GR, V22, P1181, DOI 10.1109/TVCG.2015.2500222; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Meinel C., 2019, ARXIV190608637, P1; Mishra Asit K., 2018, ICLR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nachum O, 2018, ADV NEUR IN, V31; Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465; Nech A, 2017, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2017.363; Peng HY, 2019, PR MACH LEARN RES, V97; Pirinen A, 2018, PROC CVPR IEEE, P6945, DOI 10.1109/CVPR.2018.00726; Rao YM, 2018, PROC CVPR IEEE, P6190, DOI 10.1109/CVPR.2018.00648; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Redmon J., 2016, P IEEE C COMPUTER VI, P779, DOI DOI 10.1109/CVPR.2016.91; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Supancic J, 2017, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2017.43; Sutton R. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P556; Tessler C, 2017, AAAI CONF ARTIF INTE, P1553; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443; Wang ZW, 2019, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2019.00066; Warde-Farley D., 2017, INT C LEARN REPR; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554; Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15; Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579; Zhu Chenzhuo, 2017, ICLR; Zoph B., 2017, P1	71	3	3	1	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2021	43	10					3432	3445		10.1109/TPAMI.2020.2988262	http://dx.doi.org/10.1109/TPAMI.2020.2988262			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UK8RG	32324540				2022-12-18	WOS:000692232400014
J	Zheng, WZ; Lu, JW; Zhou, J				Zheng, Wenzhao; Lu, Jiwen; Zhou, Jie			Hardness-Aware Deep Metric Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Training; Training data; Learning systems; Data mining; Geometry; Interpolation; Metric learning; deep learning; hard negative synthesis; hardness-aware learning		This paper presents a hardness-aware deep metric learning (HDML) framework for image clustering and retrieval. Most existing deep metric learning methods employ the hard negative mining strategy to alleviate the lack of informative samples for training. However, this mining strategy only utilizes a subset of training data, which may not be enough to characterize the global geometry of the embedding space comprehensively. To address this problem, we perform linear interpolation on embeddings to adaptively manipulate their hardness levels and generate corresponding label-preserving synthetics for recycled training so that information buried in all samples can be fully exploited and the metric is always challenged with proper difficulty. As a single synthetic for each sample may still not be enough to describe the unobserved distributions of the training data which is crucial for the generalization performance, we further extend HDML to generate multiple synthetics for each sample. We propose a randomly hardness-aware deep metric learning (HDML-R) method and an adaptively hardness-aware deep metric learning (HDML-A) method to sample multiple random and adaptive directions, respectively, for hardness-aware synthesis. Since the generated multiple synthetics might not all be useful and adaptive, we propose a synthetic selection method with three criteria for the selection of qualified synthetics that are beneficial to the training of the metric. Extensive experimental results on the widely used CUB-200-2011, Cars196, Stanford Online Products, In-Shop Clothes Retrieval, and VehicleID datasets demonstrate the effectiveness of the proposed framework.	[Zheng, Wenzhao; Lu, Jiwen] Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China; [Zheng, Wenzhao; Lu, Jiwen] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China; [Zhou, Jie] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Dept Automat, State Key Lab Intelligent Technol & Syst, Shenzhen 518055, Peoples R China; [Zhou, Jie] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China	Tsinghua University; Tsinghua University; Tsinghua University	Lu, JW (corresponding author), Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.; Lu, JW (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.	zhengwz18@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn; jzhou@tsinghua.edu.cn	Lu, Jiwen/C-5291-2009	Lu, Jiwen/0000-0002-6121-5529	National Key Research and Development Program of China [2016YFB1001001]; National Natural Science Foundation of China [61822603, U1813218, U1713214, 61672306]; Shenzhen Fundamental Research Fund (Subject Arrangement) [JCYJ20170412170602564]; Tsinghua University Initiative Scientific Research Program	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Fundamental Research Fund (Subject Arrangement); Tsinghua University Initiative Scientific Research Program	This work was supported in part by the National Key Research and Development Program of China under Grant 2016YFB1001001, in part by the National Natural Science Foundation of China under Grant 61822603, Grant U1813218, Grant U1713214, and Grant 61672306, in part by the Shenzhen Fundamental Research Fund (Subject Arrangement) under Grant JCYJ20170412170602564, and in part by Tsinghua University Initiative Scientific Research Program.	[Anonymous], 2006, P 18 INT C NEUR INF; [Anonymous], 2011, TECH REP CNS T 2011; Bautista MA, 2017, PROC CVPR IEEE, P1923, DOI 10.1109/CVPR.2017.208; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dinh Laurent, 2014, ARXIV14108516; Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294; Frome A, 2007, IEEE I CONF COMP VIS, P94; Ge Weifeng, 2018, ECCV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang C., 2016, ADV NEURAL INFORM PR, P1262; Kim HJ, 2017, PROC CVPR IEEE, P3251, DOI 10.1109/CVPR.2017.346; Kim W, 2018, LECT NOTES COMPUT SC, V11205, P760, DOI 10.1007/978-3-030-01246-5_45; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koh PW, 2017, PR MACH LEARN RES, V70; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Law MT, 2017, PR MACH LEARN RES, V70; Law MT, 2013, IEEE I CONF COMP VIS, P249, DOI 10.1109/ICCV.2013.38; Lin X., 2018, ECCV, P689; Liu B, 2018, PROC CVPR IEEE, P9090, DOI 10.1109/CVPR.2018.00947; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47; Vo N, 2017, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2017.286; Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925; Qi G.-J., 2018, ECCV, P501; Rice J.A., 2006, MATH STAT DATA ANAL; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sohn K., 2016, P NIPS, P1857, DOI DOI 10.5555/3157096.3157304; Song HO, 2017, PROC CVPR IEEE, P2206, DOI 10.1109/CVPR.2017.237; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tian DX, 2017, IEEE ICC; Ustinova E., 2016, ADV NEURAL INFORM PR, V29, P4170; van den Oord A, 2016, PR MACH LEARN RES, V48; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30; Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wasserstein RL, 2016, AM STAT, V70, P129, DOI 10.1080/00031305.2016.1154108; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Xuan H, 2018, ECCV; Yu BS, 2018, LECT NOTES COMPUT SC, V11210, P71, DOI 10.1007/978-3-030-01231-1_5; Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12; Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94; Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zoph B., 2019, ARXIV190611172	63	3	4	3	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3214	3228		10.1109/TPAMI.2020.2980231	http://dx.doi.org/10.1109/TPAMI.2020.2980231			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	32175855	Green Submitted			2022-12-18	WOS:000681124300027
J	Zheng, XW; Zhang, Y; Hong, SR; Li, HX; Tang, L; Xiong, YC; Zhou, J; Wang, Y; Sun, XS; Zhu, PF; Wu, CL; Ji, RR				Zheng, Xiawu; Zhang, Yang; Hong, Sirui; Li, Huixia; Tang, Lang; Xiong, Youcheng; Zhou, Jin; Wang, Yan; Sun, Xiaoshuai; Zhu, Pengfei; Wu, Chenglin; Ji, Rongrong			Evolving Fully Automated Machine Learning via Life-Long Knowledge Anchors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pipelines; Task analysis; Optimization; Data models; Computational modeling; Training; Search problems; Fully automated machine learning; life-long learning; evolutionary algorithm	NEURAL-NETWORKS	Automated machine learning (AutoML) has achieved remarkable progress on various tasks, which is attributed to its minimal involvement of manual feature and model designs. However, most of existing AutoML pipelines only touch parts of the full machine learning pipeline, e.g., neural architecture search or optimizer selection. This leaves potentially important components such as data cleaning and model ensemble out of the optimization, and still results in considerable human involvement and suboptimal performance. The main challenges lie in the huge search space assembling all possibilities over all components, as well as the generalization ability over different tasks like image, text, and tabular etc. In this paper, we present a first-of-its-kind fully AutoML pipeline, to comprehensively automate data preprocessing, feature engineering, model generation/selection/training and ensemble for an arbitrary dataset and evaluation metric. Our innovation lies in the comprehensive scope of a learning pipeline, with a novel "life-long" knowledge anchor design to fundamentally accelerate the search over the full search space. Such knowledge anchors record detailed information of pipelines and integrates them with an evolutionary algorithm for joint optimization across components. Experiments demonstrate that the result pipeline achieves state-of-the-art performance on multiple datasets and modalities. Specifically, the proposed framework was extensively evaluated in the NeurIPS 2019 AutoDL challenge, and won the only champion with a significant gap against other approaches, on all the image, video, speech, text and tabular tracks.	[Zheng, Xiawu; Li, Huixia; Tang, Lang; Sun, Xiaoshuai; Ji, Rongrong] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China; [Zheng, Xiawu; Li, Huixia; Tang, Lang; Ji, Rongrong] Peng Cheng Lab, Shenzhen 518066, Peoples R China; [Zhang, Yang; Hong, Sirui; Xiong, Youcheng; Zhou, Jin; Wu, Chenglin] DeepWisdom Ltd, Xiamen 361000, Peoples R China; [Wang, Yan] Pinterest, Seattle, WA 98100 USA; [Zhu, Pengfei] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China; [Ji, Rongrong] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China	Xiamen University; Peng Cheng Laboratory; Tianjin University; Xiamen University	Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.; Ji, RR (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.; Wu, CL (corresponding author), DeepWisdom Ltd, Xiamen 361000, Peoples R China.; Zhu, PF (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.; Ji, RR (corresponding author), Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China.	zhengxiawu@stu.xmu.edu.cn; youngzhang@fuzhi.ai; stellahong@fuzhi.ai; hxlee@stu.xmu.edu.cn; langt@stu.xmu.edu.cn; ycxiong@fuzhi.ai; jinzhou@fuzhi.ai; yanw@pinterest.com; xssun@xmu.edu.cn; zhupengfei@tju.edu.cn; alexanderwu@fuzhi.ai; rrji@xmu.edu.cn		Wang, Yan/0000-0003-4309-3166	National Science Fund for Distinguished Young Scholars [62025603]; National Natural Science Foundation of China [U1705262, 62072386, 62072387, 62072389, 62002305, 61772443, 61802324, 61702136]; Guangdong Basic and Applied Basic Research Foundation [2019B1515120049]	National Science Fund for Distinguished Young Scholars(National Natural Science Foundation of China (NSFC)National Science Fund for Distinguished Young Scholars); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Guangdong Basic and Applied Basic Research Foundation	This work was supported in part by the National Science Fund for Distinguished Young Scholars under Grant 62025603, in part by the National Natural Science Foundation of China under Grants U1705262, 62072386, 62072387, 62072389, 62002305, 61772443, 61802324, and 61702136, and in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2019B1515120049. Xiawu Zheng, Yang Zhang, Sirui Hong, Huixia Li, Lang Tang, and Youcheng Xiong contributed equally to this work.	Alaa AM, 2018, PR MACH LEARN RES, V80; Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; ANGELINE PJ, 1994, IEEE T NEURAL NETWOR, V5, P54, DOI 10.1109/72.265960; Back T, 1996, EVOLUTIONARY ALGORIT; Bengio Yoshua, 2013, ARXIV; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Brabandere B.D., 2016, ADV NEURAL INFORM PR, P667; Brazdil P., 2009, COGNITIVE TECHNOLOGI; Chen BY, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P402, DOI 10.1145/3205455.3205586; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Cheng G, 2014, ISPRS J PHOTOGRAMM, V98, P119, DOI 10.1016/j.isprsjprs.2014.10.002; Choi K., 2017, P 34 INT C MACH LEAR; Chung J., 2014, ARXIV14123555; Chung JS, 2018, INTERSPEECH, P1086; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dauphin Y. N., 2015, ARXIV150204390, V28, P1504; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dorogush A.V., 2018, ARXIV; Engels R, 1998, ECAI 1998: 13TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P430; Fahlman S.E., 1990, ADV NEURAL INFORM PR, P524; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Feurer M, 2015, AAAI CONF ARTIF INTE, P1128; Feurer Matthias, 2015, ADV NEURAL INFORM PR, P2962; Gaier A, 2019, ADV NEUR IN, V32; Gama J., 1995, Progress in Artificial Intelligence. 7th Portuguese Conference on Artificial Intelligence, EPIA '95. Proceedings, P189; Gao Y, 2019, PROC CVPR IEEE, P3200, DOI 10.1109/CVPR.2019.00332; Gil Y., 2018, AUTOML WORKSH ICML, P1; Guyon I., 2011, PROC ACTIVE LEARN EX, V16, P19; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Howard A.G, 2017, ARXIV170404861; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu W, 2018, P INT C LEARN REPR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Gao, 2017, ARXIV PREPRINT ARXIV; Hwang Y., 2020, ARXIV200101401; Escalante HJ, 2009, J MACH LEARN RES, V10, P405; Ke G., 2017, P ADV NEURAL INFORM, V30, P3146; Kim M., 2015, FEATURE EXTRACTION M, P160; Kim Y., 2014, P 2014 C EMP METH NA; Kingma D.P., 2015, INT C LEARN REPR, P1; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kleinbaum DG., 2002, LOGISTIC REGRESSION; Kohavi R., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P202; Komer B, 2014, P 13 PYTH SCI C, P32, DOI [10.25080/majora-14bd3278-006, DOI 10.25080/MAJORA-14BD3278-006]; Krizhevsky A., 2009, COMPUT SCI DEP, V1, P1; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Liam, 2019, ABS190207638 CORR; Li Lu, 2020, CLUE CHINESE LANGUAG; Lim S, 2019, ADV NEUR IN, V32; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu H, 2017, IEEE T IMAGE PROCESS, V26, P1666, DOI 10.1109/TIP.2017.2657118; Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46; Liu Yinhan, 2019, COMPUTING RES REPOSI; Liu Z, 2019, CAP 2019 C AC SPEECH; Lopez-Paz David, 2017, P 31 INT C NEUR INF, P6467; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126; Olson RS, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P485, DOI 10.1145/2908812.2908918; Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012; Park DS, 2019, INTERSPEECH, P2613, DOI 10.21437/Interspeech.2019-2680; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Rajaraman A., 2011, MINING MASSIVE DATAS; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Real Esteban, 2020, ARXIV200303384, P8007; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Reif M, 2012, MACH LEARN, V87, P357, DOI 10.1007/s10994-012-5286-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu A. A., 2016, PROGR NEURAL NETWORK; Ruvolo P., 2013, P 30 INT C MACHINE L, P507; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Serra J, 2018, PR MACH LEARN RES, V80; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Stanley KO, 2019, NAT MACH INTELL, V1, P24, DOI 10.1038/s42256-018-0006-z; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Tan MX, 2019, PR MACH LEARN RES, V97; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; THRUN S, 1994, IROS '94 - INTELLIGENT ROBOTS AND SYSTEMS: ADVANCED ROBOTIC SYSTEMS AND THE REAL WORLD, VOLS 1-3, P23, DOI 10.1109/IROS.1994.407413; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161; Wyse L., 2017, ARXIV170609559; Xie WD, 2019, INT CONF ACOUST SPEE, P5791, DOI 10.1109/ICASSP.2019.8683120; Xu J., 2018, ARXIV180512369V1; Yao Q., 2018, ARXIV181013306; Yoon Jaehong, 2017, ARXIV170801547; Zenke F, 2017, PR MACH LEARN RES, V70; Zheng X., ARXIV190513543; Zitzler E., 2001, 103 TIK, V103, DOI [10.3929/ethz-a-004284029, DOI 10.3929/ETHZ-A-004284029]; Zoph B., 2017, P1	97	3	3	4	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3091	3107		10.1109/TPAMI.2021.3069250	http://dx.doi.org/10.1109/TPAMI.2021.3069250			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	33780333				2022-12-18	WOS:000681124300019
J	Zhang, J; Dai, YC; Zhang, T; Harandi, M; Barnes, N; Hartley, R				Zhang, Jing; Dai, Yuchao; Zhang, Tong; Harandi, Mehrtash; Barnes, Nick; Hartley, Richard			Learning Saliency From Single Noisy Labelling: A Robust Model Fitting Perspective	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Noise measurement; Labeling; Predictive models; Annotations; Training; Task analysis; Saliency detection; Salinecy prediction; single noisy labelling; robust model fitting	OBJECT DETECTION	The advances made in predicting visual saliency using deep neural networks come at the expense of collecting large-scale annotated data. However, pixel-wise annotation is labor-intensive and overwhelming. In this paper, we propose to learn saliency prediction from a single noisy labelling, which is easy to obtain (e.g., from imperfect human annotation or from unsupervised saliency prediction methods). With this goal, we address a natural question: Can we learn saliency prediction while identifying clean labels in a unified framework? To answer this question, we call on the theory of robust model fitting and formulate deep saliency prediction from a single noisy labelling as robust network learning and exploit model consistency across iterations to identify inliers and outliers (i.e., noisy labels). Extensive experiments on different benchmark datasets demonstrate the superiority of our proposed framework, which can learn comparable saliency prediction with state-of-the-art fully supervised saliency methods. Furthermore, we show that simply by treating ground truth annotations as noisy labelling, our framework achieves tangible improvements over state-of-the-art methods.	[Zhang, Jing; Barnes, Nick; Hartley, Richard] Australian Natl Univ, Res Sch Elect Energy & Mat Engn, Canberra, ACT 0200, Australia; [Dai, Yuchao] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Shaanxi, Peoples R China; [Zhang, Tong] Ecole Polytech Fed Lausanne, Image & Visual Representat Lab IVRL, Lausanne, Switzerland; [Harandi, Mehrtash] Monash Univ, Dept Elect & Comp Syst Engn ECSE, Clayton, Vic 3800, Australia; [Harandi, Mehrtash] Data61 CSIRO, Sydney, NSW, Australia	Australian National University; Northwestern Polytechnical University; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Monash University; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Dai, YC (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Shaanxi, Peoples R China.	zjnwpu@gmail.com; daiyuchao@gmail.com; tong.zhang1024@icloud.com; mehrtash.harandi@monash.edu; nick.barnes@anu.edu.au; richard.hartley@anu.edu.au	Barnes, Nick/Y-2744-2018; Harandi, Mehrtash/D-6586-2018	Barnes, Nick/0000-0002-9343-9535; Harandi, Mehrtash/0000-0002-6937-6300; Zhang, Tong/0000-0001-5818-4285	National Natural Science Foundation of China [61871325, 61420106007, 61671387]; Australian Research Council Centre of Excellence for Robotics Vision [CE140100016]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Australian Research Council Centre of Excellence for Robotics Vision(Australian Research Council)	This work was supported in part by the National Natural Science Foundation of China under Grants 61871325, 61420106007, and 61671387, and the Australian Research Council Centre of Excellence for Robotics Vision (CE140100016).	Acuna D, 2019, PROC CVPR IEEE, P11067, DOI 10.1109/CVPR.2019.01133; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bordes A, 2005, J MACH LEARN RES, V6, P1579; Borji A., 2018, ABS181003716 CORR; Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9; Chang H. -S., 2017, ADV NEURAL INFORM PR, V30, P1002; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4; Nguyen DT, 2019, ADV NEUR IN, V32; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12; Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875; Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gold JR, 2017, PLAN HIST ENVIRON SE, P1; Guo S, 2018, LECT NOTES COMPUT SC, V11214, P139, DOI 10.1007/978-3-030-01249-6_9; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hu WR, 2017, IEEE T IMAGE PROCESS, V26, P724, DOI 10.1109/TIP.2016.2627803; Jing Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12543, DOI 10.1109/CVPR42600.2020.01256; Katharopoulos A, 2018, PR MACH LEARN RES, V80; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Lee D., 2013, INT C MACH LEARN ICM; Li GB, 2018, AAAI CONF ARTIF INTE, P7024; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu J, 2018, PR MACH LEARN RES, V80; Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698; Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847; Nguyen Duc Tam, 2020, ICLR; Pi Te, 2016, P 25 INT JOINT C ART, P1932; Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766; Ren Mengye, 2018, ARXIV PREPRINT ARXIV; Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594; Schein AI, 2007, MACH LEARN, V68, P235, DOI 10.1007/s10994-007-5019-5; Siddiqui Yawar, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9430, DOI 10.1109/CVPR42600.2020.00945; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; Tang M, 2018, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2018.00195; Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798; Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404; Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834; Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436; Zhang J, 2018, PROC CVPR IEEE, P9029, DOI 10.1109/CVPR.2018.00941; Zhang L, 2019, PROC CVPR IEEE, P6017, DOI 10.1109/CVPR.2019.00618; Zhang ZL, 2018, ADV NEUR IN, V31; Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887; Zhao Q., 2015, ABS151106049 CORR, Vabs; Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z; Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360	65	3	3	1	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG 1	2021	43	8					2866	2873		10.1109/TPAMI.2020.3046486	http://dx.doi.org/10.1109/TPAMI.2020.3046486			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TF2YV	33351750				2022-12-18	WOS:000670578800026
J	Sahloul, H; Shirafuji, S; Ota, J				Sahloul, Hamdi; Shirafuji, Shouhei; Ota, Jun			An Accurate and Efficient Voting Scheme for a Maximally All-Inlier 3D Correspondence Set	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Outlier rejection; post-validated voting scheme; all-inlier correspondence set; local rigidity constraint; single-point superimposition transforms	OBJECT RECOGNITION; UNIQUE SIGNATURES; ROBUST; REGISTRATION; SURFACE; HISTOGRAMS; SEGMENTATION	We present a highly accurate and efficient, yet simple, two-stage voting scheme for distinguishing inlier 3D correspondences by densely assessing and ranking their local and global geometric consistencies. The strength of the proposed method stems from both the novel idea of post-validated voting set, as well as single-point superimposition transforms, which are computationally cheap and avoid rotational ambiguities. Using a well-known dataset consisting of various 3D models and numerous scenes that include different occlusion rates, the proposed scheme is evaluated against state-of-the-art 3D voting schemes, in terms of both the correspondence PR (precision-recall) AUC (area under curve), and the execution time. A total of 374 experiments were conducted for each method, which involved a combination of four models, 50 scenes, and two down-samplings. The proposed scheme outperforms the state-of-the-art 3D voting schemes in terms of both accuracy and speed. Quantitatively, the proposed scheme scores 97.0% +/- 12.9% on the PR AUC metric, averaged over all of the experiments, while the two state-of-the-art schemes score 74.2% +/- 22.2% and 78.3% +/- 26.4%. Furthermore, the proposed scheme requires only 24.1% +/- 6.0% of the time consumed by the fastest state-of-the-art scheme. The proposed voting scheme also demonstrates high robustness against occlusions and scarce inliers.	[Sahloul, Hamdi] Grad Sch Engn, Dept Precis Engn, Bunkyo Ku, Univ Tokyo7-3-1 Hongo, Tokyo 1138656, Japan; [Shirafuji, Shouhei; Ota, Jun] Sch Engn, Ctr Engn RACE, Bunkyo Ku, Univ Tokyo7-3-1 Hongo, Tokyo 1138656, Japan		Sahloul, H (corresponding author), Grad Sch Engn, Dept Precis Engn, Bunkyo Ku, Univ Tokyo7-3-1 Hongo, Tokyo 1138656, Japan.	sahloul@race.t.u-tokyo.ac.jp; shirafuji@race.t.u-tokyo.ac.jp; ota@race.t.u-tokyo.ac.jp	Jun, Ota/CAG-2441-2022; Sahloul, Hamdi/D-8215-2017	Jun, Ota/0000-0002-4738-2275; Sahloul, Hamdi/0000-0003-3164-3028				Ariz M, 2019, COMPUT VIS IMAGE UND, V180, P13, DOI 10.1016/j.cviu.2019.01.002; Ask E, 2013, PROC CVPR IEEE, P1722, DOI 10.1109/CVPR.2013.225; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Bro R, 2008, J CHEMOMETR, V22, P135, DOI 10.1002/cem.1122; Buch AG, 2017, IEEE I CONF COMP VIS, P4137, DOI 10.1109/ICCV.2017.443; Buch AG, 2014, PROC CVPR IEEE, P2075, DOI 10.1109/CVPR.2014.266; Bustos AP, 2018, IEEE T PATTERN ANAL, V40, P2868, DOI 10.1109/TPAMI.2017.2773482; Cao MW, 2019, OPT LASER TECHNOL, V110, P120, DOI 10.1016/j.optlastec.2018.05.036; Cen SH, 2018, IEEE INT CONF ROBOT, P6045; Chang W.-C., 2008, 3D MODEL MATCHING VI, P1, DOI DOI 10.1109/CVPR.2008.4587501; Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009; Chin TJ, 2010, LECT NOTES COMPUT SC, V6315, P533, DOI 10.1007/978-3-642-15555-0_39; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O., 2004, P ACCV, V2, P812; Dong J, 2015, IEEE INT CONF ROBOT, P5807, DOI 10.1109/ICRA.2015.7140012; Drost B, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P9, DOI 10.1109/3DIMPVT.2012.53; Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fu CH, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16091406; Hast A., 2013, OPTIMAL RANSAC TOWAR; Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55; Jia K, 2016, INT J COMPUT VISION, V117, P173, DOI 10.1007/s11263-015-0858-1; Jiang Y, 2018, IEEE INT SYM MULTIM, P215, DOI 10.1109/ISM.2018.000-4; Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2; KABSCH W, 1978, ACTA CRYSTALLOGR A, V34, P827, DOI 10.1107/S0567739478001680; Klasing K, 2009, IEEE INT CONF ROBOT, P2011; Larsson V., 2016, BMVC; Lee JK, 2017, IEEE I CONF COMP VIS, P162, DOI 10.1109/ICCV.2017.27; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Lin WYD, 2014, LECT NOTES COMPUT SC, V8692, P341, DOI 10.1007/978-3-319-10593-2_23; Lin X, 2019, NEUROCOMPUTING, V325, P131, DOI 10.1016/j.neucom.2018.10.018; Liu YL, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18113908; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu GY, 2017, IEEE T MULTIMEDIA, V19, P2117, DOI 10.1109/TMM.2017.2731044; Luo N, 2018, IET COMPUT VIS, V12, P220, DOI 10.1049/iet-cvi.2017.0130; Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954; Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z; Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895; Novatnack J, 2008, LECT NOTES COMPUT SC, V5304, P440, DOI 10.1007/978-3-540-88690-7_33; Oliphant T.E., 2015, GUIDE NUMPY, V2nd edn; Olsson C., 2008, P IEEE C COMP VIS PA, p[1, 1], DOI DOI 10.1007/978-3-319-78199-0_21; Pham TT, 2014, IEEE T PATTERN ANAL, V36, P1658, DOI 10.1109/TPAMI.2013.2296310; Pitchandi N, 2019, J INTELL ROBOT SYST, V96, P65, DOI 10.1007/s10846-019-00985-4; Python, 2018, DOCUMENTATION VERSIO; Ramachandran P, 2011, COMPUT SCI ENG, V13, P40, DOI 10.1109/MCSE.2011.35; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Sahloul H, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020291; Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011; Schonberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721; SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Speers AD, 2018, HEALTHC TECHNOL LETT, V5, P208, DOI 10.1049/htl.2018.5071; Svarm L, 2017, IEEE T PATTERN ANAL, V39, P1455, DOI 10.1109/TPAMI.2016.2598331; Tennakoon RB, 2016, IEEE T PATTERN ANAL, V38, P350, DOI 10.1109/TPAMI.2015.2448103; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Varun J., 2007, INT J SHAPE MODELING, V13, P101, DOI DOI 10.1142/S0218654307000968; Wang HZ, 2015, IEEE I CONF COMP VIS, P2902, DOI 10.1109/ICCV.2015.332; Wang VT, 2017, IEEE J OCEANIC ENG, V42, P901, DOI 10.1109/JOE.2016.2634078; Wong HS, 2011, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2011.6126350; Xie XC, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11030315; Yan JC, 2018, IEEE T CYBERNETICS, V48, P765, DOI 10.1109/TCYB.2017.2655538; Yang JQ, 2019, PATTERN RECOGN LETT, V117, P1, DOI 10.1016/j.patrec.2018.11.018; Yarlagadda P, 2010, LECT NOTES COMPUT SC, V6315, P197, DOI 10.1007/978-3-642-15555-0_15; Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282; Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637; Yuan ML, 2018, I C CONT AUTOMAT ROB, P1, DOI 10.1109/ICARCV.2018.8581070; Zheng ZH, 2018, IEEE ACCESS, V6, P55501, DOI 10.1109/ACCESS.2018.2871729; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667; Zhu QP, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10110565	73	3	3	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2287	2298		10.1109/TPAMI.2020.2963980	http://dx.doi.org/10.1109/TPAMI.2020.2963980			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	31940518	hybrid			2022-12-18	WOS:000692540900009
J	Hold-Geoffroy, Y; Gotardo, P; Lalonde, JF				Hold-Geoffroy, Yannick; Gotardo, Paulo; Lalonde, Jean-Francois			Single Day Outdoor Photometric Stereo	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Sun; Image reconstruction; Clouds; Surface reconstruction; Atmospheric modeling; Meteorology; Photometric stereo; high dynamic range; deep learning; outdoor lighting		Photometric Stereo (PS) under outdoor illumination remains a challenging, ill-posed problem due to insufficient variability in illumination. Months-long capture sessions are typically used in this setup, with little success on shorter, single-day time intervals. In this paper, we investigate the solution of outdoor PS over a single day, under different weather conditions. First, we investigate the relationship between weather and surface reconstructability in order to understand when natural lighting allows existing PS algorithms to work. Our analysis reveals that partially cloudy days improve the conditioning of the outdoor PS problem while sunny days do not allow the unambiguous recovery of surface normals from photometric cues alone. We demonstrate that calibrated PS algorithms can thus be employed to reconstruct Lambertian surfaces accurately under partially cloudy days. Second, we solve the ambiguity arising in clear days by combining photometric cues with prior knowledge on material properties, local surface geometry and the natural variations in outdoor lighting through a CNN-based, weakly-calibrated PS technique. Given a sequence of outdoor images captured during a single sunny day, our method robustly estimates the scene surface normals with unprecedented quality for the considered scenario. Our approach does not require precise geolocation and significantly outperforms several state-of-the-art methods on images with real lighting, showing that our CNN can combine efficiently learned priors and photometric cues available during a single sunny day.	[Hold-Geoffroy, Yannick] Adobe, San Jose, CA 95110 USA; [Gotardo, Paulo] Disney Res Studios, CH-8006 Zurich, Switzerland; [Lalonde, Jean-Francois] Univ Laval, Quebec City, PQ G1V 0A6, Canada	Adobe Systems Inc.; Laval University	Hold-Geoffroy, Y (corresponding author), Adobe, San Jose, CA 95110 USA.	yannickhold@gmail.com; paulo.gotardo@disneyresearch.com; jflalonde@gel.ulaval.ca		Lalonde, Jean-Francois/0000-0002-6583-2364	NSERC [RGPIN-2014-05314]; FRQ-NT New Researcher Grant [2016-NC-189939]; FRQ-NT REPARTI Strategic Network	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); FRQ-NT New Researcher Grant; FRQ-NT REPARTI Strategic Network	This research was supported by the NSERC Discovery GRANT RGPIN-2014-05314, the FRQ-NT New Researcher Grant 2016-NC-189939, and the FRQ-NT REPARTI Strategic Network. The authors also thank Nvidia for the donation of the GPUs used in this research.	Abrams A, 2012, LECT NOTES COMPUT SC, V7573, P357, DOI 10.1007/978-3-642-33709-3_26; Ackermann J, 2012, PROC CVPR IEEE, P262, DOI 10.1109/CVPR.2012.6247684; Alldrin N, 2008, PROC CVPR IEEE, P2447; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; BRETAGNON P, 1988, ASTRON ASTROPHYS, V202, P309; BruceWalter Stephen R., 2007, P 18 EUR C REND TECH, P195, DOI DOI 10.2312/EGWR/EGSR07/195-206; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864; DEMENTHON DF, 1995, INT J COMPUT VISION, V15, P123, DOI 10.1007/BF01450852; Djork-Arn, ICLR 2016; Drbohlav O, 2005, IEEE I CONF COMP VIS, P1707; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Esty WW., 2003, J STAT SOFTW, V8, P1, DOI [10.18637/jss.v008.i17, DOI 10.18637/JSS.V008.I17]; Hastie T, 2009, ELEMENTS STAT LEARNI; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hernandez C, 2011, IEEE T PATTERN ANAL, V33, P419, DOI 10.1109/TPAMI.2010.181; Hold-Geoffroy Y, 2015, IEEE INT CONF COMPUT; Hold-Geoffroy Y, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P28, DOI 10.1109/3DV.2015.11; Hosek L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185591; Hung CH, 2015, IEEE WINT CONF APPL, P302, DOI 10.1109/WACV.2015.47; Inose K., 2013, INF MEDIA TECH, V8, P1095; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Johnson M. K., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2553, DOI 10.1109/CVPR.2011.5995510; Jung J, 2019, INT J COMPUT VISION, V127, P1126, DOI 10.1007/s11263-018-01145-1; Kingma D.P, P 3 INT C LEARNING R; Klaudiny M, 2014, PATTERN RECOGN LETT, V48, P81, DOI 10.1016/j.patrec.2013.12.013; Lalonde J.-F., 2016, LAVAL HDR SKY DATABA; Lalonde J.-F., 2014, INT C 3D VIS, P131; Mo ZP, 2018, PROC CVPR IEEE, P2936, DOI 10.1109/CVPR.2018.00310; ONN R, 1990, INT J COMPUT VISION, V5, P105, DOI 10.1007/BF00056773; Oxholm G, 2012, LECT NOTES COMPUT SC, V7572, P528, DOI 10.1007/978-3-642-33718-5_38; Reinhard E., 2010, HIGH DYNAMIC RANGE I, V2nd, P145; Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66; Shen FY, 2014, COMPUT GRAPH FORUM, V33, P359, DOI 10.1111/cgf.12504; Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Stumpfel J., 2004, P AFRIGRAPH, P145, DOI DOI 10.1145/1185657.1185687; Sun J, 2007, IMAGE VISION COMPUT, V25, P1073, DOI 10.1016/j.imavis.2006.04.024; Tan P., 2014, P INT C 3D VIS, P361; Taniai T, 2018, PR MACH LEARN RES, V80; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu JJ, 2017, ADV NEUR IN, V30; Yu LF, 2013, IEEE INT CONF COMPUT, DOI 10.1109/ICCPhot.2013.6528306; Yu Y, 2017, IEEE INT CONF COMP V, P526, DOI 10.1109/ICCVW.2017.69	45	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					2062	2074		10.1109/TPAMI.2019.2962693	http://dx.doi.org/10.1109/TPAMI.2019.2962693			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SA8YQ	31899414	Green Submitted			2022-12-18	WOS:000649590200017
J	Zhang, SL; Dang, X; Nguyen, D; Wilkins, D; Chen, YX				Zhang, Silu; Dang, Xin; Nguyen, Dao; Wilkins, Dawn; Chen, Yixin			Estimating Feature-Label Dependence Using Gini Distance Statistics	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Energy distance; feature selection; Gini distance covariance; Gini distance correlation; distance covariance; reproducing kernel Hilbert space; dependence test; supervised learning	FEATURE SUBSET-SELECTION; INPUT FEATURE-SELECTION; MUTUAL INFORMATION; COMPONENT ANALYSIS; CLASSIFICATION; ALGORITHMS; RELEVANCE; RANKING	Identifying statistical dependence between the features and the label is a fundamental problem in supervised learning. This paper presents a framework for estimating dependence between numerical features and a categorical label using generalized Gini distance, an energy distance in reproducing kernel Hilbert spaces (RKHS). Two Gini distance based dependence measures are explored: Gini distance covariance and Gini distance correlation. Unlike Pearson covariance and correlation, which do not characterize independence, the above Gini distance based measures define dependence as well as independence of random variables. The test statistics are simple to calculate and do not require probability density estimation. Uniform convergence bounds and asymptotic bounds are derived for the test statistics. Comparisons with distance covariance statistics are provided. It is shown that Gini distance statistics converge faster than distance covariance statistics in the uniform convergence bounds, hence tighter upper bounds on both Type I and Type II errors. Moreover, the probability of Gini distance covariance statistic under-performing the distance covariance statistic in Type II error decreases to 0 exponentially with the increase of the sample size. Extensive experimental results are presented to demonstrate the performance of the proposed method.	[Zhang, Silu] St Jude Childrens Res Hosp, Dept Diagnost Imaging, 262 Danny Thomas Pl, Memphis, TN 38105 USA; [Dang, Xin; Nguyen, Dao] Univ Mississippi, Dept Math, University, MS 38677 USA; [Wilkins, Dawn; Chen, Yixin] Univ Mississippi, Dept Comp & Informat Sci, University, MS 38677 USA	St Jude Children's Research Hospital; University of Mississippi; University of Mississippi	Zhang, SL (corresponding author), St Jude Childrens Res Hosp, Dept Diagnost Imaging, 262 Danny Thomas Pl, Memphis, TN 38105 USA.	silu.zhang@stjude.org; xdang@olemiss.edu; dxnguyen@olemiss.edu; dwilkins@olemiss.edu; yixin@olemiss.edu		Dang, Xin/0000-0002-3372-3825; Chen, Yixin/0000-0001-7645-674X				Armanfard N, 2016, IEEE T PATTERN ANAL, V38, P1217, DOI 10.1109/TPAMI.2015.2478471; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Barbu A, 2017, IEEE T PATTERN ANAL, V39, P272, DOI 10.1109/TPAMI.2016.2544315; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Belkin M, 2002, ADV NEUR IN, V14, P585; Benabdeslem K., IEEE T KNOWL DATA EN, V26, P1131; Berrendero JR, 2016, STAT SINICA, V26, P619, DOI 10.5705/ss.202014.0014; Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Bressan M, 2003, IEEE T PATTERN ANAL, V25, P1312, DOI 10.1109/TPAMI.2003.1233904; Chakraborty R, 2015, IEEE T NEUR NET LEAR, V26, P35, DOI 10.1109/TNNLS.2014.2308902; Cheng QA, 2011, IEEE T PATTERN ANAL, V33, P1217, DOI 10.1109/TPAMI.2010.195; Chow TWS, 2005, IEEE T NEURAL NETWOR, V16, P213, DOI 10.1109/TNN.2004.841414; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Constantinopoulos C, 2006, IEEE T PATTERN ANAL, V28, P1013, DOI 10.1109/TPAMI.2006.111; Cui HJ, 2015, J AM STAT ASSOC, V110, P630, DOI 10.1080/01621459.2014.920256; Damodaran BB, 2017, IEEE T GEOSCI REMOTE, V55, P2385, DOI 10.1109/TGRS.2016.2642479; Demartines P, 1997, IEEE T NEURAL NETWOR, V8, P148, DOI 10.1109/72.554199; Ding AA, 2017, J MACH LEARN RES, V18, P1; Fan JQ, 2008, J R STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x; Fan JQ, 2009, J MACH LEARN RES, V10, P2013; Gini C, 1912, VARIABILITA MUTABI 2, VIII; Gini C., 1914, ATTI REALE I VENETO, V73, P1203, DOI DOI 10.1007/BF02858128; Goldman M., 2018, BIORXIV; Gui J, 2017, IEEE T NEUR NET LEAR, V28, P1490, DOI 10.1109/TNNLS.2016.2551724; Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616; He XF, 2011, IEEE T PATTERN ANAL, V33, P2013, DOI 10.1109/TPAMI.2011.44; Hinton Geoffrey, 2002, ADV NEURAL INFORM PR, V15, P833, DOI DOI 10.1109/TSMCB.2011.2106208; Hjorland B, 2010, J AM SOC INF SCI TEC, V61, P217, DOI 10.1002/asi.21261; Hotelling H., J EDUC PSYCHOL, V24, P417; Huo XM, 2016, TECHNOMETRICS, V58, P435, DOI 10.1080/00401706.2015.1054435; Iannarilli FJ, 2003, IEEE T PATTERN ANAL, V25, P779, DOI 10.1109/TPAMI.2003.1201827; Javed K, 2012, IEEE T KNOWL DATA EN, V24, P465, DOI 10.1109/TKDE.2010.263; John G.H., 1994, MACHINE LEARNING P 1, DOI 10.1016/B978-1-55860-335-6.50023-4; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Koshevoy GA, 1997, J MULTIVARIATE ANAL, V60, P252, DOI 10.1006/jmva.1996.1655; Krishnapuram B, 2004, IEEE T PATTERN ANAL, V26, P1105, DOI 10.1109/TPAMI.2004.55; Kwak N, 2002, IEEE T NEURAL NETWOR, V13, P143, DOI 10.1109/72.977291; LeCun Y, 2010, ATT LAB; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lefakis L, 2016, J MACH LEARN RES, V17, P1; Li RZ, 2012, J AM STAT ASSOC, V107, P1129, DOI 10.1080/01621459.2012.695654; Liu H, 2005, IEEE T KNOWL DATA EN, V17, P491, DOI 10.1109/TKDE.2005.66; Lyons R, 2021, ANN PROBAB, V49, P2668, DOI 10.1214/12-AOP803; Maji P, 2010, IEEE T KNOWL DATA EN, V22, P854, DOI 10.1109/TKDE.2009.124; Mao Q, 2013, IEEE T PATTERN ANAL, V35, P2051, DOI 10.1109/TPAMI.2012.266; Mitra P, 2002, IEEE T PATTERN ANAL, V24, P301, DOI 10.1109/34.990133; Naghibi T, 2015, IEEE T PATTERN ANAL, V37, P1529, DOI 10.1109/TPAMI.2014.2372791; Nilsson R, 2007, J MACH LEARN RES, V8, P589; Novovicova J, 1996, IEEE T PATTERN ANAL, V18, P218, DOI 10.1109/34.481557; Onghena P., 2007, RANDOMIZATION TESTS, V4th; Parker JS, 2009, J CLIN ONCOL, V27, P1160, DOI 10.1200/JCO.2008.18.1370; Pearson K., 1895, P R SOC LONDON, P240, DOI DOI 10.1098/RSPL.1895.0041; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Ren SG, 2018, IEEE T PATTERN ANAL, V40, P2992, DOI 10.1109/TPAMI.2017.2776267; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sch_olkopf B., J MACH LEARN RES, V13, P723; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Serfling R., 1980, WILEY SERIES PROBABI; Shah M, 2012, IEEE T PATTERN ANAL, V34, P174, DOI 10.1109/TPAMI.2011.82; Sindhwani V, 2004, IEEE T NEURAL NETWOR, V15, P937, DOI 10.1109/TNN.2004.828772; Slutsky E, 1925, METRON, V5, P3; Somol P, 2004, IEEE T PATTERN ANAL, V26, P900, DOI 10.1109/TPAMI.2004.28; Song L, 2012, J MACH LEARN RES, V13, P1393; Stoppiglia H., 2003, J MACH LEARN RES, V3, P1399; Sun YJ, 2010, IEEE T PATTERN ANAL, V32, P1610, DOI 10.1109/TPAMI.2009.190; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Szekely GJ, 2014, ANN STAT, V42, P2382, DOI 10.1214/14-AOS1255; Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018; Szekely GJ, 2009, ANN APPL STAT, V3, P1236, DOI 10.1214/09-AOAS312; Szekely GJ, 2005, J MULTIVARIATE ANAL, V93, P58, DOI 10.1016/j.jmva.2003.12.002; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Torgerson WS, 1952, PSYCHOMETRIKA, V17, P401; Tuv E, 2009, J MACH LEARN RES, V10, P1341; Vandewalle J, 2003, ADV LEARNING THEORY, P89; Wang H, 1999, IEEE T PATTERN ANAL, V21, P271, DOI 10.1109/34.754624; Wang JL, 2014, IEEE T KNOWL DATA EN, V26, P698, DOI 10.1109/TKDE.2013.32; Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311; Wang L, 2008, IEEE T PATTERN ANAL, V30, P1534, DOI 10.1109/TPAMI.2007.70799; Wang LP, 2008, IEEE T NEURAL NETWOR, V19, P1267, DOI 10.1109/TNN.2008.2000395; Wei HL, 2007, IEEE T PATTERN ANAL, V29, P162, DOI 10.1109/TPAMI.2007.250607; Wu XD, 2013, IEEE T PATTERN ANAL, V35, P1178, DOI 10.1109/TPAMI.2012.197; Xiang ZJ, 2017, IEEE T PATTERN ANAL, V39, P1008, DOI 10.1109/TPAMI.2016.2568185; Yang SH, 2012, IEEE T KNOWL DATA EN, V24, P1422, DOI 10.1109/TKDE.2011.92; Yao C, 2017, IEEE T IMAGE PROCESS, V26, P5257, DOI 10.1109/TIP.2017.2733200; Yu L, 2004, J MACH LEARN RES, V5, P1205; Zeng H, 2011, IEEE T PATTERN ANAL, V33, P1532, DOI 10.1109/TPAMI.2010.215; Zhai YT, 2016, IEEE T PATTERN ANAL, V38, P2472, DOI 10.1109/TPAMI.2016.2533384; Zhang J., 2018, ARXIV180909793; Zhao Z, 2013, IEEE T KNOWL DATA EN, V25, P619, DOI 10.1109/TKDE.2011.222; Zhou LP, 2010, IEEE T NEURAL NETWOR, V21, P853, DOI 10.1109/TNN.2010.2044189	96	3	3	2	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					1947	1963		10.1109/TPAMI.2019.2960358	http://dx.doi.org/10.1109/TPAMI.2019.2960358			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SA8YQ	31869782	Green Submitted			2022-12-18	WOS:000649590200010
J	Kumar, S; Dai, YC; Li, HD				Kumar, Suryansh; Dai, Yuchao; Li, Hongdong			Superpixel Soup: Monocular Dense 3D Reconstruction of a Complex Dynamic Scene	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Dynamics; Image reconstruction; Heuristic algorithms; Cameras; Motion segmentation; Surface reconstruction; Dense 3D reconstruction; perspective camera; as-rigid-as-possible; relative scale ambiguity; structure from motion		This work addresses the task of dense 3D reconstruction of a complex dynamic scene from images. The prevailing idea to solve this task is composed of a sequence of steps and is dependent on the success of several pipelines in its execution. To overcome such limitations with the existing algorithm, we propose a unified approach to solve this problem. We assume that a dynamic scene can be approximated by numerous piecewise planar surfaces, where each planar surface enjoys its own rigid motion, and the global change in the scene between two frames is as-rigid-as-possible (ARAP). Consequently, our model of a dynamic scene reduces to a soup of planar structures and rigid motion of these local planar structures. Using planar over-segmentation of the scene, we reduce this task to solving a "3D jigsaw puzzle" problem. Hence, the task boils down to correctly assemble each rigid piece to construct a 3D shape that complies with the geometry of the scene under the ARAP assumption. Further, we show that our approach provides an effective solution to the inherent scale-ambiguity in structure-from-motion under perspective projection. We provide extensive experimental results and evaluation on several benchmark datasets. Quantitative comparison with competing approaches shows state-of-the-art performance.	[Kumar, Suryansh] Swiss Fed Inst Technol, Comp Vis, CH-8092 Zaurich, Switzerland; [Kumar, Suryansh] Australian Natl Univ, Canberra, ACT 0200, Australia; [Dai, Yuchao] Northwestern Polytech Univ, Fremont, CA USA; [Li, Hongdong] Australian Natl Univ, ARC Ctr Excellent Robot Vis, Canberra, ACT, Australia	Australian National University; Australian National University	Kumar, S (corresponding author), Swiss Fed Inst Technol, Comp Vis, CH-8092 Zaurich, Switzerland.	suryansh.kumar@anu.edu.au; daiyuchao@gmail.com; hongdong.li@anu.edu.au	; Dai, Yuchao/F-7832-2015	Kumar, Suryansh/0000-0003-2755-8744; li, hongdong/0000-0003-4125-1554; Dai, Yuchao/0000-0002-4432-7406	Australia Research Council ARC Centre of Excellence for Robotics Vision [CE140100016]; ARC-LIEF [190100080]; Natural Science Foundation of China [61871325, 61420106007, 61671387]; "New Generation of Artificial Intelligence" major project [2018AAA0102800]; ARC [DP 190102261, DE140100180]; NVIDIA Corporation	Australia Research Council ARC Centre of Excellence for Robotics Vision; ARC-LIEF(Australian Research Council); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); "New Generation of Artificial Intelligence" major project; ARC(Australian Research Council); NVIDIA Corporation	This research is supported in part by the Australia Research Council ARC Centre of Excellence for Robotics Vision (CE140100016), ARC-Discovery (DP 190102261), and ARC-LIEF (190100080), The Natural Science Foundation of China grants (61871325, 61420106007, 61671387), the "New Generation of Artificial Intelligence" major project under Grant 2018AAA0102800, and ARC grant DE140100180 and in part by a research gift from Baidu RAL (ApolloScapes-Robotics and Autonomous Driving Lab). The authors gratefully acknowledge the Data Science GPU gift award by NVIDIA Corporation. The authors would like to thank all of the reviewers and AE for their constructive suggestions.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457; Benson HY, 2014, COMPUT OPTIM APPL, V58, P323, DOI 10.1007/s10589-013-9626-8; Benson HY, 2002, COMPUT OPTIM APPL, V23, P257, DOI 10.1023/A:1020533003783; Blake A, 1983, PATTERN RECOGN LETT, V1, P393, DOI 10.1016/0167-8655(83)90077-6; Bleyer M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3081, DOI 10.1109/CVPR.2011.5995581; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Burago D., 2001, GRADUATE STUDIES MAT; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen Q, 2016, IEEE IC COMP COM NET; Dai YC, 2014, INT J COMPUT VISION, V107, P101, DOI 10.1007/s11263-013-0684-2; Fayad J, 2010, LECT NOTES COMPUT SC, V6314, P297, DOI 10.1007/978-3-642-15561-1_22; Fragkiadaki Katerina, 2014, ADV NEURAL INFORM PR, P55; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Garg R, 2013, PROC CVPR IEEE, P1272, DOI 10.1109/CVPR.2013.168; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Govindu VM, 2001, PROC CVPR IEEE, P218; Grimson W. E. L, IMAGES SURFACES COMP; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hinton G. E, 1977, RELAXATION ITS ROLE; Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835; Keller M., 2015, MATH TECHNOLOGY NETW, P81; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kumar S., 2017, IEEE INT C COMP VIS, P4649; Kumar S, 2019, PROC CVPR IEEE, P5341, DOI 10.1109/CVPR.2019.00549; Kumar S, 2018, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2018.00034; Kumar S, 2017, PATTERN RECOGN, V71, P428, DOI 10.1016/j.patcog.2017.05.014; Kumar S, 2016, INT CONF 3D VISION, P148, DOI 10.1109/3DV.2016.23; Lee M, 2013, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR.2013.169; Longuet-Higgins H., 1987, READINGS COMPUTER VI, P61, DOI DOI 10.1016/B978-0-08-051581-6.50012-X; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Malis E., 2007, DEEPER UNDERSTANDING; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Pearson JW, 2017, NUMER MATH, V137, P959, DOI 10.1007/s00211-017-0892-8; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Ranftl R, 2016, PROC CVPR IEEE, P4058, DOI 10.1109/CVPR.2016.440; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Savchynskyy B., 2018, P EUR C COMP VIS SEP; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Sorkine O., 2007, P 5 EUR S GEOM PROC, V4, P109, DOI [DOI 10.2312/SGP/SGP07/109-116, 10.2312/SGP/SGP07/109-116]; Taylor J, 2010, PROC CVPR IEEE, P2761, DOI 10.1109/CVPR.2010.5540002; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TOMASI C, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P913, DOI 10.1109/CVPR.1994.323924; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006; Varol A, 2012, PROC CVPR IEEE, P2248, DOI 10.1109/CVPR.2012.6247934; Varol A, 2009, IEEE I CONF COMP VIS, P1811, DOI 10.1109/ICCV.2009.5459403; Vogel C, 2015, INT J COMPUT VISION, V115, P1, DOI 10.1007/s11263-015-0806-0; Vogel C, 2011, IEEE I CONF COMP VIS, P1291, DOI 10.1109/ICCV.2011.6126381; Wang X, 2016, LECT NOTES COMPUT SC, V9911, P648, DOI 10.1007/978-3-319-46478-7_40; Whiteley W, 2004, HDB DISCRETE COMPUT, P1327; WILLIAMSON R, 1987, P AM MATH SOC, V100, P567, DOI 10.2307/2046449; Yu R, 2015, IEEE I CONF COMP VIS, P918, DOI 10.1109/ICCV.2015.111; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	61	3	3	2	21	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1705	1717		10.1109/TPAMI.2019.2955131	http://dx.doi.org/10.1109/TPAMI.2019.2955131			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31765303	Green Submitted, Green Accepted			2022-12-18	WOS:000637533800016
J	Lee, H; Kwon, H				Lee, Hyungtae; Kwon, Heesung			DBF: Dynamic Belief Fusion for Combining Multiple Object Detectors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Score-level fusion; late fusion; object detection; DBF; dempster-shafer theory	FEATURES	In this article, we propose a novel and highly practical score-level fusion approach called dynamic belief fusion (DBF) that directly integrates inference scores of individual detections from multiple object detection methods. To effectively integrate the individual outputs of multiple detectors, the level of ambiguity in each detection score is estimated using a confidence model built on a precision-recall relationship of the corresponding detector. For each detector output, DBF then calculates the probabilities of three hypotheses (target, non-target, and intermediate state (target or non-target)) based on the confidence level of the detection score conditioned on the prior confidence model of individual detectors, which is referred to as basic probability assignment. The probability distributions over three hypotheses of all the detectors are optimally fused via the Dempster's combination rule. Experiments on the ARL, PASCAL VOC 07, and 12 datasets show that the detection accuracy of the DBF is significantly higher than any of the baseline fusion approaches as well as individual detectors used for the fusion.	[Lee, Hyungtae] Booz Allen Hamilton Inc, Mclean, VA 22102 USA; [Lee, Hyungtae] US Army, Image Proc Branch, Sensors & Electron Devices Directorate SEDD, Res Lab, Adelphi, MD 20783 USA; [Kwon, Heesung] Army Res Lab SEDD, Image Proc Branch, Sensors & Electron Devices Directorate, Adelphi, MD 20783 USA	Booz Allen Hamilton Holding Corporation; United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL); United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL)	Lee, H (corresponding author), Booz Allen Hamilton Inc, Mclean, VA 22102 USA.	lee_hyungtae@bah.com; heesung.kwon.civ@mail.mil			U.S. Army Research Laboratory under a Director's Strategic Research Initiative	U.S. Army Research Laboratory under a Director's Strategic Research Initiative	This project was supported by The U.S. Army Research Laboratory under a Director's Strategic Research Initiative entitled "Heterogeneous Systems for Information Variable Environments (HIVE)" from FY14-FY16. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Dai JF, 2016, ADV NEUR IN, V29; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; DEMPSTER AP, 1967, ANN MATH STAT, V38, P325, DOI 10.1214/aoms/1177698950; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Eum S, 2017, IEEE IMAGE PROC, P875; Everingham Mark, 2010, IJCV; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fernando B, 2012, PROC CVPR IEEE, P3434, DOI 10.1109/CVPR.2012.6248084; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25; Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228; Karaoglu S, 2016, IEEE T IMAGE PROCESS, V25, P233, DOI 10.1109/TIP.2015.2499702; Khan FS, 2012, PROC CVPR IEEE, P3306, DOI 10.1109/CVPR.2012.6248068; Kim TH, 2013, IEEE I CONF COMP VIS, P3344, DOI 10.1109/ICCV.2013.415; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369; Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821; Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156; Lee HS, 2019, TOXICOL IN VITRO, V58, P256, DOI 10.1016/j.tiv.2019.02.003; Lee H, 2020, IEEE T IMAGE PROCESS, V29, P1030, DOI 10.1109/TIP.2019.2938879; Lee H, 2016, IEEE WINT CONF APPL; Lee H, 2019, INT CONF ACOUST SPEE, P1952, DOI 10.1109/ICASSP.2019.8683208; Lee H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2646; Lee H, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2751, DOI 10.1109/IROS.2016.7759427; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109; Liu JC, 2012, LECT NOTES COMPUT SC, V7576, P397, DOI [10.1007/978-3-642-33715-4_29, 10.1007/978-3-642-33167-1_23]; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Manduchi R., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P956, DOI 10.1109/ICCV.1999.790351; Mees O, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P151, DOI 10.1109/IROS.2016.7759048; Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Platt JC, 2000, ADV NEUR IN, P61; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Robinson RM, 2015, IEEE INT C INT ROBOT, P305, DOI 10.1109/IROS.2015.7353390; Sander J, 2013, 2013 WORKSHOP ON SENSOR DATA FUSION: TRENDS, SOLUTIONS, APPLICATIONS (SDF); Shafer G., 1976, MATH THEORY EVIDENCE, VVolume 1; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Spinello L, 2008, IEEE INT CONF ROBOT, P3264, DOI 10.1109/ROBOT.2008.4543708; Touryan J, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00155; Wang H, 2013, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2013.398; Wang J., 2008, P 25 INT C MACH LEAR, P1144, DOI DOI 10.1145/1390156.1390300; Wei Q, 2015, IEEE J-STSP, V9, P1117, DOI 10.1109/JSTSP.2015.2407855; Wu S, 2014, PROC CVPR IEEE, P2665, DOI 10.1109/CVPR.2014.341; XU L, 1992, IEEE T SYST MAN CYB, V22, P418, DOI 10.1109/21.155943	53	3	3	3	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1499	1514		10.1109/TPAMI.2019.2952847	http://dx.doi.org/10.1109/TPAMI.2019.2952847			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31722478	Green Submitted			2022-12-18	WOS:000637533800003
J	Shen, YT; Xiao, T; Yi, S; Chen, DP; Wang, XG; Li, HS				Shen, Yantao; Xiao, Tong; Yi, Shuai; Chen, Dapeng; Wang, Xiaogang; Li, Hongsheng			Person Re-Identification With Deep Kronecker-Product Matching and Group-Shuffling Random Walk	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Probes; Deep learning; Neural networks; Training; Visualization; Estimation; Generative adversarial networks; Computer vision; deep learning; person re-identification	NETWORK	Person re-identification (re-ID) aims to robustly measure visual affinities between person images. It has wide applications in intelligent surveillance by associating same persons' images across multiple cameras. It is generally treated as an image retrieval problem: given a probe person image, the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. There exist two main challenges for effectively solving this problem. 1) Person images usually show significant variations because of different person poses and viewing angles. The spatial layouts and correspondences between person images are therefore vital information for tackling this problem. State-of-the-art methods either ignore such spatial variation or utilize extra pose information for handling the challenge. 2) Most existing person re-ID methods rank gallery images considering only P2G affinities but ignore the affinities between the gallery images (G2G affinity). Such affinities could provide important clues for accurate gallery image ranking but were only utilized in post-processing stages by current methods. In this article, we propose a unified end-to-end deep learning framework to tackle the two challenges. For handling viewpoint and pose variations between compared person images, we propose a novel Kronecker Product Matching operation to match and warp feature maps of different persons. Comparing warped feature maps results in more accurate P2G affinities. To fully utilize all available P2G and G2G affinities for accurately ranking gallery person images, a novel group-shuffling random walk operation is proposed. Both Kronecker Product Matching and Group-shuffling Random Walk operations are end-to-end trainable and are shown to improve the learned visual features if integrated in the deep learning framework. The proposed approach outperforms state-of-the-art methods on Market-1501, CUHK03 and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach. Code is available at https://github.com/YantaoShen/kpm_rw_person_reid.	[Shen, Yantao; Xiao, Tong; Yi, Shuai; Chen, Dapeng; Wang, Xiaogang; Li, Hongsheng] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China	Chinese University of Hong Kong	Li, HS (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.	ytshen@ee.cuhk.edu.hk; tong.xiao.work@gmail.com; syi@ee.cuhk.edu.hk; dpchen@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk; hsli@ee.cuhk.edu.hk			General Research Fund through the Research Grants Council of Hong Kong [CUHK14202217, CUHK14203118, CUHK14207319, CUHK14208417, CUHK14239816]	General Research Fund through the Research Grants Council of Hong Kong	This work is supported in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants CUHK14202217, CUHK14203118, CUHK14207319, CUHK14208417, and CUHK14239816.	Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016; Aldous DJ., 1988, J THEORET PROBAB, V2, P91, DOI [10.1007/BF01048272, DOI 10.1007/BF01048272]; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bai S, 2017, AAAI CONF ARTIF INTE, P1281; Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001; Bertasius G, 2017, PROC CVPR IEEE, P6137, DOI 10.1109/CVPR.2017.650; Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X; Carr P., 2016, 2016 IEEE WINTER C A, P1; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110; Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Girshick R., 2014, PROC IEEE C COMPUT V; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gong SG, 2011, VISUAL ANALYSIS OF BEHAVIOUR: FROM PIXELS TO SEMANTICS, P301, DOI 10.1007/978-0-85729-670-2_14; Guo YL, 2018, PROC CVPR IEEE, P2335, DOI 10.1109/CVPR.2018.00248; Hamdoun O, 2008, 2008 SECOND ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P140; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hermans Alexander, 2017, ARXIV170307737; Hoffman J, 2018, PR MACH LEARN RES, V80; Huang SW, 2018, LECT NOTES COMPUT SC, V11213, P731, DOI 10.1007/978-3-030-01240-3_44; Kang K, 2017, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2017.101; Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95; Karanam S, 2019, IEEE T PATTERN ANAL, V41, P523, DOI 10.1109/TPAMI.2018.2807450; Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li HY, 2019, INT J COMPUT VISION, V127, P225, DOI 10.1007/s11263-018-1101-7; Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362; Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006; Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62; Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431; Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222; Liu ZM, 2017, IEEE I CONF COMP VIS, P2448, DOI 10.1109/ICCV.2017.266; Loy CC, 2013, IEEE IMAGE PROC, P3567, DOI 10.1109/ICIP.2013.6738736; McFee B., 2010, P 27 INT C MACHINE L, P775; Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794; Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051; Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186; Shen Y, 2015, IEEE I CONF COMP VIS, P3200, DOI 10.1109/ICCV.2015.366; Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607; Wang HX, 2016, LECT NOTES COMPUT SC, V9908, P405, DOI 10.1007/978-3-319-46493-0_25; Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wu FZ, 2016, INT C PATT RECOG, P1774, DOI 10.1109/ICPR.2016.7899893; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226; Xu YL, 2013, IEEE I CONF COMP VIS, P3152, DOI 10.1109/ICCV.2013.391; Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335; Ye M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1239, DOI 10.1145/2733373.2806326; Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Zhang H, 2019, PR MACH LEARN RES, V97; Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531; Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhou JH, 2017, IEEE I CONF COMP VIS, P2439, DOI 10.1109/ICCV.2017.265; Zhou SP, 2017, PROC CVPR IEEE, P5028, DOI 10.1109/CVPR.2017.534; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	92	3	3	2	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1649	1665		10.1109/TPAMI.2019.2954313	http://dx.doi.org/10.1109/TPAMI.2019.2954313			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31751224				2022-12-18	WOS:000637533800012
J	Cavagna, A; Melillo, S; Parisi, L; Ricci-Tersenghi, F				Cavagna, Andrea; Melillo, Stefania; Parisi, Leonardo; Ricci-Tersenghi, Federico			SpaRTA Tracking Across Occlusions via Partitioning of 3D Clouds of Points	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						3D; tracking; multi-object; occlusions; clouds of points	ALGORITHMS; FLOW	Any 3D tracking algorithm has to deal with occlusions: multiple targets get so close to each other that the loss of their identities becomes likely; hence, potentially affecting the very quality of the data with interrupted trajectories and identity switches. Here, we present a novel tracking method that addresses the problem of occlusions within large groups of featureless objects by means of three steps: i) it represents each target as a cloud of points in 3D; ii) once a 3D cluster corresponding to an occlusion occurs, it defines a partitioning problem by introducing a cost function that uses both attractive and repulsive spatio-temporal proximity links; and iii) it minimizes the cost function through a semi-definite optimization technique specifically designed to cope with the presence of multi-minima landscapes. The algorithm is designed to work on 3D data regardless of the experimental method used: multicamera systems, lidars, radars, and RGB-D systems. By performing tests on public data-sets, we show that the new algorithm produces a significant improvement over the state-of-the-art tracking methods, both by reducing the number of identity switches and by increasing the accuracy of the estimated positions of the targets in real space.	[Cavagna, Andrea; Melillo, Stefania; Parisi, Leonardo] UOS Sapienza, Collect Behav Biol Syst CoBBS Lab, Natl Res Council, Inst Complex Syst CNR ISC, I-00185 Rome, Italy; [Parisi, Leonardo] Sapienza Univ Rome, Dept Mech & Aerosp Engn DIMA, I-00185 Rome, Italy; [Ricci-Tersenghi, Federico] Sapienza Univ Rome, Phys Dept, I-00185 Rome, Italy	Consiglio Nazionale delle Ricerche (CNR); Istituto dei Sistemi Complessi (ISC-CNR); Sapienza University Rome; Sapienza University Rome	Parisi, L (corresponding author), UOS Sapienza, Collect Behav Biol Syst CoBBS Lab, Natl Res Council, Inst Complex Syst CNR ISC, I-00185 Rome, Italy.	andrea.cavagna@roma1.infn.it; stefania.melillo79@gmail.com; leonardo.parisi@gmail.com; federico.ricci@uniroma1.it	Parisi, Leonardo/G-9408-2018	Parisi, Leonardo/0000-0001-6425-5887; Ricci-Tersenghi, Federico/0000-0003-4970-7376	European Research Council [713651]	European Research Council(European Research Council (ERC)European Commission)	This work was supported by grants from the European Research Council, Proof of Concept Grant no. 713651. The authors would like to thank I. Giardina and M. Viale for the advice and the fruitful discussion on the new tracking strategy.	Asvadi A, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1255, DOI 10.1109/ITSC.2016.7795718; Attanasi A, 2015, IEEE T PATTERN ANAL, V37, P2451, DOI 10.1109/TPAMI.2015.2414427; BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Bradski G., 2008, LEARNING OPENCV COMP; Cheng XE, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129657; Choi J, 2013, IEEE INT C INTELL TR, P881, DOI 10.1109/ITSC.2013.6728343; Cox IJ, 1996, IEEE T PATTERN ANAL, V18, P138, DOI 10.1109/34.481539; Dockstader SL, 2001, 2001 IEEE WORKSHOP ON MULTI-OBJECT TRACKING, PROCEEDINGS, P95, DOI 10.1109/MOT.2001.937987; Ess A, 2010, INT J ROBOT RES, V29, P1707, DOI 10.1177/0278364910365417; Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; HOPCROFT J, 1973, COMMUN ACM, V16, P372, DOI 10.1145/362248.362272; Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113; Li Y, 2002, IMAGE VISION COMPUT, V20, P841, DOI 10.1016/S0262-8856(02)00094-X; Liu Y, 2012, LECT NOTES COMPUT SC, V7575, P730, DOI 10.1007/978-3-642-33765-9_52; Mezard M., 2009, INFORM PHYS COMPUTAT; Michel P, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P469; Mobus R, 2004, 2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P732; Mobus R, 2003, IEEE IV2003: INTELLIGENT VEHICLES SYMPOSIUM, PROCEEDINGS, P489, DOI 10.1109/IVS.2003.1212960; Moussaid M, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002442; Munaro M, 2012, IEEE INT C INT ROBOT, P2101, DOI 10.1109/IROS.2012.6385772; Ouellette NT, 2006, EXP FLUIDS, V40, P301, DOI 10.1007/s00348-005-0068-7; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; Ricci-Tersenghi F., 2016, J PHYS CONF SER, V699; Rokach L, 2005, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, P321, DOI 10.1007/0-387-25465-X_15; Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005; Theriault DH, 2014, J EXP BIOL, V217, P1843, DOI 10.1242/jeb.100529; Turk G, 1989, THESIS U N CAROLINA; Tyagi A., 2007, P IEEE WORKSH MOT VI, P1; Viale M, 2016, P INT C COMP VIS THE, V3, P679; Wen LY, 2017, INT J COMPUT VISION, V122, P313, DOI 10.1007/s11263-016-0943-0; Wu HS, 2011, OPT EXPRESS, V19, P7646, DOI 10.1364/OE.19.007646; Wu Z., 2009, P WORKSH MOT VID COM, P1, DOI [10.1109/WMVC.2009.5399245, DOI 10.1109/WMVC.2009.5399245]; Wu Z, 2016, COMPUT VIS IMAGE UND, V143, P25, DOI 10.1016/j.cviu.2015.10.006; Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39; Wu Z, 2011, PROC CVPR IEEE, P1185, DOI 10.1109/CVPR.2011.5995515; Yu S. I., 2016, P COMP VIS PATT REC, P1; Zhang L, 2013, PROCEEDINGS OF THE EIGHTH INTERNATIONAL SYMPOSIUM ON VITICULTURE AND ENOLOGY (2013), P123	40	3	3	2	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1394	1403		10.1109/TPAMI.2019.2946796	http://dx.doi.org/10.1109/TPAMI.2019.2946796			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	31689182	hybrid			2022-12-18	WOS:000626525300021
J	Yan, Z; Guo, YW; Zhang, CS				Yan, Ziang; Guo, Yiwen; Zhang, Changshui			Adversarial Margin Maximization Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Perturbation methods; Training; Support vector machines; Distortion; Radio frequency; Robustness; Neural networks; Large margin classifier; adversarial perturbation; generalization ability; deep neural networks	SUPPORT; ROBUSTNESS	The tremendous recent success of deep neural networks (DNNs) has sparked a surge of interest in understanding their predictive ability. Unlike the human visual system which is able to generalize robustly and learn with little supervision, DNNs normally require a massive amount of data to learn new concepts. In addition, research works also show that DNNs are vulnerable to adversarial examples-maliciously generated images which seem perceptually similar to the natural ones but are actually formed to fool learning models, which means the models have problem generalizing to unseen data with certain type of distortions. In this paper, we analyze the generalization ability of DNNs comprehensively and attempt to improve it from a geometric point of view. We propose adversarial margin maximization (AMM), a learning-based regularization which exploits an adversarial perturbation as a proxy. It encourages a large margin in the input space, just like the support vector machines. With a differentiable formulation of the perturbation, we train the regularized DNNs simply through back-propagation in an end-to-end manner. Experimental results on various datasets (including MNIST, CIFAR-10/100, SVHN and ImageNet) and different DNN architectures demonstrate the superiority of our method over previous state-of-the-arts. Code and models for reproducing our results will be made publicly available.	[Yan, Ziang; Zhang, Changshui] Tsinghua Univ, Inst Artificial Intelligence,Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Tsinghua Univ THUAI,State Key Lab Intelligent Tec, Beijing 100084, Peoples R China; [Guo, Yiwen] Bytedance AI Lab, Beijing 100190, Peoples R China	Tsinghua University	Yan, Z (corresponding author), Tsinghua Univ, Inst Artificial Intelligence,Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Tsinghua Univ THUAI,State Key Lab Intelligent Tec, Beijing 100084, Peoples R China.	yza18@mails.tsinghua.edu.cn; guoyiwen.ai@bytedance.com; zcs@mail.tsinghua.edu.cn			NSFC [61876095]; Beijing Academy of Artificial Intelligence (BAAI)	NSFC(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI)	This work is funded by the NSFC (Grant No. 61876095) and the Beijing Academy of Artificial Intelligence (BAAI).	Abadi M., 2022, TENSORFLOW LARGE SCA; An SJ, 2015, IEEE I CONF COMP VIS, P2515, DOI 10.1109/ICCV.2015.289; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng Jiankang, 2019, P IEEE C COMP VIS PA, P4690, DOI DOI 10.1109/CVPR.2019.00482; Ding G. W., 2018, ARXIV181202637; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Elsayed G., 2018, ADV NEURAL INFORM PR, P842; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Liu WY, 2016, PR MACH LEARN RES, V48; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; Novikoff, 1962, P S MATH THEOR AUT, P615; Osuna E, 1997, PROC CVPR IEEE, P130, DOI 10.1109/CVPR.1997.609310; Papernot N, 2018, 2018 3RD IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY (EUROS&P 2018), P399, DOI 10.1109/EuroSP.2018.00035; Platt JC, 2000, ADV NEUR IN, V12, P547; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039; Soudry D, 2018, J MACH LEARN RES, V19; Sun SZ, 2016, AAAI CONF ARTIF INTE, P2066; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tang Y, 2013, DEEP LEARNING USING; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; vander Vaart AW, 1996, WEAK CONVERGENCE EMP, P16, DOI DOI 10.1007/978-1-4757-2545-2_3; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang XB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P992; Wei C., 2018, P BRIT MACH VIS C; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Xie LX, 2016, PROC CVPR IEEE, P4753, DOI 10.1109/CVPR.2016.514; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; Yan Z, 2018, ADV NEUR IN, V31	53	3	3	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1129	1139		10.1109/TPAMI.2019.2948348	http://dx.doi.org/10.1109/TPAMI.2019.2948348			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	31634825	Green Submitted			2022-12-18	WOS:000626525300002
J	Zhang, CH; Zhang, SH				Zhang, Chihao; Zhang, Shihua			Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Matrix decomposition; Bayes methods; Data integration; Inference algorithms; Data models; Data mining; Gaussian distribution; Bayesian methods; matrix decomposition; data integration; variational Bayesian inference; maximum a posterior		Matrix decomposition is a popular and fundamental approach in machine learning and data mining. It has been successfully applied into various fields. Most matrix decomposition methods focus on decomposing a data matrix from one single source. However, it is common that data are from different sources with heterogeneous noise. A few of the matrix decomposition methods have been extended for such multi-view data integration and pattern discovery while only a few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly. To this end, in this article, we propose a joint matrix decomposition framework (BJMD), which models the heterogeneity of noise by the Gaussian distribution in a Bayesian framework. We develop two algorithms to solve this model: one is a variational Bayesian inference algorithm, which makes full use of the posterior distribution; and another is a maximum a posterior algorithm, which is more scalable and can be easily paralleled. Extensive experiments on synthetic and real-world datasets demonstrate that BJMD is superior or competitive to the state-of-the-art methods.	[Zhang, Chihao; Zhang, Shihua] Chinese Acad Sci, Acad Math & Syst Sci, RCSDS, NCMIS,CEMS, Beijing 100190, Peoples R China; [Zhang, Chihao; Zhang, Shihua] Univ Chinese Acad Sci, Sch Math Sci, Beijing 100049, Peoples R China; [Zhang, Chihao; Zhang, Shihua] Chinese Acad Sci, Ctr Excellence Anim Evolut & Genet, Kunming 650223, Yunnan, Peoples R China	Chinese Academy of Sciences; Academy of Mathematics & System Sciences, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences	Zhang, SH (corresponding author), Chinese Acad Sci, Acad Math & Syst Sci, RCSDS, NCMIS,CEMS, Beijing 100190, Peoples R China.; Zhang, SH (corresponding author), Univ Chinese Acad Sci, Sch Math Sci, Beijing 100049, Peoples R China.	zhangchihao11@outlook.com; zsh@amss.ac.cn	zhang, chi/GRX-3610-2022	Zhang, Chihao/0000-0002-2135-5690	National Natural Science Foundation of China [11661141019, 61621003]; Strategic Priority Research Program of the Chinese Academy of Sciences (CAS) [XDB13040600]; National Ten Thousand Talent Program for Young Top-notch Talents; Key Research Program of the Chinese Academy of Sciences [KFZD-SW-219]; National Key Research and Development Program of China [2017YFC0908405]; CAS Frontier Science Research Key Project for Top Young Scientist [QYZDB-SSW-SYS008]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Strategic Priority Research Program of the Chinese Academy of Sciences (CAS); National Ten Thousand Talent Program for Young Top-notch Talents; Key Research Program of the Chinese Academy of Sciences(Chinese Academy of Sciences); National Key Research and Development Program of China; CAS Frontier Science Research Key Project for Top Young Scientist	This work has been supported by the National Natural Science Foundation of China [11661141019, 61621003]; Strategic Priority Research Program of the Chinese Academy of Sciences (CAS) [XDB13040600]; National Ten Thousand Talent Program for Young Top-notch Talents; Key Research Program of the Chinese Academy of Sciences [KFZD-SW-219]; National Key Research and Development Program of China [2017YFC0908405]; and CAS Frontier Science Research Key Project for Top Young Scientist [QYZDB-SSW-SYS008].	ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; [Anonymous], 2008, P 25 INT C MACH LEAR; Bastien RRL, 2012, BMC MED GENOMICS, V5, DOI 10.1186/1755-8794-5-44; Bishop CM, 1999, ADV NEUR IN, V11, P382; Bishop CM, 1999, IEE CONF PUBL, P509, DOI 10.1049/cp:19991160; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Brunet JP, 2004, P NATL ACAD SCI USA, V101, P4164, DOI 10.1073/pnas.0308531101; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Cai D, 2008, IEEE DATA MINING, P63, DOI 10.1109/ICDM.2008.57; Carmona-Saez P, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-78; Chalise P, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176278; Chang JC, 2005, J CLIN ONCOL, V23, P1169, DOI 10.1200/JCO.2005.03.156; Curtis C, 2012, NATURE, V486, P346, DOI 10.1038/nature10983; Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277; Ding Chris, 2006, P 12 ACM SIGKDD INT, V2006, P126; Ellis MJ, 2011, J CLIN ONCOL, V29, P2342, DOI 10.1200/JCO.2010.31.6950; Gelman A, 2006, BAYESIAN ANAL, V1, P515, DOI 10.1214/06-BA117A; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Hatakenaka Masamitsu, 2008, Magn Reson Med Sci, V7, P23, DOI 10.2463/mrms.7.23; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Hofmann T, 2004, ACM T INFORM SYST, V22, P89, DOI 10.1145/963770.963774; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; Jing LP, 2012, IEEE T IMAGE PROCESS, V21, P4508, DOI 10.1109/TIP.2012.2206040; Kaban A, 2007, PATTERN RECOGN LETT, V28, P1271, DOI 10.1016/j.patrec.2007.02.010; Kim H, 2007, BIOINFORMATICS, V23, P1495, DOI 10.1093/bioinformatics/btm134; Kingma D. P, 2014, ARXIV13126114; Lawrence N.D., 2009, P 26 ANN INT C MACHI, P601, DOI DOI 10.1145/1553374.1553452; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Liu HF, 2012, IEEE T PATTERN ANAL, V34, P1299, DOI 10.1109/TPAMI.2011.217; Liu J., 2013, PROC SOC IND APPL MA, P252, DOI [10.1137/1.9781611972832.28, DOI 10.1137/1.9781611972832.28]; Liu ZQ, 2014, SCI REP-UK, V4, DOI 10.1038/srep04002; Mackey L. W., 2010, P 27 INT C MACH LEAR, P711; MONTEIRO RDC, 1989, MATH PROGRAM, V44, P43, DOI 10.1007/BF01587076; Nielsen TO, 2010, CLIN CANCER RES, V16, P5222, DOI 10.1158/1078-0432.CCR-10-1282; PAATERO P, 1994, ENVIRONMETRICS, V5, P111, DOI 10.1002/env.3170050203; Parker JS, 2009, J CLIN ONCOL, V27, P1160, DOI 10.1200/JCO.2008.18.1370; Pauca VP, 2004, SIAM PROC S, P452; Porteous I, 2010, AAAI CONF ARTIF INTE, P563; Rajan R, 2004, CANCER-AM CANCER SOC, V100, P1365, DOI 10.1002/cncr.20134; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Saddiki H, 2015, BIOINFORMATICS, V31, P225, DOI 10.1093/bioinformatics/btu618; Salvatier J, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.55; Schmidt MN, 2009, LECT NOTES COMPUT SC, V5441, P540, DOI 10.1007/978-3-642-00599-2_68; Srebro N., 2003, P 20 INT C MACHINE L, P720; Srebro N., 2005, P ADV NEURAL INFORM; Strazar M, 2016, BIOINFORMATICS, V32, P1527, DOI 10.1093/bioinformatics/btw003; Welling M, 2006, SIAM PROC S, P474; Witten DM, 2009, BIOSTATISTICS, V10, P515, DOI 10.1093/biostatistics/kxp008; Wu SQ, 2016, P NATL ACAD SCI USA, V113, P4290, DOI 10.1073/pnas.1521171113; Zhang SH, 2012, NUCLEIC ACIDS RES, V40, P9379, DOI 10.1093/nar/gks725; Zhang SH, 2011, BIOINFORMATICS, V27, pI401, DOI 10.1093/bioinformatics/btr206; Zhao Q, 2015, IEEE T NEUR NET LEAR, V26, P825, DOI 10.1109/TNNLS.2014.2387376	53	3	3	2	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1184	1196		10.1109/TPAMI.2019.2946370	http://dx.doi.org/10.1109/TPAMI.2019.2946370			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	31603812	Green Submitted			2022-12-18	WOS:000626525300006
J	Robinson, JP; Shao, M; Fu, Y				Robinson, Joseph P.; Shao, Ming; Fu, Yun			Survey on the Analysis and Modeling of Visual Kinship: A Decade in the Making	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Face recognition; Task analysis; Measurement; Tutorials; Protocols; Streaming media; Survey; facial recognition; benchmarks and evaluation; deep learning; data challenges; visual kinship recognition	FACIAL IMAGES; VERIFICATION; FACE; CLASSIFICATION; RECOGNITION	Kinship recognition is a challenging problem with many practical applications. With much progress and milestones having been reached after ten years, we are now able to survey the research and create new milestones. We review the public resources and data challenges that enabled and inspired many to hone-in on the views of automatic kinship recognition in the visual domain. The different tasks are described in technical terms and syntax consistent across the problem domain and the practical value of each discussed and measured. State-of-the-art methods for visual kinship recognition problems, whether to discriminate between or generate from, are examined. As part of such, we review systems proposed as part of a recent data challenge held in conjunction with the 2020 IEEE Conference on Automatic Face and Gesture Recognition. We establish a stronghold for the state of progress for the different problems in a consistent manner. This survey will serve as the central resource for the work of the next decade to build upon. For the tenth anniversary, the demo code is provided for the various kin-based tasks. Detecting relatives with visual recognition and classifying the relationship is an area with high potential for impact in research and practice.	[Robinson, Joseph P.; Fu, Yun] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA; [Shao, Ming] Univ Massachusetts, Dartmouth, MA 02747 USA	Northeastern University; University of Massachusetts System; University Massachusetts Dartmouth	Robinson, JP (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.	robinson.jo@northeastern.edu; mshao@umassd.edu; yunfu@ece.neu.edu			UMass Dartmouth's Marine and Undersea Technology (MUST) Research Program - Office of Naval Research (ONR) [N00014-20-1-2170]	UMass Dartmouth's Marine and Undersea Technology (MUST) Research Program - Office of Naval Research (ONR)	This work was supported in part by the UMass Dartmouth's Marine and Undersea Technology (MUST) Research Program funded by the Office of Naval Research (ONR) under Grant No. N00014-20-1-2170.	[Anonymous], 2018, Nature, V557, P5, DOI 10.1038/d41586-018-05029-9; Arnold M, 2019, IBM J RES DEV, V63, DOI 10.1147/JRD.2019.2942288; Aspandi D, 2019, IEEE INT CONF AUTOMA, P730; Bottino A., 2012, Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods. ICPRAM 2012, P405; Boutellaa E, 2016, INT CONF BIOMETR; Bredart S, 1999, EVOL HUM BEHAV, V20, P129, DOI 10.1016/S1090-5138(98)00047-6; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Cheng JC, 2019, TSINGHUA SCI TECHNOL, V24, P333, DOI 10.26599/TST.2018.9010090; Chergui A, 2020, TRAIT SIGNAL, V37, P1, DOI 10.18280/ts.370101; Chu X., 2020, TRAIT SIGNAL; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Crispim Felipe, 2020, Advanced Concepts for Intelligent Vision Systems. 20th International Conference, ACIVS 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12002), P215, DOI 10.1007/978-3-030-40605-9_19; Dahan E, 2018, SELFKIN SELF ADJUSTE; Dal Martello MF, 2015, J VISION, V15, DOI 10.1167/15.13.5; Dawson M, 2019, LECT NOTES COMPUT SC, V11363, P654, DOI 10.1007/978-3-030-20893-6_41; Dehghan A, 2014, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR.2014.227; Dibeklioglu H, 2013, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2013.189; Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38; Duan Q., 2017, P 2017 WORKSH REC FA, P21; Duan QY, 2017, IEEE INT CONF COMP V, P1590, DOI 10.1109/ICCVW.2017.187; Ertugrul IO, 2017, IEEE INT CONF AUTOMA, P33, DOI 10.1109/FG.2017.14; Fang RG, 2013, IEEE IMAGE PROC, P2983, DOI 10.1109/ICIP.2013.6738614; Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590; Fang YM, 2016, IEEE INT CONF MULTI; Frowd C. D., 2008, J MULTIMEDIA, V3; Fu Yun, 2016, P 24 ACM INT C MULTI, DOI DOI 10.1145/2964284.2967219; Fu Yun, 2011, 22 INT JOINT C ART I, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422; Furstenberg FF, 2020, J MARRIAGE FAM, V82, P364, DOI 10.1111/jomf.12628; Gao P., 2021, ICME; Georgopoulos M, 2018, IMAGE VISION COMPUT, V80, P58, DOI 10.1016/j.imavis.2018.05.003; Ghatas FS, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-1949-3; Gong SX, 2019, INT CONF BIOMETR; Goode WJ., 1963, WORLD REVOLUTION FAM; Grouver A, 2019, ASIAN C PATTERN RECO, P76; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Guo YH, 2014, INT C PATT RECOG, P4287, DOI 10.1109/ICPR.2014.735; Hanley M., 2020, ABS201113583 CORR; Harzing AWK., 2010, PUBLISH PERISH BOOK; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hettiachchi D., 2020, PACIS; Nguyen HV, 2011, LECT NOTES COMPUT SC, V6493, P709; Hormann S, 2020, IEEE INT CONF AUTOMA, P863, DOI 10.1109/FG47880.2020.00106; Hu JL, 2019, IEEE IMAGE PROC, P1178, DOI 10.1109/ICIP.2019.8803754; Hu JL, 2018, IEEE T CIRC SYST VID, V28, P1875, DOI 10.1109/TCSVT.2017.2691801; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; Huang G.B., 2008, WORKSHOP FACES REAL; Iandola F.N., 2016, ARXIV; Kaminski G, 2010, PSYCHOL SCI, V21, P1746, DOI 10.1177/0956797610388045; Kohli N., 2018, DEEP LEARN BIOMETR, V130, P127; Kohli N, 2019, IEEE T IMAGE PROCESS, V28, P1329, DOI 10.1109/TIP.2018.2840880; Kohli N, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2609811; Kollias D., 2001, ABS200111409 CORR; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Kumar C, 2020, AAAI CONF ARTIF INTE, V34, P11304; Kumar YBR, 2020, HELIYON, V6, DOI 10.1016/j.heliyon.2020.e03751; Laiadi O, 2020, IEEE INT CONF AUTOMA, P877, DOI 10.1109/FG47880.2020.00118; Laiadi O, 2020, NEUROCOMPUTING, V377, P286, DOI 10.1016/j.neucom.2019.10.055; Li WH, 2020, IEEE INT CON MULTI; Li XS, 2015, 2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS - COMPUTING TECHNOLOGY, INTELLIGENT TECHNOLOGY, INDUSTRIAL INFORMATION INTEGRATION (ICIICII), P1, DOI 10.1109/ICIICII.2015.88; Li YY, 2017, ADV SOC SCI EDUC HUM, V159, P13; Liang JQ, 2019, IEEE T IMAGE PROCESS, V28, P1149, DOI 10.1109/TIP.2018.2875346; Liang JY, 2017, COMM COM INF SC, V771, P563, DOI 10.1007/978-981-10-7299-4_47; Liu HJ, 2017, IEEE INT CON MULTI, P319, DOI 10.1109/ICME.2017.8019375; Lopez MB, 2018, MACH VISION APPL, V29, P873, DOI 10.1007/s00138-018-0943-x; Lopez MB, 2016, IEEE T PATTERN ANAL, V38, P2342, DOI 10.1109/TPAMI.2016.2522416; Lu JW, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014); Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P4269, DOI 10.1109/TIP.2017.2717505; Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134; Luo ZP, 2020, IEEE INT CONF AUTOMA, P868, DOI 10.1109/FG47880.2020.00117; Lv JN, 2019, IEICE T INF SYST, VE102D, P2568, DOI 10.1587/transinf.2019EDP7104; Mahpod S, 2018, COMPUT VIS IMAGE UND, V167, P28, DOI 10.1016/j.cviu.2017.12.003; Maryam AK., 2019, EXPERT SYST APPL X, DOI 10.1016/j.eswax.2019.100008; McNatt Z, 2018, IMPACT SEPARATION RE, DOI [10.7916/D80P2GF9, DOI 10.7916/D80P2GF9]; Merler Michele, 2019, ARXIV190110436; Mitchell M, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P220, DOI 10.1145/3287560.3287596; Mukherjee M, 2019, INT CONF COMPUT; Naini FB, 2004, AM J ORTHOD DENTOFAC, V126, P655, DOI 10.1016/j.ajodo.2003.08.034; Nandy A, 2019, IEEE INT CONF AUTOMA, P739; Nguyen TDH, 2020, IEEE INT CONF AUTOMA, P887, DOI 10.1109/FG47880.2020.00130; Ozkan S, 2018, IEEE IMAGE PROC, P2142, DOI 10.1109/ICIP.2018.8451305; Qin X., 2015, ABS150102555 CORR; Qin XQ, 2020, NEUROCOMPUTING, V377, P213, DOI 10.1016/j.neucom.2019.09.089; Qin XQ, 2019, APPL SOFT COMPUT, V77, P344, DOI 10.1016/j.asoc.2019.01.027; Qin XQ, 2018, NEURAL PROCESS LETT, V47, P1253, DOI 10.1007/s11063-017-9694-3; Ramos J., 2003, P 1 INSTR C MACH LEA, DOI DOI 10.15804/TNER.2015.42.4.03; Robinson J. P., 2017, 12 IEEE AMFG RFIW WO, P5; Robinson J. P., 2016, ACMM; Robinson JP, 2020, IEEE INT CONF AUTOMA, P857, DOI 10.1109/FG47880.2020.00138; Robinson JP, 2020, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW50498.2020.00008; Robinson JP, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2096, DOI 10.1145/3240508.3241471; Robinson JP, 2018, IEEE T PATTERN ANAL, V40, P2624, DOI 10.1109/TPAMI.2018.2826549; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shadrikov A, 2020, IEEE INT CONF AUTOMA, P872, DOI 10.1109/FG47880.2020.00137; Shao M., 2011, PROC CVPR WORKSHOPS, P60, DOI DOI 10.1109/CVPRW.2011.5981801; Sun Y., 2018, 2018 IEEE C EV COMP, P1, DOI DOI 10.1109/CEC.2018.8477921; Taherkhani F, 2018, IEEE COMPUT SOC CONF, P666, DOI 10.1109/CVPRW.2018.00097; Taleb-Ahmed A., 2019, P 14 IEEE INT C AUT, P1, DOI 10.1109/FG.2019.8756627; Vijayan V, 2011, 2011 INT JOINT C BIO, P1, DOI DOI 10.1109/IJCB.2011.6117491; Walsh S., 2016, HDB FORENSIC GENETIC, P415, DOI DOI 10.1142/9781786340788_0017; Wang M, 2019, IEEE I CONF COMP VIS, P692, DOI 10.1109/ICCV.2019.00078; Wang MY, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patog.2020.10732; Wang SY, 2019, IEEE T PATTERN ANAL, V41, P2783, DOI 10.1109/TPAMI.2018.2861871; Wang SY, 2017, IEEE INT CONF AUTOMA, P216, DOI 10.1109/FG.2017.35; Wang W., 2020, ABS200406382 CORR; Wei ZQ, 2019, IEEE ACCESS, V7, P100029, DOI 10.1109/ACCESS.2019.2929939; Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87; Wu X, 2016, INT C PAR DISTRIB SY, P8, DOI [10.1109/ICPADS.2016.9, 10.1109/ICPADS.2016.0011]; Wu XT, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P187, DOI 10.1109/SIPROCESS.2018.8600423; Wu XW, 2021, PLANT SOIL, V458, P191, DOI 10.1007/s11104-019-04373-7; Wu Y, 2018, IEEE INT CONF AUTOMA, P143, DOI 10.1109/FG.2018.00030; Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436; Xiaojun Wu, 2016, 2016 13th International Conference on Service Systems and Service Management (ICSSSM), P1, DOI 10.1109/ICSSSM.2016.7538581; Yan HB, 2018, PATTERN RECOGN, V75, P15, DOI 10.1016/j.patcog.2017.03.001; Yan HB, 2015, IEEE T CYBERNETICS, V45, P2535, DOI 10.1109/TCYB.2014.2376934; Yan HB, 2014, IEEE T INF FOREN SEC, V9, P1169, DOI 10.1109/TIFS.2014.2327757; Yu J, 2020, IEEE INT CONF AUTOMA, P882, DOI 10.1109/FG47880.2020.00136; Yu J, 2020, IEEE INT CONF AUTOMA, P892, DOI 10.1109/FG47880.2020.00127; Zhang HM, 2019, IEEE IMAGE PROC, P3856, DOI 10.1109/ICIP.2019.8803647; Zhang K., 2015, P BMVC, P1, DOI DOI 10.5244/C.29.148; Zhang L, 2021, IEEE T CYBERNETICS, V51, P5883, DOI 10.1109/TCYB.2019.2959403; Zhang QC, 2018, INFORM FUSION, V42, P146, DOI 10.1016/j.inffus.2017.10.006; Zhang R, 2019, INFORM FUSION, V50, P158, DOI 10.1016/j.inffus.2018.11.019; Zhang Y., 2002, ABS200211376 CORR; Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463; Zhou XZ, 2019, INFORM FUSION, V48, P84, DOI 10.1016/j.inffus.2018.07.011; Zhou XZ, 2016, NEUROCOMPUTING, V197, P136, DOI 10.1016/j.neucom.2016.02.039; Zhu S., 2019, ABS190912977 CORR	130	3	3	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 2	2021	44	8					4432	4453		10.1109/TPAMI.2021.3063078	http://dx.doi.org/10.1109/TPAMI.2021.3063078			22	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	2Q6HV	33651684	Green Submitted			2022-12-18	WOS:000820522500002
J	Wang, SP; Yang, Y; Sun, J; Xu, ZB				Wang, Shipeng; Yang, Yan; Sun, Jian; Xu, Zongben			Variational HyperAdam: A Meta-Learning Approach to Network Training	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Network training; meta-learning; learning to optimize; variational inference; variational hyperadam	BACKPROPAGATION; NOISE	Stochastic optimization algorithms have been popular for training deep neural networks. Recently, there emerges a new approach of learning-based optimizer, which has achieved promising performance for training neural networks. However, these blackbox learning-based optimizers do not fully take advantage of the experience in human-designed optimizers and heavily rely on learning from meta-training tasks, therefore have limited generalization ability. In this paper, we propose a novel optimizer, dubbed as Variational HyperAdam, which is based on a parametric generalized Adam algorithm, i.e., HyperAdam, in a variational framework. With Variational HyperAdam as optimizer for training neural network, the parameter update vector of the neural network at each training step is considered as random variable, whose approximate posterior distribution given the training data and current network parameter vector is predicted by Variational HyperAdam. The parameter update vector for network training is sampled from this approximate posterior distribution. Specifically, in Variational HyperAdam, we design a learnable generalized Adam algorithm for estimating expectation, paired with a VarBlock for estimating the variance of the approximate posterior distribution of parameter update vector. The Variational HyperAdam is learned in a meta-learning approach with meta-training loss derived by variational inference. Experiments verify that the learned Variational HyperAdam achieved state-of-the-art network training performance for various types of networks on different datasets, such as multilayer perceptron, CNN, LSTM and ResNet.	[Wang, Shipeng; Yang, Yan; Sun, Jian; Xu, Zongben] Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Shaanxi, Peoples R China; [Sun, Jian; Xu, Zongben] Xi An Jiao Tong Univ, Natl Engn Lab Big Data Algorithms & Anal Technol, Xian 710049, Shaanxi, Peoples R China; [Sun, Jian; Xu, Zongben] Pazhou Lab Guangzhou, Guangzhou 510335, Guangdong, Peoples R China	Xi'an Jiaotong University; Xi'an Jiaotong University; Pazhou Lab	Sun, J (corresponding author), Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Shaanxi, Peoples R China.; Sun, J (corresponding author), Xi An Jiao Tong Univ, Natl Engn Lab Big Data Algorithms & Anal Technol, Xian 710049, Shaanxi, Peoples R China.	wangshipeng8128@stu.xjtu.edu.cn; yangyan92@stu.xjtu.edu.cn; jiansun@xjtu.edu.cn; zbxu@xjtu.edu.cn			NSFC [U20B2075, 11690011, 11971373, U1811461, 12026605]; National Key RD Program [2018AAA0102201]	NSFC(National Natural Science Foundation of China (NSFC)); National Key RD Program	This work was supported by NSFC under Grants U20B2075, 11690011, 11971373, U1811461, 12026605 and National Key R&D Program 2018AAA0102201.	Amit R, 2018, PR MACH LEARN RES, V80; Andrychowicz M, 2016, ADV NEUR IN, V29; Balaji Y, 2018, ADV NEUR IN, V31; Bello I, 2017, PR MACH LEARN RES, V70; Bishop C.M., 2006, PATTERN RECOGN, DOI [DOI 10.1007/978-0-387-45528-0, DOI 10.1016/C2009-0-22409-3]; BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chen YT, 2017, PR MACH LEARN RES, V70; Dauphin YN, 2014, ADV NEUR IN, V27; Djork-Arn, ICLR 2016; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Finn C, 2017, PR MACH LEARN RES, V70; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Higgins I, 2016, BETA VAE LEARNING BA; Dau HA, 2019, IEEE-CAA J AUTOMATIC, V6, P1293, DOI 10.1109/JAS.2019.1911747; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang HB, 2018, ADV NEUR IN, V31; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jaakkola TS, 2001, NEU INF PRO, P129; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kingma DP, 2017, P INT C LEARN REPR I; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Kuncheva LI, 2003, MACH LEARN, V51, P181, DOI 10.1023/A:1022859003006; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee S, 2007, PROC MONOGR ENG WATE, P489, DOI 10.1145/1273496.1273558; Li D, 2018, AAAI CONF ARTIF INTE, P3490; Lillicrap T. P., 2016, P 4 INT C LEARN REPR; Liu Q, 2018, ADV NEUR IN, V31; Liu RS, 2018, AAAI CONF ARTIF INTE, P1371; Luo Liangchen, 2019, INT C LEARN REPR; Luo P, 2018, ARXIV PREPRINT ARXIV; Lv KF, 2017, PR MACH LEARN RES, V70; Marino J., 2018, PROC INT C LEARN REP; McMahan B, 2018, AAAI CONF ARTIF INTE, P378; Naik D. K., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P437, DOI 10.1109/IJCNN.1992.287172; Reddi S. J., 2018, P INT C LEARN REPR; Ren M., 2018, P 6 INT C LEARN REPR; Rifai S., 2011, ARXIV11043250; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rusu Andrei A, 2019, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Simonyan K., 2014, 3 INT C LEARN REPR I; Snell J, 2017, ADV NEUR IN, V30; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tseng P, 1998, SIAM J OPTIMIZ, V8, P506, DOI 10.1137/S1052623495294797; Wager Stefan, 2013, ADV NEURAL INFORM PR, P351; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang SP, 2019, AAAI CONF ARTIF INTE, P5297; Wang YC, 2017, ADV NEUR IN, V30; Wang YX, 2016, ADV NEUR IN, V29; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Wichrowska O, 2017, PR MACH LEARN RES, V70; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Wu LJ, 2018, ADV NEUR IN, V31; Yang Y, 2016, ADV NEUR IN, V29; Yang Y, 2020, IEEE T PATTERN ANAL, V42, P521, DOI 10.1109/TPAMI.2018.2883941; Younger AS, 2001, IEEE IJCNN, P2001, DOI 10.1109/IJCNN.2001.938471; Zeiler M.D., 2012, ADADELTA ADAPTIVE LE, DOI DOI 10.48550/ARXIV.1212.5701; Zhang RX, 2018, ADV NEUR IN, V31; Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759; Zhou Z., 2019, PROC INT C LEARN REP	73	3	3	6	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 23	2021	44	8					4469	4484		10.1109/TPAMI.2021.3061581	http://dx.doi.org/10.1109/TPAMI.2021.3061581			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	2Q6HN	33621172				2022-12-18	WOS:000820521700005
J	Gillis, N; Hien, LTK; Leplat, V; Tan, VYF				Gillis, Nicolas; Le Thi Khanh Hien; Leplat, Valentin; Tan, Vincent Y. F.			Distributionally Robust and Multi-Objective Nonnegative Matrix Factorization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Linear programming; Optimization; Standards; Data models; Minimization; Image reconstruction; Dimensionality reduction; Nonnegative matrix factorization; multiple objectives; distributional robustness; multiplicative updates	ALGORITHMS; CONVERGENCE; DIVERGENCE	Nonnegative matrix factorization (NMF) is a linear dimensionality reduction technique for analyzing nonnegative data. A key aspect of NMF is the choice of the objective function that depends on the noise model (or statistics of the noise) assumed on the data. In many applications, the noise model is unknown and difficult to estimate. In this paper, we define a multi-objective NMF (MO-NMF) problem, where several objectives are combined within the same NMF model. We propose to use Lagrange duality to judiciously optimize for a set of weights to be used within the framework of the weighted-sum approach, that is, we minimize a single objective function which is a weighted sum of the all objective functions. We design a simple algorithm based on multiplicative updates to minimize this weighted sum. We show how this can be used to find distributionally robust NMF (DR-NMF) solutions, that is, solutions that minimize the largest error among all objectives, using a dual approach solved via a heuristic inspired from the Frank-Wolfe algorithm. We illustrate the effectiveness of this approach on synthetic, document and audio data sets. The results show that DR-NMF is robust to our incognizance of the noise model of the NMF problem.	[Gillis, Nicolas; Le Thi Khanh Hien; Leplat, Valentin] Univ Mons, Fac Polytech, Dept Math & Operat Res, B-7000 Mons, Belgium; [Tan, Vincent Y. F.] Natl Univ Singapore, Dept Elect & Comp Engn, Dept Math, Singapore 119077, Singapore	University of Mons; National University of Singapore	Gillis, N (corresponding author), Univ Mons, Fac Polytech, Dept Math & Operat Res, B-7000 Mons, Belgium.	nicolas.gillis@umons.ac.be; thikhanhhien.le@umons.ac.be; valentin.leplat@umons.ac.be; vtan@nus.edu.sg		Leplat, Valentin/0000-0002-3313-1547	Fonds de la Recherche Scientifique -FNRS; Fonds Wetenschappelijk Onderzoek -Vlanderen (FWO) under EOS [O005318F-RG47]; European Research Council (ERC) [679515]; Singapore National Research Foundation (NRF) Fellowship [R-263-000-D02-281]	Fonds de la Recherche Scientifique -FNRS(Fonds de la Recherche Scientifique - FNRS); Fonds Wetenschappelijk Onderzoek -Vlanderen (FWO) under EOS(FWO); European Research Council (ERC)(European Research Council (ERC)European Commission); Singapore National Research Foundation (NRF) Fellowship(National Research Foundation, Singapore)	The authors would like to thank the reviewers and the handling editor for their insightful comments that helped us improve the paper. This work was supported by the Fonds de la Recherche Scientifique -FNRS and the Fonds Wetenschappelijk Onderzoek -Vlanderen (FWO) under EOS Project no O005318F-RG47, and by the European Research Council (ERC starting grant no 679515). This work was also supported by a Singapore National Research Foundation (NRF) Fellowship (R-263-000-D02-281).	[Anonymous], 2015, FOUND TRENDS MACH LE, V8, P232, DOI 10.1561/2200000050; Araujo MCU, 2001, CHEMOMETR INTELL LAB, V57, P65, DOI 10.1016/S0169-7439(01)00119-8; BenTal A, 2009, PRINC SER APPL MATH, P1; Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006; Bertsekas D. P., 2009, CONVEX OPTIMIZATION; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Chen YM, 2014, SIAM J OPTIMIZ, V24, P1779, DOI 10.1137/130919362; Cichocki Andrzej, 2009, NONNEGATIVE MATRIX T, P2; Deb Kalyanmoy, 2014, SEARCH METHODOLOGIES, V4, P5, DOI DOI 10.1007/978-1-4614-6940-7_15; Dikmen O, 2015, IEEE T PATTERN ANAL, V37, P1442, DOI 10.1109/TPAMI.2014.2366144; Fevotte C, 2011, NEURAL COMPUT, V23, P2421, DOI 10.1162/NECO_a_00168; Fevotte C, 2009, NEURAL COMPUT, V21, P793, DOI 10.1162/neco.2008.04-08-771; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Fritsch J., 2012, THESIS UPMC; Fu X, 2019, IEEE SIGNAL PROC MAG, V36, P59, DOI 10.1109/MSP.2018.2877582; Gillis N., 2019, SIAM NEWS, V25, P1; Gillis N., 2011, THESIS U CATHOLIQUE; Gillis N, 2014, REGULARIZATION OPTIM, V12, P257; Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226; Hamedani E. Y., 2018, ARXIV 180301401; Hien L. T. K., 2017, ARXIV 171103669; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Juditsky A, 2012, OPTIMIZATION FOR MACHINE LEARNING, P149; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee DD, 2001, ADV NEUR IN, V13, P556; Liu XS, 2011, IEEE T GEOSCI REMOTE, V49, P757, DOI 10.1109/TGRS.2010.2068053; Lu ST, 2019, INT CONF ACOUST SPEE, P4754, DOI 10.1109/ICASSP.2019.8683795; Marler RT, 2010, STRUCT MULTIDISCIP O, V41, P853, DOI 10.1007/s00158-009-0460-7; Mertikopoulos P., 2019, INT C LEARN REPR; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Takahashi N, 2014, COMPUT OPTIM APPL, V57, P417, DOI 10.1007/s10589-013-9593-0; Tan VYF, 2013, IEEE T PATTERN ANAL, V35, P1592, DOI 10.1109/TPAMI.2012.240; Virtanen T, 2007, IEEE T AUDIO SPEECH, V15, P1066, DOI 10.1109/TASL.2006.885253; Zhao RB, 2018, IEEE T SIGNAL PROCES, V66, P129, DOI 10.1109/TSP.2017.2757914; Zhiyun Lu, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. 22nd International Conference on Artificial Neural Networks, P419, DOI 10.1007/978-3-642-33266-1_52; Zhong S, 2005, KNOWL INF SYST, V8, P374, DOI 10.1007/s10115-004-0194-1; Zhu F, 2016, IEEE T GEOSCI REMOTE, V54, P4012, DOI 10.1109/TGRS.2016.2535298	51	3	3	3	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 11	2021	44	8					4052	4064		10.1109/TPAMI.2021.3058693	http://dx.doi.org/10.1109/TPAMI.2021.3058693			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	2Q6HZ	33571089	Green Submitted			2022-12-18	WOS:000820522900002
J	Kasaei, SH; Lopes, LS; Tome, AM				Kasaei, S. Hamidreza; Lopes, Luis Seabra; Tome, Ana Maria			Local-LDA: Open-Ended Learning of Latent Topics for 3D Object Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Object recognition; Robots; Visualization; Task analysis; Training; Shape; Latent dirichlet allocation (LDA); local-LDA; open-ended learning; 3D object learning and recognition; object perception	MODEL; HISTOGRAMS; PERCEPTION	Service robots are expected to be more autonomous and work effectively in human-centric environments. This implies that robots should have special capabilities, such as learning from past experiences and real-time object category recognition. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e., visual topics), from low-level feature co-occurrences, for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. In this way, the advantages of both the (hand-crafted) local features and the (learned) structural semantic features have been considered and combined in an efficient way. An extensive set of experiments has been performed to assess the performance of the proposed Local-LDA in terms of descriptiveness, scalability, and computation time. Experimental results show that the overall classification performance obtained with Local-LDA is clearly better than the best performances obtained with the state-of-the-art approaches. Moreover, the best scalability, in terms of number of learned categories, was obtained with the proposed Local-LDA approach, closely followed by a Bag-of-Words (BoW) approach. Concerning computation time, the best result was obtained with BoW, immediately followed by the Local-LDA approach.	[Kasaei, S. Hamidreza] Univ Groningen, Inst Artificial Intelligence & Cognit Engn, POB 407, NL-9700 AK Groningen, Netherlands; [Kasaei, S. Hamidreza; Lopes, Luis Seabra; Tome, Ana Maria] Univ Aveiro, IEETA Inst Engn Elect & Telemat Aveiro, P-3810193 Aveiro, Portugal; [Lopes, Luis Seabra; Tome, Ana Maria] Univ Aveiro, Dept Elect Telecommun & Informat, P-3810193 Aveiro, Portugal	University of Groningen; Universidade de Aveiro; Universidade de Aveiro	Kasaei, SH (corresponding author), Univ Groningen, Inst Artificial Intelligence & Cognit Engn, POB 407, NL-9700 AK Groningen, Netherlands.	hamidreza.kasaei@rug.nl; lsl@ua.pt; ana@ua.pt	; Seabra Lopes, Luis/A-6012-2012	Kasaei, Hamidreza/0000-0001-9408-7730; Seabra Lopes, Luis/0000-0002-5719-5019	FCT project [PEst-OE/EEI/UI0127/2016]; FCT scholarship [SFRH/BD/94183/2013]	FCT project(Portuguese Foundation for Science and Technology); FCT scholarship(Portuguese Foundation for Science and Technology)	This work was partially funded by Portuguese funds through FCT project PEst-OE/EEI/UI0127/2016 and FCT scholarship SFRH/BD/94183/2013.	Aldoma A, 2012, IEEE ROBOT AUTOM MAG, V19, P80, DOI 10.1109/MRA.2012.2206675; Banerjee A, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P431; Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bo LF, 2011, PROC CVPR IEEE, P1729, DOI 10.1109/CVPR.2011.5995719; Canini K., 2009, ARTIF INTELL, V65, P72, DOI 10.1.1.187.6726; Celikkanat H, 2016, IEEE T COGN DEV SYST, V8, P42, DOI 10.1109/TAMD.2015.2476374; Chauhan A, 2011, COGN PROCESS, V12, P341, DOI 10.1007/s10339-011-0407-y; Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPRW.2009.5206800, 10.1109/CVPR.2009.5206800]; Doumanoglou A, 2016, PROC CVPR IEEE, P3583, DOI 10.1109/CVPR.2016.390; Fei-Fei L, 2005, PROC CVPR IEEE, P524; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gao SH, 2014, IEEE T IMAGE PROCESS, V23, P623, DOI 10.1109/TIP.2013.2290593; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Kasaei SH, 2018, NEUROCOMPUTING, V291, P151, DOI 10.1016/j.neucom.2018.02.066; Kasaei SH, 2015, IEEE INT CONF AUTON, P221, DOI 10.1109/ICARSC.2015.37; Hoffman M., 2010, ADV NEURAL INFORM PR, P856; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; Jeong S, 2012, NEURAL NETWORKS, V25, P130, DOI 10.1016/j.neunet.2011.06.020; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kasaei S. H., 2019, P 2019 CHI C HUM FAC, P1; Kasaei S. H., 2019, ARXIV190203057; Kasaei S.H., 2016, ADV NEURAL INFORM PR, P1948; Kasaei SH, 2018, AAAI CONF ARTIF INTE, P596; Kasaei SH, 2018, IEEE INT C INT ROBOT, P6806; Kasaei SH, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4158, DOI 10.1109/IROS.2016.7759612; Kasaei SH, 2016, PATTERN RECOGN LETT, V83, P312, DOI 10.1016/j.patrec.2016.07.006; Kasaei SH, 2015, J INTELL ROBOT SYST, V80, P537, DOI 10.1007/s10846-015-0189-z; Katz D, 2013, IEEE INT CONF ROBOT, P154, DOI 10.1109/ICRA.2013.6630570; Kim JG, 2009, VISION RES, V49, P2297, DOI 10.1016/j.visres.2009.06.020; Lai K, 2014, IEEE INT CONF ROBOT, P3050, DOI 10.1109/ICRA.2014.6907298; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Li Y., 2016, ADV NEURAL INFORM PR, P307; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Lopes LS, 2007, INTERACT STUD, V8, P53, DOI 10.1075/is.8.1.05lop; Lopes LS, 2002, IEEE ROMAN 2002, PROCEEDINGS, P312, DOI 10.1109/ROMAN.2002.1045641; Mcauliffe Jon D., 2008, P ADV NEURAL INFORM, P121; Oliveira M, 2016, ROBOT AUTON SYST, V75, P614, DOI 10.1016/j.robot.2015.09.019; Oliveira M, 2015, IEEE INT C INT ROBOT, P2488, DOI 10.1109/IROS.2015.7353715; Porteous I., 2008, P 14 ACM SIGKDD INT, P569; Ramage D., 2009, P 2009 C EMP METH NA, V1, P248, DOI DOI 10.3115/1699510.1699543; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Rodrigues F., 2015, P 3 AAAI C HUM COMP, P160; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Sock J, 2017, IEEE INT CONF COMP V, P2228, DOI 10.1109/ICCVW.2017.260; Steels L., 2000, EVOLUTION COMMUNICAT, V4, P3, DOI DOI 10.1075/EOC.4.1.03STE; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; van Hoof H, 2013, IEEE-RAS INT C HUMAN, P169, DOI 10.1109/HUMANOIDS.2013.7029972; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang Y, 2007, LECT NOTES COMPUT SC, V4814, P240; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289	55	3	3	2	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2020	42	10					2567	2580		10.1109/TPAMI.2019.2926459	http://dx.doi.org/10.1109/TPAMI.2019.2926459			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NL5QY	31283495	Green Submitted			2022-12-18	WOS:000567471300018
J	Scholefield, A; Ghasemi, A; Vetterli, M				Scholefield, Adam; Ghasemi, Alireza; Vetterli, Martin			Bound and Conquer: Improving Triangulation by Enforcing Consistency	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Uncertainty; Biological system modeling; Matrix decomposition; Geometry; Computational modeling; Multi-camera imaging; multiple-view geometry; triangulation	PRACTICAL GLOBAL OPTIMIZATION; OVERCOMPLETE EXPANSIONS; QUANTIZATION; POLYTOPES	We study the accuracy of triangulation in multi-camera systems with respect to the number of cameras. We show that, under certain conditions, the optimal achievable reconstruction error decays quadratically as more cameras are added to the system. Furthermore, we analyze the error decay-rate of major state-of-the-art algorithms with respect to the number of cameras. To this end, we introduce the notion of consistency for triangulation, and show that consistent reconstruction algorithms achieve the optimal quadratic decay, which is asymptotically faster than some other methods. Finally, we present simulations results supporting our findings. Our simulations have been implemented in MATLAB and the resulting code is available in the supplementary material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2939530.	[Scholefield, Adam; Ghasemi, Alireza; Vetterli, Martin] Ecole Polytech Fed Lausanne EPFL, Sch Comp & Commun Sci, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Scholefield, A (corresponding author), Ecole Polytech Fed Lausanne EPFL, Sch Comp & Commun Sci, CH-1015 Lausanne, Switzerland.	adam.scholefield@epfl.ch; alireza.ghasemi@epfl.ch; martin.vetterli@epfl.ch		Scholefield, Adam/0000-0002-5083-2185	Commission for Technology and Innovation (CTI) [14842.1 PFES-ES]; ERC Advanced Grant-Support for Frontier Research-SPAR-SAM [247006]; Qualcomm Innovation Fellowship	Commission for Technology and Innovation (CTI); ERC Advanced Grant-Support for Frontier Research-SPAR-SAM; Qualcomm Innovation Fellowship	The authors would like to thank Richard Hartley, Fredrik Kahl, and Pascal Frossard for their advice and fruitful discussions, which have greatly improved the manuscript. This work was in part presented in [1] and [2]. This work was supported by the Commission for Technology and Innovation (CTI) project no. 14842.1 PFES-ES and ERC Advanced Grant-Support for Frontier Research-SPAR-SAM Nr: 247006. A. Ghasemi was additionally supported by a Qualcomm Innovation Fellowship.	Agarwal S, 2006, LECT NOTES COMPUT SC, V3951, P592; [Anonymous], 2005, IEEE I CONF COMP VIS; [Anonymous], 2007, LECT NOTES COMPUT SC; [Anonymous], 2009, LECT NOTES COMPUT SC; [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.97; [Anonymous], 2010, PROC CVPR IEEE; Arvo J., 1992, GRAPHICS GEMS 3 IBM, P117; Beferull-Lozano B, 2003, IEEE T INFORM THEORY, V49, P129, DOI 10.1109/TIT.2002.806117; BLOSTEIN SD, 1987, IEEE T PATTERN ANAL, V9, P752, DOI 10.1109/TPAMI.1987.4767982; Cvetkovic Z, 1999, IEEE DATA COMPR CONF, P344, DOI 10.1109/DCC.1999.755684; Freundlich C, 2015, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2015.7298950; Ghasemi A, 2016, INT CONF ACOUST SPEE, P1776, DOI 10.1109/ICASSP.2016.7471982; Ghasemi A, 2015, IEEE IMAGE PROC, P981, DOI 10.1109/ICIP.2015.7350946; GOODMAN JE, 1986, DISCRETE COMPUT GEOM, V1, P219, DOI 10.1007/BF02187696; Goyal VK, 1998, IEEE T INFORM THEORY, V44, P16, DOI 10.1109/18.650985; Hartley R., 2003, CAMBRIDGE U PRESS CA, V2, DOI DOI 10.1017/CBO9780511811685; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley R, 2007, LECT NOTES COMPUT SC, V4843, P13; Hartley R, 2013, INT J COMPUT VISION, V101, P288, DOI 10.1007/s11263-012-0569-9; Hedborg J, 2014, IEEE COMPUT SOC CONF, P152, DOI 10.1109/CVPRW.2014.28; Kahl F, 2008, IEEE T PATTERN ANAL, V30, P1603, DOI 10.1109/TPAMI.2007.70824; Kahl F, 2008, INT J COMPUT VISION, V79, P271, DOI 10.1007/s11263-007-0117-1; Kanatani K., 2008, P 19 BRIT MACH VIS C, P173; Kanatani K., 2010, P 10 AS C COMP VIS, P242; Nordberg K., 2008, P BRIT MACH VIS C, P1; Powell AM, 2016, FOUND COMPUT MATH, V16, P395, DOI 10.1007/s10208-015-9251-2; Rangan S, 2001, IEEE T INFORM THEORY, V47, P457, DOI 10.1109/18.904562; Raynor R., 2013, P AIPR WORKSH, V1, P1; RODRIGUEZ JJ, 1990, IEEE T PATTERN ANAL, V12, P467, DOI 10.1109/34.55106; Yu L, 2007, IEEE INT C BIOINFORM, P9, DOI 10.1109/BIBM.2007.19	32	3	3	2	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2020	42	9					2321	2326		10.1109/TPAMI.2019.2939530	http://dx.doi.org/10.1109/TPAMI.2019.2939530			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MW9MI	31535981	Green Submitted, hybrid			2022-12-18	WOS:000557354900017
J	Tavassolipour, M; Motahari, SA; Shalmani, MTM				Tavassolipour, Mostafa; Motahari, Seyed Abolfazl; Shalmani, Mohammad Taghi Manzuri			Learning of Gaussian Processes in Distributed and Communication Limited Systems	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Training; Machine learning; Gaussian processes; Task analysis; Optimization; Distortion; Distributed learning; Gaussian processes; communication constraints; vector quantization		It is of fundamental importance to find algorithms obtaining optimal performance for learning of statistical models in distributed and communication limited systems. Aiming at characterizing the optimal strategies, we consider learning of Gaussian Processes (GP) in distributed systems as a pivotal example. We first address a very basic problem: how many bits are required to estimate the inner-products of some Gaussian vectors across distributed machines? Using information theoretic bounds, we obtain an optimal solution for the problem which is based on vector quantization. Two suboptimal and more practical schemes are also presented as substitutes for the vector quantization scheme. In particular, it is shown that the performance of one of the practical schemes which is called per-symbol quantization is very close to the optimal one. Schemes provided for the inner-product calculations are incorporated into our proposed distributed learning methods for GPs. Experimental results show that with spending few bits per symbol in our communication scheme, our proposed methods outperform previous zero rate distributed GP learning schemes such as Bayesian Committee Model (BCM) and Product of experts (PoE).	[Tavassolipour, Mostafa; Motahari, Seyed Abolfazl; Shalmani, Mohammad Taghi Manzuri] Sharif Univ Technol, Dept Comp Engn, Tehran 111558639, Iran	Sharif University of Technology	Shalmani, MTM (corresponding author), Sharif Univ Technol, Dept Comp Engn, Tehran 111558639, Iran.	tavassolipour@ce.sharif.edu; motahari@sharif.edu; manzuri@sharif.edu	Motahari, Seyed Abolfazl/ABB-2459-2020	Manzuri Shalmani, Mohammad Taghi/0000-0002-3451-0338; Tavassolipour, Mostafa/0000-0003-0662-0115				AHLSWEDE R, 1986, IEEE T INFORM THEORY, V32, P533, DOI 10.1109/TIT.1986.1057194; Balcan MF, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P725, DOI 10.1145/2939672.2939796; Boronea SA, 2010, PROCEEDINGS OF THE 2ND EUROPEAN CONFERENCE ON INTELLECTUAL CAPITAL, P100; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Cao Y., 2014, ARXIV14107827; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Deisenroth M. P., 2015, P INT C MACH LEARN; Duchi J. C., 2014, ARXIV14050782; Forero PA, 2010, J MACH LEARN RES, V11, P1663; Gal Yarin, 2014, ADV NEURAL INF PROCE, V2, P3257; HAN TS, 1987, IEEE T INFORM THEORY, V33, P759, DOI 10.1109/TIT.1987.1057383; Han TS, 1995, IEEE T INFORM THEORY, V41, P1802, DOI 10.1109/18.476308; Han TS, 1998, IEEE T INFORM THEORY, V44, P2300, DOI 10.1109/18.720540; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Howard S. G., 2003, ACM SIGOPS OPERATING, P29, DOI [10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]; Jordan MI, 2015, SCIENCE, V349, P255, DOI 10.1126/science.aaa8415; Jordan M. I., 2016, J AM STAT ASSOC, V1050; Kwok J., 2014, P 31 INT C MACH LEAR, P1701; Liu Q., 2014, ADV NEURAL INFORM PR, P1098; Liu Qiang, 2012, INT C MACH LEARN ICM, P1487; Meng XR, 2016, J MACH LEARN RES, V17; Meng Z., 2013, P ART INT STAT, P39; Ng Jun Wei, 2014, ARXIV14123078; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahman MS, 2012, IEEE T INFORM THEORY, V58, P6282, DOI 10.1109/TIT.2012.2206793; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Shvachko K., 2010, SYMPOSIUM, P1, DOI DOI 10.1109/MSST.2010.5496972; Snelson E., 2005, ADV NEURAL INFORM PR, P1257; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908; Zhang Y., 2013, NEURAL INFORM PROCES, P2328	34	3	3	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG. 1	2020	42	8					1928	1941		10.1109/TPAMI.2019.2906207	http://dx.doi.org/10.1109/TPAMI.2019.2906207			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MF5XR	30908258	Green Submitted			2022-12-18	WOS:000545415400009
J	Eghbali, S; Ashtiani, H; Tahvildari, L				Eghbali, Sepehr; Ashtiani, Hassan; Tahvildari, Ladan			Online Nearest Neighbor Search Using Hamming Weight Trees	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Nearest neighbor search; binary codes; sublinear search; tree search; Hamming Weight	BINARY; SPACE	Nearest neighbor search is a basic and recurring proximity problem that has been studied for several decades. The goal is to preprocess a dataset of points so that we can quickly report the closet point(s) to any query point. Many recent applications of NNS involve datasets that are very large and dynamic, that is items of data items become available gradually. In this study, we propose a data structure for solving NNS for dynamic binary data where both query and dataset items are represented as binary strings. The proposed tree data structure, called the Hamming Weight Tree, is simple and as the names suggests, is based on partitioning the feature space of binary strings by exploiting the Hamming weights of the binary codes and their substrings. Given a Hamming Weight Tree of binary codes, we propose two search algorithms that accommodate nearest neighbor search for two different distance functions, the Hamming distance and the angular distance. Our empirical results show significant speedup in comparison with the best known large-scale solutions.	[Eghbali, Sepehr; Tahvildari, Ladan] Univ Waterloo, Dept Elect & Comp Engn, 200 Univ Ave West, Waterloo, ON N2L 3G1, Canada; [Ashtiani, Hassan] McMaster Univ, Dept Comp & Software, 1280 Main St W, Hamilton, ON L8S 4L8, Canada	University of Waterloo; McMaster University	Eghbali, S (corresponding author), Univ Waterloo, Dept Elect & Comp Engn, 200 Univ Ave West, Waterloo, ON N2L 3G1, Canada.	s2eghbal@uwaterloo.ca; zokaeiam@mcmaster.ca; ladan.tahvildari@uwaterloo.ca		Ashtiani, Hassan/0000-0003-1758-7330	Natural Sciences and Engineering Research Council (NSERC) Canada	Natural Sciences and Engineering Research Council (NSERC) Canada(Natural Sciences and Engineering Research Council of Canada (NSERC))	This research was supported by Natural Sciences and Engineering Research Council (NSERC) Canada, Ontario Research Fund: Research Excellence (ORF-RE) and Collaborative Research and Development (CRD). The authors would like to express their appreciations to the associate editor and the anonymous reviewers for their valuable suggestions.	Alman J, 2015, ANN IEEE SYMP FOUND, P136, DOI 10.1109/FOCS.2015.18; Aumuller M, 2017, LECT NOTES COMPUT SC, V10609, P34, DOI 10.1007/978-3-319-68474-1_3; Balntas V, 2018, IEEE T PATTERN ANAL, V40, P555, DOI 10.1109/TPAMI.2017.2679193; Barkol O., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P388, DOI 10.1145/335305.335350; Bayardo R. J., 2007, WWW, P131, DOI DOI 10.1145/1242572.1242591; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Beygelzimer A, 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857; Buhlmann P, 2016, HDB OF BIG DATA; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646; Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965; Da C, 2017, PROC CVPR IEEE, P898, DOI 10.1109/CVPR.2017.102; Eghbali S, 2019, IEEE T KNOWL DATA EN, V31, P329, DOI 10.1109/TKDE.2018.2828095; Eghbali S, 2017, IEEE DATA MINING, P853, DOI 10.1109/ICDM.2017.104; Esmaeili MM, 2012, IEEE T PATTERN ANAL, V34, P2481, DOI 10.1109/TPAMI.2012.170; Gao JY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P349, DOI 10.1145/2783258.2783284; Gong Y., 2012, ADV NEURAL INFORM PR, P1196; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; GREENE D, 1994, AN S FDN CO, P722; Hein M, 2005, P 10 INT WORKSH ART, P136; Hoffmann, 2003, ACM MULTIMEDIA, V8, P119; Huang LK, 2018, IEEE T NEUR NET LEAR, V29, P2309, DOI 10.1109/TNNLS.2017.2689242; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Jegou H, 2011, INT CONF ACOUST SPEE, P861; Jiang ZY, 2016, AER ADV ENG RES, V80, P325; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Kuettel D, 2012, LECT NOTES COMPUT SC, V7578, P459, DOI 10.1007/978-3-642-33786-4_34; Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Lin K, 2018, IEEE INT INTERC TECH, P2; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu L, 2017, PROC CVPR IEEE, P5140, DOI 10.1109/CVPR.2017.546; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mic V, 2018, INT CONF DAT MIN WOR, P945, DOI 10.1109/ICDMW.2018.00137; Minsky M., 1969, PERCEPTRONS; Muja M., 2012, 2012 Canadian Conference on Computer and Robot Vision, P404, DOI 10.1109/CRV.2012.60; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231; Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043; Omohundro S. M., 1989, 5 BALLTREE CONSTRUCT; Ong EJ, 2016, PROC CVPR IEEE, P2000, DOI 10.1109/CVPR.2016.220; Patrascu M., 2008, THESIS; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; Raziperchikolaei R, 2016, IEEE DATA MINING, P1173, DOI [10.1109/ICDM.2016.0155, 10.1109/ICDM.2016.84]; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Samet H, 2006, MORGAN KAUFMANN SERI; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shrivastava A, 2014, JMLR WORKSH CONF PRO, V33, P886; Sivic J, 2008, PROC CVPR IEEE, P2182; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Zhang X., 2013, P 25 INT C SCI STAT	57	3	4	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2020	42	7					1729	1740		10.1109/TPAMI.2019.2902391	http://dx.doi.org/10.1109/TPAMI.2019.2902391			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MC0DH	30843799				2022-12-18	WOS:000542967200016
J	Shi, J; Wang, YL				Shi, Jie; Wang, Yalin		Alzheimer's Dis Neuroimaging	Hyperbolic Wasserstein Distance for Shape Indexing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Shape; Three-dimensional displays; Measurement; Space vehicles; Indexing; Face; Shape space; hyperbolic conformal geometry; Wasserstein distance; shape indexing	MILD COGNITIVE IMPAIRMENT; OPTIMAL MASS-TRANSPORT; ALZHEIMERS-DISEASE; FACE RECOGNITION; SEGMENTATION; REARRANGEMENT; REGISTRATION; MORPHOMETRY; FRAMEWORK; SURFACES	Shape space is an active research topic in computer vision and medical imaging fields. The distance defined in a shape space may provide a simple and refined index to represent a unique shape. This work studies the Wasserstein space and proposes a novel framework to compute the Wasserstein distance between general topological surfaces by integrating hyperbolic Ricci flow, hyperbolic harmonic map, and hyperbolic power Voronoi diagram algorithms. The resulting hyperbolic Wasserstein distance can intrinsically measure the similarity between general topological surfaces. Our proposed algorithms are theoretically rigorous and practically efficient. It has the potential to be a powerful tool for 3D shape indexing research. We tested our algorithm with human face classification and Alzheimer's disease (AD) progression tracking studies. Experimental results demonstrated that our work may provide a succinct and effective shape index.	[Shi, Jie; Wang, Yalin; Alzheimer's Dis Neuroimaging] Arizona State Univ, Sch Comp Informat & Decis Syst Engn, Tempe, AZ 85281 USA	Arizona State University; Arizona State University-Tempe	Wang, YL (corresponding author), Arizona State Univ, Sch Comp Informat & Decis Syst Engn, Tempe, AZ 85281 USA.	jshi28@asu.edu; ylwang@asu.edu	; Shi, Jie/D-1147-2017	Wang, Yalin/0000-0002-6241-735X; Shi, Jie/0000-0002-9095-0557	National Health Institutes [R21AG043760, R21AG049216, RF1AG051710, R01EB025032, U54EB020403]; National Science Foundation [DMS-1413417, IIS-1421165]; Arizona Alzheimer's Consortium; Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health) [U01 AG024904]; DOD ADNI (Department of Defense) [W81XWH-12-20012]; National Institute on Aging; National Institute of Biomedical Imaging and Bioengineering; Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen Idec Inc.; Bristol-Myers Squibb Company; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. HoffmannLa Roche Ltd and its Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Medpace, Inc.; Merck Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Synarc Inc.; Takeda Pharmaceutical Company; Canadian Institutes of Health Research; NATIONAL HEART, LUNG, AND BLOOD INSTITUTE [R01HL128818] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING [R01EB025032, U54EB020403] Funding Source: NIH RePORTER; NATIONAL INSTITUTE ON AGING [R21AG049216] Funding Source: NIH RePORTER	National Health Institutes(National Health Research Institutes - Taiwan); National Science Foundation(National Science Foundation (NSF)); Arizona Alzheimer's Consortium; Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); DOD ADNI (Department of Defense); National Institute on Aging(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute on Aging (NIA)); National Institute of Biomedical Imaging and Bioengineering(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB)); Alzheimer's Association(Alzheimer's Association); Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen Idec Inc.(Biogen); Bristol-Myers Squibb Company(Bristol-Myers Squibb); Eisai Inc.(Eisai Co Ltd); Elan Pharmaceuticals, Inc.; Eli Lilly and Company(Eli Lilly); EuroImmun; F. HoffmannLa Roche Ltd and its Genentech, Inc.; Fujirebio; GE Healthcare(General ElectricGE Healthcare); IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.(Johnson & JohnsonJohnson & Johnson USA); Medpace, Inc.; Merck Co., Inc.(Merck & Company); Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation(Novartis); Pfizer Inc.(Pfizer); Piramal Imaging; Servier(Servier); Synarc Inc.; Takeda Pharmaceutical Company(Takeda Pharmaceutical Company Ltd); Canadian Institutes of Health Research(Canadian Institutes of Health Research (CIHR)); NATIONAL HEART, LUNG, AND BLOOD INSTITUTE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Heart Lung & Blood Institute (NHLBI)); NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB)); NATIONAL INSTITUTE ON AGING(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute on Aging (NIA))	The authors would like to thankWen Zhang and Yonghui Fan of Arizona State University for their helpful discussion. This work was partially supported by National Health Institutes (R21AG043760, R21AG049216, RF1AG051710, R01EB025032, U54EB020403), National Science Foundation (DMS-1413417, IIS-1421165) and Arizona Alzheimer's Consortium. Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award #W81XWH-12-20012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen Idec Inc.; Bristol-Myers Squibb Company; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. HoffmannLa Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Medpace, Inc.; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Synarc Inc.; and Takeda Pharmaceutical Company. The Canadian Institutes of Rev December 5, 2013 Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Disease Cooperative Study at the University of California, San Diego. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California. Data used in preparation of this article were obtained from the Alzheimer' s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: https://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf	Ambrosio L, 2013, LECT NOTES MATH, V2062, P1, DOI 10.1007/978-3-642-32160-3_1; Andersen Simon Kragh, 2015, Image Analysis. 19th Scandinavian Conference, SCIA 2015. Proceedings: LNCS 9127, P103, DOI 10.1007/978-3-319-19665-7_9; [Anonymous], PATTERN RECOGNITION; [Anonymous], GEOMETRY TOPOLOGY TH; [Anonymous], TEXAS 3D FACE RECOGN; [Anonymous], 13 6 POWER DIAGRAMS; Bauer M, 2012, SIAM J IMAGING SCI, V5, P244, DOI 10.1137/100807983; Ben-Chen M., 2008, P 1 EUROGRAPHICS C 3, P1; Bonnotte N, 2013, SIAM J MATH ANAL, V45, P64, DOI 10.1137/120874850; BRENIER Y, 1991, COMMUN PUR APPL MATH, V44, P375, DOI 10.1002/cpa.3160440402; Brignell CJ, 2010, BIOSTATISTICS, V11, P609, DOI 10.1093/biostatistics/kxq016; Bronstein AM, 2005, INT J COMPUT VISION, V64, P5, DOI 10.1007/s11263-005-1085-y; Castellani U, 2011, LECT NOTES COMPUT SC, V6892, P426, DOI 10.1007/978-3-642-23629-7_52; Chow B., 2006, HAMILTONS RICCI FLOW; Chung MK, 2008, IEEE T MED IMAGING, V27, P1143, DOI 10.1109/TMI.2008.918338; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dale AM, 1999, NEUROIMAGE, V9, P179, DOI 10.1006/nimg.1998.0395; Davatzikos C, 1996, J COMPUT ASSIST TOMO, V20, P656, DOI 10.1097/00004728-199607000-00031; de Goes F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366190; de Goes F, 2011, COMPUT GRAPH FORUM, V30, P1593, DOI 10.1111/j.1467-8659.2011.02033.x; Du AT, 2007, BRAIN, V130, P1159, DOI 10.1093/brain/awm016; Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Gutman Boris A, 2015, Inf Process Med Imaging, V24, P205, DOI 10.1007/978-3-319-19992-4_16; Haker S, 2004, INT J COMPUT VISION, V60, P225, DOI 10.1023/B:VISI.0000036836.66311.97; Jermyn IH, 2012, LECT NOTES COMPUT SC, V7576, P804, DOI 10.1007/978-3-642-33715-4_58; Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017; KANTOROVICH L. V., 2006, J MATH SCI-U TOKYO, V133, P1383, DOI [DOI 10.1007/S10958-006-0050-9, 10.1007/s10958-006-0050-9]; KENDALL DG, 1977, ADV APPL PROBAB, V9, P428, DOI 10.2307/1426091; Kurtek S, 2012, IEEE T PATTERN ANAL, V34, P1717, DOI 10.1109/TPAMI.2011.233; Lai RJ, 2010, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2010.5540023; Li SY, 2014, J NEUROSCI, V34, P10541, DOI 10.1523/JNEUROSCI.4356-13.2014; Li X, 2009, IEEE T VIS COMPUT GR, V15, P558, DOI 10.1109/TVCG.2008.200; Lipman Y, 2013, MATH COMPUT, V82, P331; Lipman Y, 2011, ADV MATH, V227, P1047, DOI 10.1016/j.aim.2011.01.020; Memoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y; Miller MI, 2002, ANNU REV BIOMED ENG, V4, P375, DOI 10.1146/annurev.bioeng.4.092101.125733; Mueller SG, 2005, NEUROIMAG CLIN N AM, V15, P869, DOI 10.1016/j.nic.2005.09.008; Ni K, 2009, INT J COMPUT VISION, V84, P97, DOI 10.1007/s11263-009-0234-0; Pizer SM, 1999, IEEE T MED IMAGING, V18, P851, DOI 10.1109/42.811263; rachev s.t., 1998, PROB APPL S, VII, DOI 10.1007/b98894; Raviv D., 2010, PROCEDINGS ACM WORKS, P39, DOI DOI 10.1145/1877808.1877817; Rehman TU, 2009, MED IMAGE ANAL, V13, P931, DOI 10.1016/j.media.2008.10.008; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Schmitzer Bernhard, 2013, Energy Minimization Methods in Computer Vision and Pattern Recognition. 9th International Conference, EMMCVPR 2013. Proceedings. LNCS 8081, P123, DOI 10.1007/978-3-642-40395-8_10; Schoen R., 1997, P C LECT NOT GEOM TO, VII; Shi J, 2016, PROC CVPR IEEE, P5051, DOI 10.1109/CVPR.2016.546; Shi J, 2017, MED IMAGE ANAL, V35, P517, DOI 10.1016/j.media.2016.09.001; Shi J, 2015, NEUROIMAGE, V104, P1, DOI 10.1016/j.neuroimage.2014.09.062; Shi R, 2017, IEEE T PATTERN ANAL, V39, P965, DOI 10.1109/TPAMI.2016.2567398; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86; Srivastava A, 2009, IEEE T PATTERN ANAL, V31, P1616, DOI 10.1109/TPAMI.2008.223; Styner M, 2005, P NATL ACAD SCI USA, V102, P4872, DOI 10.1073/pnas.0501117102; Styner M., 2001, Information Processing in Medical Imaging. 17th International Conference, IPMI 2001. Proceedings (Lecture Notes in Computer Science Vol.2082), P502; Su ZY, 2015, IEEE T PATTERN ANAL, V37, P2246, DOI 10.1109/TPAMI.2015.2408346; Su Zhengyu, 2015, Inf Process Med Imaging, V24, P411, DOI 10.1007/978-3-319-19992-4_32; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; Thompson PM, 2001, CEREB CORTEX, V11, P1, DOI 10.1093/cercor/11.1.1; Villani C., 2003, TOPICS OPTIMAL TRANS; Wang W, 2013, INT J COMPUT VISION, V101, P254, DOI 10.1007/s11263-012-0566-z; Wang YL, 2012, IEEE T MED IMAGING, V31, P251, DOI 10.1109/TMI.2011.2168233; Younes L., 2010, SHAPES DIFFEOMORPHIS; Younes L, 2012, IMAGE VISION COMPUT, V30, P389, DOI 10.1016/j.imavis.2011.09.009; Zeng W, 2013, INT J COMPUT VISION, V105, P155, DOI 10.1007/s11263-012-0586-8; Zeng W, 2010, IEEE T PATTERN ANAL, V32, P662, DOI 10.1109/TPAMI.2009.201; Zhang L, 2006, VISUAL COMPUT, V22, P43, DOI 10.1007/s00371-005-0352-9; Zhang W, 2017, I S BIOMED IMAGING, P520, DOI 10.1109/ISBI.2017.7950574; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342	72	3	3	0	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2020	42	6					1362	1376		10.1109/TPAMI.2019.2898400	http://dx.doi.org/10.1109/TPAMI.2019.2898400			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LR3TM	30763239	Green Accepted, hybrid			2022-12-18	WOS:000535615700006
J	Nauata, N; Hu, HX; Zhou, GT; Deng, ZW; Liao, ZC; Mori, G				Nauata, Nelson; Hu, Hexiang; Zhou, Guang-Tong; Deng, Zhiwei; Liao, Zicheng; Mori, Greg			Structured Label Inference for Visual Understanding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Visualization; Task analysis; Hidden Markov models; Deep learning; Feature extraction; Neural networks; Computer vision; multi-label classification; image classification; video recognition; action detection; structured inference	CLASSIFICATION; RECOGNITION; DATABASE	Visual data such as images and videos contain a rich source of structured semantic labels as well as a wide range of interacting components. Visual content could be assigned with fine-grained labels describing major components, coarse-grained labels depicting high level abstractions, or a set of labels revealing attributes. Such categorization over different, interacting layers of labels evinces the potential for a graph-based encoding of label information. In this paper, we exploit this rich structure for performing graph-based inference in label space for a number of tasks: multi-label image and video classification and action detection in untrimmed videos. We consider the use of the Bidirectional Inference Neural Network (BINN) and Structured Inference Neural Network (SINN) for performing graph-based inference in label space and propose a Long Short-Term Memory (LSTM) based extension for exploiting activity progression on untrimmed videos. The methods were evaluated on (i) the Animal with Attributes (AwA), Scene Understanding (SUN) and NUS-WIDE datasets for multi-label image classification, (ii) the first two releases of the YouTube-8M large scale dataset for multi-label video classification, and (iii) the THUMOS'14 and MultiTHUMOS video datasets for action detection. Our results demonstrate the effectiveness of structured label inference in these challenging tasks, achieving significant improvements against baselines.	[Nauata, Nelson] Simon Fraser Univ, Fac Appl Sci, Comp Sci, Burnaby, BC V5A 1S6, Canada; [Hu, Hexiang] Univ Southern Calif, Viterbi Sch Engn, Los Angeles, CA 90089 USA; [Zhou, Guang-Tong] Oracle Corp, Oracle Labs, Vancouver, BC V7X 1M5, Canada; [Deng, Zhiwei; Mori, Greg] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada; [Liao, Zicheng] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China	Simon Fraser University; University of Southern California; Simon Fraser University; Zhejiang University	Nauata, N (corresponding author), Simon Fraser Univ, Fac Appl Sci, Comp Sci, Burnaby, BC V5A 1S6, Canada.	nnauata@sfu.ca; frank.hexiang@gmail.com; zhouguangtong@gmail.com; zhiweid@sfu.ca; zliao@zju.edu.cn; mori@cs.sfu.ca	Hu, Hexiang/GNW-4536-2022					Abu-El-Haija S., 2016, ARXIV; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2007, 2007 IEEE C COMP VIS; [Anonymous], 2003, THESIS; [Anonymous], 2011, P NIPS; [Anonymous], 2004, P WORKSH STAT LEARN; [Anonymous], ARXIV170605028; Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4; Battaglia Peter W, 2018, ARXIV 180601261; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Deng Jia, 2011, ADV NEURAL INFORM PR, V1, P567, DOI [10.5555/2986459.2986523, DOI 10.5555/2986459.2986523]; Ding N, 2015, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2015.138; Donahue J, 2014, PR MACH LEARN RES, V32; Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174; Du N, 2007, PROCEEDINGS OF THE IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, P100, DOI 10.1109/WI.2007.36; Gong Y, 2013, ARXIV13124894; Graves A., 2008, ADV NEURAL INFORM PR, P545, DOI DOI 10.1007/978-1-4471-4072-6; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu HX, 2016, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2016.323; Hwang SJ, 2012, ADV NEURAL INFORM PR, P1718; Hwang Sung Ju, 2014, ADV NEURAL INFORM PR, P271; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia Y., 2014, P 22 ACM INT C MULT, P675; Jiang Y.-G., 2014, ECCV WORKSH; Johnson J, 2015, IEEE I CONF COMP VIS, P4624, DOI 10.1109/ICCV.2015.525; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Ke Y, 2007, IEEE I CONF COMP VIS, P1424; King DB, 2015, ACS SYM SER, V1214, P1; Kohli P, 2008, INT J COMPUT VISION, V79, P285, DOI 10.1007/s11263-007-0120-6; Kong C, 2014, PROC CVPR IEEE, P3558, DOI 10.1109/CVPR.2014.455; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Kumar MP, 2005, PROC CVPR IEEE, P18; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214; McAuley J, 2012, LECT NOTES COMPUT SC, V7575, P828, DOI 10.1007/978-3-642-33765-9_59; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Montes Alberto, 2016, ARXIV160808128; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sak H, 2014, INTERSPEECH, P338; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216; Soomro K., 2012, CRCVTR1201; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Taskar B, 2004, ADV NEUR IN, V16, P25; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161; Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	59	3	4	1	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2020	42	5					1257	1271		10.1109/TPAMI.2019.2893215	http://dx.doi.org/10.1109/TPAMI.2019.2893215			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LA0ZT	30668494	Green Submitted			2022-12-18	WOS:000523685800018
J	Poullis, C				Poullis, Charalambos			Large-Scale Urban Reconstruction with Tensor Clustering and Global Boundary Refinement	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Laser radar; Three-dimensional displays; Buildings; Data mining; Solid modeling; Measurement; Tensor clustering; pointcloud segmentation; pointcloud tensor field; parameter-free clustering; LiDAR reconstruction; boundary refinement		Accurate and efficient methods for large-scale urban reconstruction are of significant importance to the computer vision and computer graphics communities. Although rapid acquisition techniques such as airborne LiDAR have been around for many years, creating a useful and functional virtual environment from such data remains difficult and labor intensive. This is due largely to the necessity in present solutions for data dependent user defined parameters. In this paper we present a new solution for automatically converting large LiDAR data pointcloud into simplified polygonal 3D models. The data is first divided into smaller components which are processed independently and concurrently to extract various metrics about the points. Next, the extracted information is converted into tensors. A robust agglomerate clustering algorithm is proposed to segment the tensors into clusters representing geospatial objects e.g., roads, buildings, etc. Unlike previous methods, the proposed tensor clustering process has no data dependencies and does not require any user-defined parameter. The required parameters are adaptively computed assuming a Weibull distribution for similarity distances. Lastly, to extract boundaries from the clusters a new multi-stage boundary refinement process is developed by reformulating this extraction as a global optimization problem. We have extensively tested our methods on several pointcloud datasets of different resolutions which exhibit significant variability in geospatial characteristics e.g., ground surface inclination, building density, etc and the results are reported. The source code for both tensor clustering and global boundary refinement will be made publicly available with the publication on the author's website.	[Poullis, Charalambos] Concordia Univ, Dept Comp Sci & Software Engn, Immers & Creat Technol Lab, Montreal, PQ H3G 1M8, Canada	Concordia University - Canada	Poullis, C (corresponding author), Concordia Univ, Dept Comp Sci & Software Engn, Immers & Creat Technol Lab, Montreal, PQ H3G 1M8, Canada.	charalambos@poullis.org			Natural Sciences and Engineering Research Council of Canada [DG-N01670, DND-N01885]; Microsoft Research [0518306]	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); Microsoft Research(Microsoft)	This research is based upon work supported by the Natural Sciences and Engineering Research Council of Canada Grants DG-N01670 (Discovery Grant) and DND-N01885 (Collaborative Research and Development with the Department of National Defence Grant). The author would also like to thank Microsoft Research for making processing large data sets possible through its Azure for Research Award No. 0518306.	Burghouts G., 2008, P ADV NEUR INF PROC, P201; Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236; Douglas DH, 1973, CARTOGR INT J GEOGR, V10, P112, DOI [10.3138/fm57-6770-u75u-7727, DOI 10.3138/FM57-6770-U75U-7727]; Forstner W, 2003, GEODESY THE CHALLENG, P299, DOI DOI 10.1007/978-3-662-05296-9_31; Herdin M, 2005, IEEE VTS VEH TECHNOL, P136; Lafarge F, 2013, COMPUT GRAPH FORUM, V32, P225, DOI 10.1111/cgf.12042; Lin H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461969; Malcolm J, 2007, IEEE I CONF COMP VIS, P2726; Matei BC, 2008, PROC CVPR IEEE, P903; Medioni G., 2000, COMPUTATIONAL FRAMEW; Mitra NJ, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12010; Musialski P, 2013, COMPUT GRAPH FORUM, V32, P146, DOI 10.1111/cgf.12077; Poullis C, 2014, ISPRS J PHOTOGRAMM, V95, P93, DOI 10.1016/j.isprsjprs.2014.06.006; Poullis C, 2013, IEEE T PATTERN ANAL, V35, P2563, DOI 10.1109/TPAMI.2013.64; Poullis C, 2009, PROC CVPR IEEE, P2767; Poullis C, 2009, IEEE T VIS COMPUT GR, V15, P654, DOI 10.1109/TVCG.2008.189; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7; Xiao JX, 2014, INT J COMPUT VISION, V110, P243, DOI 10.1007/978-3-642-33718-5_48; Zhou QY, 2012, PROC CVPR IEEE, P326, DOI 10.1109/CVPR.2012.6247692; Zhou QY, 2010, LECT NOTES COMPUT SC, V6313, P115	21	3	3	6	25	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2020	42	5					1132	1145		10.1109/TPAMI.2019.2893671	http://dx.doi.org/10.1109/TPAMI.2019.2893671			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LA0ZT	30668463	Bronze			2022-12-18	WOS:000523685800009
J	Fujimura, Y; Iiyama, M; Hashimoto, A; Minoh, M				Fujimura, Yuki; Iiyama, Masaaki; Hashimoto, Atsushi; Minoh, Michihiko			Photometric Stereo in Participating Media Using an Analytical Solution for Shape-Dependent Forward Scatter	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Media; Three-dimensional displays; Shape; Image reconstruction; Backscatter; Computational modeling; Scattering; Photometric stereo; participating media; single scattering	SINGLE SCATTERING; LIGHT	Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from prvious studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. We discuss the approximation in the proposed method using synthesized data. Then, experiments with real data demonstrate that the proposed method improves 3D reconstruction in participating media.	[Fujimura, Yuki] Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan; [Iiyama, Masaaki; Minoh, Michihiko] Kyoto Univ, Acad Ctr Comp & Media Studies, Kyoto 6068501, Japan; [Hashimoto, Atsushi] OMRON SINIC X Corp, R&D Dept, Tokyo 1130033, Japan	Kyoto University; Kyoto University	Fujimura, Y (corresponding author), Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan.	fujimura@mm.media.kyoto-u.ac.jp; iiyama@mm.media.kyoto-u.ac.jp; a_hasimoto@mm.media.kyoto-u.ac.jp; minoh@mm.media.kyoto-u.ac.jp	Iiyama, Masaaki/AAO-2344-2020	Hashimoto, Atsushi/0000-0002-0799-4269; Fujimura, Yuki/0000-0002-7225-8452	Japan Society for the Promotion of Science KAKENHI [15K00237]	Japan Society for the Promotion of Science KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by the Japan Society for the Promotion of Science KAKENHI Grant Number 15K00237.	Agrawal A, 2006, LECT NOTES COMPUT SC, V3951, P578; [Anonymous], P SIGGRAPH AS 2014 T; [Anonymous], P AS C COMP VIS; Asano Y, 2016, LECT NOTES COMPUT SC, V9910, P635, DOI 10.1007/978-3-319-46466-4_38; Dong B, 2014, PROC CVPR IEEE, P2299, DOI 10.1109/CVPR.2014.294; Forsyth David A, 2012, COMPUTER VISION MODE; Fujimura Y, 2018, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2018.00777; Gu JW, 2013, IEEE T PATTERN ANAL, V35, P555, DOI 10.1109/TPAMI.2012.130; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hullin MB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360686; Ikehata S, 2014, IEEE T PATTERN ANAL, V36, P1816, DOI 10.1109/TPAMI.2014.2299798; Inoshita C, 2014, LECT NOTES COMPUT SC, V8690, P346, DOI 10.1007/978-3-319-10605-2_23; Inoshita C, 2012, LECT NOTES COMPUT SC, V7573, P371, DOI 10.1007/978-3-642-33709-3_27; Kim J, 2010, LECT NOTES COMPUT SC, V6311, P86; Liao M, 2011, PROC CVPR IEEE, P689, DOI 10.1109/CVPR.2011.5995343; Logothetis F., 2016, P 27 BRIT MACHINE VI, P1; Logothetis F, 2017, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2017.481; Midorikawa K, 2016, PROC CVPR IEEE, P4350, DOI 10.1109/CVPR.2016.471; Murez Z, 2017, IEEE T PATTERN ANAL, V39, P1880, DOI 10.1109/TPAMI.2016.2613862; Nam G, 2014, IEEE COMPUT GRAPH, V34, P57, DOI 10.1109/MCG.2014.108; Narasimhan SG, 2005, IEEE I CONF COMP VIS, P420; Narasimhan SG, 2006, ACM T GRAPHIC, V25, P1003, DOI 10.1145/1141911.1141986; Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; Negahdaripour S, 2002, OCEANS 2002 MTS/IEEE CONFERENCE & EXHIBITION, VOLS 1-4, CONFERENCE PROCEEDINGS, P1010; Negahdaripour Shahriar, 2014, IEEE Trans Image Process, V23, P5743, DOI 10.1109/TIP.2014.2358882; Papadhimitri T., 2014, PROC BRIT MACH VIS C, P1; Papadhimitri T, 2013, PROC CVPR IEEE, P1474, DOI 10.1109/CVPR.2013.194; Pegoraro V, 2010, COMPUT GRAPH FORUM, V29, P1365, DOI 10.1111/j.1467-8659.2010.01732.x; Roser M, 2014, IEEE INT CONF ROBOT, P3840, DOI 10.1109/ICRA.2014.6907416; Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66; Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309; Treibitz T, 2009, IEEE T PATTERN ANAL, V31, P385, DOI 10.1109/TPAMI.2008.85; Tsiotsios C, 2014, PROC CVPR IEEE, P2259, DOI 10.1109/CVPR.2014.289; VANDERVORST HA, 1992, SIAM J SCI STAT COMP, V13, P631, DOI 10.1137/0913035; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhou K, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P116, DOI 10.1109/PG.2007.48	40	3	3	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2020	42	3					708	719		10.1109/TPAMI.2018.2889088	http://dx.doi.org/10.1109/TPAMI.2018.2889088			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LC5KN	30582528				2022-12-18	WOS:000525365300014
J	Hasan, M; Paul, S; Mourikis, AI; Roy-Chowdhury, AK				Hasan, Mahmudul; Paul, Sujoy; Mourikis, Anastasios, I; Roy-Chowdhury, Amit K.			Context-Aware Query Selection for Active Learning in Event Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Labeling; Activity recognition; Context modeling; Streaming media; Feature extraction; Entropy; Manuals; Active learning; activity recognition; visual context; information theory	HISTOGRAMS; FLOW	Activity recognition is a challenging problem with many practical applications. In addition to the visual features, recent approaches have benefited from the use of context, e.g., inter-relationships among the activities and objects. However, these approaches require data to be labeled, entirely available beforehand, and not designed to be updated continuously, which make them unsuitable for surveillance applications. In contrast, we propose a continuous-learning framework for context-aware activity recognition from unlabeled video, which has two distinct advantages over existing methods. First, it employs a novel active-learning technique that not only exploits the informativeness of the individual activities but also utilizes their contextual information during query selection; this leads to significant reduction in expensive manual annotation effort. Second, the learned models can be adapted online as more data is available. We formulate a conditional random field model that encodes the context and devise an information-theoretic approach that utilizes entropy and mutual information of the nodes to compute the set of most informative queries, which are labeled by a human. These labels are combined with graphical inference techniques for incremental updates. We provide a theoretical formulation of the active learning framework with an analytic solution. Experiments on six challenging datasets demonstrate that our framework achieves superior performance with significantly less manual labeling.	[Hasan, Mahmudul] Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92521 USA; [Hasan, Mahmudul] Comcast Labs, Washington, DC 20005 USA; [Paul, Sujoy; Mourikis, Anastasios, I; Roy-Chowdhury, Amit K.] Univ Calif Riverside, Dept Elect & Comp Engn, Riverside, CA 92521 USA	University of California System; University of California Riverside; University of California System; University of California Riverside	Hasan, M (corresponding author), Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92521 USA.; Hasan, M (corresponding author), Comcast Labs, Washington, DC 20005 USA.; Paul, S (corresponding author), Univ Calif Riverside, Dept Elect & Comp Engn, Riverside, CA 92521 USA.	mhasa004@ucr.edu; spaul003@ucr.edu; mourikis@ee.ucr.edu; amitrc@ee.ucr.edu			ONR through Mayachitra Inc [N00014-15-C5113]; NSF [IIS-1316934]; Google	ONR through Mayachitra Inc; NSF(National Science Foundation (NSF)); Google(Google Incorporated)	This work was partially supported by ONR contract N00014-15-C5113 through a sub-contract from Mayachitra Inc, NSF grant IIS-1316934, and Google. The first two authors should be considered as joint first authors.	Amer MR, 2012, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2012.6247816; [Anonymous], [No title captured]; [Anonymous], 2011, P 24 INT C NEUR INF; [Anonymous], [No title captured]; [Anonymous], 2007, EE364B STANF U; Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080; Cauwenberghs G, 2001, ADV NEUR IN, V13, P409; Chakraborty S, 2015, IEEE T PATTERN ANAL, V37, P1945, DOI 10.1109/TPAMI.2015.2389848; Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Druck G., 2009, P C EMP METH NAT LAN, P81, DOI DOI 10.3115/1699510.1699522; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Elhamifar E, 2013, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2013.33; Fathi A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.78; Felzenszwalb P. F., DISCRIMINATIVELY TRA; Grant M., 2009, 711 U CAMBR; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; Hasan M, 2015, IEEE I CONF COMP VIS, P4543, DOI 10.1109/ICCV.2015.516; Hasan M, 2016, COMPUT VIS IMAGE UND, V144, P24, DOI 10.1016/j.cviu.2015.10.018; Hasan M, 2014, PROC CVPR IEEE, P796, DOI 10.1109/CVPR.2014.107; Hasan M, 2014, LECT NOTES COMPUT SC, V8691, P705, DOI 10.1007/978-3-319-10578-9_46; He HB, 2011, IEEE T NEURAL NETWOR, V22, P1901, DOI 10.1109/TNN.2011.2171713; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Lan T., 2010, ADV NEURAL INFORM PR; Lan T, 2015, IEEE I CONF COMP VIS, P4552, DOI 10.1109/ICCV.2015.517; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113; Lea C, 2016, LECT NOTES COMPUT SC, V9907, P36, DOI 10.1007/978-3-319-46487-9_3; Li Y, 2008, LECT NOTES COMPUT SC, V5305, P409; Mac Aodha O, 2014, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2014.79; Minhas R, 2012, IEEE T CIRC SYST VID, V22, P1529, DOI 10.1109/TCSVT.2011.2177182; Ni BB, 2014, PROC CVPR IEEE, P756, DOI 10.1109/CVPR.2014.102; Oh SM, 2011, PROC CVPR IEEE; Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009; Pei MT, 2011, IEEE I CONF COMP VIS, P487, DOI 10.1109/ICCV.2011.6126279; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Ramanan D., 2006, NIPS, P1129; Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4; Reddy KK, 2009, IEEE I CONF COMP VIS, P1010, DOI 10.1109/ICCV.2009.5459374; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806; Settles B., 2012, ACTIVE LEARNING; Settles B, 2008, P 2008 C EMP METH NA, ppp1070, DOI DOI 10.3115/1613715.1613855; Shi LX, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089109; Stein S, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P729, DOI 10.1145/2493432.2493482; Sun Q, 2015, PROC CVPR IEEE, P3612, DOI 10.1109/CVPR.2015.7298984; Todorovic S, 2012, LECT NOTES COMPUT SC, V7573, P130, DOI 10.1007/978-3-642-33709-3_10; Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9; Wang X, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ESTIMATION, DETECTION AND INFORMATION FUSION ICEDIF 2015, P8, DOI 10.1109/ICEDIF.2015.7280148; Wang ZH, 2013, PROC CVPR IEEE, P1690, DOI 10.1109/CVPR.2013.221; Xianghang Liu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3253, DOI 10.1109/ICIP.2011.6116363; Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235; Zhu YY, 2013, PROC CVPR IEEE, P2491, DOI 10.1109/CVPR.2013.322	54	3	3	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2020	42	3					554	567		10.1109/TPAMI.2018.2878696	http://dx.doi.org/10.1109/TPAMI.2018.2878696			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LC5KN	30387722	Green Submitted, hybrid			2022-12-18	WOS:000525365300003
J	Dutta, A; Engels, J; Hahn, M				Dutta, Avishek; Engels, Johannes; Hahn, Michael			Segmentation of Laser Point Clouds in Urban Areas by a Modified Normalized Cut Method	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Three-dimensional displays; Laser beam cutting; Cost function; Urban areas; Minimization; Eigenvalues and eigenfunctions; Graph; cut; segmentation; laser point cloud		Normalized Cut is a well-established divisive image segmentation method, which we adapt in this paper for the segmentation of laser point clouds in urban areas. Our focus is on polyhedral objects with planar surfaces. Due to its target function, Normalized Cut favours cuts with & x201C;short cut lines & x201D; or & x201C;small cut surfaces & x201D;, which is a drawback for our application. We therefore modify the target function, weighting the similarity measures with distance-dependent weights. We call the induced minimization problem <italic>& x201C;Distance-weighted Cut & x201D;</italic> (<italic>DWCut</italic>). The new target function leads to a generalized eigenvalue problem, which is slightly more complicated than the corresponding problem for the Normalized Cut; on the other hand, the new target function is easier to interpret and avoids some drawbacks of the Normalized Cut. We point out an efficient method for the numerical solution of the eigenvalue problem which is based on a Krylov subspace method. <italic>DWCut</italic> can be beneficially combined with an aggregation in order to reduce the computational effort and to avoid shortcomings due to insufficient plane parameters. We present examples for the successful application of the Distance-weighted Cut principle and evaluate its results by comparison with the results of corresponding manual segmentations.	[Dutta, Avishek; Engels, Johannes; Hahn, Michael] Univ Appl Sci, Schellingstr 24, D-70174 Stuttgart, Germany		Hahn, M (corresponding author), Univ Appl Sci, Schellingstr 24, D-70174 Stuttgart, Germany.	avishek.dutta@hft-stuttgart.de; jolumnes.engels@hft-stuttgart.de; michael.hahn@hft-stuttgart.de			German ministery of education and research	German ministery of education and research(Federal Ministry of Education & Research (BMBF))	This work was funded by the German ministery of education and research within the project "mms -Automatisierte Extraktion vertikaler Strukturen im stadtischen Bereich aus Multisensor Mobile Mapping Daten". The data used in this study was kindly provided by TopScan GmbH. Last but not the least, helpful discussions with V. Wichmann and F. Petrini of Laserdata GmbH, Innsbruck, Austria are gratefully acknowledged.	Vo AV, 2015, ISPRS J PHOTOGRAMM, V104, P88, DOI 10.1016/j.isprsjprs.2015.01.011; Arbelaez P., 2007, BERKELEY SEGMENTATIO; Barnhardt WA, 2002, J COASTAL RES, P28; Boykov Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P26; Carballido-Gamio J, 2004, IEEE T MED IMAGING, V23, P36, DOI 10.1109/TMI.2003.819929; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; Dutta A., 2014, ISPRS INT ARCH PHOTO, V40, P81, DOI [10.5194/isprsarchives-XL-3-81-2014, DOI 10.5194/ISPRSARCHIVES-XL-3-81-2014, DOI 10.5194/ISPRSARCHIVES-XL-3-81]; El-Zehiry N, 2007, PROCEEDINGS OF THE SEVENTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P182; ELIAS P, 1956, IRE T INFORM THEOR, V2, P117, DOI 10.1109/TIT.1956.1056816; Eriksson A, 2011, J MATH IMAGING VIS, V39, P45, DOI 10.1007/s10851-010-0223-5; Golub G., 2013, MATRIX COMPUTATIONS; Golub GH, 2002, SIAM J SCI COMPUT, V24, P312, DOI 10.1137/S1064827500382579; Gross H., 2006, INT ARCH PHOTOGRAMM, V36, P86; Gross H, 2009, INT ARCH PHOTOGRAMM; Hackel T., 2017, ISPRS ANN PHOTOGRAMM, DOI [10.5194/isprs-annals-iv-1-w1-91-2017, DOI 10.5194/ISPRS-ANNALS-IV-1-W1-91-2017]; Huang Q, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC53; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Linli Xu, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2866, DOI 10.1109/CVPRW.2009.5206561; Maji S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2057, DOI 10.1109/CVPR.2011.5995630; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Oesau S, 2014, ISPRS J PHOTOGRAMM, V90, P68, DOI 10.1016/j.isprsjprs.2014.02.004; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Reitberger J., 2010, THESIS; Santalo L., 2009, INTEGRAL GEOMETRY GE; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; WU Z, 1993, IEEE T PATTERN ANAL, V15, P1101, DOI 10.1109/34.244673; Yu YZ, 2001, IEEE T VIS COMPUT GR, V7, P351, DOI 10.1109/2945.965349	33	3	4	2	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2019	41	12					3034	3047		10.1109/TPAMI.2018.2869744	http://dx.doi.org/10.1109/TPAMI.2018.2869744			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JQ0XI	30222551				2022-12-18	WOS:000498677600018
J	Da, C; Meng, GF; Xiang, SM; Ding, K; Xu, SB; Yang, Q; Pan, CH				Da, Cheng; Meng, Gaofeng; Xiang, Shiming; Ding, Kun; Xu, Shibiao; Yang, Qing; Pan, Chunhong			Nonlinear Asymmetric Multi-Valued Hashing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Asymmetric hashing; multi-valued embeddings; binary sparse representation; nonlinear transformation	LEARNING BINARY-CODES; DEEP; RANKING; OBJECT; SCENE	Most existing hashing methods resort to binary codes for large scale similarity search, owing to the high efficiency of computation and storage. However, binary codes lack enough capability in similarity preservation, resulting in less desirable performance. To address this issue, we propose Nonlinear Asymmetric Multi-Valued Hashing (NAMVH) supported by two distinct non-binary embeddings. Specifically, a real-valued embedding is used for representing the newly-coming query by an ideally nonlinear transformation. Besides, a multi-integer-embedding is employed for compressing the whole database, which is modeled by Binary Sparse Representation (BSR) with fixed sparsity. With these two non-binary embeddings, NAMVH preserves more precise similarities between data points and enables access to the incremental extension with database samples evolving dynamically. To perform meaningful asymmetric similarity computation for efficient semantic search, these embeddings are jointly learnt by preserving the pairwise label-based similarity. Technically, this results in a mixed integer programming problem, which is efficiently solved by a well-designed alternative optimization method. Extensive experiments on seven large scale datasets demonstrate that our approach not only outperforms the existing binary hashing methods in search accuracy, but also retains their query and storage efficiency.	[Da, Cheng; Meng, Gaofeng; Xiang, Shiming; Ding, Kun; Xu, Shibiao; Yang, Qing; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Da, Cheng; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Xiang, SM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.	chen.da@nlpr.ia.ac.cn; gfmeng@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn; kding@nlpr.ia.ac.cn; shibiao.xu@nlpr.ia.ac.cn; qyang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn			National Natural Science Foundation of China [91646207, 61620106003, 61573352, 61671451]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The authors would like to thank the reviewers for their valuable suggestions. This work was supported by the National Natural Science Foundation of China under Grants 91646207, 61620106003, 61573352 and 61671451.	Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chen JX, 2017, PROC CVPR IEEE, P5330, DOI 10.1109/CVPR.2017.566; Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Da C, 2017, PROC CVPR IEEE, P898, DOI 10.1109/CVPR.2017.102; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding K, 2018, IEEE T NEUR NET LEAR, V29, P87, DOI 10.1109/TNNLS.2016.2615085; Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Glorot X., 2011, P 14 INT C ART INT S, P315; Gong YC, 2015, PROC CVPR IEEE, P19, DOI 10.1109/CVPR.2015.7298596; Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101; Huiskes Mark J, 2008, P 1 ACM INT C MULTIM, P39, DOI DOI 10.1145/1460096.1460104; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342; Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248; Kang WC, 2016, AAAI CONF ARTIF INTE, P1230; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li P., 2006, P 12 ACM SIGKDD INT, P287, DOI [10.1145/1150402.1150436, DOI 10.1145/1150402.1150436]; Li Q, 2017, ADV NEUR IN, V30; Li W., 2016, INT JOINT C ARTIFICI, P1711; Li W.-J., 2012, P ADV NEUR INF PROC, P1646; Lin GS, 2015, IEEE T PATTERN ANAL, V37, P2317, DOI 10.1109/TPAMI.2015.2404776; Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu W., 2014, ADV NEURAL INFORM PR, V4, P3419; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lyu HL, 2015, CHIN CONT DECIS CONF, P2885; Mohammad Norouzi, 2012, ADV NEURAL INFORM PR, P1061; Neyshabur B, 2013, NIPS, P2823; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345; Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shi XS, 2017, AAAI CONF ARTIF INTE, P2541; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Sivic J, 2008, PROC CVPR IEEE, P2182; Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; von Ahn Luis, 2004, P SIGCHI C HUM FACT, DOI [10.1145/985692.985733, DOI 10.1145/985692.985733]; Wang J, 2017, IEEE INFOCOM SER, DOI 10.1007/s12083-017-0556-6; Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976; Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994; Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Xu DN, 2018, IEEE T KNOWL DATA EN, V30, P2185, DOI 10.1109/TKDE.2018.2817526; Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812; Zhang JP, 2019, IEEE ACM T COMPUT BI, V16, P396, DOI 10.1109/TCBB.2017.2701379; Zhang L, 2013, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2013.208; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhong ZC, 2008, INT CONF NANO MICRO, P120; Zhu FQ, 2017, IEEE T IMAGE PROCESS, V26, P4806, DOI 10.1109/TIP.2017.2695101; Zhu H, 2016, AAAI CONF ARTIF INTE, P2415	71	3	3	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2019	41	11					2660	2676		10.1109/TPAMI.2018.2867866	http://dx.doi.org/10.1109/TPAMI.2018.2867866			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JD2XM	30176580				2022-12-18	WOS:000489838200008
J	Arrigoni, F; Fusiello, A				Arrigoni, Federica; Fusiello, Andrea			Bearing-Based Network Localizability: A Unifying View	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bearing-based localization; direction-based localization; parallel rigidity; bearing rigidity; structure from motion	SENSOR NETWORKS; CAMERA MOTION; LOCALIZATION; RIGIDITY	This paper provides a unifying view and offers new insights on bearing-based network localizability, that is the problem of establishing whether a set of directions between pairs of nodes uniquely determines (up to translation and scale) the position of the nodes in d-space. If nodes represent cameras then we are in the context of global structure from motion. The contribution of the paper is theoretical: first, we rewrite and link in a coherent structure several results that have been presented in different communities using disparate formalisms; second, we derive some new localizability results within the edge-based formulation.	[Arrigoni, Federica] Czech Tech Univ, Prague 16636 6, Czech Republic; [Arrigoni, Federica; Fusiello, Andrea] Univ Udine, DPIA, I-33100 Udine, Italy	Czech Technical University Prague; University of Udine	Fusiello, A (corresponding author), Univ Udine, DPIA, I-33100 Udine, Italy.	arrigoni.federica@spes.uniud.it; andrea.fusiello@uniud.it	Fusiello, Andrea/GOJ-9893-2022	Fusiello, Andrea/0000-0003-2963-0316; Arrigoni, Federica/0000-0003-0331-4032	European Regional Development Fund under the project IMPACT [02.1.01/0.0/0.0/15_ 003/0000468]	European Regional Development Fund under the project IMPACT	The authors would like to thank Beatrice Rossi for her support and valuable discussions on bearing-based localizability that were at the basis of this work. This work was supported by the European Regional Development Fund under the project IMPACT (reg. no CZ. 02.1.01/0.0/0.0/15_ 003/0000468).	Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; Arie-Nachimson M, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P81, DOI 10.1109/3DIMPVT.2012.46; Arrigoni F, 2016, INT CONF 3D VISION, P546, DOI 10.1109/3DV.2016.64; Arrigoni F, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P300, DOI 10.1109/3DV.2015.41; Aspnes J, 2006, IEEE T MOBILE COMPUT, V5, P1663, DOI 10.1109/TMC.2006.174; Bishop AN, 2009, IEEE T AERO ELEC SYS, V45, P308, DOI 10.1109/TAES.2009.4805281; Biswas P, 2006, ACM T SENSOR NETWORK, V2; Bollobas B., 1998, MODERN GRAPH THEORY; Brand M, 2004, LECT NOTES COMPUT SC, V3022, P262; Chartrand G, 1985, INTRO GRAPH THEORY; Chatterjee A, 2013, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2013.70; Connelly R, 2005, DISCRETE COMPUT GEOM, V33, P549, DOI 10.1007/s00454-004-1124-4; Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626; Eren T, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P3064; Eren T., 2004, CUCSO32M; Eren T, 2006, IEEE DECIS CONTR P, P4677; GAVISH M, 1992, IEEE T AERO ELEC SYS, V28, P817, DOI 10.1109/7.256302; Goldstein T, 2016, LECT NOTES COMPUT SC, V9911, P289, DOI 10.1007/978-3-319-46478-7_18; Govindu V. M., 2017, TUTORIAL CVPR; Govindu VM, 2001, PROC CVPR IEEE, P218; Haas R, 2002, ARS COMBINATORIA, V63, P129; Hartley R., 2004, ROBOTICA; Havlena M, 2010, LECT NOTES COMPUT SC, V6312, P100, DOI 10.1007/978-3-642-15552-9_8; Hendrickx JM, 2007, INT J ROBUST NONLIN, V17, P960, DOI 10.1002/rnc.1145; Jacobs DJ, 1997, J COMPUT PHYS, V137, P346, DOI 10.1006/jcph.1997.5809; Ji X, 2004, IEEE INFOCOM SER, P2652; Jiang NJ, 2013, IEEE I CONF COMP VIS, P481, DOI 10.1109/ICCV.2013.66; Karimian A., 2017, P2228; Katz B, 2007, LECT NOTES COMPUT SC, V4362, P330; Kavitha T, 2009, COMPUT SCI REV, V3, P199, DOI 10.1016/j.cosrey.2009.08.001; Kennedy R, 2014, IEEE INT C INT ROBOT, P149, DOI 10.1109/IROS.2014.6942554; Kennedy R, 2012, IEEE INT C INT ROBOT, P194, DOI 10.1109/IROS.2012.6386132; Khatri C. G., 1968, SANKHYA, V30, P167, DOI DOI 10.2307/25049527; Levi N, 2003, PROC CVPR IEEE, P518; Li K, 1998, P AMER CONTR CONF, P2510, DOI 10.1109/ACC.1998.703086; Liu S., 2008, INT J INFORM SYSTEMS, V4, P160, DOI 10.1155/2016/8301709; Lou Y, 2012, LECT NOTES COMPUT SC, V7573, P45, DOI 10.1007/978-3-642-33709-3_4; Malapelle F, 2013, INT SYMP IMAGE SIG, P224; Mao GQ, 2007, COMPUT NETW, V51, P2529, DOI 10.1016/j.comnet.2006.11.018; Martinec D., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383115; Trinh MH, 2016, IEEE DECIS CONTR P, P922, DOI 10.1109/CDC.2016.7798385; Moulon P, 2013, IEEE I CONF COMP VIS, P3248, DOI 10.1109/ICCV.2013.403; Niculescu D., 2003, P IEEE INFOCOM, P4104; Ozyesil O., 2014, THESIS; Ozyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X; Ozyosil O, 2015, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2015.7298883; Postnikov A., 2007, PARALLEL RIGIDITY IN; Russell WJ, 2011, IEEE T SIGNAL PROCES, V59, P2834, DOI 10.1109/TSP.2011.2117422; Servatius B, 1999, SIAM J DISCRETE MATH, V12, P136, DOI 10.1137/S0895480196307342; Shames I, 2013, IEEE T AUTOMAT CONTR, V58, P247, DOI 10.1109/TAC.2012.2206693; Shen TW, 2016, LECT NOTES COMPUT SC, V9907, P139, DOI 10.1007/978-3-319-46487-9_9; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; STOICA P, 1990, IEEE T ACOUST SPEECH, V38, P1132, DOI 10.1109/29.57542; TARJAN RE, 1985, SIAM J COMPUT, V14, P862, DOI 10.1137/0214061; Toldo R, 2015, COMPUT VIS IMAGE UND, V140, P127, DOI 10.1016/j.cviu.2015.05.011; Tron R, 2015, P AMER CONTR CONF, P3911, DOI 10.1109/ACC.2015.7171940; Tron R, 2014, IEEE T AUTOMAT CONTR, V59, P3325, DOI 10.1109/TAC.2014.2351912; Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9; WHITELEY W, 1997, AMS CONT MATH, P171; Whiteley W., 1986, PARALLEL REDRAWING C; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25; Zhao SY, 2016, AUTOMATICA, V69, P334, DOI 10.1016/j.automatica.2016.03.010; Zhao SY, 2015, IEEE DECIS CONTR P, P6115, DOI 10.1109/CDC.2015.7403181	66	3	3	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2019	41	9					2049	2069		10.1109/TPAMI.2018.2848225	http://dx.doi.org/10.1109/TPAMI.2018.2848225			21	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IP9BY	29994655	Green Published			2022-12-18	WOS:000480343900002
J	Son, K; Hays, J; Cooper, DB				Son, Kilho; Hays, James; Cooper, David B.			Solving Square Jigsaw Puzzle by Hierarchical Loop Constraints	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computational jigsaw puzzle; discrete optimization algorithm; hierarchical loop constraints; maximizing consensus	RECONSTRUCTION	We present a novel computational puzzle solver for square-piece image jigsaw puzzles with no prior information such as piece orientation or anchor pieces. By "piece" we mean a square d x d block of pixels, where we investigate pieces as small as 7 x 7 pixels. To reconstruct such challenging puzzles, we propose to find maximum geometric consensus between pieces, specifically hierarchical piece loops. The proposed algorithm seeks out loops of four pieces and aggregates the smaller loops into higher order "loops of loops" in a bottom-up fashion. In contrast to previous puzzle solvers which aim to maximize compatibility measures between all pairs of pieces and thus depend heavily on the pairwise compatibility measures used, our approach reduces the dependency on the pairwise compatibility measures which become increasingly uninformative for small scales and instead exploits geometric agreement among pieces. Our contribution also includes an improved pairwise compatibility measure which exploits directional derivative information along adjoining boundaries of the pieces. We verify the proposed algorithm as well as its individual components with mathematical analysis and reconstruction experiments.	[Son, Kilho; Cooper, David B.] Brown Univ, Sch Engn, Providence, RI 02912 USA; [Hays, James] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA	Brown University; University System of Georgia; Georgia Institute of Technology	Son, K (corresponding author), Brown Univ, Sch Engn, Providence, RI 02912 USA.	kilho.son@gmail.com; hays@gatech.edu; david_cooper@brown.edu			NSF [0808718]	NSF(National Science Foundation (NSF))	Kilho Son and David B. Cooper acknowledge partial support from NSF Grant #0808718.	Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Alajlan Naif, 2009, American Journal of Applied Sciences, V6, P1941, DOI 10.3844/ajassp.2009.1941.1947; Andalo Fernanda A., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P63, DOI 10.1109/SIBGRAPI.2012.18; [Anonymous], 2010, OXFORD DICT ENGLISH; Barabasi A. L., 2013, NAT GENET, V5, P101; Cao SJ, 2010, IEEE INT CON MULTI, P358, DOI 10.1109/ICME.2010.5582544; Chakrabarti A, 2015, PROC CVPR IEEE, P4009, DOI 10.1109/CVPR.2015.7299027; Cho TS, 2010, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2010.5540212; Cho TS, 2010, IEEE T PATTERN ANAL, V32, P1489, DOI 10.1109/TPAMI.2009.133; Dekel T, 2015, PROC CVPR IEEE, P2021, DOI 10.1109/CVPR.2015.7298813; Demaine ED, 2007, GRAPH COMBINATOR, V23, P195, DOI 10.1007/s00373-007-0713-4; FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781; Gallagher AC, 2012, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2012.6247699; Goldenberg A, 2010, FOUND TRENDS MACH LE, V2, P129, DOI 10.1561/2200000005; Hartley R., 2003, MULTIPLE VIEW GEOMET; Kalal Zdenek, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2756, DOI 10.1109/ICPR.2010.675; Kato H, 2014, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2014.127; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Kurihara K, 2015, IEICE T FUND ELECTR, VE98A, P2238, DOI 10.1587/transfun.E98.A.2238; Li XF, 2011, IEEE INT C INT ROBOT, P2879, DOI 10.1109/IROS.2011.6048529; Liu P., 2012, THESIS; Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321; Oxholm G., 2011, 12 INT S VIRT REAL A, P49; Paikin G, 2015, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR.2015.7299116; Pomeranz D, 2011, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2011.5995331; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Sholomon D, 2013, PROC CVPR IEEE, P1767, DOI 10.1109/CVPR.2013.231; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Son K, 2016, PROC CVPR IEEE, P1193, DOI 10.1109/CVPR.2016.134; Son K, 2014, LECT NOTES COMPUT SC, V8694, P32, DOI 10.1007/978-3-319-10599-4_3; Son K, 2013, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2013.40; Willis AR, 2008, IEEE SIGNAL PROC MAG, V25, P65, DOI 10.1109/MSP.2008.923101; Yu R., 2016, P BRIT MACH VIS C, DOI 10.5244/C.30.139; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801; Zhu LJ, 2008, IEEE T PATTERN ANAL, V30, P1, DOI 10.1109/TPAMI.2007.1163	35	3	3	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2019	41	9					2222	2235		10.1109/TPAMI.2018.2857776	http://dx.doi.org/10.1109/TPAMI.2018.2857776			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IP9BY	30028692				2022-12-18	WOS:000480343900013
J	Algarni, M; Sundaramoorthi, G				Algarni, Marei; Sundaramoorthi, Ganesh			SurfCut: Surfaces of Minimal Paths from Topological Structures	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Segmentation; surface extraction; minimal paths; computational topology; cubical complex; Morse-Smale complex	ALGORITHMS; SEGMENTATION; EXTRACTION	We present SurfCut, an algorithm for extracting a smooth, simple surface with an unknown 3D curve boundary from a noisy 3D image and a seed point. Our method is built on the novel observation that ridge curves of the Euclidean length of minimal paths ending on a level set of the solution of the eikonal equation lie on the surface. Our method extracts these ridges and cuts them to form the surface boundary. Our surface extraction algorithm is built on the novel observation that the surface lies in a valley of the eikonal equation solution. The resulting surface is a collection of minimal paths. Using the framework of cubical complexes and Morse theory, we design algorithms to extract ridges and valleys robustly. Experiments on three 3D datasets show the robustness of our method, and that it achieves higher accuracy with lower computational cost than state-of-the-art.	[Algarni, Marei; Sundaramoorthi, Ganesh] King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia	King Abdullah University of Science & Technology	Sundaramoorthi, G (corresponding author), King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia.	marei.algarni@kaust.edu.sa; ganesh.sundaramoorthi@kaust.edu.sa		Algarni, Marei/0000-0003-2713-0852	KAUST [OCRF-2014-CRG3-62140401]	KAUST(King Abdullah University of Science & Technology)	Funded by KAUST OCRF-2014-CRG3-62140401 and VCC.	Algarni M., 2017, THESIS; Algarni M, 2016, LECT NOTES COMPUT SC, V9911, P171, DOI 10.1007/978-3-319-46478-7_11; [Anonymous], 2015, FLOWS NETWORKS; Ardon R, 2005, LECT NOTES COMPUT SC, V3757, P520, DOI 10.1007/11585978_34; Ardon R, 2007, APPL MATH OPT, V55, P127, DOI 10.1007/s00245-006-0885-y; Benmansour F, 2009, LECT NOTES COMPUT SC, V5567, P648, DOI 10.1007/978-3-642-02256-2_54; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Brunsch T, 2015, SIAM J COMPUT, V44, P1798, DOI 10.1137/140989893; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chaussard J, 2009, LECT NOTES COMPUT SC, V5852, P135, DOI 10.1007/978-3-642-10210-3_11; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; CRANDALL MG, 1983, T AM MATH SOC, V277, P1, DOI 10.2307/1999343; Dezso B, 2011, ELECTRON NOTES THEOR, V264, P23, DOI 10.1016/j.entcs.2011.06.003; Eberly D., 1994, Journal of Mathematical Imaging and Vision, V4, P353, DOI 10.1007/BF01262402; Edelsbrunner H., 2001, P 17 ANN S COMPUTATI, P70, DOI DOI 10.1145/378583.378626; Edelsbrunner H., 2003, P 19 ANN S COMPUTATI, P361, DOI DOI 10.1145/777792.777846; Goldberg AV, 1997, J ALGORITHM, V22, P1, DOI 10.1006/jagm.1995.0805; GRADY L, 2006, P IEEE C COMP VIS PA, V1, P69; Grady L, 2010, IEEE T PATTERN ANAL, V32, P321, DOI 10.1109/TPAMI.2008.289; Gyulassy A, 2008, IEEE T VIS COMPUT GR, V14, P1619, DOI 10.1109/TVCG.2008.110; Gyulassy A, 2012, IEEE T VIS COMPUT GR, V18, P2014, DOI 10.1109/TVCG.2012.209; Hale D, 2013, GEOPHYSICS, V78, pS105, DOI 10.1190/GEO2012-0327.1; Hugo GD, 2017, MED PHYS, V44, P762, DOI 10.1002/mp.12059; Kaczynski T., 2006, COMPUTATIONAL HOMOLO, V157; Kaul V, 2012, IEEE T PATTERN ANAL, V34, P1952, DOI 10.1109/TPAMI.2011.267; Kolomenkin M, 2013, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2013.36; Kovacs P, 2015, OPTIM METHOD SOFTW, V30, P94, DOI 10.1080/10556788.2014.895828; KOVALEVSKY VA, 1989, COMPUT VISION GRAPH, V46, P141, DOI 10.1016/0734-189X(89)90165-5; Lassen B, 2013, IEEE T MED IMAGING, V32, P210, DOI 10.1109/TMI.2012.2219881; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; Mille J, 2015, INT J COMPUT VISION, V112, P1, DOI 10.1007/s11263-014-0751-3; Milnor J, 2016, MORSE THEORY AM 51, V51; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Pock T, 2008, LECT NOTES COMPUT SC, V5304, P792, DOI 10.1007/978-3-540-88690-7_59; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Schultz T, 2010, IEEE T VIS COMPUT GR, V16, P109, DOI 10.1109/TVCG.2009.44; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8; Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187; Sullivan J. M, 1990, THESIS; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624; Ulen J, 2015, IEEE T PATTERN ANAL, V37, P2588, DOI 10.1109/TPAMI.2015.2409869; van Rikxoort EM, 2008, IEEE T MED IMAGING, V27, P1, DOI 10.1109/TMI.2007.900447; Wiemker R, 2005, INT CONGR SER, V1281, P1121, DOI 10.1016/j.ics.2005.03.130; Xiao CY, 2016, IEEE T MED IMAGING, V35, P1488, DOI 10.1109/TMI.2016.2517680; Yezzi A, 1997, IEEE T MED IMAGING, V16, P199, DOI 10.1109/42.563665; Zomorodian, 2009, TOPOLOGY COMPUTING, V16	51	3	3	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2019	41	3					726	739		10.1109/TPAMI.2018.2811810	http://dx.doi.org/10.1109/TPAMI.2018.2811810			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HK7LA	29993597	Green Submitted, Green Published			2022-12-18	WOS:000458168800015
J	Setti, F; Cristani, M				Setti, Francesco; Cristani, Marco			Evaluating the Group Detection Performance: The GRODE Metrics	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Group detection; performance metrics; videosurveillance; social signal processing	CROWDS	The detection of groups of individuals is attracting the attention of many researchers in diverse fields, from automated surveillance to human-computer interaction, with a growing number of approaches published every year. Unexpectedly, the evaluation metrics for this problem are not consolidated, with some measures inherited from the people detection field, other from clustering, other designed specifically for a particular approach, thus lacking in generalization and making the comparisons between different approaches hard to be carried out. Moreover, most of the existent metrics are scarcely expressive, addressing groups as they are atomic entities, ignoring that they may have different cardinalities, and that group detection approaches may fail in capturing the exact number of individuals that compose it. This paper fills this gap presenting the GROup DEtection (GRODE) metrics, which formally define precision and recall on the groups, including the group cardinality as a variable. This gives the possibility to investigate aspects never considered so far, such as the tendency of a method of over-or under-segmenting, or of better dealing with specific group cardinalities. The GRODE metrics have been evaluated first on controlled scenarios, where the differences with alternative metrics are evident. Then, the metrics have been applied to eight approaches of group detection, on eight public datasets, providing a fresh-new panorama of the state-of-the-art, discovering interesting strengths and pitfalls of the recent approaches.	[Setti, Francesco; Cristani, Marco] Univ Verona, Dept Comp Sci, Vis Image Proc & Sound Lab VIPS, Str Le Grazie 15, I-37135 Verona, Italy	University of Verona	Setti, F (corresponding author), Univ Verona, Dept Comp Sci, Vis Image Proc & Sound Lab VIPS, Str Le Grazie 15, I-37135 Verona, Italy.	francesco.setti@univr.it; marco.cristani@univr.it		CRISTANI, Marco/0000-0002-0523-6042; Setti, Francesco/0000-0002-0015-5534				Amigo E, 2009, INFORM RETRIEVAL, V12, P461, DOI 10.1007/s10791-008-9066-8; [Anonymous], 1991, MYTH MADDING CROWD; Bassetti C, 2017, COMPUT VIS PATT REC, P15, DOI 10.1016/B978-0-12-809276-7.00003-5; Bazzani L, 2013, EXPERT SYST, V30, P115, DOI 10.1111/j.1468-0394.2012.00622.x; Bazzani L, 2015, IEEE T PATTERN ANAL, V37, P746, DOI 10.1109/TPAMI.2014.2353641; Bazzani L, 2012, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2012.6247888; BOURGEOIS F, 1971, COMMUN ACM, V14, P802, DOI 10.1145/362919.362945; Cehovin L, 2016, IEEE T IMAGE PROCESS, V25, P1261, DOI 10.1109/TIP.2016.2520370; Chang MC, 2011, IEEE I CONF COMP VIS, P747, DOI 10.1109/ICCV.2011.6126312; Choi SS., 2010, J SYSTEMICS CYBERNET, V8910, P43; Choi WG, 2014, LECT NOTES COMPUT SC, V8692, P417, DOI 10.1007/978-3-319-10593-2_28; Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16; Ciolek T., 1980, MAN ENV SYSTEMS, V10, P57; CIOLEK TM, 1983, J NONVERBAL BEHAV, V8, P55, DOI 10.1007/BF00986330; Cristani M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.23; Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176; Goffman E., 1972, ENCOUNTERS 2 STUDIES; Goffman Ervin, 1966, BEHAV PUBLIC PLACES; Goffman Erving., 2009, RELATIONS PUBLIC; Goldberg Mark K., 2010, Proceedings of the 2010 IEEE Second International Conference on Social Computing (SocialCom 2010). the Second IEEE International Conference on Privacy, Security, Risk and Trust (PASSAT 2010), P303, DOI 10.1109/SocialCom.2010.50; Harold Garfinkel, 1967, STUDIES ETHNOMETHODO; Hung H., 2011, P 13 INT C MULTIMODA, P231; Inaba S, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P526, DOI 10.1109/SITIS.2016.89; Ramirez OAI, 2016, IEEE ROMAN, P1104, DOI 10.1109/ROMAN.2016.7745246; Kendon A., 1988, E GOFFMAN EXPLORING, P14; Kendon A., 1990, CUP ARCH, V7; Khamis S, 2012, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2012.6247804; Lanz O, 2008, LECT NOTES COMPUT SC, V4625, P287; Lanz O, 2006, IEEE T PATTERN ANAL, V28, P1436, DOI 10.1109/TPAMI.2006.177; Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x; Manfredi M, 2014, PATTERN RECOGN LETT, V44, P39, DOI 10.1016/j.patrec.2013.11.001; Mazzon R, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P202, DOI 10.1109/AVSS.2013.6636640; McKenna SJ, 2000, COMPUT VIS IMAGE UND, V80, P42, DOI 10.1006/cviu.2000.0870; Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013; Pantel P., 2002, P PAC RIM INT C TREN, P527; Ricci E, 2015, IEEE I CONF COMP VIS, P4660, DOI 10.1109/ICCV.2015.529; Rosales-Mendez H., 2013, PROGR PATTERN RECOGN, V8258, P157, DOI [DOI 10.1007/978-3-642-41822-820, 10.1007/978-3-642-41822-820]; Rota P, 2012, LECT NOTES COMPUT SC, V7585, P111, DOI 10.1007/978-3-642-33885-4_12; Scott S, 2009, SYMB INTERACT, V32, P123, DOI 10.1525/si.2009.32.2.123; Setti F., 2013, 14 INT WORKSHOP IMAG, P1, DOI DOI 10.1109/WIAMIS.2013.6616147; Setti F, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123783; Setti F, 2013, IEEE IMAGE PROC, P3547, DOI 10.1109/ICIP.2013.6738732; Solera F, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P7, DOI 10.1109/AVSS.2013.6636608; Tosato D, 2013, IEEE T PATTERN ANAL, V35, P1972, DOI 10.1109/TPAMI.2012.263; Tran Khai N., 2013, International Conference on Computer Vision Theory and Applications (VISAPP 2013). Proceedings, P539; Varadarajan J, 2018, INT J COMPUT VISION, V126, P410, DOI 10.1007/s11263-017-1026-6; Vascon S, 2015, LECT NOTES COMPUT SC, V9007, P658, DOI 10.1007/978-3-319-16814-2_43; Vilain Marc, 1995, P MESS UND C, V6, P45, DOI [DOI 10.3115/1072399.1072405, 10.3115/1072399.1072405]; White JV, 2004, INTERFACE; Yamaguchi K, 2011, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2011.5995468; Ye N., 2003, HDB DATA MINING, V24; Zanotto M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.111; Zhang L, 2016, PROC CVPR IEEE, P1086, DOI 10.1109/CVPR.2016.123	54	3	3	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2019	41	3					566	580		10.1109/TPAMI.2018.2806970	http://dx.doi.org/10.1109/TPAMI.2018.2806970			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HK7LA	29994145				2022-12-18	WOS:000458168800004
J	Yalniz, IZ; Manmatha, R				Yalniz, Ismet Zeki; Manmatha, R.			Dependence Models for Searching Text in Document Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Markov random fields; document image search; image retrieval; word spotting	RECOGNITION; RETRIEVAL; WORDS	The main goal of existing word spotting approaches for searching document images has been the identification of visually similar word images in the absence of high quality text recognition output. Searching for a piece of arbitrary text is not possible unless the user identifies a sample word image from the document collection or generates the query word image synthetically. To address this problem, a Markov Random Field (MRF) framework is proposed for searching document images and shown to be effective for searching arbitrary text in real time for books printed in English (Latin script), Telugu and Ottoman scripts. The English experiments demonstrate that the dependencies between the visual terms and letter bigrams can be automatically learned using noisy OCR output. It is also shown that OCR text search accuracy can be significantly improved if it is combined with the proposed approach. No commercial OCR engine is available for Telugu or Ottoman script. In these cases the dependencies are trained using manually annotated document images. It is demonstrated that the trained model can be directly used to resolve arbitrary text queries across books despite font type and size differences. The proposed approach outperforms a state-of-the-art BLSTM baseline in these contexts.	[Yalniz, Ismet Zeki; Manmatha, R.] Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Yalniz, IZ (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.	zeki@cs.umass.edu; manmatha@cs.umass.edu			Center for Intelligent Information Retrieval; NSF [IIS-0910884]	Center for Intelligent Information Retrieval; NSF(National Science Foundation (NSF))	Thanks to i) Pramod Sankar & C. V. Jawahar for providing us the Telugu datasets and ii) Bilkent University Multimedia Databases Group for the Ottoman datasets. Special thanks to W. Bruce Croft for his valuable discussions and feedback. This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.	Abadi M, 2015, P 12 USENIX S OPERAT; Almazan J, 2013, IEEE I CONF COMP VIS, P1017, DOI 10.1109/ICCV.2013.130; Ataer E., 2007, P 6 ACM INT C IM VID, P341; Ball G.R., 2006, P 10 INT WORKSH FRON; Beitzel S. M., 2003, P SDUIT C; Feng S., 2008, P 2008 INT C CONT BA, P427; Feng SL, 2004, PROC CVPR IEEE, P1002; Frinken V., 2010, Proceedings 2010 12th International Conference on Frontiers in Handwriting Recognition (ICFHR 2010), P352, DOI 10.1109/ICFHR.2010.61; Govindaraju V, 2009, ADV PATTERN RECOGNIT, P1; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; Harding SM, 1997, LECT NOTES COMPUT SC, V1324, P345, DOI 10.1007/BFb0026737; Kompalli Pramod Sankar, 2010, P DAS, P207, DOI DOI 10.1145/1815330.1815357; Konidaris T, 2007, INT J DOC ANAL RECOG, V9, P167, DOI 10.1007/s10032-007-0042-4; KUKICH K, 1992, COMPUT SURV, V24, P377; Louloudis G, 2007, PROC INT CONF DOC, P599; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu SJ, 2008, IEEE T PATTERN ANAL, V30, P1913, DOI 10.1109/TPAMI.2008.89; Manmatha R, 2005, IEEE T PATTERN ANAL, V27, P1212, DOI 10.1109/TPAMI.2005.150; Marinai S, 2008, PROCEEDINGS OF THE 8TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, P671, DOI 10.1109/DAS.2008.85; Marti UV, 2001, PROC INT CONF DOC, P159, DOI 10.1109/ICDAR.2001.953775; Mathew M, 2016, PROCEEDINGS OF 12TH IAPR WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, (DAS 2016), P186, DOI 10.1109/DAS.2016.68; Metzler D., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P472, DOI 10.1145/1076034.1076115; Nister D, 2006, IEEE COMP SOC C COMP, V2, P2161, DOI DOI 10.1109/CVPR.2006.264; Pal U, 2004, PATTERN RECOGN, V37, P1887, DOI 10.1016/j.patcog.2004.02.003; Parapar J, 2009, LECT NOTES COMPUT SC, V5478, P680, DOI 10.1007/978-3-642-00958-7_66; Qaralleh M., 2013, J SOFTW ENG APPL, V6, P533; Rath T., 2004, P SIGIR04, P297; Rath TM, 2003, PROC CVPR IEEE, P521; Rath TM, 2003, PROC INT CONF DOC, P218; Rath TM, 2007, INT J DOC ANAL RECOG, V9, P139, DOI 10.1007/s10032-006-0027-8; Rodriguez-Serrano J.A., 2008, INT C FRONTIERS HAND, P7; Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34; Sankar P.K., 2007, P CVPR MINN MN US, P1; Tong X., 1996, P WVLC, P88; UKKONEN E, 1992, THEOR COMPUT SCI, V92, P191, DOI 10.1016/0304-3975(92)90143-4; Wshah S, 2012, INT C PATT RECOG, P310; Yalniz I. Z., 2012, Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS 2012), P48, DOI 10.1109/DAS.2012.18; Yalniz IZ, 2011, PROC INT CONF DOC, P754, DOI 10.1109/ICDAR.2011.157; Yalniz IZ, 2009, OPT ENG, V48, DOI 10.1117/1.3262346; Yalniz S., 2009, JOCCH, V2, P8	40	3	4	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2019	41	1					49	63		10.1109/TPAMI.2017.2780108	http://dx.doi.org/10.1109/TPAMI.2017.2780108			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HD3QX	29990277	hybrid			2022-12-18	WOS:000452434800005
J	Chrysos, GG; Zafeiriou, S				Chrysos, Grigorios G.; Zafeiriou, Stefanos			(PDT)-T-2: Person-Specific Detection, Deformable Tracking	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deformable models; adaptive tracking; person-specific learning; long-term tracking		Face detection/alignment methods have reached a satisfactory state in static images captured under arbitrary conditions. Such methods typically perform (joint) fitting for each frame and are used in commercial applications; however in the majority of the real-world scenarios the dynamic scenes are of interest. We argue that generic fitting per frame is suboptimal (it discards the informative correlation of sequential frames) and propose to learn person-specific statistics from the video to improve the generic results. To that end, we introduce a meticulously studied pipeline, which we name (PDT)-T-2, that performs person-specific detection and landmark localisation. We carry out extensive experimentation with a diverse set of i) generic fitting results, ii) different objects (human faces, animal faces) that illustrate the powerful properties of our proposed pipeline and experimentally verify that (PDT)-T-2 outperforms all the compared methods.	[Chrysos, Grigorios G.; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London SW7 2AZ, England	Imperial College London	Chrysos, GG (corresponding author), Imperial Coll London, Dept Comp, London SW7 2AZ, England.	g.chrysos@imperial.ac.uk; s.zafeiriou@imperial.ac.uk	Chrysos, Grigorios/ABE-2026-2021	Chrysos, Grigorios/0000-0002-0650-1856	FiDiPro program of Tekes [1849/31/2015]; EPSRC [EP/N007743/1]; EPSRC project Adaptive Facial Deformable Models for Tracking (ADAManT) [EP/L026813/1]; Imperial College DTA; EPSRC [EP/L026813/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/L026813/1, EP/N007743/1] Funding Source: researchfish	FiDiPro program of Tekes; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC project Adaptive Facial Deformable Models for Tracking (ADAManT)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Imperial College DTA; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We would like to thank Epameinondas Antonakos and Patrick Snape for our fruitful conversations and their contributions in the preliminary version of this work. The work of Stefanos Zafeiriou has been partially funded by the FiDiPro program of Tekes (project number: 1849/31/2015), as well as the EPSRC project EP/N007743/1 (FACER2VM). The work of Grigorios Chrysos has been funded by a) the EPSRC project EP/L026813/1 Adaptive Facial Deformable Models for Tracking (ADAManT), as well as b) an Imperial College DTA.	Alabort-i-Medina J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P679, DOI 10.1145/2647868.2654890; Alabort-i-Medina J, 2014, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2014.439; [Anonymous], 2015, ARXIV150905520; [Anonymous], 2015, P IEEE INT C COMP VI; Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; Bertinetto L., 2016, ARXIV160609549; Bulat A., 2016, P BRIT MACH VIS C; Cheng X, 2012, LECT NOTES COMPUT SC, V7583, P133, DOI 10.1007/978-3-642-33863-2_14; Chrysos GG, 2018, INT J COMPUT VISION, V126, P198, DOI 10.1007/s11263-017-0999-5; Chrysos GG, 2017, IEEE COMPUT SOC CONF, P2015, DOI 10.1109/CVPRW.2017.252; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Essa I., 1996, Proceedings. Computer Animation '96, P68, DOI 10.1109/CA.1996.540489; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb P., 2004, DISTANCE TRANSFORMS, P1963; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Guler Riza Alp, 2017, P IEEE C COMP VIS PA, P6799; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Isard M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P343, DOI 10.1007/BFb0015549; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Koelstra S, 2010, IEEE T PATTERN ANAL, V32, P1940, DOI 10.1109/TPAMI.2010.50; Kossatfi J, 2015, IEEE IMAGE PROC, P1135, DOI 10.1109/ICIP.2015.7350977; Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79; LANITIS A, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P368, DOI 10.1109/ICCV.1995.466919; Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177; MARONNA R.A., 2006, ROBUST STAT; Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47; Nam H., 2016, P IEEE C COMP VIS PA; Nebehay G., 2015, P IEEE C COMP VIS PA; Ning Jifeng, 2016, P IEEE C COMP VIS PA; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Sagonas C., 2015, IMAGE VIS COMPUT; Sagonas C, 2017, INT J COMPUT VISION, V122, P270, DOI 10.1007/s11263-016-0920-7; Sagonas C, 2014, PROC CVPR IEEE, P1789, DOI 10.1109/CVPR.2014.231; Sanchez-Lozano E, 2016, LECT NOTES COMPUT SC, V9912, P645, DOI 10.1007/978-3-319-46484-8_39; Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891; Shen J., 2015, P IEEE INT C COMP VI; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Xiong X., 2014, ARXIV14050601; Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823; Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342; Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	55	3	3	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2018	40	11					2555	2568		10.1109/TPAMI.2017.2769654	http://dx.doi.org/10.1109/TPAMI.2017.2769654			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GW2AF	29990150	Green Accepted			2022-12-18	WOS:000446683700003
J	Raposo, C; Antunes, M; Barreto, JP				Raposo, Carolina; Antunes, Michel; Barreto, Joao P.			Piecewise-Planar StereoScan: Sequential Structure and Motion Using Plane Primitives	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Structure and motion; piecewise-planar reconstruction; stereo image sequences; MRF	WORLD	The article describes a pipeline that receives as input a sequence of stereo images, and outputs the camera motion and a Piecewise-Planar Reconstruction (PPR) of the scene. The pipeline, named Piecewise-Planar StereoScan (PPSS), works as follows: the planes in the scene are detected for each stereo view using semi-dense depth estimation; the relative pose is computed by a new closed-form minimal algorithm that only uses point correspondences whenever plane detections do not fully constrain the motion; the camera motion and the PPR are jointly refined by alternating between discrete optimization and continuous bundle adjustment; and, finally, the detected 3D planes are segmented in images using a new framework that handles low texture and visibility issues. PPSS is extensively validated in indoor and outdoor datasets, and benchmarked against two popular point-based SfM pipelines. The experiments confirm that plane-based visual odometry is resilient to situations of small image overlap, poor texture, specularity, and perceptual aliasing where the fast LIBVISO2 [1] pipeline fails. The comparison against VisualSfM+CMVS/PMVS [2], [3] shows that, for a similar computational complexity, PPSS is more accurate and provides much more compelling and visually pleasant 3D models. These results strongly suggest that plane primitives are an advantageous alternative to point correspondences for applications of SfM and 3D reconstruction in man-made environments.	[Raposo, Carolina; Barreto, Joao P.] Univ Coimbra, Inst Syst & Robot, P-3000213 Coimbra, Portugal; [Antunes, Michel] Univ Luxembourg, Interdisciplinary Ctr Secur Reliabil & Trust SnT, L-4365 Esch Sur Alzette, Luxembourg	Universidade de Coimbra; University of Luxembourg	Raposo, C (corresponding author), Univ Coimbra, Inst Syst & Robot, P-3000213 Coimbra, Portugal.	carolinasraposo@gmail.com; michel.antunes@uni.lu; jpbar@isr.uc.pt	Raposo, Carolina/AAA-7879-2021; Barreto, Joao P/I-2845-2012	Raposo, Carolina/0000-0002-2884-5000; Barreto, Joao P/0000-0001-5220-9170; Antunes, Michel/0000-0001-6115-5186	Google, Inc; Portuguese Science Foundation (FCT) [SFRH/BD/88446/2012]; FCT; COMPETE program [AMS-HMI12: RECI/EEI-AUT/0181/2012]	Google, Inc(Google Incorporated); Portuguese Science Foundation (FCT)(Portuguese Foundation for Science and Technology); FCT(Portuguese Foundation for Science and TechnologyEuropean Commission); COMPETE program	The authors thank Google, Inc for the support through a Faculty Research Award. Carolina Raposo acknowledges the Portuguese Science Foundation (FCT) for funding her PhD under grant SFRH/BD/88446/2012. The work was also partially supported by FCT and COMPETE program under Grant AMS-HMI12: RECI/EEI-AUT/0181/2012. The authors would also like to thank Joao Marcos for his contribution in the acquisition of datasets S<INF>2</INF> and S<INF>3</INF>.	Alcantarilla P., 2013, P 5 WORKSH PLANN PER; Antunes M., 2009, INT J COMPUT VISION, V109, P187; Antunes M., 2009, IMAGE VISION COMPUT, V46, P47; Antunes M, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P230, DOI 10.1109/3DIMPVT.2012.49; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bouguet J. Y., 2008, CAMERA CALIBRATION T; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Dunn E, 2011, IEEE I CONF COMP VIS, P1187, DOI 10.1109/ICCV.2011.6126368; Fraundorfer F, 2010, LECT NOTES COMPUT SC, V6314, P269, DOI 10.1007/978-3-642-15561-1_20; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Furukawa Y, 2009, IEEE I CONF COMP VIS, P80, DOI 10.1109/ICCV.2009.5459145; Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802; Furukawa Y, 2009, PROC CVPR IEEE, P1422, DOI 10.1109/CVPRW.2009.5206867; Gallup D., 2008, 2008 IEEE C COMPUTER, P1, DOI 10.1109/CVPR.2008.4587671; Gallup D, 2010, PROC CVPR IEEE, P1418, DOI 10.1109/CVPR.2010.5539804; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7; Kazik T, 2012, PROC CVPR IEEE, P1529, DOI 10.1109/CVPR.2012.6247843; Lazic N., 2010, EXPERT SYST APPL, V9, P429; Micusik Branislav, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2906, DOI 10.1109/CVPRW.2009.5206535; NISTER D, 2004, PROC CVPR IEEE, P652, DOI DOI 10.1109/CVPR.2004.1315094; Pathak K, 2010, IEEE T ROBOT, V26, P424, DOI 10.1109/TRO.2010.2042989; Raposo C., 2013, BMVC; Raposo C, 2014, LECT NOTES COMPUT SC, V8690, P48, DOI 10.1007/978-3-319-10605-2_4; Sinha SN, 2009, IEEE I CONF COMP VIS, P1881, DOI 10.1109/ICCV.2009.5459417; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Taguchi Y, 2012, INT SYM MIX AUGMENT, P321, DOI 10.1109/ISMAR.2012.6402594; Werner T, 2002, LECT NOTES COMPUT SC, V2351, P541; Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25	34	3	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2018	40	8					1918	1931		10.1109/TPAMI.2017.2737425	http://dx.doi.org/10.1109/TPAMI.2017.2737425			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GL6DT	28796609				2022-12-18	WOS:000437271100010
J	Sosic, A; Zoubir, AM; Koeppl, H				Sosic, Adrian; Zoubir, Abdelhak M.; Koeppl, Heinz			A Bayesian Approach to Policy Recognition and State Representation Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Learning from demonstration; policy recognition; imitation learning; Bayesian nonparametric modeling; Markov chain Monte Carlo; Gibbs sampling; distance dependent Chinese restaurant process	MODEL	Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used, e.g., for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g., they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert's controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the optimality of the demonstrations. Following a Bayesian methodology, we model the full posterior distribution of possible expert controllers that explain the provided demonstration data. Moreover, we show that our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert, and to learn task-appropriate partitionings of the system state space.	[Sosic, Adrian; Zoubir, Abdelhak M.] Tech Univ Darmstadt, Signal Proc Grp, D-64283 Darmstadt, Germany; [Sosic, Adrian; Koeppl, Heinz] Tech Univ Darmstadt, Bioinspired Commun Syst Lab, D-64283 Darmstadt, Germany; [Koeppl, Heinz] Tech Univ Darmstadt, Ctr Cognit Sci, D-64283 Darmstadt, Germany	Technical University of Darmstadt; Technical University of Darmstadt; Technical University of Darmstadt	Sosic, A (corresponding author), Tech Univ Darmstadt, Signal Proc Grp, D-64283 Darmstadt, Germany.; Sosic, A (corresponding author), Tech Univ Darmstadt, Bioinspired Commun Syst Lab, D-64283 Darmstadt, Germany.	adrian.sosic@spg.tu-darmstadt.de; zoubir@spg.tu-darmstadt.de; heinz.koeppl@bcs.tu-darmstadt.de	Zoubir, Abdelhak M/AAW-6349-2021					Abbeel P., 2004, P 21 INT C MACHINE L, P1; Abbeel P, 2005, ICML ACM INT C PROCE, V119, P1, DOI 10.1145/1102351.1102352; Abbeel P, 2010, INT J ROBOT RES, V29, P1608, DOI 10.1177/0278364910371999; Aldous D.J., 1985, EXCHANGEABILITY RELA; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Atkeson C. G., 1997, P 14 INT C MACHINE L, V97, P12, DOI [10.1007/springerreference_, DOI 10.1007/978-3-7091-6874-5_4]; Atkeson CG, 1997, IEEE INT CONF ROBOT, P3557, DOI 10.1109/ROBOT.1997.606886; Blei DM, 2011, J MACH LEARN RES, V12, P2461; CHARNIAK E, 1993, ARTIF INTELL, V64, P53, DOI 10.1016/0004-3702(93)90060-O; CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568; Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218; Doshi-Velez F, 2015, IEEE T PATTERN ANAL, V37, P394, DOI 10.1109/TPAMI.2013.191; Dvijotham Krishnamurthy, 2010, P 27 INT C MACH LEAR, P335, DOI DOI 10.0RG/PAPERS/571.PDF; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Hindriks K., 2008, PROC 7 INT JOINT C A, P331, DOI [10.5555/1402383.1402433, DOI 10.5555/1402383.1402433]; Koller D., 2009, PROBABILISTIC GRAPHI; Meuleau N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P427; Michini Bernard, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P148, DOI 10.1007/978-3-642-33486-3_10; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Panella A, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1875; Parsons S.D., 2012, GAME THEORY DECISION, V5; Pietquin O., 2013, P 2 WORKSH MACH LEAR, P71, DOI 10.1145/2493525.2493529; Piot Bilal, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P17, DOI 10.1007/978-3-642-40988-2_2; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; POTTS RB, 1952, P CAMB PHILOS SOC, V48, P106, DOI 10.1017/S0305004100027419; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; Rothkopf CA, 2011, LECT NOTES ARTIF INT, V6913, P34, DOI 10.1007/978-3-642-23808-6_3; Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701; Sammut C., 1992, P 9 INT WORKSH MACH, P385; Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3; Sosic A, 2016, INT CONF ACOUST SPEE, P4801, DOI 10.1109/ICASSP.2016.7472589; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Syed U., 2007, ADV NEURAL INFORM PR; WALTZ MD, 1965, IEEE T AUTOMAT CONTR, VAC10, P390, DOI 10.1109/TAC.1965.1098193; Ziebart B. D., 2008, AAAI, V8, P1433	38	3	3	3	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2018	40	6					1295	1308		10.1109/TPAMI.2017.2711024	http://dx.doi.org/10.1109/TPAMI.2017.2711024			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GE9BK	28622668	Green Submitted			2022-12-18	WOS:000431524700002
J	Alterman, M; Schechner, YY; Swirski, Y				Alterman, Marina; Schechner, Yoav Y.; Swirski, Yohay			Triangulation in Random Refractive Distortions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Underwater; stereo; triangulation; probability; likelihood	RECONSTRUCTION; STEREO; MOTION; IMAGE; WATER; FIELD; SHAPE	Random refraction occurs in turbulence and through a wavy water-air interface. It creates distortion that changes in space, time and with viewpoint. Localizing objects in three dimensions (3D) despite this random distortion is important to some predators and also to submariners avoiding the salient use of periscopes. We take a multiview approach to this task. Refracted distortion statistics induce a probabilistic relation between any pixel location and a line of sight in space. Measurements of an object's random projection from multiple views and times lead to a likelihood function of the object's 3D location. The likelihood leads to estimates of the 3D location and its uncertainty. Furthermore, multiview images acquired simultaneously in a wide stereo baseline have uncorrelated distortions. This helps reduce the acquisition time needed for localization. The method is demonstrated in stereoscopic video sequences, both in a lab and a swimming pool.	[Alterman, Marina] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA; [Schechner, Yoav Y.; Swirski, Yohay] Technion Israel Inst Technol, Viterbi Fac Elect Engn, IL-32000 Haifa, Israel	Northwestern University; Technion Israel Institute of Technology	Alterman, M (corresponding author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.	amarinago@gmail.com; yoav@ee.technion.ac.il; syohays@gmail.com			Taub Foundation; Israel Science Foundation (ISF) [1467/12]; R. L. Kohns Eye Research Fund; BMBF	Taub Foundation; Israel Science Foundation (ISF)(Israel Science Foundation); R. L. Kohns Eye Research Fund; BMBF(Federal Ministry of Education & Research (BMBF))	We thank the anonymous reviewers for their useful comments and Joseph Shamir for useful discussions. We thank Daniel Yagodin for his help in the lab experiment, and the swimming pool facilities of the Technion for letting us conduct the experiments. We thank the authors of [19] for letting us use their image matching code. Yoav Schechner is a Landau Fellow - supported by the Taub Foundation. This research was mainly conducted while Marina Alterman was with the Department of Electrical Engineering, Technion Israel Inst. Technology. The work was supported by Israel Science Foundation (ISF) (Grant 1467/12) and the R. L. Kohns Eye Research Fund. This work was conducted in the Ollendorff Minerva Center. Minerva is funded through the BMBF.	Adato Y, 2010, IEEE T PATTERN ANAL, V32, P2054, DOI 10.1109/TPAMI.2010.126; Alterman M., 2014, P IEEE, P1; Alterman M., 2013, ICCP, P1; Alterman M, 2014, LECT NOTES COMPUT SC, V8692, P47, DOI 10.1007/978-3-319-10593-2_4; Alterman M, 2013, IEEE T PATTERN ANAL, V35, P245, DOI 10.1109/TPAMI.2012.192; Baglio S, 1998, OCEANS'98 - CONFERENCE PROCEEDINGS, VOLS 1-3, P449, DOI 10.1109/OCEANS.1998.725787; Ben-Simon A, 2012, J VISION, V12, DOI 10.1167/12.12.18; Bradler D, 2009, 2009 6TH IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, VOLS 1 AND 2, P1, DOI 10.1109/CVPR.2009.5204340; Brox T, 2011, LARGE DISPLACEMENT O; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Chang YJ, 2011, IEEE I CONF COMP VIS, P351, DOI 10.1109/ICCV.2011.6126262; Chari V., 2009, P BMVC; COX C, 1954, J MAR RES, V13, P198; Dabiri D., 2000, P INT S FLOW VIS; Dellaert F, 2000, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2000.854916; Ding YY, 2011, IEEE I CONF COMP VIS, P2478, DOI 10.1109/ICCV.2011.6126533; Dolin L. S., 2007, P SPIE, V6615; Donate A, 2007, COMM COM INF SC, V4, P264; Efros A., 2004, ADV NEURAL INFORM PR, V17, P393; Ferreira R, 2005, LECT NOTES COMPUT SC, V3522, P102; FRASER CS, 1984, PHOTOGRAMM ENG REM S, V50, P1115; Gallup D., 2008, 2008 IEEE C COMPUTER, P1, DOI 10.1109/CVPR.2008.4587671; Gershenson M, 2008, PROTON CHARGED PARTI, P1; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hong SH, 2004, OPT EXPRESS, V12, P483, DOI 10.1364/OPEX.12.000483; HORVATH G, 1991, B MATH BIOL, V53, P425, DOI 10.1016/S0092-8240(05)80396-9; Hullin MB, 2011, COMPUT GRAPH FORUM, V30, P475, DOI 10.1111/j.1467-8659.2011.01891.x; Ihrke I., 2008, EUROGRAPHICS STAR P, P87; Ikeuchi K, 2004, INT J COMPUT VISION, V58, P237, DOI 10.1023/B:VISI.0000019686.74089.5d; JAHNE B, 1994, J OPT SOC AM A, V11, P2197, DOI 10.1364/JOSAA.11.002197; Jordt-Sedlazeck A, 2012, LECT NOTES COMPUT SC, V7576, P846, DOI 10.1007/978-3-642-33715-4_61; Joshi N., 2010, P IEEE C COMP PHOT, P28; KATZIR G, 1987, J COMP PHYSIOL A, V160, P517, DOI 10.1007/BF00615085; Kidger M. J., 2001, FUNDAMENTAL OPTICAL; Kleerekoper H., 2008, CIB FDN S HEAR MECH, P187; Koppal SJ, 2011, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2011.5995338; Lipman Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602142; Loktev M, 2011, OPT LETT, V36, P2656, DOI 10.1364/OL.36.002656; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Milder DM, 2006, WAVE RANDOM COMPLEX, V16, P521, DOI 10.1080/17455030600557202; Miranda J, 2005, INT GEOSCI REMOTE SE, P2527; Miyazaki D, 2002, J OPT SOC AM A, V19, P687, DOI 10.1364/JOSAA.19.000687; Molkov AA, 2012, IZV ATMOS OCEAN PHY+, V48, P552, DOI 10.1134/S0001433812050088; Morris NJW, 2005, IEEE I CONF COMP VIS, P1573; MURASE H, 1992, IEEE T PATTERN ANAL, V14, P1045, DOI 10.1109/34.159906; Oreifej O, 2011, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2011.5995428; Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895; Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880; Saito H., 2002, P IEEE 21 INT C IND, P1231; Schechner YY, 2013, MAR TECHNOL SOC J, V47, P148; Schechner YY, 2004, OCEANS '04 MTS/IEEE TECHNO-OCEAN '04, VOLS 1- 2, CONFERENCE PROCEEDINGS, VOLS. 1-4, P1262; SCHMALZ MS, 1993, P SOC PHOTO-OPT INS, V1943, P115, DOI 10.1117/12.157138; Schultz H., 2007, US Patent, Patent No. [7,630,077, 7630077]; Schultz H., 1994, SURFACE PHOTOGRAMMET, V62, P93; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Steen J. B., 1970, P413; Swirski Y., 2013, COMP PHOT ICCP 2013, P1; Swirski Y, 2011, IEEE I CONF COMP VIS, P1124, DOI 10.1109/ICCV.2011.6126360; Swirski Y, 2011, APPL OPTICS, V50, pF89, DOI 10.1364/AO.50.000F89; Tian YD, 2012, PROC CVPR IEEE, P246, DOI 10.1109/CVPR.2012.6247682; Tian YD, 2010, PROC CVPR IEEE, P1277, DOI 10.1109/CVPR.2010.5539822; Tian YD, 2009, IEEE I CONF COMP VIS, P2303, DOI 10.1109/ICCV.2009.5459440; Treibitz T., 2008, IEEE C COMPUT VIS PA, P1, DOI [10.1109/CVPR.2008.4587844, DOI 10.1109/CVPR.2008.4587844]; Tsin Y, 2003, PROC CVPR IEEE, P702; Wen ZY, 2010, APPL OPTICS, V49, P6376, DOI 10.1364/AO.49.006376; Westaway RM, 2001, PHOTOGRAMM ENG REM S, V67, P1271; Wetzstein G, 2011, IEEE I CONF COMP VIS, P1180, DOI 10.1109/ICCV.2011.6126367; Xue TF, 2014, LECT NOTES COMPUT SC, V8691, P767, DOI 10.1007/978-3-319-10578-9_50; Zhu X, 2013, IEEE T PATTERN ANAL, V35, P157, DOI 10.1109/TPAMI.2012.82; [No title captured]	71	3	3	3	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2017	39	3					603	616		10.1109/TPAMI.2016.2551740	http://dx.doi.org/10.1109/TPAMI.2016.2551740			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EM8IP	27071162	Green Submitted			2022-12-18	WOS:000395555100014
J	Luu, K; Savvides, M; Bui, TD; Suen, CY				Luu, Khoa; Savvides, Marios; Bui, Tien D.; Suen, Ching Y.			Compressed Submanifold Multifactor Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Tensor analysis; multifactor analysis; compressed sensing; norm optimization; random projection	FACE RECOGNITION; ILLUMINATION; COMPLETION; EIGENFACES	Although widely used, Multilinear PCA (MPCA), one of the leading multilinear analysis methods, still suffers from four major drawbacks. First, it is very sensitive to outliers and noise. Second, it is unable to cope with missing values. Third, it is computationally expensive sinceMPCAdeals with large multi-dimensional datasets. Finally, it is unable to maintain the local geometrical structures due to the averaging process. This paper proposes a novel approach named Compressed Submanifold Multifactor Analysis (CSMA) to solve the four problems mentioned above. Our approach can deal with the problemof missing values and outliers via SVD-L1. The Random Projection method is used to obtain the fast low-rank approximation of a given multifactor dataset. In addition, it is able to preserve the geometry of the original data. Our CSMA method can be used efficiently for multiple purposes, e. g., noise and outlier removal, estimation of missing values, biometric applications. We show that CSMA method can achieve good results and is very efficient in the inpainting problemas compared to [1], [2]. Our method also achieves higher face recognition rates compared to LRTC, SPMA, MPCA and some other methods, i. e., PCA, LDA and LPP, on three challenging face databases, i. e., CMU-MPIE, CMU-PIE and Extended YALE-B.	[Luu, Khoa; Savvides, Marios] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA; [Luu, Khoa; Savvides, Marios] Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA; [Bui, Tien D.; Suen, Ching Y.] Concordia Univ, Montreal, PQ H3G 1M8, Canada	Carnegie Mellon University; Carnegie Mellon University; Concordia University - Canada	Luu, K (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.; Luu, K (corresponding author), Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA.	kluu@andrew.cmu.edu; msavvid@ri.cmu.edu; bui@encs.concordia.ca; suen@encs.concordia.ca	Luu, Khoa/AAQ-8540-2021	Luu, Khoa/0000-0003-2104-0901				Abate AF, 2007, PATTERN RECOGN LETT, V28, P1885, DOI 10.1016/j.patrec.2006.12.018; Alex M, 2002, LECT NOTES COMPUT SC, V2350, P447; Bartkowiak A, 2010, COMP INF SYST IND MA, V1-6, DOI [10.1109/CISIM.2010.5643699, DOI 10.1109/CISIM.2010.5643699, 10.1109/cisim.2010.5643699]; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Bendapudi S., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P311, DOI 10.1109/BTAS.2012.6374594; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Chen YL, 2014, IEEE T PATTERN ANAL, V36, P577, DOI 10.1109/TPAMI.2013.164; Dasgupta S., 1999, TR99006 INT COMP SCI; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Donoho D., 2006, FAST SOLUTION L 1 NO; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Gower J. C., 2004, PROCRUSTES PROBLEMS; Gross R., 2008, P IEEE INT C AUT FAC, P1; He X., 2003, P ADV NEUR INF PROC, P1413; Inoue K, 2009, IEEE I CONF COMP VIS, P591, DOI 10.1109/ICCV.2009.5459186; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Ke Q., 2005, P IEEE C COMP VIS PA, P592; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kwak N, 2008, IEEE T PATTERN ANAL, V30, P1672, DOI 10.1109/TPAMI.2008.114; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Liu J, 2009, IEEE I CONF COMP VIS, P2114; Liu J, 2010, IEEE T NEURAL NETWOR, V21, P621, DOI 10.1109/TNN.2010.2040290; Lu HP, 2008, IEEE T NEURAL NETWOR, V19, P18, DOI 10.1109/TNN.2007.901277; Lu HP, 2011, PATTERN RECOGN, V44, P1540, DOI 10.1016/j.patcog.2011.01.004; Luu K, 2012, INT C PATT RECOG, P2715; Pang YW, 2010, IEEE T CIRC SYST VID, V20, P172, DOI 10.1109/TCSVT.2009.2020337; Park S., 2011, THESIS; Park SW, 2010, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2010.5539980; Pizarro D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2409, DOI 10.1109/CVPR.2011.5995677; Prabhu U, 2011, IEEE T PATTERN ANAL, V33, P1952, DOI 10.1109/TPAMI.2011.123; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; Smeets D, 2012, IEEE T SYST MAN CY C, V42, P710, DOI 10.1109/TSMCC.2011.2174221; Strelow D, 2012, PROC CVPR IEEE, P1584, DOI 10.1109/CVPR.2012.6247850; Sung Won Park, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2817, DOI 10.1109/CVPR.2011.5995397; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Vasilescu MAO, 2005, PROC CVPR IEEE, P547; Vasilescu MAO, 2002, INT C PATT RECOG, P511, DOI 10.1109/ICPR.2002.1048350; Vempala Santosh S., 2004, RANDOM PROJECTION ME; Wexler Y, 2004, PROC CVPR IEEE, P120; XU J, 2011, PROC IEEE INT JOINT, P1; Ye J., 2005, MACH LEARN, V61; YUAN X, 2009, PACIFIC J OPTIM, V9, P1; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828	49	3	3	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2017	39	3					444	456		10.1109/TPAMI.2016.2554107	http://dx.doi.org/10.1109/TPAMI.2016.2554107			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EM8IP	27101597				2022-12-18	WOS:000395555100003
J	Lu, N; Miao, HY				Lu, Na; Miao, Hongyu			Clustering Tree-Structured Data on Manifold	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Clustering; geodesic; tree-structured data; nonnegative matrix factorization	COMPUTING GEODESIC DISTANCES; PRINCIPAL COMPONENT ANALYSIS; ALGORITHM; PATTERNS; VESSELS; IMAGES; SPACE	Tree-structured data usually contain both topological and geometrical information, and are necessarily considered on manifold instead of euclidean space for appropriate data parameterization and analysis. In this study, we propose a novel tree-structured data parameterization, called Topology-Attribute matrix (T-A matrix), so the data clustering task can be conducted on matrix manifold. We incorporate the structure constraints embedded in data into the non-negative matrix factorization method to determine meta-trees from the T-A matrix, and the signature vector of each single tree can then be extracted by meta-tree decomposition. The meta-tree space turns out to be a cone space, in which we explore the distance metric and implement the clustering algorithm based on the concepts like Frechet mean. Finally, the T-A matrix based clustering (TAMBAC) framework is evaluated and compared using both simulated data and real retinal images to illustrate its efficiency and accuracy.	[Lu, Na] Xi An Jiao Tong Univ, Syst Engn Inst, State Key Lab Mfg Syst Engn, Xian 710049, Shaanxi, Peoples R China; [Miao, Hongyu] Univ Texas Houston, Hlth Sci Ctr, Sch Publ Hlth, Dept Biostat, Houston, TX 77030 USA	Xi'an Jiaotong University; University of Texas System; University of Texas Health Science Center Houston; University of Texas School Public Health	Lu, N (corresponding author), Xi An Jiao Tong Univ, Syst Engn Inst, State Key Lab Mfg Syst Engn, Xian 710049, Shaanxi, Peoples R China.	lvna2009@mail.xjtu.edu.cn; Hongyu.Miao@uth.tmc.edu			Fundamental Research Funds for the Central Universities; NIH [HHSN272201000055C]; National Natural Foundation of China [61105034]; China Postdoctoral Science Foundation grant [20110491662, 2012T50805]	Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Natural Foundation of China(National Natural Science Foundation of China (NSFC)); China Postdoctoral Science Foundation grant(China Postdoctoral Science Foundation)	This work is supported by Fundamental Research Funds for the Central Universities, NIH grant HHSN272201000055C, the National Natural Foundation of China grant 61105034, China Postdoctoral Science Foundation grant 20110491662, 2012T50805.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Allen B. L., 2001, ANN COMB, V5, P1, DOI DOI 10.1007/S00026-001-8006-8; Amenta N, 2007, INFORM PROCESS LETT, V103, P61, DOI 10.1016/j.ipl.2007.02.008; Aydin B, 2009, ANN APPL STAT, V3, P1597, DOI 10.1214/09-AOAS263; BARTHELEMY JP, 1986, J CLASSIF, V3, P329, DOI 10.1007/BF01894194; Billera LJ, 2001, ADV APPL MATH, V27, P733, DOI 10.1006/aama.2001.0759; Carmeliet P, 2005, NATURE, V438, P932, DOI 10.1038/nature04478; Collins M, 2002, ADV NEUR IN, V14, P625; DAY WHE, 1985, J CLASSIF, V2, P7, DOI 10.1007/BF01908061; Dengler N, 2001, CURR OPIN PLANT BIOL, V4, P50, DOI 10.1016/S1369-5266(00)00135-7; Donoho D., 2004, ADV NEURAL INFORM PR, V16; Feragen A., 2011, 2011 IEEE International Conference on Computer Vision (ICCV 2011), P736, DOI 10.1109/ICCV.2011.6126311; Feragen A., 2011, 10 AS C COMP VIS Q 2; Feragen A, 2013, IEEE T PATTERN ANAL, V35, P2008, DOI 10.1109/TPAMI.2012.265; Feragen A, 2011, LECT NOTES COMPUT SC, V6493, P160; Gross J., 1998, GRAPH THEORY ITS APP; Hamarneh G, 2010, COMPUT MED IMAG GRAP, V34, P605, DOI 10.1016/j.compmedimag.2010.06.002; HEIN J, 1990, MATH BIOSCI, V98, P185, DOI 10.1016/0025-5564(90)90123-G; Herbert SP, 2011, NAT REV MOL CELL BIO, V12, P551, DOI 10.1038/nrm3176; Holder MT, 2008, SYST BIOL, V57, P814, DOI 10.1080/10635150802422308; Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178; Howard S. G., 2003, ACM SIGOPS OPERATING, P29, DOI [10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]; Hsia CCW, 2010, AM J RESP CRIT CARE, V181, P394, DOI 10.1164/rccm.200809-1522ST; Huang LG, 2007, J MATH ANAL APPL, V332, P1468, DOI 10.1016/j.jmaa.2005.03.087; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee DD, 2001, ADV NEUR IN, V13, P556; LEE TC, 1994, CVGIP-GRAPH MODEL IM, V56, P462, DOI 10.1006/cgip.1994.1042; LESS JR, 1991, CANCER RES, V51, P265; Li JS, 1997, LINEAR ALGEBRA APPL, V265, P93; Li L, 2013, CANCER RES, V73, P2082, DOI 10.1158/0008-5472.CAN-12-4200; Liu K, 2009, SCIENCE, V324, P392, DOI 10.1126/science.1170540; Locantore N, 1999, TEST, V8, P1, DOI 10.1007/BF02595862; Lu N, 2013, PATTERN RECOGN, V46, P1933, DOI 10.1016/j.patcog.2013.01.011; Lui YM, 2012, IMAGE VISION COMPUT, V30, P380, DOI 10.1016/j.imavis.2011.08.002; MANN RE, 1985, DRUG ALCOHOL DEPEN, V15, P61, DOI 10.1016/0376-8716(85)90030-4; Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156; McMullen C., 2015, AM J MATH IN PRESS; Moschitti A., 2006, P 11 C EUR CHAPT ASS, P113; Nieman K, 2002, CIRCULATION, V106, P2051, DOI 10.1161/01.CIR.0000037222.58317.3D; Nye TMW, 2011, ANN STAT, V39, P2716, DOI 10.1214/11-AOS915; Owen M, 2011, SIAM J DISCRETE MATH, V25, P1506, DOI 10.1137/090751396; Owen M, 2011, IEEE ACM T COMPUT BI, V8, P2, DOI 10.1109/TCBB.2010.3; Rieck K, 2010, J MACH LEARN RES, V11, P555; Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X; Risser L, 2008, IEEE T MED IMAGING, V27, P674, DOI 10.1109/TMI.2007.913248; ROBINSON DF, 1981, MATH BIOSCI, V53, P131, DOI 10.1016/0025-5564(81)90043-2; Robinson DF., 1971, J COMBIN THEORY B, V11, P105, DOI DOI 10.1016/0095-8956(71)90020-7; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shin K, 2011, P 28 INT C MACH LEAR, P961; Shirinifard A, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0007190; Torsello A, 2005, IEEE T PATTERN ANAL, V27, P1087, DOI 10.1109/TPAMI.2005.146; Tyrrell JA, 2007, IEEE T MED IMAGING, V26, P223, DOI 10.1109/TMI.2006.889722; Wang H, 2007, ANN STAT, V35, P1849, DOI 10.1214/009053607000000217; Wang Y, 2012, J AM STAT ASSOC, V107, P1272, DOI 10.1080/01621459.2012.699348; Welter M, 2008, J THEOR BIOL, V250, P257, DOI 10.1016/j.jtbi.2007.09.031; Yim PJ, 2000, IEEE T MED IMAGING, V19, P568, DOI 10.1109/42.870662	57	3	3	1	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2016	38	10					1956	1968		10.1109/TPAMI.2015.2505282	http://dx.doi.org/10.1109/TPAMI.2015.2505282			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DX2YV	26660696	Green Accepted, Green Submitted			2022-12-18	WOS:000384240600003
J	Seshadri, K; Savvides, M				Seshadri, Keshav; Savvides, Marios			Towards a Unified Framework for Pose, Expression, and Occlusion Tolerant Automatic Facial Alignment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Facial alignment; automatic facial landmark localization; l(1)-regularized least squares; active shape models (ASMs)	ACTIVE APPEARANCE MODEL; STATISTICAL VARIABLES; SHAPE MODELS; RECOGNITION; COMPLEX	We propose a facial alignment algorithm that is able to jointly deal with the presence of facial pose variation, partial occlusion of the face, and varying illumination and expressions. Our approach proceeds from sparse to dense landmarking steps using a set of specific models trained to best account for the shape and texture variation manifested by facial landmarks and facial shapes across pose and various expressions. We also propose the use of a novel l(1)-regularized least squares approach that we incorporate into our shape model, which is an improvement over the shape model used by several prior Active Shape Model (ASM) based facial landmark localization algorithms. Our approach is compared against several state-of-the-art methods on many challenging test datasets and exhibits a higher fitting accuracy on all of them.	[Seshadri, Keshav; Savvides, Marios] Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA; [Seshadri, Keshav; Savvides, Marios] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Seshadri, K (corresponding author), Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA.; Seshadri, K (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.	kseshadr@andrew.cmu.edu; msavvid@ri.cmu.edu			Federal Highway Administration's Exploratory Advanced Research Program [DTFH61-14-C-00006]	Federal Highway Administration's Exploratory Advanced Research Program	This work was supported by the Federal Highway Administration's Exploratory Advanced Research Program under contract DTFH61-14-C-00006. The authors would also like to thank Dr. Craig Thor for his support and feedback on this work.	Abiantun R., 2011, P IEEE WORKSH APPL C, P212; [Anonymous], 2013, FACIAL POINT ANNOTAT; [Anonymous], 2013, INTELLIGENT BEHAV UN; [Anonymous], 2013, 300 FACES IN THE WIL; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015; Choi HC, 2006, IEEE SYS MAN CYBERN, P1559, DOI 10.1109/ICSMC.2006.384939; Cootes T. F., 2004, IMAG SCI BIOMED ENG; Cootes T. F., 1994, P BRIT VIS C BMVA PR; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cristinacce D., 2006, P BRIT MACH VIS C, V3, P929; Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976; Dollar P., 2005, PIOTRS COMPUTER VISI; Edwards G. J., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P581, DOI 10.1007/BFb0054766; Faggian N, 2006, INT C PATT RECOG, P287; Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306; GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; Gross R., 2008, P IEEE INT C AUT FAC, P1; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Heo J, 2012, IEEE T INF FOREN SEC, V7, P563, DOI 10.1109/TIFS.2012.2184755; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Huang Gary B., 2007, 0749 U MASS, P7; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Kim CM, 2007, TISSUE ENG REGEN MED, V4, P60; Kim S., 2007, L1 LS SIMPLE MATLAB; Le T. H. N., 2012, P IEEE INT C COMM SY, P1; Le T. H. N., 2011, P IEEE 19 INT C IM P, P165; Luu Khoa, 2011, BIOM IJCB 2011 INT J, P1, DOI DOI 10.1109/IJCB.2011.6117601; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Rudovic Ognjen, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4121, DOI 10.1109/ICPR.2010.1001; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Seshadri K, 2012, IEEE T INF FOREN SEC, V7, P1255, DOI 10.1109/TIFS.2012.2195175; Smith BM, 2014, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2014.225; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tamersoy B, 2013, IEEE COMPUT SOC CONF, P838, DOI 10.1109/CVPRW.2013.160; Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126; Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244; Zhao XW, 2013, IEEE I CONF COMP VIS, P1033, DOI 10.1109/ICCV.2013.132; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zisserman, 2006, P 17 BRIT MACH VIS C, V2, P6	56	3	3	0	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2016	38	10					2110	2122		10.1109/TPAMI.2015.2505301	http://dx.doi.org/10.1109/TPAMI.2015.2505301			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DX2YV	26660702				2022-12-18	WOS:000384240600014
J	Jiang, H; Tian, TP; Sclaroff, S				Jiang, Hao; Tian, Tai-Peng; Sclaroff, Stan			Scale and Rotation Invariant Matching Using Linearly Augmented Trees	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Object matching; scale and rotation invariance; high-order model; linearly augmented tree; linear optimization; decomposition method	GRAPH; ALGORITHM	We propose a novel linearly augmented tree method for efficient scale and rotation invariant object matching. The proposed method enforces pairwise matching consistency defined on trees, and high-order constraints on all the sites of a template. The pairwise constraints admit arbitrary metrics while the high-order constraints use L1 norms and therefore can be linearized. Such a linearly augmented tree formulation introduces hyperedges and loops into the basic tree structure. But, different from a general loopy graph, its special structure allows us to relax and decompose the optimization into a sequence of tree matching problems that are efficiently solvable by dynamic programming. The proposed method also works on continuous scale and rotation parameters; we can match with a scale up to any large value with the same efficiency. Our experiments on ground truth data and a variety of real images and videos show that the proposed method is efficient, accurate and reliable.	[Jiang, Hao] Boston Coll, Dept Comp Sci, Chestnut Hill, MA 02167 USA; [Tian, Tai-Peng] GE Global Res, Niskayuna, NY USA; [Sclaroff, Stan] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA	Boston College; General Electric; Boston University	Jiang, H (corresponding author), Boston Coll, Dept Comp Sci, Chestnut Hill, MA 02167 USA.				US National Science Foundation [0713168, 0910908, 1029430, 1018641]; Direct For Computer & Info Scie & Enginr [1029430] Funding Source: National Science Foundation	US National Science Foundation(National Science Foundation (NSF)); Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work was supported in part through US National Science Foundation grants 0713168, 0910908, 1029430 and 1018641. H. Jiang is the corresponding author.	Albarelli A, 2012, INT J COMPUT VISION, V97, P36, DOI 10.1007/s11263-011-0432-4; ALMOHAMAD HA, 1993, IEEE T PATTERN ANAL, V15, P522, DOI 10.1109/34.211474; [Anonymous], 2011, GRAPHM GRAPH MATCHIN; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Berg AC, 2005, PROC CVPR IEEE, P26; Bradley S. P., 1977, APPL MATH PROGRAMMIN; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Chvatal V., 1983, LINEAR PROGRAMMING; Cour Timothee, 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0044; DANTZIG GB, 1960, OPER RES, V8, P101, DOI 10.1287/opre.8.1.101; Dawande MW, 2000, OPER RES, V48, P623, DOI 10.1287/opre.48.4.623.12420; Duchenne O., 2009, IEEE C COMP VIS PATT, P2383; DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Felzenszwalb PF, 2000, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.2000.854739; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Hao Jiang, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2474, DOI 10.1109/CVPRW.2009.5206776; Hedau V., 2008, IEEE C COMP VIS PATT, P1; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N, 2007, IEEE T PATTERN ANAL, V29, P1436, DOI 10.1109/TPAMI.2007.1061; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu Marius, 2009, ADV NEURAL INFORM PR; Li HS, 2013, IEEE T PATTERN ANAL, V35, P411, DOI 10.1109/TPAMI.2012.99; Li HS, 2010, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2010.5539776; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151; Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763; Singh R, 2007, LECT NOTES COMPUT SC, V4453, P16; Szummer M, 2008, LECT NOTES COMPUT SC, V5303, P582, DOI 10.1007/978-3-540-88688-4_43; Taylor CJ, 2008, LECT NOTES COMPUT SC, V5305, P638, DOI 10.1007/978-3-540-88693-8_47; Todorovic S, 2005, IEEE T PATTERN ANAL, V27, P1762, DOI 10.1109/TPAMI.2005.219; Torresani Lorenzo, 2008, EUR C COMP VIS; UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zass R., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587500; Zhou F, 2013, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2013.376	42	3	4	0	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2015	37	12					2558	2572		10.1109/TPAMI.2015.2409880	http://dx.doi.org/10.1109/TPAMI.2015.2409880			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CW2OK	26539858				2022-12-18	WOS:000364831700016
J	Gould, S				Gould, Stephen			Learning Weighted Lower Linear Envelope Potentials in Binary Markov Random Fields	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Higher-order MRFs; lower linear envelope potentials; max-margin learning	ENERGY MINIMIZATION; GRAPH CUTS	Markov random fields containing higher-order terms are becoming increasingly popular due to their ability to capture complicated relationships as soft constraints involving many output random variables. In computer vision an important class of constraints encode a preference for label consistency over large sets of pixels and can be modeled using higher-order terms known as lower linear envelope potentials. In this paper we develop an algorithm for learning the parameters of binary Markov random fields with weighted lower linear envelope potentials. We first show how to perform exact energy minimization on these models in time polynomial in the number of variables and number of linear envelope functions. Then, with tractable inference in hand, we show how the parameters of the lower linear envelope potentials can be estimated from labeled training data within a max-margin learning framework. We explore three variants of the lower linear envelope parameterization and demonstrate results on both synthetic and real-world problems.	Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia	Australian National University	Gould, S (corresponding author), Australian Natl Univ, Res Sch Comp Sci, GPO Box 4, Canberra, ACT 0200, Australia.	stephen.gould@anu.edu.au			Australian Research Council's Discovery Projects funding scheme [DP110103819]	Australian Research Council's Discovery Projects funding scheme(Australian Research Council)	This research was supported under Australian Research Council's Discovery Projects funding scheme (project number DP110103819).	Bertsekas D. P., 2004, NONLINEAR PROGRAMMIN; Borenstein E, 2002, LECT NOTES COMPUT SC, V2351, P109; Borenstein E., 2004, P IEEE WORKSH PERC O, P46; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; BOYKOV Y, 1999, P INT C COMP VIS; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Everingham M., 2010, PASCAL VISUAL OBJECT; Finley T., 2008, INT C MACHINE LEARNI, P304, DOI DOI 10.1145/1390156.1390195; Freedman D, 2005, PROC CVPR IEEE, P939; Gould Stephen, 2011, P INT C MACH LEARN, P193; HAMMER PL, 1965, OPER RES, V13, P388; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Ishikawa H, 2009, PROC CVPR IEEE, P2985; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kohli P., 2008, TR08 OXF BROOK U MIC; Kohli P., 2007, P IEEE C COMP VIS PA, P1; Kohli P, 2007, IEEE T PATTERN ANAL, V29, P2079, DOI 10.1109/TPAMI.2007.1128; Kohli P, 2010, PROC CVPR IEEE, P1863, DOI 10.1109/CVPR.2010.5539858; Koller D., 2009, PROBABILISTIC GRAPHI; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025; Komodakis N., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1841, DOI 10.1109/CVPR.2011.5995375; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Nowozin S, 2009, PROC CVPR IEEE, P818, DOI 10.1109/CVPRW.2009.5206567; Orlin JB, 2009, MATH PROGRAM, V118, P237, DOI 10.1007/s10107-007-0189-2; Park K, 2012, LECT NOTES COMPUT SC, V7573, P202, DOI 10.1007/978-3-642-33709-3_15; Pletscher P., 2012, P INT C ART INT STAT, P886; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rother C, 2009, PROC CVPR IEEE, P1382, DOI 10.1109/CVPRW.2009.5206739; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Szummer M, 2008, LECT NOTES COMPUT SC, V5303, P582, DOI 10.1007/978-3-540-88688-4_43; Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tsochantaridis I., 2004, P 21 INT C MACH LEAR, P104, DOI DOI 10.1145/1015330.1015341; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]	39	3	4	1	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2015	37	7					1336	1346		10.1109/TPAMI.2014.2366760	http://dx.doi.org/10.1109/TPAMI.2014.2366760			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CK0YG	26352443	Green Submitted			2022-12-18	WOS:000355931100003
J	Werner, T				Werner, Tomas			Marginal Consistency: Upper-Bounding Partition Functions over Commutative Semirings	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Partition function; commutative semiring; graphical model; Markov random field; linear programming relaxation; message passing; max-sum diffusion; soft constraint satisfaction; local consistency; constraint propagation	CONSTRAINT SATISFACTION; ENERGY MINIMIZATION; ALGORITHMS	Many inference tasks in pattern recognition and artificial intelligence lead to partition functions in which addition and multiplication are abstract binary operations forming a commutative semiring. By generalizing max-sum diffusion (one of convergent message passing algorithms for approximate MAP inference in graphical models), we propose an iterative algorithm to upper bound such partition functions over commutative semirings. The iteration of the algorithm is remarkably simple: change any two factors of the partition function such that their product remains the same and their overlapping marginals become equal. In many commutative semirings, repeating this iteration for different pairs of factors converges to a fixed point when the overlapping marginals of every pair of factors coincide. We call this state marginal consistency. During that, an upper bound on the partition function monotonically decreases. This abstract algorithm unifies several existing algorithms, including max-sum diffusion and basic costraint propagation (or local consistency) algorithms in constraint programming. We further construct a hierarchy of marginal consistencies of increasingly higher levels and show than any such level can be enforced by adding identity factors of higher arity (order). Finally, we discuss instances of the framework for several semirings, including the distributive lattice and the max-sum and sum-product semirings.	Czech Tech Univ, Dept Cybernet, Prague 12135, Czech Republic	Czech Technical University Prague	Werner, T (corresponding author), Czech Tech Univ, Dept Cybernet, Karlovo Namesti 13, Prague 12135, Czech Republic.	werner@cmp.felk.cvut.cz			Czech Science Foundation [P202/12/2071]; European Commission [FP7-ICT-270138]	Czech Science Foundation(Grant Agency of the Czech Republic); European Commission(European CommissionEuropean Commission Joint Research Centre)	The author was supported by the Czech Science Foundation under project P202/12/2071 and by the European Commission under project FP7-ICT-270138.	Aji SM, 2000, IEEE T INFORM THEORY, V46, P325, DOI 10.1109/18.825794; [Anonymous], 1990, INTRO LATTICES ORDER; Apt KR, 1999, LECT NOTES COMPUT SC, V1713, P1; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bessiere C, 2006, FOUND ARTIF INTELL, P29; Bistarelli S, 1997, J ACM, V44, P201, DOI 10.1145/256303.256306; Bistarelli S, 2004, LECT NOTES COMPUT SC, V2962, P1; Bistarelli S., 1999, Constraints, V4, P199, DOI 10.1023/A:1026441215081; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Bulatov AA, 2007, INFORM COMPUT, V205, P651, DOI 10.1016/j.ic.2006.09.005; Chekuri C, 2005, SIAM J DISCRETE MATH, V18, P608, DOI 10.1137/S0895480101396937; Cohen David, 2006, HDB CONSTRAINT PROGR; Cooper M, 2004, ARTIF INTELL, V154, P199, DOI 10.1016/j.artint.2003.09.002; Cooper MC, 2010, ARTIF INTELL, V174, P449, DOI 10.1016/j.artint.2010.02.001; Debruyne R, 2001, J ARTIF INTELL RES, V14, P205, DOI 10.1613/jair.834; DECHTER R, 2003, P 19 C UNC ART INT U, P175; DUBOIS D, 1993, SECOND IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOLS 1 AND 2, P1131, DOI 10.1109/FUZZY.1993.327356; Eisner J, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P1; Franc V, 2012, OPTIMIZATION FOR MACHINE LEARNING, P185; Freuder E., 2006, HDB CONSTRAINT PROGR; FREUDER EC, 1978, COMMUN ACM, V21, P958, DOI 10.1145/359642.359654; GAUBERT S, 1997, 3088 INRIA; Globerson Amir, 2008, ADV NEURAL INFORM PR, P553; Golan J.S., 2013, SEMIRINGS AFFINE EQU; Golan J. S., 1999, SEMIRINGS THEIR APPL, P381; Gondran M, 2008, OPER RES COMPUT SCI, V41, P1; HARALICK RM, 1979, IEEE T PATTERN ANAL, V1, P173, DOI 10.1109/TPAMI.1979.4766903; Haralick RM., 1992, COMPUTER ROBOT VISIO; Hazan T., 2008, UAI, P264; Heskes T, 2006, J ARTIF INTELL RES, V26, P153, DOI 10.1613/jair.1933; Janssen P., 1989, IEEE International Workshop on Tools for Artificial Intelligence. Architectures, Languages and Algorithms, P420, DOI 10.1109/TAI.1989.65349; Johnson J., 2007, P ALL C COMM CONTR C; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Klement E., 2000, TRIANGULAR NORMS; Kohlas J, 2008, ARTIF INTELL, V172, P1360, DOI 10.1016/j.artint.2008.03.003; Kohlas J., 2003, INFORM ALGEBRAS GENE; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N, 2008, LECT NOTES COMPUT SC, V5304, P806, DOI 10.1007/978-3-540-88690-7_60; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Koster AMCA, 1998, OPER RES LETT, V23, P89, DOI 10.1016/S0167-6377(98)00043-1; KOVALEVSKY VA, 1975, DIFFUSION ALGO UNPUB; Kumar M.P., 2008, P 25 INT C MACH LEAR, P680; Li Z., 2009, P 2009 C EMP METH NA, V1, P40; Litvinov G.L., 2007, J MATH SCI-U TOKYO, V140, P426, DOI [10.1007/s10958-007-0450-5, DOI 10.1007/S10958-007-0450-5, DOI 10.1007/s10958-007-0450-5]; MACKWORTH A, 1991, ENCY ARTIFICIAL INTE, P285; MACKWORTH AK, 1977, ARTIF INTELL, V8, P99, DOI 10.1016/0004-3702(77)90007-8; Meltzer T., 2009, P UAI, P393; Meseguer P, 2006, FOUND ARTIF INTELL, P281; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; MONTANAR.U, 1974, INFORM SCIENCES, V7, P95, DOI 10.1016/0020-0255(74)90008-5; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pouly M., 2011, GENERIC INFERENCE UN; Ravikumar P, 2010, J MACH LEARN RES, V11, P1043; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; Rossi F, 2006, FOUND ARTIF INTELL, P1; Schiex T, 1995, INT JOINT CONF ARTIF, P631; Schlesinger M., 2000, CZECH PATT REC WORKS, V2000, P55; Schlesinger MI, 2011, CYBERN SYST ANAL+, V47, P175, DOI 10.1007/s10559-011-9300-z; Schlesinger M. I., 1989, MATEMATICHESKIE SRED; Shenoy P. P., 1990, P 6 WORKSH UNC ART I, P169; Shlezinger M. I., 1976, Cybernetics, V12, P612, DOI 10.1007/BF01070399; Sontag D., 2010, THESIS MIT CAMBRIDGE; Sontag D, 2007, ADV NEURAL INFORM PR, P1393; Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Thapper J, 2012, ANN IEEE SYMP FOUND, P669, DOI 10.1109/FOCS.2012.25; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; WALTZ D, 1972, AI271 MIT; Weiss Y., 2008, P UAI, P503; Weiss Y., 2007, PROC C UNCERTAINTY A, P416; Werner T., 2011, CONTROL SYST COMPUT, V2, P35; Werner Toma, 2008, INT WORKSH PREF SOFT, P43; Werner Toma, 2007, 12 COMP VIS WINT WOR, P27; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Werner T, 2010, IEEE T PATTERN ANAL, V32, P1474, DOI 10.1109/TPAMI.2009.134; Zivny S., 2012, COMPLEXITY VALUED CO	76	3	3	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2015	37	7					1455	1468		10.1109/TPAMI.2014.2363833	http://dx.doi.org/10.1109/TPAMI.2014.2363833			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CK0YG	26352452				2022-12-18	WOS:000355931100012
J	Wu, TF; Zhu, SC				Wu, Tianfu; Zhu, Song-Chun			Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Decision policy; cost-sensitive computing; risk minimization; dynamic programming; object detection		Many popular object detectors, such as AdaBoost, SVM and deformable part-based models (DPM), compute additive scoring functions at a large number of windows in an image pyramid, thus computational efficiency is an important consideration in real time applications besides accuracy. In this paper, a decision policy refers to a sequence of two-sided thresholds to execute early reject and early accept based on the cumulative scores at each step. We formulate an empirical risk function as the weighted sum of the cost of computation and the loss of false alarm and missing detection. Then a policy is said to be cost-sensitive and optimal if it minimizes the risk function. While the risk function is complex due to high-order correlations among the two-sided thresholds, we find that its upper bound can be optimized by dynamic programming efficiently. We show that the upper bound is very tight empirically and thus the resulting policy is said to be near-optimal. In experiments, we show that the decision policy outperforms state-of-the-art cascade methods significantly, in several popular detection tasks and benchmarks, in terms of computational efficiency with similar accuracy of detection.	[Wu, Tianfu] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Zhu, Song-Chun] Univ Calif Los Angeles, Dept Stat & Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles	Wu, TF (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.	tfwu@stat.ucla.edu; sczhu@stat.ucla.edu		Wu, Tianfu/0000-0001-8911-5506	Defense Advanced Research Projects Agency (DARPA) MSEE [FA 8650-11-1-7149]; US National Science Foundation (NSF) [IIS1018751]; MURI grant ONR [N00014-10-1-0933]	Defense Advanced Research Projects Agency (DARPA) MSEE(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); US National Science Foundation (NSF)(National Science Foundation (NSF)); MURI grant ONR	This work was supported by Defense Advanced Research Projects Agency (DARPA) MSEE grant FA 8650-11-1-7149, US National Science Foundation (NSF) IIS1018751 and MURI grant ONR N00014-10-1-0933. The authors thank Dr. Ying Nian Wu for helpful discussions.	Amer MR, 2012, LECT NOTES COMPUT SC, V7575, P187, DOI 10.1007/978-3-642-33765-9_14; Amit Y, 2004, IEEE T PATTERN ANAL, V26, P1606, DOI 10.1109/TPAMI.2004.111; Barbu A., 2013, MULTIPATH MARGINAL S; Blanchard G, 2005, ANN STAT, V33, P1155, DOI 10.1214/009053605000000174; Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310; Brubaker SC, 2008, INT J COMPUT VISION, V77, P65, DOI 10.1007/s11263-007-0060-1; Chen M., 2012, P INT C ART INT STAT, P218; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Elkan C., 2001, INT JOINT C ART INT, P973; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2007, J ARTIF INTELL RES, V29, P153, DOI 10.1613/jair.2187; Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906; Gangaputra S., 2006, P IEEE C COMP VIS PA, V2, P1877, DOI 10.1109/CVPR.2006.21; Gao T., 2011, NEURIPS 2011, P1062; Girshick RB, 2012, DISCRIMINATIVELY TRA; Grubb Alexander, 2012, AISTATS, P458; Karayev S, 2014, PROC CVPR IEEE, P572, DOI 10.1109/CVPR.2014.80; Kokkinos I., 2011, ADV NEURAL INFORM PR, VVol. 24, P2681; Kokkinos I, 2011, INT J COMPUT VISION, V93, P201, DOI 10.1007/s11263-010-0398-7; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Lefakis L, 2010, ADV NEURAL INFORM PR, P1315; Masnadi-Shirazi H., 2010, ICML, P759; Masnadi-Shirazi H, 2011, IEEE T PATTERN ANAL, V33, P294, DOI 10.1109/TPAMI.2010.71; Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009; Pele O, 2008, IEEE T PATTERN ANAL, V30, P1427, DOI 10.1109/TPAMI.2007.70794; Pirsiavash H, 2012, PROC CVPR IEEE, P3226, DOI 10.1109/CVPR.2012.6248058; Poczos B., 2009, P INT C MACH LEARN, P104; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Saberian MJ, 2012, IEEE T PATTERN ANAL, V34, P2005, DOI 10.1109/TPAMI.2011.281; Sahbi H, 2006, J MACH LEARN RES, V7, P2087; Schneiderman H, 2004, PROC CVPR IEEE, P29; SEIGMUND DO, 1985, SEQUENTIAL ANAL TEST; Sochman J, 2005, PROC CVPR IEEE, P150; Song X, 2013, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR.2013.421; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; Ting Kai Ming, 2000, P 17 INT C MACH LEAR, P983; ULLMAN S, 1984, COGNITION, V18, P97, DOI 10.1016/0010-0277(84)90023-4; Vapnik V.N, 1998, STAT LEARNING THEORY; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wald A., 1947, SEQUENTIAL ANAL; Wu JX, 2008, IEEE T PATTERN ANAL, V30, P369, DOI 10.1109/TPAMI.2007.1181; Wu TF, 2013, IEEE I CONF COMP VIS, P753, DOI 10.1109/ICCV.2013.98; Wu TF, 2011, INT J COMPUT VISION, V93, P226, DOI 10.1007/s11263-010-0346-6; Xiao R, 2007, IEEE I CONF COMP VIS, P1679; Yan JJ, 2014, PROC CVPR IEEE, P2497, DOI 10.1109/CVPR.2014.320; Zhu ML, 2014, LECT NOTES COMPUT SC, V8695, P281, DOI 10.1007/978-3-319-10584-0_19; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	48	3	4	0	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2015	37	5					1013	1027		10.1109/TPAMI.2014.2359653	http://dx.doi.org/10.1109/TPAMI.2014.2359653			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CF4PS	26353325				2022-12-18	WOS:000352533000009
J	Barbu, A				Barbu, Adrian			Hierarchical Object Parsing from Structured Noisy Point Clouds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Object parsing; hierarchical models; markov random field optimization; active shape model	SEGMENTATION	Object parsing and segmentation from point clouds are challenging tasks because the relevant data is available only as thin structures along object boundaries or other features, and is corrupted by large amounts of noise. To handle this kind of data, flexible shape models are desired that can accurately follow the object boundaries. Popular models such as active shape and active appearance models (AAMs) lack the necessary flexibility for this task, while recent approaches such as the recursive compositional models make model simplifications to obtain computational guarantees. This paper investigates a hierarchical Bayesian model of shape and appearance in a generative setting. The input data is explained by an object parsing layer which is a deformation of a hidden principal component analysis (PCA) shape model with Gaussian prior. The paper also introduces a novel efficient inference algorithm that uses informed data-driven proposals to initialize local searches for the hidden variables. Applied to the problem of object parsing from structured point clouds such as edge detection images, the proposed approach obtains state-of-the-art parsing errors on two standard datasets without using any intensity information.	Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA	State University System of Florida; Florida State University	Barbu, A (corresponding author), Florida State Univ, Dept Stat, 820 Concord Rd, Tallahassee, FL 32306 USA.	abarbu@stat.fsu.edu	Barbu, Adrian/C-6865-2009	Barbu, Adrian/0000-0002-9548-7872	Division Of Mathematical Sciences [0915003] Funding Source: National Science Foundation	Division Of Mathematical Sciences(National Science Foundation (NSF)NSF - Directorate for Mathematical & Physical Sciences (MPS))		AMIT Y, 1991, J AM STAT ASSOC, V86, P376, DOI 10.2307/2290581; Bai X, 2009, IEEE I CONF COMP VIS, P575, DOI 10.1109/ICCV.2009.5459188; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Barbu A, 2009, IEEE T IMAGE PROCESS, V18, P2451, DOI 10.1109/TIP.2009.2028254; Behiels G, 1999, LECT NOTES COMPUT SC, V1679, P128; Besbes A, 2009, PROC CVPR IEEE, P1295, DOI 10.1109/CVPRW.2009.5206649; Borenstein E., 2002, P EUR C COMP VIS, P639; Borenstein E, 2008, IEEE T PATTERN ANAL, V30, P2109, DOI 10.1109/TPAMI.2007.70840; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chui H., 2003, COMPUTER VISION IMAG, V89, P141; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; COUR T, 2007, P IEEE C COMP VIS PA; Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024; Falcao AX, 1998, GRAPH MODEL IM PROC, V60, P233, DOI 10.1006/gmip.1998.0475; Felzenszwalb P., 2007, P IEEE C COMP VIS PA; Fernandes LAF, 2008, PATTERN RECOGN, V41, P299, DOI 10.1016/j.patcog.2007.04.003; Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144; Freedman D, 2005, PROC CVPR IEEE, P755; Jojic V., 2010, P INT C MACH LEARN; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Komodakis Nikos, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2985, DOI 10.1109/CVPRW.2009.5206846; Kumar MP, 2005, PROC CVPR IEEE, P18; Leibe B., 2004, EUROPEAN C COMPUTER, P17; Levin A, 2009, INT J COMPUT VISION, V81, P105, DOI 10.1007/s11263-008-0166-0; Li H, 2005, J COMPUT SCI TECH-CH, V20, P849, DOI 10.1007/s11390-005-0849-8; Li Y., 2009, P IEEE C COMP VIS PA; Li Y., 2008, P 10 EUR C COMP VIS; Liu JM, 2009, IEEE T MED IMAGING, V28, P571, DOI 10.1109/TMI.2008.2007820; Malcolm J, 2007, IEEE IMAGE PROC, P2061; Rangarajan A, 1997, LECT NOTES COMPUT SC, V1230, P29; Ren X., 2006, ADV NEURAL INFORM PR, V18, P1121; Rogers M., 2006, LECT NOTES COMPUTER, P289; Shotton J, 2008, IEEE T PATTERN ANAL, V30, P1270, DOI 10.1109/TPAMI.2007.70772; Sofka M, 2010, PROC CVPR IEEE, P1735, DOI 10.1109/CVPR.2010.5539842; Stark M., 2010, P 12 IEEE INT C COMP, P373; Stegmann M. B., 2000, ACTIVE APPEARANCE MO; Stegmann MB, 2003, IEEE T MED IMAGING, V22, P1319, DOI 10.1109/TMI.2003.817780; Stegmann MB, 2002, TECHNICAL REPORT; Torresani L., 2003, P C NEUR INF PROC SY; Tresadern P., 2009, P BRIT MACH VIS C; Winn J., 2005, P 10 IEEE INT C COMP; Wu Y., 2009, INT J COMPUT VISION, V90, P1; Yang X., 2010, P 12 IEEE INT C COMP, P1042; Zhu L, 2010, IEEE T PATTERN ANAL, V32, P1029, DOI 10.1109/TPAMI.2009.65; Zhu Q., 2007, INT C COMP VIS, p[2065, 2066, 2067, 2068, 2070], DOI DOI 10.1109/ICCV.2007.4408929	47	3	3	0	21	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2013	35	7					1649	1659		10.1109/TPAMI.2012.262	http://dx.doi.org/10.1109/TPAMI.2012.262			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	146AG	23681993	Green Submitted			2022-12-18	WOS:000319060600009
J	Sanchez-Vega, F; Eisner, J; Younes, L; Geman, D				Sanchez-Vega, Francisco; Eisner, Jason; Younes, Laurent; Geman, Donald			Learning Multivariate Distributions by Competitive Assembly of Marginals	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Graphs and networks; statistical models; machine learning; linear programming	GENE; NETWORKS; MODELS	We present a new framework for learning high-dimensional multivariate probability distributions from estimated marginals. The approach is motivated by compositional models and Bayesian networks, and designed to adapt to small sample sizes. We start with a large, overlapping set of elementary statistical building blocks, or "primitives," which are low-dimensional marginal distributions learned from data. Each variable may appear in many primitives. Subsets of primitives are combined in a Lego-like fashion to construct a probabilistic graphical model; only a small fraction of the primitives will participate in any valid construction. Since primitives can be precomputed, parameter estimation and structure search are separated. Model complexity is controlled by strong biases; we adapt the primitives to the amount of training data and impose rules which restrict the merging of them into allowable compositions. The likelihood of the data decomposes into a sum of local gains, one for each primitive in the final structure. We focus on a specific subclass of networks which are binary forests. Structure optimization corresponds to an integer linear program and the maximizing composition can be computed for reasonably large numbers of variables. Performance is evaluated using both synthetic data and real datasets from natural language processing and computational biology.	[Sanchez-Vega, Francisco; Younes, Laurent; Geman, Donald] Johns Hopkins Univ, Ctr Imaging Sci, Dept Appl Math & Stat, Baltimore, MD 21218 USA; [Sanchez-Vega, Francisco; Younes, Laurent; Geman, Donald] Johns Hopkins Univ, Inst Computat Med, Baltimore, MD 21218 USA; [Eisner, Jason] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA; [Eisner, Jason] Johns Hopkins Univ, Ctr Language & Speech Proc, Baltimore, MD 21218 USA	Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University	Sanchez-Vega, F (corresponding author), Johns Hopkins Univ, Ctr Imaging Sci, Dept Appl Math & Stat, Clark Hall,3400 N Charles St, Baltimore, MD 21218 USA.	sanchez@cis.jhu.edu; jason@cs.jhu.edu; laurent.younes@jhu.edu; geman@jhu.edu	Geman, Donald/A-3325-2010; Younes, E. Laurent/A-3349-2010	Sanchez-Vega, Francisco/0000-0002-2990-4893; Younes, Laurent/0000-0003-2017-9565	"la Caixa" Foundation; Caja Madrid Foundation; US National Science Foundation (NSF) [CCF-0625687]; NIH-NCRR [UL1 RR 025005]; NSF [CCF-0625687]; NATIONAL CENTER FOR RESEARCH RESOURCES [UL1RR025005] Funding Source: NIH RePORTER	"la Caixa" Foundation(La Caixa Foundation); Caja Madrid Foundation; US National Science Foundation (NSF)(National Science Foundation (NSF)); NIH-NCRR(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR)); NSF(National Science Foundation (NSF)); NATIONAL CENTER FOR RESEARCH RESOURCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR))	Francisco Sanchez-Vega thanks "la Caixa" Foundation and Caja Madrid Foundation for support through graduate fellowships. The work of Laurent Younes is partially supported by US National Science Foundation (NSF) CCF-0625687. The work of Donald Geman is partially supported by NIH-NCRR Grant UL1 RR 025005 and NSF CCF-0625687.	Amit Y, 2007, INT J COMPUT VISION, V75, P267, DOI 10.1007/s11263-006-0033-9; Bach F. R., 2001, ADV NEURAL INFORM PR, P569; Brown D. T., 1959, INFORM CONTROL, V4, P386, DOI DOI 10.1016/S0019-9958(59)80016-4; Chechetka A., 2007, P NEUR INF PROC SYST; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; de Campos C.P., 2009, P 26 ANN INT C MACH; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Dobra A, 2004, J MULTIVARIATE ANAL, V90, P196, DOI 10.1016/j.jmva.2004.02.009; Duggan DJ, 1999, NAT GENET, V21, P10, DOI 10.1038/4434; Elidan G., 2008, P 22 ANN C NEUR INF, P417; Faith JJ, 2007, PLOS BIOL, V5, P54, DOI 10.1371/journal.pbio.0050008; Friedman N, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1300; Geman S, 2002, Q APPL MATH, V60, P707, DOI 10.1090/qam/1939008; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; GREENBLATT MS, 1994, CANCER RES, V54, P4855; Gyftodimos E, 2004, LECT NOTES COMPUT SC, V3025, P291; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Heckerman D, 2001, J MACH LEARN RES, V1, P49, DOI 10.1162/153244301753344614; Heckerman D, 1995, MSRTR9506; Hoffgen K.-U., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P77, DOI 10.1145/168304.168314; Jaakkola T, 2010, P 13 INT C ART INT S, P358; KARGER D, 2001, P 12 ACM SIAM S DISC; Koller D., 1997, P 13 C UNC ART INT U, P302; Lang K., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P331; Lee Su-In, 2007, ADV NEURAL INFORM PR, P817; Levine Arnold J., 2004, Cell, VS116, pS67, DOI 10.1016/S0092-8674(04)00036-4; Lipshutz RJ, 1999, NAT GENET, V21, P20, DOI 10.1038/4447; Margolin AA, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-S1-S7; Martins A., 2009, P JOINT C 47 ANN M A, P342; McCarthy M.I., 2004, NATURE REV GENETICS, V9, P356; Meila M, 1999, MACHINE LEARNING, PROCEEDINGS, P249; MILLER CE, 1960, J ACM, V7, P326, DOI 10.1145/321043.321046; Moore JH, 2004, JAMA-J AM MED ASSOC, V291, P1642, DOI 10.1001/jama.291.13.1642; Morley M, 2004, NATURE, V430, P743, DOI 10.1038/nature02797; Narasimhan M., 2004, UAI 04, P410; Neumann B, 2010, IFIP ADV INF COMM TE, V331, P155; Pe'er D., 2005, SCI STKE, V281, P14, DOI DOI 10.1126/STKE.2812005P14; Petitjean A, 2007, HUM MUTAT, V28, P622, DOI 10.1002/humu.20495; RIMEY RD, 1994, INT J COMPUT VISION, V12, P173, DOI 10.1007/BF01421202; Schafer J, 2005, BIOINFORMATICS, V21, P754, DOI 10.1093/bioinformatics/bti062; Segal E., 2005, J MACHINE LEARNING R, P525; Shahaf D., 2009, J MACHINE LEARNING R, P113; Srebro N, 2003, ARTIF INTELL, V143, P123, DOI 10.1016/S0004-3702(02)00360-0; Szantai T., 2010, ANN OPERATING RES, P1; Utans J., 1994, P NEUR INF PROC SYST, P285; Vogelstein B, 2000, NATURE, V408, P307, DOI 10.1038/35042675; Wang Y, 2008, BRIT J CANCER, V98, P1023, DOI 10.1038/sj.bjc.6604207; Wilks SS, 1938, ANN MATH STAT, V9, P60, DOI 10.1214/aoms/1177732360; Xiang Y., 1993, COMPUTATIONAL INTELL, V9, P680; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	53	3	3	0	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2013	35	2					398	410		10.1109/TPAMI.2012.96	http://dx.doi.org/10.1109/TPAMI.2012.96			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	057JX	22529323				2022-12-18	WOS:000312560600012
J	Felsberg, M; Larsson, F; Wiklund, J; Wadstromer, N; Ahlberg, J				Felsberg, Michael; Larsson, Fredrik; Wiklund, Johan; Wadstromer, Niclas; Ahlberg, Jorgen			Online Learning of Correspondences between Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Online learning; correspondence problem; channel representation; computer vision; surveillance	TRACKING; OBJECTS; TIME	We propose a novel method for iterative learning of point correspondences between image sequences. Points moving on surfaces in 3D space are projected into two images. Given a point in either view, the considered problem is to determine the corresponding location in the other view. The geometry and distortions of the projections are unknown, as is the shape of the surface. Given several pairs of point sets but no access to the 3D scene, correspondence mappings can be found by excessive global optimization or by the fundamental matrix if a perspective projective model is assumed. However, an iterative solution on sequences of point-set pairs with general imaging geometry is preferable. We derive such a method that optimizes the mapping based on Neyman's chi-square divergence between the densities representing the uncertainties of the estimated and the actual locations. The densities are represented as channel vectors computed with a basis function approach. The mapping between these vectors is updated with each new pair of images such that fast convergence and high accuracy are achieved. The resulting algorithm runs in real time and is superior to state-of-the-art methods in terms of convergence and accuracy in a number of experiments.	[Felsberg, Michael; Larsson, Fredrik; Wiklund, Johan] Linkoping Univ, Dept Elect Engn, Comp Vis Lab, SE-58183 Linkoping, Sweden; [Wadstromer, Niclas] FOI Swedish Def Res Agcy, Div Informat Syst Sensor & EW Syst, SE-58111 Linkoping, Sweden; [Ahlberg, Jorgen] Termisk Systemteknik AB, SE-58335 Linkoping, Sweden	Linkoping University; FOI - Swedish Defence Research Agency	Felsberg, M (corresponding author), Linkoping Univ, Dept Elect Engn, Comp Vis Lab, SE-58183 Linkoping, Sweden.	michael.felsberg@liu.se; larsson@isy.liu.se; jowi@isy.liu.se; niclas.wadstromer@foi.se; jorgen@termisk.se		Ahlberg, Jorgen/0000-0002-6763-5487; Felsberg, Michael/0000-0002-6096-3648	EC [215078, 247947]; ELLIIT; Strategic Area for ICT research; CADICS; Swedish Government; Swedish Research Council; CUAS; FOCUS; Swedish Foundation for Strategic Research	EC(European CommissionEuropean Commission Joint Research Centre); ELLIIT; Strategic Area for ICT research; CADICS; Swedish Government; Swedish Research Council(Swedish Research CouncilEuropean Commission); CUAS; FOCUS; Swedish Foundation for Strategic Research(Swedish Foundation for Strategic Research)	This research has received funding from the EC's seventh Framework Programme (FP7/2007-2013), grant agreements 215078 (DIPLECS) and 247947 (GARNICS), from ELLIIT, Strategic Area for ICT research, and CADICS, funded by the Swedish Government, Swedish Research Council project ETT, and from CUAS and FOCUS, funded by the Swedish Foundation for Strategic Research.	Bishop C.M, 2006, PATTERN RECOGN; Bottou L, 1998, ON LINE LEARNING NEU; Cichocki A., 2008, P IEEE INT WORKSH MA; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Felsberg M, 2006, IEEE T PATTERN ANAL, V28, P209, DOI 10.1109/TPAMI.2006.29; Felsberg M, 2009, LECT NOTES COMPUT SC, V5876, P184, DOI 10.1007/978-3-642-10520-3_17; Forssen P.-E., 2003, P 13 SCAND C IM AN; Gilbert A, 2008, COMPUT VIS IMAGE UND, V111, P43, DOI 10.1016/j.cviu.2007.06.005; Grollman D., 2010, THESIS BROWN U; Grollman D., 2010, P IEEE RSJ INT C INT; Grollman DH, 2008, IEEE INT CONF ROBOT, P3315, DOI 10.1109/ROBOT.2008.4543716; Hartley R., 2004, ROBOTICA; Haykin S., 1999, NEURAL NETWORKS COMP; Javed O, 2008, COMPUT VIS IMAGE UND, V109, P146, DOI 10.1016/j.cviu.2007.01.003; Jimenez R, 2001, TEST, V10, P241, DOI 10.1007/BF02595695; Johansson B, 2006, MATH COMPUT MODEL, V43, P892, DOI 10.1016/j.mcm.2005.12.010; Jonsson E., 2006, P 18 INT C PATT REC; JONSSON E, 2008, THESIS LINKOPING U S; Kass M., 2010, ACM SIGGRAPH PAPERS; Khan S, 2003, IEEE T PATTERN ANAL, V25, P1355, DOI 10.1109/TPAMI.2003.1233912; Kim Y.-D., 2008, P IEEE INT C AC SPEE; Larsson F, 2009, IMAGE VISION COMPUT, V27, P1729, DOI 10.1016/j.imavis.2009.04.003; Lee L, 2000, IEEE T PATTERN ANAL, V22, P758, DOI 10.1109/34.868678; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119; Micusik B, 2006, IEEE T PATTERN ANAL, V28, P1135, DOI 10.1109/TPAMI.2006.151; Pouget A, 2003, ANNU REV NEUROSCI, V26, P381, DOI 10.1146/annurev.neuro.26.041002.131112; Rangarajan A., 1999, P 2 INT WORKSH EN MI, P734; Saad D., 1998, ON LINE LEARNING NEU; Schaal S, 2002, APPL INTELL, V17, P49, DOI 10.1023/A:1015727715131; SNIPPE HP, 1992, BIOL CYBERN, V66, P543, DOI 10.1007/BF00204120; Vijayakumar S., 2000, P 17 INT C MACH LEAR, P1079; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818; [No title captured]; [No title captured]	35	3	3	0	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2013	35	1					118	129		10.1109/TPAMI.2012.65	http://dx.doi.org/10.1109/TPAMI.2012.65			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	037SV	22392708	Green Published, Green Submitted			2022-12-18	WOS:000311127700012
J	Bouaynaya, N; Charif-Chefchaouni, M; Schonfeld, D				Bouaynaya, Nidhal; Charif-Chefchaouni, Mohammed; Schonfeld, Dan			M-Idempotent and Self-Dual Morphological Filters	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Mathematical morphology; spatially-invariant mathematical morphology; duality; idempotence	THEORETICAL FOUNDATIONS; RESTORATION; OPERATORS; IMAGE	In this paper, we present a comprehensive analysis of self-dual and m-idempotent operators. We refer to an operator as m-idempotent if it converges after m iterations. We focus on an important special case of the general theory of lattice morphology: spatially variant morphology, which captures the geometrical interpretation of spatially variant structuring elements. We demonstrate that every increasing self-dual morphological operator can be viewed as a morphological center. Necessary and sufficient conditions for the idempotence of morphological operators are characterized in terms of their kernel representation. We further extend our results to the representation of the kernel of m-idempotent morphological operators. We then rely on the conditions on the kernel representation derived and establish methods for the construction of m-idempotent and self-dual morphological operators. Finally, we illustrate the importance of the self-duality and m-idempotence properties by an application to speckle noise removal in radar images.	[Bouaynaya, Nidhal] Univ Arkansas, George W Donaghey Coll Engn & Informat Technol, Dept Syst Engn, Little Rock, AR 72204 USA; [Charif-Chefchaouni, Mohammed] Inst Natl Postes & Telecommun, Rabat, Morocco; [Schonfeld, Dan] Univ Illinois, Dept Elect & Comp Engn, Chicago, IL 60607 USA	University of Arkansas System; University of Arkansas Fayetteville; University of Arkansas Little Rock; University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Bouaynaya, N (corresponding author), Univ Arkansas, George W Donaghey Coll Engn & Informat Technol, Dept Syst Engn, 2801 S Univ Ave, Little Rock, AR 72204 USA.	nxbouaynaya@ualr.edu; charifm@inpt.ac.ma; dans@uic.edu						Bloch I, 2007, HANDBOOK OF SPATIAL LOGICS, P857, DOI 10.1007/978-1-4020-5587-4_14; Bouaynaya N, 2008, IEEE T PATTERN ANAL, V30, P823, DOI 10.1109/TPAMI.2007.70754; Bouaynaya N, 2008, IEEE T PATTERN ANAL, V30, P837, DOI 10.1109/TPAMI.2007.70756; Bouaynaya N, 2006, IEEE T IMAGE PROCESS, V15, P3579, DOI 10.1109/TIP.2006.877475; GOLES E, 1981, DISCRETE APPL MATH, V3, P93, DOI 10.1016/0166-218X(81)90034-2; Heijmans H., 1994, MORPHOLOGICAL IMAGE; Heijmans HJAM, 2002, J MATH IMAGING VIS, V17, P55, DOI 10.1023/A:1020726725590; HEIJMANS HJAM, 1991, IEEE T PATTERN ANAL, V13, P568, DOI 10.1109/34.87343; HEIJMANS HJAM, 1994, SIGNAL PROCESS, V38, P13, DOI 10.1016/0165-1684(94)90053-1; Heijmans HJAM, 1999, IEEE T IMAGE PROCESS, V8, P1330, DOI 10.1109/83.791959; Heijmans HJAM, 1996, J MATH IMAGING VIS, V6, P15, DOI 10.1007/BF00127373; Keshet R, 2005, J MATH IMAGING VIS, V22, P309, DOI 10.1007/s10851-005-4896-0; MARAGOS P, 1987, IEEE T ACOUST SPEECH, V35, P1153, DOI 10.1109/TASSP.1987.1165259; MARAGOS P, 1989, IEEE T PATTERN ANAL, V11, P586, DOI 10.1109/34.24793; Matheron G., 1975, RANDOM SETS INTEGRAL; Mehnert AJH, 2000, COMP IMAG VIS, V18, P99; MEYER F, 1989, SIGNAL PROCESS, V16, P303, DOI 10.1016/0165-1684(89)90028-5; Meyer F, 1998, COMP IMAG VIS, V12, P191; RONSE C, 1993, IEEE T PATTERN ANAL, V15, P484, DOI 10.1109/34.211468; RONSE C, 1990, SIGNAL PROCESS, V21, P129, DOI 10.1016/0165-1684(90)90046-2; Ronse C., 2010, MATH MORPHOLOGY THEO, P35; Ronse C, 2008, APPL ALGEBR ENG COMM, V19, P51, DOI 10.1007/s00200-008-0064-2; SAFA F, 1989, SIGNAL PROCESS, V16, P319, DOI 10.1016/0165-1684(89)90029-7; Salembier P, 1998, COMP IMAG VIS, V12, P183; SCHONFELD D, 1991, IEEE T PATTERN ANAL, V13, P14, DOI 10.1109/34.67627; SERRA J, 1986, COMPUT VISION GRAPH, V35, P283, DOI 10.1016/0734-189X(86)90002-2; Serra J, 1988, IMAGE ANAL MATH MORP; Sheng YW, 1996, INT GEOSCI REMOTE SE, P1559; Soille P, 2005, IMAGE VISION COMPUT, V23, P249, DOI 10.1016/j.imavis.2004.06.002; Soille P, 2002, IEEE T GEOSCI REMOTE, V40, P2042, DOI 10.1109/TGRS.2002.804618; Sternberg S., 1979, P IEEE COMP SOFTW AP; STEVENSON RL, 1987, IEEE T CIRCUITS SYST, V34, P1292, DOI 10.1109/TCS.1987.1086067; Verdu-Monedero R, 2009, LECT NOTES COMPUT SC, V5720, P115, DOI 10.1007/978-3-642-03613-2_11; Verdu-Monedero R, 2008, LECT NOTES COMPUT SC, V5259, P542, DOI 10.1007/978-3-540-88458-3_49; Vichik A., 2007, P 8 INT S MATH MORPH, P49; YLIHARIA O, 1991, IEEE T SIGNAL PROCES, V39, P395, DOI 10.1109/78.80823	36	3	5	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2012	34	4					805	813		10.1109/TPAMI.2011.244	http://dx.doi.org/10.1109/TPAMI.2011.244			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	896PO	22184254				2022-12-18	WOS:000300581700013
J	Chen, DZ; Bilgic, M; Getoor, L; Jacobs, D				Chen, Daozheng; Bilgic, Mustafa; Getoor, Lise; Jacobs, David			Dynamic Processing Allocation in Video	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Video processing; resource allocation; graphical models; optimization; background subtraction; face detection; dynamic programming	FACE DETECTION; INFORMATION; TRACKING; MODELS	Large stores of digital video pose severe computational challenges to existing video analysis algorithms. In applying these algorithms, users must often trade off processing speed for accuracy, as many sophisticated and effective algorithms require large computational resources that make it impractical to apply them throughout long videos. One can save considerable effort by applying these expensive algorithms sparingly, directing their application using the results of more limited processing. We show how to do this for retrospective video analysis by modeling a video using a chain graphical model and performing inference both to analyze the video and to direct processing. We apply our method to problems in background subtraction and face detection, and show in experiments that this leads to significant improvements over baseline algorithms.	[Chen, Daozheng; Getoor, Lise; Jacobs, David] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Chen, Daozheng; Getoor, Lise; Jacobs, David] UMIACS, College Pk, MD 20742 USA; [Bilgic, Mustafa] IIT, Dept Comp Sci, Chicago, IL 60616 USA	University System of Maryland; University of Maryland College Park; Illinois Institute of Technology	Chen, DZ (corresponding author), Univ Maryland, Dept Comp Sci, AV Williams Bldg, College Pk, MD 20742 USA.	dchen@cs.umd.edu; mbilgic@iit.edu; getoor@cs.umd.edu; djacobs@cs.umd.edu			US Army Research Office, ARO [W911NF0810466]; US National Science Foundation (NSF) [0746930]	US Army Research Office, ARO; US National Science Foundation (NSF)(National Science Foundation (NSF))	The authors would like to thank the reviewers for their helpful comments. This work was supported by the US Army Research Office, ARO #W911NF0810466. Mustafa Bilgic was also supported under US National Science Foundation (NSF) Grant #0746930.	BAROTTI S, 2003, P INT C IM AN PROC; BAYERZUBEK V, 2004, P C UNC ART INT; BILGIC M, 2008, P INT C KNOWL DISC D; Cheung SCS, 2004, PROC SPIE, V5308, P881, DOI 10.1117/12.526886; DEAN N, 2005, INDEPENDENT      SEP; Elgammal A. M., 2000, P EUR C COMP VIS; Felzenszwalb P., 2010, P IEEE C COMP VIS PA; FISCUS J, 2009, P IEEE INT C ADV VID, P219; HOWARD RA, 1966, IEEE T SYST SCI CYB, VSSC2, P22, DOI 10.1109/TSSC.1966.300074; Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242; Huang C, 2005, IEEE I CONF COMP VIS, P446; *INT, 2011, OP OP SOURC COMP VIS; JAIN R, 1979, IEEE T PATTERN ANAL, V1, P206, DOI 10.1109/TPAMI.1979.4766907; Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198; Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004; Krause A, 2009, J ARTIF INTELL RES, V35, P557, DOI 10.1613/jair.2737; KRISHNA R, 2008, P ACM IEEE INT C DIS; Lienhart R, 2002, P IEEE INT C IM PROC; Lo BPL, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P158, DOI 10.1109/ISIMP.2001.925356; LOY C, 2009, P IEEE C COMP VIS PA; Mita T, 2005, IEEE I CONF COMP VIS, P1619; Murphy K.P., 2002, DYNAMIC BAYESIAN NET; Osuna E., 1997, P IEEE C COMP VIS PA; Piccardi M., 2004, P IEEE INT C SYST MA; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; RADOVILSKY Y, 2006, P IEEE INT C SYST MA; Rattigan M., 2007, P ICDM WORKSH MIN GR; RITTSCHER J, 2000, P EUR C COMP VIS; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Schneiderman H, 1998, P IEEE C COMP VIS PA, P4551; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Settles B., 2009, ACT LEARN LIT SURV; Sinha P., 1994, INVEST OPHTHALMOL, V35, P1735; Stauffer C., 1999, P IEEE COMP SOC C CO, V2; Sutton C., 2007, INTRO STAT RELATIONA, P93; Turney PD, 1994, J ARTIF INTELL RES, V2, P369, DOI 10.1613/jair.120; VIJAYANARASIMHA.S, 2010, P IEEE C COMP VIS PA; Vijayanarasimhan S., 2009, P IEEE C COMP VIS PA; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; WEISS D, 2010, P INT C ART INT STAT; WESTROP AJR, 2011, FACE DETECTION TECHN; Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236; Xiao R, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P709; YANG GZ, 1994, PATTERN RECOGN, V27, P53, DOI 10.1016/0031-3203(94)90017-5; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zivkovic Z, 2004, P INT C PATT REC; 2008, BRICKSTREAM LAUNCHES	48	3	3	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2011	33	11					2174	2187		10.1109/TPAMI.2011.55	http://dx.doi.org/10.1109/TPAMI.2011.55			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	820MM	21422485	Green Submitted			2022-12-18	WOS:000294910000005
J	Jagannathan, S; Horn, BKP; Ratilal, P; Makris, NC				Jagannathan, Srinivasan; Horn, Berthold Klaus Paul; Ratilal, Purnima; Makris, Nicholas Constantine			Force Estimation and Prediction from Time-Varying Density Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Force estimation; density prediction; compressible flow estimation; minimum energy flow	OPTICAL-FLOW; MOTION ESTIMATION; NONRIGID MOTION; POPULATION; VELOCITY; COMPUTATION; BEHAVIOR; SCHOOLS; FIELDS; CELL	We present methods for estimating forces which drive motion observed in density image sequences. Using these forces, we also present methods for predicting velocity and density evolution. To do this, we formulate and apply a Minimum Energy Flow (MEF) method which is capable of estimating both incompressible and compressible flows from time-varying density images. Both the MEF and force-estimation techniques are applied to experimentally obtained density images, spanning spatial scales from micrometers to several kilometers. Using density image sequences describing cell splitting, for example, we show that cell division is driven by gradients in apparent pressure within a cell. Using density image sequences of fish shoals, we also quantify 1) intershoal dynamics such as coalescence of fish groups over tens of kilometers, 2) fish mass flow between different parts of a large shoal, and 3) the stresses acting on large fish shoals.	[Jagannathan, Srinivasan; Makris, Nicholas Constantine] MIT, Dept Mech Engn, Cambridge, MA 02139 USA; [Horn, Berthold Klaus Paul] MIT, Comp Sci & Artificial Intelligence Lab, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA; [Ratilal, Purnima] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Northeastern University	Jagannathan, S (corresponding author), MIT, Dept Mech Engn, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	jsrini@mit.edu; bkph@csail.mit.edu; purnima@ece.neu.edu; makris@mit.edu		/0000-0003-3434-391X	US Office of Naval Research; Alfred P. Sloan Foundation; US National Oceanographic Partnership Program	US Office of Naval Research(Office of Naval Research); Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); US National Oceanographic Partnership Program	This research was supported by the US Office of Naval Research, the Alfred P. Sloan Foundation, the US National Oceanographic Partnership Program, and is a contribution to the Census of Marine Life. The authors thank Margrit Betke for her useful discussions and her suggestion to apply MEF to the problem of cell division.	AMINI A, 1994, P EUR C COMP VIS, P125; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Andrews M, 2009, J ACOUST SOC AM, V126, P1057, DOI 10.1121/1.3177271; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Batchelor G., 2000, INTRO FLUID DYNAMICS; BATTITI R, 1991, INT J COMPUT VISION, V6, P133, DOI 10.1007/BF00128153; Bereziat D, 2000, ENVIRON MODELL SOFTW, V15, P513, DOI 10.1016/S1364-8152(00)00046-3; BEREZIAT D, 2000, P C COMP VIS PATT RE, V2, P487; Corpetti T, 2006, EXP FLUIDS, V40, P80, DOI 10.1007/s00348-005-0048-y; Couzin ID, 2003, ADV STUD BEHAV, V32, P1, DOI 10.1016/S0065-3454(03)01001-5; Danuser G, 2003, J MICROSC-OXFORD, V211, P191, DOI 10.1046/j.1365-2818.2003.01222.x; Devenport WJ, 1996, J FLUID MECH, V312, P67, DOI 10.1017/S0022112096001929; DEVLAMINCK V, 1996, P INT C IM PROC SEPT, V1, P125; EKATARINA LG, 2005, NATURE, V438, P384; ENKELMANN W, 1988, COMPUT VISION GRAPH, V43, P150, DOI 10.1016/0734-189X(88)90059-X; Farmer DM, 1999, J ACOUST SOC AM, V106, P2481, DOI 10.1121/1.428082; Fitzpatrick J. M., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P78; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; Freeman S., 2002, BIOL SCI; Gong Z, 2010, J ACOUST SOC AM, V127, P104, DOI 10.1121/1.3268595; Guerrero T, 2006, PHYS MED BIOL, V51, P777, DOI 10.1088/0031-9155/-51/4/002; HEEGER DJ, 1987, INT J COMPUT VISION, V1, P279, DOI 10.1007/BF00133568; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Huse I, 1996, ICES J MAR SCI, V53, P863, DOI 10.1006/jmsc.1996.9999; INOUE S, 1995, MOL BIOL CELL, V6, P1619, DOI 10.1091/mbc.6.12.1619; INOUE S, 1967, J GEN PHYSIOL, V50, P259, DOI 10.1085/jgp.50.6.259; JAGANNATHAN S, 2009, MARINE ECOLOGY PROGR, V395; Kasza KE, 2007, CURR OPIN CELL BIOL, V19, P101, DOI 10.1016/j.ceb.2006.12.002; Landau L.D., 1976, MECHANICS; Lodish H., 2004, MOL CELL BIOL, V5th; Lucas B. D., 1981, IJCAI, P121, DOI DOI 10.5555/1623264.1623280; Makris NC, 2006, SCIENCE, V311, P660, DOI 10.1126/science.1121756; Makris NC, 2009, SCIENCE, V323, P1734, DOI 10.1126/science.1169441; Maupertuis P-LMd., 1744, MEM ACAD R SCI PARIS, P417; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; Misund OA, 1998, ICES J MAR SCI, V55, P58, DOI 10.1006/jmsc.1997.0228; NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5; Nakajima Y, 2003, PATTERN RECOGN, V36, P1203, DOI 10.1016/S0031-3203(02)00078-X; Ortega R, 2009, J R SOC INTERFACE, V6, pS649, DOI 10.1098/rsif.2009.0166.focus; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P730, DOI 10.1109/34.85661; Pitcher TJ., 1993, Behaviour of teleost fishes, P363, DOI 10.1007/978-94-011-1578-0_12; POTAPOVA T, 2007, ASCB IMAGE VIDEO LIB; PRINCE JL, 1992, IEEE T MED IMAGING, V11, P238, DOI 10.1109/42.141648; Rhode K, 2000, PROC SPIE, V3979, P1414, DOI 10.1117/12.387652; Saffman P., 1992, VORTEX DYNAMICS; SIMPSON JJ, 1994, IEEE T GEOSCI REMOTE, V32, P479, DOI 10.1109/36.297966; Singh A., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P168, DOI 10.1109/ICCV.1990.139516; SONG SM, 1991, IEEE T MED IMAGING, V10, P295, DOI 10.1109/42.97579; SUTER D, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P939, DOI 10.1109/CVPR.1994.323929; TERZOPOULOS D, 1993, IEEE T PATTERN ANAL, V15, P569, DOI 10.1109/34.216726; TONER J, 1995, PHYS REV LETT, V75, P4326, DOI 10.1103/PhysRevLett.75.4326; Toner J, 1998, PHYS REV E, V58, P4828, DOI 10.1103/PhysRevE.58.4828; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; VLASENKO A, 2009, IMAGING MEASUREMENT, P247; WAXMAN AM, 1988, P IEEE C COMP VIS PA, P723; Wildes RP, 2000, COMPUT VIS IMAGE UND, V80, P246, DOI 10.1006/cviu.2000.0874	56	3	5	1	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2011	33	6					1132	1146		10.1109/TPAMI.2010.185	http://dx.doi.org/10.1109/TPAMI.2010.185			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	750DE	20921583	Green Published			2022-12-18	WOS:000289524000005
J	Rozenfeld, S; Shimshoni, I; Lindenbaum, M				Rozenfeld, Stas; Shimshoni, Ilan; Lindenbaum, Michael			Dense Mirroring Surface Recovery from 1D Homographies and Sparse Correspondences	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Mirroring objects; 3D shape reconstruction; 1D homographies; stability	SHAPE	In this work, we recover the 3D shape of mirrors, sunglasses, and stainless steel implements. A computer monitor displays several images of parallel stripes, each image at a different angle. Reflections of these stripes in a mirroring surface are captured by the camera. For every image point, the direction of the displayed stripes and their reflections in the image are related by a 1D homography matrix, computed with a robust version of the statistically accurate heteroscedastic approach. By focusing on a sparse set of image points for which monitor-image correspondence is computed, the depth and the local shape may be estimated from these homographies. The depth estimation relies on statistically correct minimization and provides accurate, reliable results. Even for the image points where the depth estimation process is inherently unstable, we are able to characterize this instability and develop an algorithm to detect and correct it. After correcting the instability, dense surface recovery of mirroring objects is performed using constrained interpolation, which does not simply interpolate the surface depth values but also uses the locally computed 1D homographies to solve for the depth, the correspondence, and the local surface shape. The method was implemented and the shape of several objects was densely recovered at submillimeter accuracy.	[Rozenfeld, Stas] Technion Israel Inst Technol, Fac Ind Engn & Management, IL-32000 Haifa, Israel; [Shimshoni, Ilan] Univ Haifa, Dept Informat Syst, IL-31905 Haifa, Israel; [Lindenbaum, Michael] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology; University of Haifa; Technion Israel Institute of Technology	Rozenfeld, S (corresponding author), Technion Israel Inst Technol, Fac Ind Engn & Management, IL-32000 Haifa, Israel.	rozefeld@tx.technion.ac.il; ishimshoni@is.haifa.ac.il; mic@cs.technion.ac.il						Adato Y, 2007, IEEE I CONF COMP VIS, P433; Baba M, 2001, OPT ENG, V40, P53, DOI 10.1117/1.1331269; BLAKE A, 1988, P IEEE INT C COMP VI, P394; Bonfort T, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P591; HALSTEAD MA, 1996, P ACM SIGGRAPH, P335; Knauer MC, 2004, PROC SPIE, V5457, P366, DOI 10.1117/12.545704; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; Lellmann J, 2008, INT J COMPUT VISION, V80, P226, DOI 10.1007/s11263-007-0123-3; Morano RA, 1998, IEEE T PATTERN ANAL, V20, P322, DOI 10.1109/34.667888; OREN M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P740, DOI 10.1109/ICCV.1995.466864; PARK WS, 1996, P PAC C MAN SEOUL KO, V1, P389; Press WH, 1988, NUMERICAL RECIPES C; RIPSMAN A, 2001, P IEEE INT S COMP IN; ROTH S, 2006, P IEEE C COMP VIS PA, P1869; ROZENFELD S, 2007, P IEEE C COMP VIS PA, P1; Savarese S, 2005, INT J COMPUT VISION, V64, P31, DOI 10.1007/s11263-005-1086-x; Solem JE, 2007, IEEE T PATTERN ANAL, V29, P181, DOI 10.1109/TPAMI.2007.250610; Tarini M, 2005, GRAPH MODELS, V67, P233, DOI 10.1016/j.gmod.2004.11.002; Zheng JY, 2000, IEEE T PATTERN ANAL, V22, P913, DOI 10.1109/34.868691; ZISSERMAN A, 1989, IMAGE VISION COMPUT, V7, P38, DOI 10.1016/0262-8856(89)90018-8	20	3	3	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2011	33	2					325	337		10.1109/TPAMI.2010.76	http://dx.doi.org/10.1109/TPAMI.2010.76			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	694QR	21193810				2022-12-18	WOS:000285313200009
J	Shafait, F; Keysers, D; Breuel, TM				Shafait, Faisal; Keysers, Daniel; Breuel, Thomas M.			Response to "Projection Methods Require Black Border Removal"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Document page segmentation; OCR; performance evaluation; performance metric	SEGMENTATION ALGORITHMS; PERFORMANCE EVALUATION; TECHNICAL JOURNALS; IMAGES	In contrast to prior experimental work, our results support the conclusion that RXYC can perform well after marginal noise removal. However, marginal noise removal on page images like those found in UW3 remains a hard problem and it therefore remains an open question whether RXYC can actually achieve competitive performance on such databases.	[Shafait, Faisal; Keysers, Daniel] German Res Ctr Artificial Intelligence DFKI GmbH, Image Understanding & Pattern Recognit IUPR Res G, D-67663 Kaiserslautern, Germany; [Breuel, Thomas M.] Tech Univ Kaiserslautern, Dept Comp Sci, D-67663 Kaiserslautern, Germany	German Research Center for Artificial Intelligence (DFKI); University of Kaiserslautern	Shafait, F (corresponding author), German Res Ctr Artificial Intelligence DFKI GmbH, Image Understanding & Pattern Recognit IUPR Res G, D-67663 Kaiserslautern, Germany.	faisal.shafait@dfki.de; daniel.keysers@dfki.de; tmb@informatik.uni-kl.de	Shafait, Faisal/A-1342-2012	Shafait, Faisal/0000-0002-0922-0566				AVILA BT, 2004, P INT C IM AN REC SE, P249; Cinque L, 2002, PATTERN RECOGN, V35, P1167, DOI 10.1016/S0031-3203(01)00082-6; Fan KC, 2002, PATTERN RECOGN, V35, P2593, DOI 10.1016/S0031-3203(01)00205-9; Guyon Isabelle, 1997, HDB CHARACTER RECOGN, P779; KRISHNAMOORTHY M, 1993, IEEE T PATTERN ANAL, V15, P737, DOI 10.1109/34.221173; Mao S, 2001, IEEE T PATTERN ANAL, V23, P242, DOI 10.1109/34.910877; NAGY G, 1992, COMPUTER, V25, P10, DOI 10.1109/2.144436; Nagy G, 2009, IEEE T PATTERN ANAL, V31, P762, DOI 10.1109/TPAMI.2008.192; Peerawit W., 2004, P 4 INF COMP ENG POS; SHAFAIT F, 2006, P 7 IAPR WORKSH DOC, P368, DOI 10.1007/11669487_33; Shafait F, 2008, INT J DOC ANAL RECOG, V11, P81, DOI 10.1007/s10032-008-0071-7; Shafait F, 2008, IEEE T PATTERN ANAL, V30, P941, DOI 10.1109/TPAMI.2007.70837; Shafait F, 2007, LECT NOTES COMPUT SC, V4522, P651; Stamatopoulos N., 2007, P 2 INT WORKSH CAM B, P71	14	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2009	31	4					763	764		10.1109/TPAMI.2008.220	http://dx.doi.org/10.1109/TPAMI.2008.220			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	407WX					2022-12-18	WOS:000263396100017
J	Popovici, I; Withers, WD				Popovici, Irina; Withers, William Douglas			Curve Parameterization by Moments	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Circle detection; ellipse detection; conic location; conic detection; conic parameterization; moments	HOUGH TRANSFORM	We present a method for deriving a parametric description of a conic section (quadratic curve) in an image from the moments of the image with respect to several specially constructed kernel functions. In contrast to Hough-transform-type methods, the moment approach requires no large accumulator array. Judicious implementation allows the parameters to be determined using five multiplication operations and six addition operations per pixel. The use of moments renders the calculation robust in the presence of high-frequency noise or texture and resistant to small-scale irregularities in the edge. Our method is generalizable to more complex classes of curves with more parameters and to surfaces in higher dimensions.	[Popovici, Irina; Withers, William Douglas] USN Acad, Dept Math, Annapolis, MD 21402 USA	United States Department of Defense; United States Navy; United States Naval Academy	Popovici, I (corresponding author), USN Acad, Dept Math, 572C Holloway Rd, Annapolis, MD 21402 USA.	popovici@usna.edu; wdw@usna.edu			US Office of Naval Research	US Office of Naval Research(Office of Naval Research)	This work was supported by a grant from the US Office of Naval Research.	AGUADO AS, 1995, RES J; AGUADO AS, 1995, P 5 INT C IM PROC IT, P375; Atherton TJ, 1999, IMAGE VISION COMPUT, V17, P795, DOI 10.1016/S0262-8856(98)00160-7; ATHERTON TJ, 1999, P IEE C HOUGH TRANSF; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; CHAN R, 1990, P IEEE INT C AC SPEE, V4, P2201; DAVIES ER, 1989, PATTERN RECOGN LETT, V9, P87, DOI 10.1016/0167-8655(89)90041-X; Donoho DL, 1999, ANN STAT, V27, P859, DOI 10.1214/aos/1018031261; DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242; GHOSAL S, 1993, PATTERN RECOGN, V26, P295, DOI 10.1016/0031-3203(93)90038-X; Ghosal S, 1997, IEEE T IMAGE PROCESS, V6, P781, DOI 10.1109/83.585230; Goneid A, 1997, IEEE SYS MAN CYBERN, P3154, DOI 10.1109/ICSMC.1997.633079; Gustafsson B, 2000, INVERSE PROBL, V16, P1053, DOI 10.1088/0266-5611/16/4/312; Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468; Heikkila J, 1998, INT C PATT RECOG, P734, DOI 10.1109/ICPR.1998.711250; HINTON CJ, 1993, P IEE C HOUGH TRANSF; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; JAFRI MZM, 1995, P SOC PHOTO-OPT INS, V2356, P53, DOI 10.1117/12.198623; KANATANI K, 1994, IEEE T PATTERN ANAL, V16, P320, DOI 10.1109/34.276132; Krein M.G., 1977, TRANSLATIONS MATH MO, V50; Lei YW, 1999, PATTERN RECOGN LETT, V20, P41, DOI 10.1016/S0167-8655(98)00127-5; LISOWKSA A, 2005, THESIS U SILESIA; LYVERS EP, 1989, IEEE T PATTERN ANAL, V11, P1293, DOI 10.1109/34.41367; MCLAUGHLIN R, 1996, P IEEE INT C DIG SIG, V1, P409; Milanfar P, 2000, P SOC PHOTO-OPT INS, V4116, P406, DOI 10.1117/12.406519; Popovici I, 2006, IEEE T PATTERN ANAL, V28, P637, DOI 10.1109/TPAMI.2006.75; Popovici I, 2006, IEEE IMAGE PROC, P753, DOI 10.1109/ICIP.2006.312501; Popovici I, 2007, IEEE T IMAGE PROCESS, V16, P1470, DOI 10.1109/TIP.2007.891782; Putinar M, 1995, ARK MAT, V33, P357, DOI 10.1007/BF02559714; Reeves A. P., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P312; Romberg JK, 2003, P SOC PHOTO-OPT INS, V5150, P1265, DOI 10.1117/12.509903; SAFAEERAD R, 1991, CVGIP-IMAG UNDERSTAN, V54, P259, DOI 10.1016/1049-9660(91)90067-Y; SAKAI M, 1982, SPRINGER VERLAG LECT, V934; WEAST RC, 1975, CRC HDB TABLES MATH; Xie YH, 2002, INT C PATT RECOG, P957, DOI 10.1109/ICPR.2002.1048464; XU L, 1990, PATTERN RECOGN LETT, V11, P331, DOI 10.1016/0167-8655(90)90042-Z; YOO JH, 1993, PATTERN RECOGN, V26, P307, DOI 10.1016/0031-3203(93)90039-Y	37	3	3	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2009	31	1					15	26		10.1109/TPAMI.2008.54	http://dx.doi.org/10.1109/TPAMI.2008.54			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	372GI	19029543				2022-12-18	WOS:000260889700003
J	Wong, KYK; Zhang, GQ; Liang, C; Zhang, H				Wong, Kwan-Yee K.; Zhang, Guoqiang; Liang, Chen; Zhang, Hui			1D Camera Geometry and Its Application to the Self-Calibration of Circular Motion Sequences	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						1D projective camera; homography; self-calibration; circular motion		This paper proposes a novel method for robustly recovering the camera geometry of an uncalibrated image sequence taken under circular motion. Under circular motion, all the camera centers lie on a circle and the mapping from the plane containing this circle to the horizon line observed in the image can be modeled as a 1D projection. A 2 x 2 homography is introduced in this paper to relate the projections of the camera centers in two 1D views. It is shown that the two imaged circular points of the motion plane and the rotation angle between the two views can be derived directly from such a homography. This way of recovering the imaged circular points and rotation angles is intrinsically a multiple view approach as all the sequence geometry embedded in the epipoles is exploited in the estimation of the homography for each view pair. This results in a more robust method compared to those computing the rotation angles using adjacent views only. The proposed method has been applied to self-calibrate turntable sequences using either point features or silhouettes, and highly accurate results have been achieved.	[Wong, Kwan-Yee K.; Zhang, Guoqiang; Liang, Chen] Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China; [Zhang, Hui] United Int Coll, Div Sci & Technol, Zhuhai 519085, Guangdong, Peoples R China	University of Hong Kong; Beijing Normal University - Hong Kong Baptist University United International College	Wong, KYK (corresponding author), Univ Hong Kong, Dept Comp Sci, Pokfulam Rd, Hong Kong, Hong Kong, Peoples R China.	kykwong@cs.hku.hk; gqzhang@cs.hku.hk; cliang@cs.hku.hk; amyzhang@uic.edu.hk	Wong, Kenneth Kwan Yee/C-1577-2009	Wong, Kenneth Kwan Yee/0000-0001-8560-9007; ZHANG, Hui/0000-0002-1681-7926				ARMSTRONG M, 1996, P 4 EUR C COMP VIS C, P3; BOYER E, 2006, P AS C COMP VIS, V1, P1; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; FAUGERAS O, 1998, P 5 EUR C COMP VIS F, P36; FITZGIBBON AW, 1998, P EUR WORKSH 3D STRU, P155; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hernandez C, 2007, IEEE T PATTERN ANAL, V29, P343, DOI 10.1109/TPAMI.2007.42; Jiang G, 2004, IEEE T PATTERN ANAL, V26, P721, DOI 10.1109/TPAMI.2004.4; Jiang G, 2003, IEEE T PATTERN ANAL, V25, P1343, DOI 10.1109/TPAMI.2003.1233910; Mendonca PRS, 2001, IEEE T PATTERN ANAL, V23, P604, DOI 10.1109/34.927461; NIEM W, 1994, P SOC PHOTO-OPT INS, V2182, P388, DOI 10.1117/12.171088; Sinha SN, 2004, PROC CVPR IEEE, P195; Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; Wong KYK, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P217, DOI 10.1109/ICCV.2001.937627; ZHANG G, 2006, P BRIT MACH VIS C, V1, P67; ZHANG H, 2005, P BRIT MACH VIS C, V1, P79	17	3	3	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2008	30	12					2243	2248		10.1109/TPAMI.2008.169	http://dx.doi.org/10.1109/TPAMI.2008.169			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	360CF	18988956	Green Submitted			2022-12-18	WOS:000260033900014
J	Xu, YL; Roy-Chowdhury, AK				Xu, Yilei; Roy-Chowdhury, Amit K.			Inverse compositional estimation of 3D pose and lighting in dynamic scenes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						inverse composition; tracking; 3D pose; illumination	ILLUMINATION; TRACKING; MODELS	In this paper, we show how we can estimate, accurately and efficiently, the 3D motion of a rigid object and time-varying lighting in a dynamic scene. This is achieved in an inverse compositional tracking framework with a novel warping function that involves a 2D --> 3D --> 2D transformation. This also allows us to extend traditional two-frame inverse compositional tracking to a sequence of frames, leading to even higher computational savings. We prove the theoretical convergence of this method and show that it leads to significant reduction in computational burden. Experimental analysis on multiple video sequences shows impressive speedup over existing methods while retaining a high level of accuracy.	[Xu, Yilei; Roy-Chowdhury, Amit K.] Univ Calif Riverside, Dept Elect Engn, Riverside, CA 92521 USA	University of California System; University of California Riverside	Xu, YL (corresponding author), Univ Calif Riverside, Dept Elect Engn, Riverside, CA 92521 USA.	yxu@ee.ucr.edu; amitrc@ee.ucr.edu	Xu, Yilei/F-5095-2012	Roy-Chowdhury, Amit/0000-0001-6690-9725				Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; BARTOLI A, 2006, P 7 BRIT MACH VIS C; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; EISERT P, 1996, 3D IMAGE ANAL SYNTHE, P61; Finlayson G. D., 2002, REMOVING SHADOWS IMA; Freedman D., 2005, P IEEE C COMP VIS PA; GOUIFFES M, 2006, P 9 EUR C COMP VIS M; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; JIN H, 2001, P 8 IEEE INT C COMP; KALE A, 2006, P IEEE CVPR, P602; LATHAUWER LD, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI DOI 10.1137/S0895479896305696; LUCAS BD, 1981, P DARPA IM UND WORKS; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; PIZARRO D, 2007, P 15 SCAND C IM AN J; Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204; RAMAMOORTHI R, 2001, J OPTICAL SOC AM, V18; ROMDHANI S, 2003, P 10 IEEE INT C COMP; Shi J, 1994, P IEEE C COMP VIS PA; SHUM HY, 2000, INT J COMPUT VISION, V16, P63; Szeliski R., 1994, Journal of Visual Communication and Image Representation, V5, P10, DOI 10.1006/jvci.1994.1002; Xu YL, 2007, IEEE T PATTERN ANAL, V29, P793, DOI 10.1109/TPAMI.2007.1047	22	3	3	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2008	30	7					1300	1307		10.1109/TPAMI.2008.81	http://dx.doi.org/10.1109/TPAMI.2008.81			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	307CA	18550911				2022-12-18	WOS:000256294100015
J	Kuncheva, LI; Hoare, ZSJ				Kuncheva, Ludmila I.; Hoare, Zoe S. J.			Error-dependency relationships for the Naive Bayes classifier with binary features	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						pattern recognition; Naive Bayes classifier; dependency between features; classification error; Q statistic	OPTIMALITY	We derive a tight dependency-related bound on the difference between the NB error and Bayes error for the case of two binary features and two classes. A measure of feature dependency is proposed for multiple features. Simulations and experiments with 23 real data sets were carried out.	[Kuncheva, Ludmila I.] Bangor Univ, Sch Comp Sci, Bangor LL57 1UT, Gwynedd, Wales; [Hoare, Zoe S. J.] KSS Ltd, Manchester M1 6SS, Lancs, England	Bangor University	Kuncheva, LI (corresponding author), Bangor Univ, Sch Comp Sci, Dean St, Bangor LL57 1UT, Gwynedd, Wales.	l.i.kuncheva@bangor.ac.uk; hoarez@kssg.com	Kuncheva, Ludmila/J-4357-2014	Kuncheva, Ludmila/0000-0002-0415-6964				Blake C. L., UCI REPOSITORY MACHI; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Hand DJ, 2001, INT STAT REV, V69, P385, DOI 10.1111/j.1751-5823.2001.tb00465.x; Kohavi R., 1997, IMPROVING SIMPLE BAY; Kohavi R, 1996, P 2 INT C KNOWL DISC; KONONENKO I, 1993, APPL ARTIF INTELL, V7, P317, DOI 10.1080/08839519308949993; Kuncheva LI, 2006, PATTERN RECOGN LETT, V27, P830, DOI 10.1016/j.patrec.2005.12.001; LANGLEY P, 1992, P 10 NAT C ART INT, P00399; LANGLEY P, 1994, P 10 C UNC ART INT, P399, DOI DOI 10.1016/B978-1-55860-332-5.50055-9; Ripley BD., 1996; Rish I., 2001, RC21993 IBM TJ WATS; RISH I, 2001, EMPIRICAL METHODS A; Webb GI, 2005, MACH LEARN, V58, P5, DOI 10.1007/s10994-005-4258-6; Yule GU., 1940, INTRO THEORY STAT, V12; Zhang H., 2004, P 17 INT FLAIRS C; ZHANG H, 2003, P CAN C ART INT, P591	17	3	3	1	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2008	30	4					735	740		10.1109/TPAMI.2007.70845	http://dx.doi.org/10.1109/TPAMI.2007.70845			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	262FY	18276977				2022-12-18	WOS:000253135600015
J	Enderli, C; Savy, L; Refregier, P				Enderli, Cyrille; Savy, Laurent; Refregier, Philippe			Application of the deflection criterion to classification of radar SAR images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						classification; deflection; likelihood ratio approximation; Fisher ratio; radar	BHATTACHARYYA DISTANCE	An original application of the weighted deflection is proposed for radar target classification by quadratic filters. An explicit formulation of optimal filters is derived. We analyze the impact of the weighting parameter on real data recognition and show that performances are better when the deflection coincides with the Fisher ratio.	Thales Airborne Syst, F-78852 Elancourt, France; Off Natl Etud & Rech Aerosp, FR-91761 Palaiseau, France; DU St Jerome, Inst Fresnel, F-13397 Marseille, France	Thales Group; National Office for Aerospace Studies & Research (ONERA); UDICE-French Research Universities; Universite Paris Saclay; UDICE-French Research Universities; Aix-Marseille Universite	Enderli, C (corresponding author), Thales Airborne Syst, 2 Ave Gay Lussac, F-78852 Elancourt, France.	cyrille-jean.enderli@fr.thalesgroup.com; Laurent.Savy@onera.fr; philippe.refregier@ec-marseille.fr						Cover T. M., 2006, ELEMENTS INFORM THEO, V2; DEVORE MD, 2000, P SPIE; DEVORE MD, 1999, IEEE T AEROSPACE ELE, V12; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; GANDHI PP, 1988, IEEE T AERO ELEC SYS, V24, P427, DOI 10.1109/7.7185; GARDNER WA, 1980, IEEE T COMMUN, V28, P807, DOI 10.1109/TCOM.1980.1094735; Gopinath RA, 1998, INT CONF ACOUST SPEE, P661, DOI 10.1109/ICASSP.1998.675351; Goudail F, 2004, J OPT SOC AM A, V21, P1231, DOI 10.1364/JOSAA.21.001231; Liao SX, 1998, IEEE T PATTERN ANAL, V20, P1358, DOI 10.1109/34.735809; LIAO SX, 1993, THESIS U MANITOBA CA; LIN Y, 1999, 1014 U WISC DEP STAT; Mangasaian OL, 2001, J MACH LEARN RES, V1, P161, DOI 10.1162/15324430152748218; OSULLIVAN JA, 1999, P 33 ANN C INF SCI S; OSULLIVAN JA, 2000, P SPIE; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Shikh-Bahaei MR, 2000, IEICE T COMMUN, VE83B, P1619; Tou JT, 1974, PATTERN RECOGN	18	3	3	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2007	29	9					1668	U2		10.1109/TPAMI.2007.1121	http://dx.doi.org/10.1109/TPAMI.2007.1121			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	189CD	17627054				2022-12-18	WOS:000247965600016
J	Liu, XF; Kanungo, T; Haralick, RM				Liu, XF; Kanungo, T; Haralick, RM			On the use of error propagation for statistical validation of computer vision software	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						statistical analysis; multivariate hypothesis testing; 3D parameter estimation; error propagation; software validation; software engineering		Computer vision software is complex, involving many tens of thousands of lines of code. Coding mistakes are not uncommon. When the vision algorithms are run on controlled data which meet all the algorithm assumptions, the results are often statistically predictable. This renders it possible to statistically validate the computer vision software and its associated theoretical derivations. In this paper, we review the general theory for some relevant kinds of statistical testing and then illustrate this experimental methodology to validate our building parameter estimation software. This software estimates the 3D positions of buildings vertices based on the input data obtained from multi-image photogrammetric resection calculations and 3D geometric information relating some of the points, lines and planes of the buildings to each other.	Cisco Syst, Parsippany, NJ 07054 USA; IBM Corp, Almaden Res Ctr, San Jose, CA 95120 USA; CUNY, Grad Ctr, Dept Comp Sci, New York, NY 10016 USA	Cisco Systems Inc; International Business Machines (IBM); City University of New York (CUNY) System	Liu, XF (corresponding author), Cisco Syst, 600 Lanidex Pl, Parsippany, NJ 07054 USA.	xuliu@cisco.com; kanungo@us.ibm.com; haralick@netscape.net	Haralick, Robert/AAW-5151-2020	manickam, vijayabhama.M/0000-0001-9437-9477				ALTMAN P, 2002, PS POLITICAL SCI POL, V24, P681; Anderson T. W, 1984, INTRO MULTIVARIATE S; Arnold S.F., 1990, MATH STAT; Bertsekas D.P., 2019, REINFORCEMENT LEARNI; BROMILEY PD, 2004, P MED IM AN C; Casella G., 1990, STAT INFERENCE; CHEN A, 2003, COMPUTATION STAT, V18, P107; Cho K, 1997, IEEE T PATTERN ANAL, V19, P1185, DOI 10.1109/34.632979; CHOWDHURY AR, 2003, P IEEE WORKSH STAT A; Courtney P, 2001, ADV COMP THE PRACT, V9, P109; CROSSLEY S, 1998, P BRIT MACH VIS C, P346; Efron B., 1994, MONOGR STAT APPL PRO, DOI DOI 10.1007/978-1-4899-4541-9; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Fletcher R, 1987, PRACTICAL METHODS OP, V1; GHEZZI J, 2002, FUNDAMENTALS SOFTWAR; GILL PE, 1981, PRACTICAL OPTIMIZTIO; HARALICK RM, 1994, INT C PATT RECOG, P493, DOI 10.1109/ICPR.1994.576335; HUDSON DA, 1990, THESIS U WASHINGTON; KANATANI K, 1996, STAT OPITMIZATION GE; KANUNGO T, 1995, IEEE T IMAGE PROCESS, V4, P1667, DOI 10.1109/83.475516; KANUNGO T, 1995, ISLTR9505 U WASH; Koch K.R., 1987, PARAMETER ESTIMATION; LIU X, 1996, P 1996 DARPA IM UND; LIU X, 1994, P 1994 DARPA IM UND, P1017; LIU X, 1995, THESIS U WASHINGTON; LIU X, 1996, P 1996 DARPA IM UND, P1533; McCullough BD, 1999, AM STAT, V53, P149, DOI 10.2307/2685736; McCullough BD, 1998, AM STAT, V52, P358, DOI 10.2307/2685442; Press W. H., 1990, NUMERICAL RECIPES C; WILKINSON L, 1994, COMPUTATIONAL STAT C	30	3	3	1	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2005	27	10					1603	1614		10.1109/TPAMI.2005.203	http://dx.doi.org/10.1109/TPAMI.2005.203			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	953OM	16237995				2022-12-18	WOS:000231086700008
J	Kim, SW; Oommen, BJ				Kim, SW; Oommen, BJ			On using prototype reduction schemes and classifier fusion strategies to optimize kernel-based nonlinear subspace methods	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article; Proceedings Paper	16th Australian Conference on Artificial Intelligence	DEC 03-05, 2003	UNIV WESTERN AUSTRALIA, PERTH, AUSTRALIA	Natl Comm Artificial Intellignece & Expert Syst	UNIV WESTERN AUSTRALIA	Kernel Principal Component Analysis (kPCA); kernel-based nonlinear subspace (KNS) method; prototype reduction schemes (PRS); classifier fusion strategies (CFS)	COMPONENT ANALYSIS; ALGORITHMS	In Kernel-based Nonlinear Subspace (KNS) methods, the length of the projections onto the principal component directions in the feature space, is computed using a kernel matrix, K, whose dimension is equivalent to the number of sample data points. Clearly this is problematic, especially, for large data sets. In this paper, we solve this problem by subdividing the data into smaller subsets, and utilizing a Prototype Reduction Scheme (PRS) as a preprocessing module, to yield more refined representative prototypes. Thereafter, a Classifier Fusion Strategy (CFS) is invoked as a postprocessing module, to combine the individual KNS classification results to derive a consensus decision. Essentially, the PRS is used to yield computational advantage, and the CFS, in turn, is used to compensate for the decreased efficiency caused by the data set division. Our experimental results demonstrate that the proposed mechanism significantly reduces the prototype extraction time as well as the computation time without sacrificing the classification accuracy. The results especially demonstrate a significant computational advantage for large data sets within a parallel processing philosophy.	Myongji Univ, Dept Comp Sci & Engn, Yongin 449728, South Korea; Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada	Myongji University; Carleton University	Kim, SW (corresponding author), Myongji Univ, Dept Comp Sci & Engn, Yongin 449728, South Korea.	kimsw@mju.ac.kr; oommen@scs.carleton.ca	Oommen, B. John/P-6323-2017	Oommen, B. John/0000-0002-5105-1575				Achlioptas D., 2001, P 33 ANN ACM S THEOR, P611; Bezdek JC, 2001, INT J INTELL SYST, V16, P1445, DOI 10.1002/int.1068; Dasarathy B.V., 1991, NEAREST NEIGHBOR NN; HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155; Kim SW, 2004, IEEE T SYST MAN CY B, V34, P1384, DOI 10.1109/TSMCB.2004.824524; Kim SW, 2003, PATTERN ANAL APPL, V6, P232, DOI 10.1007/s10044-003-0191-0; Kim SW, 2003, PATTERN RECOGN, V36, P1083, DOI 10.1016/S0031-3203(02)00115-2; Kim SW, 2004, PATTERN RECOGN, V37, P227, DOI 10.1016/j.patcog.2003.07.006; Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881; Kuncheva LI, 2002, IEEE T PATTERN ANAL, V24, P281, DOI 10.1109/34.982906; Kuncheva LI, 2001, PATTERN RECOGN, V34, P299, DOI 10.1016/S0031-3203(99)00223-X; MAEDA E, 1999, P IEEE INT C AC SPEE; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; OJA E, 1983, SUBSPACE METHODS PAT; SAKANO H, 2001, IEICE T INFORM J D 2, V84, P1549; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Tipping ME, 2001, ADV NEUR IN, V13, P633; TSUDA K, 1999, IEICE T INF SYST, V82, P592; WILLIAMS CKI, 2001, ADV NEURAL INFORMATI, V13	21	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2005	27	3												6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	887IW					2022-12-18	WOS:000226300200013
J	Gluckman, J				Gluckman, J			Visually distinct patterns with matching subband statistics	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						texture; statistical models; feature representation; moments		A commonly used representation of a visual pattern is a statistical distribution measured from the output of a bank of filters (Gaussian, Laplacian, Gabor, etc.). Both marginal and joint distributions of filter responses have been advocated and effectively used for a variety of vision tasks, including texture classification, texture synthesis, object detection, and image retrieval. This paper examines the ability of these representations to discriminate between an arbitrary pair of visual stimuli. Examples of patterns are derived that provably possess the same marginal and joint statistical properties, yet are "visually distinct." This is accomplished by showing sufficient conditions for matching the first k moments of the marginal distributions of a pair of images. Then, given a set of filters, we show how to match the marginal statistics of the subband images formed through convolution with the filter set. Next, joint statistics are examined and images with similar joint distributions of subband responses are shown. Finally, distinct periodic patterns are derived that possess approximately the same subband statistics for any arbitrary filter set.	Polytech Univ, Dept Comp Sci, Brooklyn, NY 11201 USA	New York University; Polytechnic University Puerto Rico	Gluckman, J (corresponding author), Polytech Univ, Dept Comp Sci, 6 Metrotech Ctr, Brooklyn, NY 11201 USA.	jgluckma@poly.edu						[Anonymous], P IEEE C COMP VIS PA; BERGEN JR, 1991, SPATIAL VISION; BOVIC A, 1990, IEEE T PATTERN ANAL, V12; CAELLI T, 1978, BIOL CYBERN, V28, P167, DOI 10.1007/BF00337138; Candes EJ, 1999, PHILOS T R SOC A, V357, P2495, DOI 10.1098/rsta.1999.0444; Chubb C., 1991, COMPUTATIONAL MODELS; DANA KJ, 1998, P IEEE C COMP VIS PA; DEBONET JS, 1997, ADV NEURAL INFORMATI, V10; FAUGERAS OD, 1980, IEEE T PATTERN ANAL, V2, P323, DOI 10.1109/TPAMI.1980.4767031; GILBERT EN, 1980, SIAM J ALGEBRA DISCR, V1, P152, DOI 10.1137/0601018; GRAHAM N, 1992, VISION RES, V32, P719, DOI 10.1016/0042-6989(92)90188-O; HADJIDEMETRIOU E, 2000, P IEEE C COMP VIS PA; HADJIDEMETRIOU E, 2001, P IEEE C COMP VIS PA; HEEGER D, 1995, P SIGGRAPH C 95; Huang J., 1999, P IEEE C COMP VIS PA; JULESZ B, 1978, BIOL CYBERNETICS, V31; Julesz B., 1962, IRE T INFORM THEOR, V8, P84, DOI 10.1109/TIT.1962.1057698; KONISHI S, 2000, P IEEE C COMP VIS PA; LEUNG T, 1999, INT J COMPUTER V DEC; LIU X, 2003, IEEE T IMAGE PROCESS, V12; MALLAT S, 1989, IEEE T PATTERN A JUL; Mandal MK, 1996, IEEE T CONSUM ELECTR, V42, P557, DOI 10.1109/30.536156; Meyer P.L., 1970, INTRO PROBABILITY ST; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261; RUDERMAN DL, 1994, NETWORK; SCHIELE B, 2000, INT J COMPUTER VISIO, V36; Schmoldt DL, 2001, MANAG FOR ECOSYST, V3, P1; Schneiderman H., 2000, P IEEE C COMP VIS PA; Schofield AJ, 2003, VISION RES, V43, P243, DOI 10.1016/S0042-6989(02)00542-4; SILVERMAN M, 1989, P NATL ACAD SCI US; SIMONCELLI EP, 1996, INT C IM PROC; Srivastava A, 2002, IEEE T PATTERN ANAL, V24, P1200, DOI 10.1109/TPAMI.2002.1033212; Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI 10.1117/12.408568; SUTTER A, 1995, VISION RES, V35; Varma M., 2002, P EUR C COMP VIS; YELLOTT JI, 1993, J OPT SOC AM A, V10, P777, DOI 10.1364/JOSAA.10.000777; ZETZSCHE C, 1997, NAT SCEN STAT M; Zhu SC, 2000, IEEE T PATTERN ANAL, V22, P554, DOI 10.1109/34.862195; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420	40	3	6	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2005	27	2					252	264		10.1109/TPAMI.2005.42	http://dx.doi.org/10.1109/TPAMI.2005.42			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	879AR	15688562				2022-12-18	WOS:000225689300008
J	Navab, N; Genc, Y; Appel, M				Navab, N; Genc, Y; Appel, M			Lines in one orthographic and two perspective views	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						motion from line correspondences; perspective and orthographic cameras; trifocal tensor; camera calibration	CORRESPONDENCES; MOTION; CAMERAS	We introduce a linear algorithm to recover the Euclidean motion between an orthographic and two perspective cameras from straight line correspondences filling the gap in the analysis of motion estimation from line correspondences for various projection models. The general relationship between lines in three views is described by the trifocal tensor. Euclidean structure from motion for three perspective views is a special case in which the relationship is defined by a collection of three matrices. Here, we describe the case of two calibrated perspective views and an orthographic view. Similar to the other cases, our linear algorithm requires 13 or more line correspondences to recover 27 coefficients of the trifocal tensor.	Siemens Corp Res, Princeton, NJ 08540 USA; Framatome ANP, D-91058 Erlangen, Germany	Siemens AG; Framatome	Navab, N (corresponding author), Siemens Corp Res, 755 Coll Rd E, Princeton, NJ 08540 USA.	Nassir.Navab@scr.siemens.com; Yakup.Genc@scr.siemens.com; Mirko.Appel@framatome-anp.de	Genc, Yakup/AAG-4668-2019	Genc, Yakup/0000-0002-6952-6735				ASTROM K, 1999, ICCV, P285; Hartley R. I., 1993, P DARPA IM UND WORKS, P745; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P882, DOI 10.1109/ICCV.1995.466843; Holt RJ, 1997, INT J IMAG SYST TECH, V8, P301, DOI 10.1002/(SICI)1098-1098(1997)8:3<301::AID-IMA8>3.0.CO;2-E; LIU YC, 1988, COMPUT VISION GRAPH, V44, P35, DOI 10.1016/S0734-189X(88)80030-6; LIU YC, 1990, IEEE T PATTERN ANAL, V12, P28, DOI 10.1109/34.41381; Marugame A, 1999, IEEE T PATTERN ANAL, V21, P628, DOI 10.1109/34.777373; Navab N, 1997, INT J COMPUT VISION, V23, P17, DOI 10.1023/A:1007911807871; Navab N, 2000, PROC CVPR IEEE, P607, DOI 10.1109/CVPR.2000.854928; NAVAB N, 1999, P IEEE INT WORKSH AU; Quan L, 1997, IEEE T PATTERN ANAL, V19, P834, DOI 10.1109/34.608285; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; Steger C, 1998, IEEE T PATTERN ANAL, V20, P113, DOI 10.1109/34.659930; TAYLOR J, 1995, STRENGTH COND, V17, P10; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327; ZHANG Z, 1999, P INT C COMP VIS, P680; ZHANG ZY, 1995, IEEE T PATTERN ANAL, V17, P1129	17	3	3	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2003	25	7					912	917		10.1109/TPAMI.2003.1206519	http://dx.doi.org/10.1109/TPAMI.2003.1206519			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	692NN					2022-12-18	WOS:000183667300012
J	Granger, E; Savaria, Y; Lavoie, P				Granger, E; Savaria, Y; Lavoie, P			A pattern reordering approach based on ambiguity detection for online category learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						ambiguity; online category learning; partitional clustering; pattern recognition; reject option	ADAPTIVE RESONANCE; FUZZY ART; QUANTIZATION	Pattern reordering is proposed as an alternative to sequential and batch processing for online category learning. Upon detecting that the categorization of a new input pattern is ambiguous, the input is postponed for a predefined time, after which it is reexamined and categorized for good. This approach is shown to improve the categorization performance over purely sequential processing, while yielding a shorter input response time, or latency, than batch processing. In order to examine the response time of processing schemes, the latency of a typical implementation is derived and compared to lower bounds. Gaussian and softmax models are derived from reject option theory and are considered for detecting ambiguity and triggering pattern postponement. The average latency and Rand Adjusted clustering score of reordered, sequential, and batch processing are compared through computer simulation using two unsupervised competitive learning neural networks and a radar pulse data set.	Mitel Networks, Integrated Syst Grp, Ottawa, ON K1N 7T8, Canada; Def R&D Canada Ottawa, Dept Natl Def, Ottawa, ON K1A 0Z4, Canada; Ecole Polytech, Dept Elect & Comp Engn, Montreal, PQ H3C 3A7, Canada	Mitel Networks Corporation; Universite de Montreal; Polytechnique Montreal	Granger, E (corresponding author), Mitel Networks, Integrated Syst Grp, 98 Sweetland Ave, Ottawa, ON K1N 7T8, Canada.	eric.granger@rogers.com; savaria@vlsi.polymtl.ca; pierre.lavoie@drdc-rddc.gc.ca						AHALT SC, 1990, NEURAL NETWORKS, V3, P277, DOI 10.1016/0893-6080(90)90071-R; Anderberg MR, 1973, CLUSTER ANAL APPL; BRIDLE JS, 1989, NATO ASI F, V68, P227, DOI DOI 10.1007/978-3-642-76153-9_; CARPENTER GA, 1991, NEURAL NETWORKS, V4, P759, DOI 10.1016/0893-6080(91)90056-B; CARPENTER GA, 1987, COMPUT VISION GRAPH, V37, P54, DOI 10.1016/S0734-189X(87)80014-2; CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; DAVIES CL, 1982, P IEEE F, V129, P164; DUBUISSON B, 1993, PATTERN RECOGN, V26, P155, DOI 10.1016/0031-3203(93)90097-G; Duda R.O., 1973, J ROYAL STAT SOC SER; Frank T, 1998, IEEE T NEURAL NETWOR, V9, P544, DOI 10.1109/72.668896; FUKUNAGA K, 1972, IEEE T INFORMATI NOV, P814; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; GERSHO A, 1982, IEEE T INFORM THEORY, V28, P157, DOI 10.1109/TIT.1982.1056457; Granger E, 1998, SIGNAL PROCESS, V64, P249, DOI 10.1016/S0165-1684(97)00194-1; GRANGER E, 2001, EPMRT0102 DEP GEN EL, P40; Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229; GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x; GROSSBERG S, 1976, BIOL CYBERN, V23, P121, DOI 10.1007/BF00344744; Hartigan J.A., 1975, CLUSTERING ALGORITHM; HELLMAN ME, 1970, IEEE T SYST SCI CYB, VSSC6, P179, DOI 10.1109/TSSC.1970.300339; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Jain A. K., 1988, ALGORITHMS CLUSTERIN; Kohonen T., 1989, SELF ORG ASSOCIATIVE, V3rd; LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; RUMELHART DE, 1985, COGNITIVE SCI, V9, P75, DOI 10.1207/s15516709cog0901_5	28	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2003	25	4					524	528						5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	659BV					2022-12-18	WOS:000181758100014
J	Paragios, N; Deriche, R				Paragios, N; Deriche, R			Geodesic active contours and level sets for the detection and tracking of moving objects (vol 22, pg 279, 2000)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction													Deriche, Rachid/AAM-9869-2021	Deriche, Rachid/0000-0002-4643-8417				Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758	1	3	3	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2000	22	4					415	415		10.1109/TPAMI.2000.845385	http://dx.doi.org/10.1109/TPAMI.2000.845385			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	317WT					2022-12-18	WOS:000087250500012
J	Hoover, A; Goldgof, D; Bowyer, KW				Hoover, A; Goldgof, D; Bowyer, KW			Dynamic-scale model construction from range imagery	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						range image processing; 3D shape; model construction	OBJECTS; VIEWS; SEGMENTATION	The construction of a surface model from range data may be undertaken at any point in a continuum of scales that reflects the level of detail of the resulting model. This continuum relates the construction parameters to the scale of the model. We propose methods to dynamically reprocess range data at different scales. The construction result from a single scale is automatically evaluated, causing reconstruction at a different scale when user-defined criteria are not met. We demonstrate our methods in constructing a planar b-rep space envelope (a scene representation) for over 400 range images. The experiments demonstrate the ability to construct 100 percent valid models, with the scale of detail within specified requirements.	Univ Calif San Diego, Dept Elect Engn, Visual Comp Lab, La Jolla, CA 92093 USA; Univ Calif San Diego, Dept Comp Engn, La Jolla, CA 92093 USA; Univ S Florida, Dept Comp Sci & Engn, Tampa, FL 33620 USA	University of California System; University of California San Diego; University of California System; University of California San Diego; State University System of Florida; University of South Florida	Hoover, A (corresponding author), Univ Calif San Diego, Dept Elect Engn, Visual Comp Lab, La Jolla, CA 92093 USA.	hoover@vision.ucsd.edu; goldgof@csee.usf.edu; kwb@csee.usf.edu	Goldgof, Dmitry/ABF-1366-2020	Bowyer, Kevin/0000-0002-7562-4390				Ballard D.H., 1982, COMPUTER VISION; BERGEVIN R, 1995, COMPUT VIS IMAGE UND, V61, P1, DOI 10.1006/cviu.1995.1001; BHANDARKAR SM, 1992, PATTERN RECOGN, V25, P947, DOI 10.1016/0031-3203(92)90060-V; BLAIS G, 1995, IEEE T PATTERN ANAL, V17, P820, DOI 10.1109/34.400574; Blake A., 1992, ACTIVE VISION; CHEN Y, 1995, COMPUT VIS IMAGE UND, V61, P325, DOI 10.1006/cviu.1995.1026; CHEN Z, 1993, PATTERN RECOGN, V26, P33, DOI 10.1016/0031-3203(93)90086-C; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; DAVIS HF, 1991, INTRO VECTOR ANAL; FERRIE FP, 1993, IEEE T PATTERN ANAL, V15, P771, DOI 10.1109/34.236252; FOLEY JD, 1991, COMPUTER GRAPHICS PR; HIGUCHI K, 1995, GRAPH MODEL IM PROC, V57, P315, DOI 10.1006/gmip.1995.1028; HOOPE H, 1992, P SIGGRAPH 92, P71; HOOVER A, 1995, IEEE T PATTERN ANAL, V17, P920, DOI 10.1109/34.406660; Hoover A, 1996, IEEE T PATTERN ANAL, V18, P673, DOI 10.1109/34.506791; Hoover A, 1998, COMPUT VIS IMAGE UND, V69, P310, DOI 10.1006/cviu.1998.0666; Hoover A., 1996, THESIS U S FLORIDA; Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216; Lindeberg T., 1994, SCALE SPACE THEORY C; Mantyla M., 1988, INTRODUCTION; MULGAONKAR PG, 1992, IEEE T PATTERN ANAL, V14, P303, DOI 10.1109/34.121798; Parvin B, 1996, INT J COMPUT VISION, V20, P81, DOI 10.1007/BF00144118; Requicha A. G., 1980, ACM COMPUT SURV, P437; Roberts L, 1965, MACHINE PERCEPTION 3; Roth G, 1997, PROC GRAPH INTERF, P173; RUTISHAUSER M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P573, DOI 10.1109/CVPR.1994.323797; SOUCY M, 1995, MACH VISION APPL, V8, P53, DOI 10.1007/BF01213638; STENSTROM JR, 1992, INT J COMPUT VISION, V9, P185, DOI 10.1007/BF00133701	28	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	1998	20	12					1352	1357		10.1109/34.735808	http://dx.doi.org/10.1109/34.735808			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	147QV					2022-12-18	WOS:000077578300007
J	Jackway, PT				Jackway, PT			On the scale-space theorem of Chen and Yan	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						scale-space filtering; mathematical morphology; morphological opening; zero crossings; monotone theorems		In an earlier paper, Chen and Yan presented a theorem concerning zero crossings of boundary curvature under morphological openings. In this correspondence, we show by means of a counterexample a problem with this theorem and suggest how the theorem may be modified to make it correct.	Univ Queensland, Dept Comp Sci & Elect Engn, Cooperat Res Ctr Sensor Signal & Informat Proc, Brisbane, Qld 4072, Australia	University of Queensland	Jackway, PT (corresponding author), Univ Queensland, Dept Comp Sci & Elect Engn, Cooperat Res Ctr Sensor Signal & Informat Proc, Brisbane, Qld 4072, Australia.		Jackway, Paul T/A-3636-2009	Jackway, Paul T/0000-0002-5848-4832				CHEN MH, 1989, IEEE T PATTERN ANAL, V11, P694, DOI 10.1109/34.192464; JACKWAY PT, 1994, THESIS QUEENSLAND U; JANG BK, 1991, P 25 ANN C INF SCI S, P1; NACKEN PFM, 1994, IEEE T PATTERN ANAL, V16, P656, DOI 10.1109/34.295918; Serra J, 1982, IMAGE ANAL MATH MORP; VANDENBOOMGAARD R, 1994, IEEE T PATTERN ANAL, V16, P1101, DOI 10.1109/34.334389; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729	7	3	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1998	20	3					351	352		10.1109/34.667893	http://dx.doi.org/10.1109/34.667893			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZH156					2022-12-18	WOS:000073078400013
J	Hu, S; Lehner, PE				Hu, S; Lehner, PE			Multipurpose strategic planning in the game of Go	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						computer Go; adversarial planning; automated planning; artificial intelligence		A heuristic method for adversarial planning is developed to address the problem of multipurpose planning in the game of Go. Static analysis and dynamic look ahead on both strategic and tactical levels are used to generate possible goals and to identify interactions among the achievability of various goals. Strategic, multipurpose goals are composed of sets of interacting goals.	MITRE CORP,MCLEAN,VA 22101	MITRE Corporation	Hu, S (corresponding author), HUGHES INFORMAT TECHNOL SYST,1616 MCCORMIC DR,UPPER MARLBORO,MD 20774, USA.							BENSON DB, 1976, INFORM SCIENCES, V10, P17, DOI 10.1016/0020-0255(76)90059-1; CHEN K, 1990, COMPUTER CHESS COGNI, P271; Chen K., 1989, HEURISTIC PROGRAMMIN, P195; FRIEDENBACH KJ, 1980, THESIS U CALIFORNIA; HU S, 1995, THESIS G MASON U; KAGEYAMA T, 1978, LESSONS FUNDAMENTALS; KIERULF A, 1990, COMM ACM         FEB, P152; LEHNER PE, 1990, ADV TECHNOLOGY COMMA; LEHNER PE, 1983, COMPUTER GAME PLAYIN; LEVY D, 1988, COMPUTER GAME, V1; LEVY D, 1988, COMPUTER GAME, V2; MIURA Y, 1995, GO ASIAN PARADIGM BU; POPMA R, 1992, HEURISTIC PROGRAMMIN, V3; REITMAN W, 1979, P INT JOINT C ART IN, P711; RYDER JL, 1971, THESIS STANFORD U, P162; Wilcox B., 1979, AM GO J, V14; WILCOX B, 1978, AM GO J, V13; WILCOX B, 1984, AM GO J, V19	18	3	3	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1997	19	9					1048	1051		10.1109/34.615454	http://dx.doi.org/10.1109/34.615454			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XX985					2022-12-18	WOS:A1997XX98500011
J	PARVIN, BA; PENG, C; JOHNSTON, W; MAESTRE, FM				PARVIN, BA; PENG, C; JOHNSTON, W; MAESTRE, FM			TRACKING OF TUBULAR MOLECULES FOR SCIENTIFIC APPLICATIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter							FLUORESCENCE MICROSCOPY; DYNAMICS; MODEL	In this paper, we present a system for detection and tracking of tubular molecules in images. The automatic detection and characterization of the shape, location, and motion of these molecules can enable new laboratory protocols in several scientific disciplines. The uniqueness of the proposed system is twofold: At the macro level, the novelty of the system lies in the integration of object localization and tracking using geometric properties; at the micro level, in the use of high and low level constraints to model the detection and tracking subsystem. The underlying philosophy for object detection is to extract perceptually significant features from the pixel level image, and then use these high level cues to refine the precise boundaries. In the case of tubular molecules, the perceptually significant features are antiparallel line segments or, equivalently, their axis of symmetries. The axis of symmetry infers a coarse description of the object in terms of a bounding polygon. The polygon then provides the necessary boundary condition for the refine ment process, which is based on dynamic programming. For tracking the object in a time sequence of images, the refined contour is then projected onto each consecutive frame.			PARVIN, BA (corresponding author), UNIV CALIF BERKELEY, LAWRENCE BERKELEY LAB, DIV INFORMAT & COMP SCI, BERKELEY, CA 94720 USA.							AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681; AMINI AA, 1994, AAAI C APPLICATIONS, P126; Bellman RE, 1957, DYNAMIC PROGRAMMING; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; BUSTAMANTE C, 1991, ANNU REV BIOPHYS BIO, V20, P415, DOI 10.1146/annurev.biophys.20.1.415; BUSTAMANTE C, 1990, J BIOMOL STRUCT DYN, V8, P643, DOI 10.1080/07391102.1990.10507833; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; HUANG Q, 1993, JUN P C COMP VIS PAT, P104; HUERTAS A, 1990, COMPUT VISION GRAPH, V51, P107, DOI 10.1016/0734-189X(90)90027-S; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; LEYMARIE F, 1993, IEEE T PATTERN ANAL, V15, P617, DOI 10.1109/34.216733; NASTAR C, 1994, IEEE WORKSH BIOM IM, P61; PARVIN B, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P295, DOI 10.1109/CVPR.1994.323843; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P730, DOI 10.1109/34.85661; SONG L, 1991, J BIOMOL STRUCT DYN, V9, P87, DOI 10.1080/07391102.1991.10507895; WILLIAMS DJ, 1992, CVGIP-IMAG UNDERSTAN, V55, P14, DOI 10.1016/1049-9660(92)90003-L	17	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1995	17	8					800	805		10.1109/34.400570	http://dx.doi.org/10.1109/34.400570			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RL035					2022-12-18	WOS:A1995RL03500006
J	REDDI, S; LOIZOU, G				REDDI, S; LOIZOU, G			ANALYSIS OF CAMERA BEHAVIOR DURING TRACKING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						OPTICAL FLOW; GAZE CONTROL; ACTIVE VISION	VISION	A camera is mounted on a moving robot and can rotate, relative to the robot, about two axes. We show how the optical now field can be used to control the camera's motion to keep a target at the center of the camera's field of view, but that this is not always possible when the target lies close to the plane defined by the camera's two axes of rotation. When the target is held at the center of the camera's field of view, then the magnitude of the camera's angular velocity about one axis never exceeds the magnitude of the flow vector associated with the target, but the angular velocity about the other axis is dependent on the inverse distance of the target from this axis, and hence can became large as this distance becomes small. Situations, where the magnitudes of the camera's angular velocity and acceleration become large, are considered in the special case where the relative motion between the robot and its environment is purely translational. The tracking strategy is experimentally evaluated using computer-generated optical flow fields.			REDDI, S (corresponding author), UNIV LONDON BIRKBECK COLL,DEPT COMP SCI,MALET ST,LONDON WC1E 7HX,ENGLAND.							Abbott A. L., 1992, IEEE Control Systems Magazine, V12, P25, DOI 10.1109/37.204948; ABBOTT AL, 1993, INT J COMPUT VISION, V11, P109; ALOIMONOS J, 1987, 1ST P INT C COMP VIS, P35; ALOIMONOS JY, 1991, IMAGE VISION COMPUT, V9, P235, DOI 10.1016/0262-8856(91)90028-N; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BALLARD DH, 1987, 218 U ROCH TECHN REC; BROWN C, 1990, IEEE T SYST MAN CYB, V20, P518, DOI 10.1109/21.52563; COOMBS D, 1993, INT J COMPUT VISION, V11, P147, DOI 10.1007/BF01469226; CRAIG JJ, 1986, INTRO ROBOTICS MECHA, P11; FERMULLER C, 1993, INT J COMPUT VISION, V11, P165, DOI 10.1007/BF01469227; FRANCESCHINI N, 1992, PHILOS T R SOC B, V337, P283, DOI 10.1098/rstb.1992.0106; Horn B., 1986, ROBOT VISION, P1; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; MAYBANK SJ, 1988, THESIS BIRKBECK COLL, P16; MAYBANK SJ, 1984, 6TH P EUR C ART INT, P641; MCLAUCHLAN PF, 1992, P BIRT MACHINE VISIO, P357; PAHLAVAN K, 1992, CVGIP-IMAG UNDERSTAN, V56, P41, DOI 10.1016/1049-9660(92)90084-G; RAMSEY AS, 1944, DYNAMICS 2, P59; Raviv D., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P217, DOI 10.1109/WVM.1991.212804; REDDI S, 1993, THESIS BIRKBECK COLL; REECE DA, 1991, CMUCS91199 CARN MELL; SANDINI G, 1980, COMPUTER VISION GRAP, V14, P364; SOKOLNIKOFF IS, 1966, MATH PHYSIS MODERN E, P271; TISTARELLI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P108, DOI 10.1016/1049-9660(92)90089-L; TISTARELLI M, 1993, IEEE T PATTERN ANAL, V14, P401; VERRI A, 1986, MIT917 AI LAB MEM; Yarbus A. L., 1967, EYE MOVEMENTS VISION, P171	28	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1995	17	8					765	778		10.1109/34.400566	http://dx.doi.org/10.1109/34.400566			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RL035					2022-12-18	WOS:A1995RL03500003
J	CHHABRA, AK; GROGAN, TA				CHHABRA, AK; GROGAN, TA			ON POISSON SOLVERS AND SEMIDIRECT METHODS FOR COMPUTING AREA BASED OPTICAL-FLOW	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						OPTICAL FLOW; DIRECT METHODS; POISSON SOLVERS; ITERATIVE METHODS; CONVERGENCE ANALYSIS; EARLY VISION	EQUATIONS	Simchony, Chellappa, and Shao [1] recently proposed a semi-direct method for computing area based optical flow. Their method is based on the iterative application of a direct Poisson solver. This method is restricted to Dirichlet boundary conditions, i.e., it is applicable only when velocity vectors at the boundary of the domain are known a priori. We show, both experimentally and through analysis, that the semi-direct method converges only for very large smoothness. At such revels of smoothness, the solution is obtained merely by filling in the known boundary values; the data from the image is almost totally ignored. Next, we consider the Concus and Golub method [2], another semi-direct method, for computing optical flow. This method always converges, but the convergence is too slow to be of any practical value. We conclude that semi-direct methods are not suited for the computation of area based optical flow.	UNIV CINCINNATI,DEPT ELECT & COMP ENGN,CINCINNATI,OH 45221	University System of Ohio; University of Cincinnati								BUZBEE BL, 1970, SIAM J NUMER ANAL, V7, P627, DOI 10.1137/0707049; BUZBEE BL, 1971, SIAM J NUMER ANAL, V8, P722, DOI 10.1137/0708066; CHHABRA A, 1992, JUN P IEEE C COMP VI; CHHABRA A, 1990, 3RD P INT C COMP VIS; CHHABRA A, 1990, THESIS U CINCINNATI; CONCUS P, 1973, SIAM J NUMER ANAL, V10, P1103, DOI 10.1137/0710092; DYAKONOV Y, 1966, ZH VYCHISL MAT MAT F, V6, P12; Hildreth E., 1984, MEASUREMENT VISUAL M; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Lee D., 1989, Proceedings. Workshop on Visual Motion (IEEE Cat. No.89CH2716-9), P99, DOI 10.1109/WVM.1989.47099; PICKERING M, 1986, INTRO FAST FOURIER T; Polak E., 1971, COMPUTATIONAL METHOD; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; SWARZTRAUBER PN, 1977, SIAM REV, V19, P490, DOI 10.1137/1019071; TEMPERTON C, 1979, J COMPUT PHYS, V31, P1, DOI 10.1016/0021-9991(79)90059-7; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P129, DOI 10.1109/TPAMI.1986.4767767; Varga RS., 1999, MATRIX ITERATIVE ANA; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781	19	3	3	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1994	16	11					1133	1138		10.1109/34.334395	http://dx.doi.org/10.1109/34.334395			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PW081					2022-12-18	WOS:A1994PW08100009
J	RAGHUNATH, KJ; CHERKASSKY, V				RAGHUNATH, KJ; CHERKASSKY, V			NOISE PERFORMANCE OF LINEAR ASSOCIATIVE MEMORIES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						ASSOCIATIVE MEMORY; CORRELATION MEMORY MATRIX; GENERALIZED INVERSE; NOISE PERFORMANCE		The performance of two commonly used linear models of associative memories, generalized inverse (GI) and correlation matrix memory (CMM) is studied analytically in the presence of a new type of noise (training noise due to noisy training patterns). Theoretical expressions are determined for the SNR (signal-to-noise ratio) gain of the GI and CMM memories in the auto-associative and hetero-associative modes of operation. It is found that the GI method performance degrades significantly in the presence of training noise while the CMM method is relatively unaffected by it. The theoretical expressions are plotted and compared with the results obtained from Monte Carlo simulations and the two are found to be in excellent agreement.			RAGHUNATH, KJ (corresponding author), UNIV MINNESOTA, DEPT ELECT ENGN, 200 UNION ST SE, MINNEAPOLIS, MN 55455 USA.							CASASENT D, 1989, APPL OPTICS, V28, P272, DOI 10.1364/AO.28.000272; CHAR JM, 1988, IEEE T COMPUT, V37, P484, DOI 10.1109/12.2196; CHERKASSKY V, 1992, ENG APPL ARTIF INTEL, V5, P223, DOI 10.1016/0952-1976(92)90006-6; CHERKASSKY V, 1991, IEEE T COMPUT, V40, P1429, DOI 10.1109/12.106229; GILES CL, 1987, APPL OPTICS, V26, P4972, DOI 10.1364/AO.26.004972; HASSOUN MH, 1989, OPT ENG, V28, P46, DOI 10.1117/12.7976900; KOHONEN T, 1973, IEEE T COMPUT, VC 22, P701, DOI 10.1109/TC.1973.5009138; KOHONEN T, 1972, IEEE T COMPUT, VC 21, P353, DOI 10.1109/TC.1972.5008975; Kohonen T, 1984, SELF ORG ASS MEMORY; MAXWELL T, 1988, EVOLUTION LEARNING C, P347; Minsky M., 1969, PERCEPTRONS; MURAKAMI K, 1987, IEEE T SYST MAN CYB, V17, P699, DOI 10.1109/TSMC.1987.289364; MURAKAMI K, 1981, IEEE T PATTERN ANAL, V3, P210, DOI 10.1109/TPAMI.1981.4767083; MURAKAMI K, 1989, IEEE T SYST MAN CYB, V19, P1230, DOI 10.1109/21.44041; NAKANO K, 1972, IEEE T SYST MAN CYB, VSMC2, P380, DOI 10.1109/TSMC.1972.4309133; OLIVIER PD, 1988, IEEE T SYST MAN CYB, V18, P814, DOI 10.1109/21.21607; Pao Y.H., 1989, ADAPTIVE PATTERN REC; RAGHUNATH KJ, 1992, IEEE T SIGNAL PROCES, V40, P2726, DOI 10.1109/78.165659; STILES GS, 1985, IEEE T PATTERN ANAL, V7, P358, DOI 10.1109/TPAMI.1985.4767667; STILES GS, 1987, IEEE T COMPUT, V36, P257, DOI 10.1109/TC.1987.1676898; TELFER B, 1990, APPL OPTICS, V29, P1191, DOI 10.1364/AO.29.001191; ZHANG SW, 1992, NEURAL NETWORKS, V5, P163, DOI 10.1016/S0893-6080(05)80015-2	22	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1994	16	7					757	764		10.1109/34.297959	http://dx.doi.org/10.1109/34.297959			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NY134					2022-12-18	WOS:A1994NY13400011
J	MAST, M; KUMMERT, F; EHRLICH, U; FINK, GA; KUHN, T; NIEMANN, H; SAGERER, G				MAST, M; KUMMERT, F; EHRLICH, U; FINK, GA; KUHN, T; NIEMANN, H; SAGERER, G			A SPEECH UNDERSTANDING AND DIALOG SYSTEM WITH A HOMOGENEOUS LINGUISTIC KNOWLEDGE-BASE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						SPEECH UNDERSTANDING; DIALOG SYSTEM; DIALOG MODEL; SYNTACTIC AND SEMANTIC ANALYSIS; SEMANTIC NETWORK; PROBLEM-INDEPENDENT CONTROL		This article presents the speech understanding and dialog system EVAR. All levels of linguistic knowledge are used both to control the analysis process and for the interpretation of an utterance. All kinds of knowledge are integrated in a homogeneous knowledge base. The control algorithm used for the analysis is defined within the representation scheme and does not depend on the application. One of the aims of EVAR is to develop a system structure where linguistic and nonlinguistic expectations could be used not only for the interpretation but also as predictions for the recognition process.	UNIV BIELEFELD,AG ANGEW INFORMAT,FAK TECH,W-4800 BIELEFELD,GERMANY	University of Bielefeld	MAST, M (corresponding author), UNIV ERLANGEN NURNBERG,LEHRSTUHL INFORMAT 5,MARTENSSTR 3,W-8520 ERLANGEN,GERMANY.			Fink, Gernot/0000-0002-7446-7813				BUNT HC, 1988, RECENT ADV SPEECH UN, V46, P349; Cappelli A., 1984, Computational models of natural language processing, P33; DORTA P, 1987, 3RD C EUR CHAPT ACL, P80; EHRLICH U, 1990, SPRACHE INFORMATION, V22; Fillmore Charles J., 1968, UNIVERSALS LINGUIST, P1; FINK PK, 1983, THESIS INFORMATION S; HAVENS WS, 1983, COMPUTATIONAL LINGUI, P185; Hayes P. J., 1986, 11th International Conference on Computational Linguistics. Proceedings of Coling '86, P587; Hitzenberger L., 1989, Eurospeech 89. European Conference on Speech Communication and Technology, P597; INGRIA JP, 1988, 4TH P C PORT, P59; KUHN T, 1992, SIGNAL PROCESS, V6, P439; KUMMERT F, 1992, ERWEITERUNGEN DIALOG; KUMMERT F, 1992, THESIS, V12; KUNZMANN S, 1988, RECENT ADV SPEECH UN, V46, P311; LEVINSON SE, 1985, BIBLIOTHECA PHONETIC, V12, P149; MUDLER J, 1988, RECENT ADV SPEECH UN, V46, P473; NEY H, 1988, RECENT ADV SPEECH UN, V46, P305; NIEDERMAIR G, 1992, OCT INT C SPOK LANG, P635; NIEDERMAIR G, 1992, SPRACHLICHE MENSCH M, P91; NIEMANN H, 1990, IEEE T PATTERN ANAL, V12, P883, DOI 10.1109/34.57683; Niemann H., 1992, Speech Recognition and Understanding. Recent Advances, Trends and Applications. Proceedings of the NATO Advanced Study Institute, P425; Nilsson N., 1982, PRINCIPLES ARTIFICIA; NORTON LM, 1990, JUN P DARPA WORKSH, P141; NOTH E, 1988, MUSTERERKENNUNG 88, P2; Sagerer G., 1988, 9th International Conference on Pattern Recognition (IEEE Cat. No.88CH2614-6), P788, DOI 10.1109/ICPR.1988.28360; SAGERER G, 1988, RECENT ADV SPEECH UN, V46, P421; SAGERER G, 1990, AUTOMATISCHES VERSTE, V74; SCHUKATTALAMAZZ.EG, 1992, P INT C ACOUST SPEEC, V1, P577; SHIGENAGA M, 1986, P ICASSP TOKYO, P1577; Sondheimer N. K., 1984, 10th International Conference on Computational Linguistics. 22nd Annual Meeting of the Association for Computational Linguistics. Proceedings of Coling 84, P101; TESNIERE L, 1966, ELEMENTS SYNTAXE STR; THURMAIR G, 1988, RECENT ADV SPEECH UN, V46, P397; YOUD N, 1992, OCT INT C SPOK LANG, P643; Young S. J., 1989, Computer Speech and Language, V3, P329, DOI 10.1016/0885-2308(89)90002-8; YOUNG SR, 1989, COMMUN ACM, V32, P183, DOI 10.1145/63342.63344; ZUE V, 1990, JUN DARPA SPEECH NAT	36	3	5	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1994	16	2					179	194		10.1109/34.273733	http://dx.doi.org/10.1109/34.273733			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NA631		Green Submitted			2022-12-18	WOS:A1994NA63100005
J	TAGARE, HD; DEFIGUEIREDO, RJP				TAGARE, HD; DEFIGUEIREDO, RJP			ON THE LOCALIZATION PERFORMANCE MEASURE AND OPTIMAL EDGE DETECTION - REPLY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						EDGE DETECTION; OPTIMALITY CRITERIA; SCALE SPACE			YALE UNIV,DEPT COMP SCI,NEW HAVEN,CT 06471; UNIV CALIF IRVINE,DEPT ELECT & COMP ENGN,IRVINE,CA 92717; UNIV CALIF IRVINE,DEPT MATH,IRVINE,CA 92717	Yale University; University of California System; University of California Irvine; University of California System; University of California Irvine	TAGARE, HD (corresponding author), YALE UNIV,DEPT DIAGNOST RADIOL,NEW HAVEN,CT 06471, USA.							BOYER KL, 1994, IEEE T PATTERN ANAL, V16, P107; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; SARKAR S, 1991, IEEE T PATTERN ANAL, V13, P1154, DOI 10.1109/34.103275; TAGARE HD, 1990, IEEE T PATTERN ANAL, V12, P1186, DOI [10.1109/34.62607, 10.1117/12.19530]	4	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1994	16	1					108	110		10.1109/34.273709	http://dx.doi.org/10.1109/34.273709			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MV733					2022-12-18	WOS:A1994MV73300013
J	DUMAY, ACM; CLAESSENS, MNAJ; ROOS, C; GERBRANDS, JJ; REIBER, JHC				DUMAY, ACM; CLAESSENS, MNAJ; ROOS, C; GERBRANDS, JJ; REIBER, JHC			OBJECT DELINEATION IN NOISY IMAGES BY A MODIFIED POLICY-ITERATION METHOD	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						CLOSED MINIMAL COST PATH; CONTOUR DETECTION; DYNAMIC PROGRAMMING; POLICY-ITERATION METHOD; SCINTIGRAPHY		The contours of isolated objects in noisy images may be detected with a minimal cost contour detection algorithm. This paper describes a new algorithm that is based on the policy-iteration method for locating the closed minimal cost path. Computational results indicate that this algorithm is computationally more efficient than the dynamic programming approach. The method is applied to left ventricular contours in scintigraphic images, although it is applicable to any domain where a closed minimal cost path is to be computed in a matrix of cost coefficients.	LEIDEN UNIV HOSP,FAC ELECT ENGN,2333 AA LEIDEN,NETHERLANDS; DELFT UNIV TECHNOL,FAC TECH MATH & INFORMAT,DELFT,NETHERLANDS	Leiden University; Leiden University Medical Center (LUMC); Delft University of Technology	DUMAY, ACM (corresponding author), LEIDEN UNIV HOSP,DEPT DIAGNOST RADIOL,2333 AA LEIDEN,NETHERLANDS.			Roos, Kees (Cornelis)/0000-0002-7963-9127				Bellman R., 1962, APPL DYNAMIC PROGRAM; Cooper D. B., 1978, Proceedings of the 1978 Conference on Pattern Recognition and Image Processing, P25; COOPER DB, 1979, IEEE T PATTERN ANAL, V1, P372, DOI 10.1109/TPAMI.1979.4766946; COOPER DB, 1980, 5TH P IAPR INT C PAT, P1278; Derman C., 1970, FINITE STATE MARKOVI; DIXON WJ, 1969, INTRO STATISTICAL AN; Gerbrands J. J., 1985, Sixth Symposium on Information Theory in the Benelux, P35; Gerbrands J. J., 1981, 2nd International Conference on Visual Psychophysics and Medical Imaging, P155; GERBRANDS JJ, 1988, THESIS DELFT U TECHN; Howard RonaldA., 1960, DYNAMIC PROGRAMMING; MARTELLI A, 1976, COMMUN ACM, V19, P73, DOI 10.1145/359997.360004; MARTELLI A, 1972, COMPUTER GRAPHICS IM, V1, P169, DOI DOI 10.1016/S0146-664X(72)80013-3; MONTANARI U, 1971, COMMUN ACM, V14, P335, DOI 10.1145/362588.362594; REIBER JHC, 1985, EUR J NUCL MED, V10, P97, DOI 10.1007/BF00252715	15	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1992	14	9					952	958		10.1109/34.161354	http://dx.doi.org/10.1109/34.161354			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JL924					2022-12-18	WOS:A1992JL92400008
J	WESTBERG, L				WESTBERG, L			HIERARCHICAL CONTOUR-BASED SEGMENTATION OF DYNAMIC SCENES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						HYPOTHESIS TEST; IMAGE SEQUENCE; OBJECT RECOGNITION; PYRAMIDS; SEGMENTATION	BOUNDARY	A new motion segmentation algorithm is presented. The algorithm is based on the assumption of one coherent moving area (without holes) on a static background. Our algorithm does coarse-to-fine pyramid-based boundary refinement that attempts to classify the blocks into three classes: Inside, Border, and Outside.			WESTBERG, L (corresponding author), ELLEMTEL TELECOM LAB,ALVSJO,SWEDEN.							BAUGHER ES, 1986, PATTERN RECOGN, V19, P373, DOI 10.1016/0031-3203(86)90004-X; BERGHOLM F, 1987, IEEE T PATTERN ANAL, V9, P726, DOI 10.1109/TPAMI.1987.4767980; CANTONI V, 1986, NATO ASI SERIES F, V25; CHEN PC, 1981, IMAGE MODELLING, P9; GROSS AD, 1987, COMPUT VISION GRAPH, V39, P102, DOI 10.1016/S0734-189X(87)80204-9; HARTLEY R, 1985, PATTERN RECOGN LETT, V3, P253, DOI 10.1016/0167-8655(85)90005-4; JAIN R, 1979, IEEE T PATTERN ANAL, V1, P206, DOI 10.1109/TPAMI.1979.4766907; JAIN R, 1984, P IVC, V2; JAIN R, 1979, P CGIP, P13; JAIN R, 1981, IEEE T PATT ANAL MAC, V3; KASS M, 1987, 1ST P INT C COMP VIS, P259; KOLLER D, 12 P DAGM S AAL 1990, P625; LEUNG MK, 1987, PATTERN RECOGN, V20, P55, DOI 10.1016/0031-3203(87)90017-3; NAHI NE, 1977, IEEE T COMPUT, V26, P772, DOI 10.1109/TC.1977.1674915; ROSENFELD A, 1984, MULTIRESOLUTION IMAG; ROSENFELD A, 1986, NATO ASI SER, V25, P261; SCHNEIER M, 1983, IEEE T PATTERN ANAL, V5, P345; SHER A, 1987, IVC, V7; TANOMITO S, 1975, P CGIP, V4, P101; THOMPSON WB, 1980, IEEE T PATTERN ANAL, V2, P543, DOI 10.1109/TPAMI.1980.6447701; Van Trees H., 2013, DETECTION ESTIMATION; WESTBERG L, 1989, THESIS ROYAL I TECH; YALAMANCHILI S, 1982, COMPUT VISION GRAPH, V18, P188, DOI 10.1016/0146-664X(82)90171-X	23	3	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1992	14	9					946	952		10.1109/34.161353	http://dx.doi.org/10.1109/34.161353			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JL924					2022-12-18	WOS:A1992JL92400007
J	LEE, S; HAHN, H				LEE, S; HAHN, H			AN OPTIMAL SENSING STRATEGY FOR RECOGNITION AND LOCALIZATION OF 3-D NATURAL QUADRIC OBJECTS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						ACTIVE SENSING; MEASURE OF DISCRIMINATION POWER; NATURAL QUADRIC OBJECTS; OBJECT LOCALIZATION; OBJECT RECOGNITION; OBJECT REPRESENTATION; PROBING	RANGE	Active sensing aims at achieving a goal-directed collection of data critical for sensing goals by controlling sensor configurations and poses. Active sensing thus requires an optimal sensing strategy for controlling sensor configurations and poses in such a way as to minimize data collection operations necessary for achieving sensing goals. This paper presents an optimal sensing strategy of an optical proximity sensor system engaged in the recognition and localization of 3-D natural quadric objects. The optimal sensing strategy consists of the selection of an optimal beam orientation and the determination of an optimal probing plane that compose an optimal data collection operation known as an optimal probing. The decision of an optimal probing is based on the measure of discrimination power for a cluster of surfaces on a multiple interpretation image (MII), where the measure of discrimination power is defined in terms of a utility function computing the expected number of interpretations that can be pruned out by a probing. This paper also presents an object representation suitable for active sensing based on a surface description vector (SDV) distribution graph and hierarchical tables. Experimental results are shown.	UNIV SO CALIF,DEPT ELECT ENGN SYST,LOS ANGELES,CA 90089	University of Southern California	LEE, S (corresponding author), CALTECH,JET PROP LAB,TECH STAFF,PASADENA,CA 91109, USA.							ELLIS RE, 1986, AAAI, P632; GASTON PC, 1984, IEEE T PATTERN ANAL, V6, P257, DOI 10.1109/TPAMI.1984.4767518; GRIMSON WEL, 1986, IEEE J ROBOT AUTOM, V2, P196, DOI 10.1109/JRA.1986.1087057; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; HUTCHINSON SA, 1988, 1988 P IEEE INT C RO, P1068; IDESAWA M, 1986, J ROBOTIC SYST, V3, P165, DOI 10.1002/rob.4620030205; JOSHI S, 1988, COMPUT AIDED DESIGN, V20, P58, DOI 10.1016/0010-4485(88)90050-4; KANADE T, 1983, TR8315 CARN U TECH R; KIM H, 1985, 1985 IEEE INT C ROB, P28; LEE S, 1991, USCIRIS270 TECH REP; LEE S, 1991, USCIRIS269 TECH REP; LEE S, 1989, 1ST P WORKSH INT 3D, P75; LEE S, 1990, 1990 P IEEE INT C RO, P1666; LEVIN J, 1976, COMMUN ACM, V19, P555, DOI 10.1145/360349.360355; LUO RC, 1987, J ROBOTIC SYST, V4, P199, DOI 10.1002/rob.4620040204; MAGEE M, 1986, INT J PATT RECOGNITI, V19, P169; MAGEE M, 1987, P SPATIAL REASONING, P262; MILLER JR, 1987, ACM T GRAPHIC, V6, P274, DOI 10.1145/35039.35041; Requicha A. G., 1980, ACM COMPUT SURV, P437; REQUICHA AAG, 1982, IEEE COMPUT GRAPH, P9; ROY U, 1988, ROBOT CIM-INT MANUF, V4, P335, DOI 10.1016/0736-5845(88)90004-X; SCHNIETER JL, 1986, 1986 P IEEE C ROB AU, P1262; SHAPIRO LG, 1987, APPL OPTICS, V26, P1845, DOI 10.1364/AO.26.001845; Shirai Y., 1986, Robotics, V2, P175, DOI 10.1016/0167-8493(86)90028-8; TSAI R, 1987, P SPATIAL REASONING, P282; XIE SE, 1988, IEEE T PATTERN ANAL, V10, P221, DOI 10.1109/34.3884	27	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1991	13	10					1018	1037		10.1109/34.99236	http://dx.doi.org/10.1109/34.99236			20	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GM763					2022-12-18	WOS:A1991GM76300005
J	SAITO, N; CUNNINGHAM, MA				SAITO, N; CUNNINGHAM, MA			GENERALIZED E-FILTER AND ITS APPLICATION TO EDGE-DETECTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											SAITO, N (corresponding author), SCHLUMBERGER DOLL RES CTR, RIDGEFIELD, CT 06877 USA.		Saito, Naoki/A-6138-2012	Saito, Naoki/0000-0001-5234-4719				BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749; BURT PJ, 1981, COMPUT VISION GRAPH, V16, P20, DOI 10.1016/0146-664X(81)90092-7; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837; MOORE DJH, 1973, IEEE T INFORM THEORY, V19, P415, DOI 10.1109/TIT.1973.1055044; NIEMINEN A, 1987, IEEE T PATTERN ANAL, V9, P74, DOI 10.1109/TPAMI.1987.4767873; OPPENHEI.AV, 1968, PR INST ELECTR ELECT, V56, P1264, DOI 10.1109/PROC.1968.6570; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI [10.1109/34.41383, 10.1109/TPAMI.1986.4767748]	9	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1990	12	8					814	817		10.1109/34.57671	http://dx.doi.org/10.1109/34.57671			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DQ388					2022-12-18	WOS:A1990DQ38800007
J	FORSGREN, PO; SEIDEMAN, P				FORSGREN, PO; SEIDEMAN, P			AN INTEROBJECT DISTANCE MEASURE BASED ON MEDIAL AXES RETRIEVED FROM DISCRETE DISTANCE MAPS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									KAROLINSKA INST,DEPT INTERNAL MED,DIV RHEUMATOL,S-18288 DANDERYD,SWEDEN	Karolinska Institutet	FORSGREN, PO (corresponding author), SARASTRO INC,6660 CRANE RD,YPSILANTI,MI 48197, USA.							ARCELLI C, 1981, IEEE T PATTERN ANAL, V3, P134, DOI 10.1109/TPAMI.1981.4767071; ARCELLI C, 1986, PATTERN RECOGN LETT, V4, P383, DOI 10.1016/0167-8655(86)90060-7; Ballard D.H., 1982, COMPUTER VISION; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5; DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4; Dorst L., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P286; Hilditch C J, 1983, IMAGE VISION COMPUT, V1, P115, DOI DOI 10.1016/0262-8856(83)90063-X; IANNUZZI L, 1983, NEW ENGL J MED, V309, P1023, DOI 10.1056/NEJM198310273091704; KLEIN F, 1987, PATTERN RECOGN LETT, V5, P19, DOI 10.1016/0167-8655(87)90022-5; LUMELSKY VJ, 1985, INFORM PROCESS LETT, V21, P55, DOI 10.1016/0020-0190(85)90032-8; Mehrang Saeed, IEEE T GEOSCI REMOTE, V20, P7957, DOI [10.1109/JSEN.2020.2981334, DOI 10.1109/TGRS.2018.2872081]; MONTANARI U, 1968, J ACM, V15, P600, DOI 10.1145/321479.321486; OLSSON L, 1982, 1ST P IEEE COMP SOC, P77; ROSENFELD A, 1968, PATTERN RECOGN, V1, P33, DOI 10.1016/0031-3203(68)90013-7; SHARP JT, 1985, ARTHRITIS RHEUM, V28, P16, DOI 10.1002/art.1780280104; Yamada H., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P69	18	3	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	1990	12	4					390	397		10.1109/34.50624	http://dx.doi.org/10.1109/34.50624			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CV929					2022-12-18	WOS:A1990CV92900005
J	SANDS, OS; GARBER, FD				SANDS, OS; GARBER, FD			PATTERN REPRESENTATIONS AND SYNTACTIC CLASSIFICATION OF RADAR MEASUREMENTS OF COMMERCIAL AIRCRAFT	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											SANDS, OS (corresponding author), OHIO STATE UNIV,DEPT ELECT ENGN,ELECTROSCI LAB,COLUMBUS,OH 43210, USA.							BHANU B, 1986, IEEE T AERO ELEC SYS, V22, P364, DOI 10.1109/TAES.1986.310772; BIERMANN AW, 1972, FRONTIERS PATTERN RE, P31; CURTIS SR, 1985, IEEE T ACOUST SPEECH, V33, P643, DOI 10.1109/TASSP.1985.1164589; FU KS, 1975, IEEE T SYST MAN CYB, VSMC5, P95, DOI 10.1109/TSMC.1975.5409159; FU KS, 1982, SYNTACTIC PATTERN RE; Hildebrand F. B., 1974, INTRO NUMERICAL ANAL; KAMIS A, 1985, 7165591 OH STAT U DE; LOGAN BF, 1977, AT&T TECH J, V56, P487, DOI 10.1002/j.1538-7305.1977.tb00522.x; PARL S, 1980, IEEE T INFORM THEORY, V26, P121, DOI 10.1109/TIT.1980.1056127; VADIM VM, 1979, MAY P IEEE, V67, P714; WALTON EK, 1984, IEEE T ANTENN PROPAG, V32, P1218, DOI 10.1109/TAP.1984.1143228	11	3	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1990	12	2					204	211		10.1109/34.44406	http://dx.doi.org/10.1109/34.44406			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CL282					2022-12-18	WOS:A1990CL28200008
J	ZHUANG, XH; HARALICK, RM; JOO, H				ZHUANG, XH; HARALICK, RM; JOO, H			A SIMPLEX-LIKE ALGORITHM FOR THE RELAXATION LABELING PROCESS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											ZHUANG, XH (corresponding author), UNIV WASHINGTON,DEPT ELECT ENGN,SEATTLE,WA 98195, USA.		Haralick, Robert/AAW-5151-2020	manickam, vijayabhama.M/0000-0001-9437-9477				HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; KINDERLEHRER D, 1980, INTRO VARIATIONAL IN; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; WALTZ DL, 1972, MIT AI271 TECH REP	4	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	1989	11	12					1316	1321		10.1109/34.41370	http://dx.doi.org/10.1109/34.41370			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CB716					2022-12-18	WOS:A1989CB71600008
J	VANWARMERDAM, WLG; ALGAZI, VR				VANWARMERDAM, WLG; ALGAZI, VR			DESCRIBING 1-D INTENSITY TRANSITIONS WITH GAUSSIAN DERIVATIVES AT THE RESOLUTIONS MATCHING THE TRANSITION WIDTHS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											VANWARMERDAM, WLG (corresponding author), UNIV CALIF DAVIS,DEPT ELECT ENGN & COMP SCI,DAVIS,CA 95616, USA.							ALGAZI VR, 1989, J OPT SOC AM     JUN; BERGHOLM F, 1987, IEEE T PATTERN ANAL, V9, P726, DOI 10.1109/TPAMI.1987.4767980; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CLARK JJ, 1989, IEEE T PATTERN ANAL, V11, P43, DOI 10.1109/34.23112; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P55; HILDRETH EC, 1983, COMPUT VISION GRAPH, V22, P1, DOI 10.1016/0734-189X(83)90093-2; KOENDERINK, 1988, NEURAL COMPUTERS; KORN AF, 1988, IEEE T PATTERN ANAL, V10, P610, DOI 10.1109/34.6770; LUNSCHER WHHJ, 1986, IEEE T PATTERN ANAL, V8, P164, DOI 10.1109/TPAMI.1986.4767770; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; NALWA VS, 1986, IEEE T PATTERN ANAL, V8, P699, DOI 10.1109/TPAMI.1986.4767852; VANWARMERDAM WLG, 1987 P SPIE C INT RO, P102; VANWARMERDAM WLG, 1988, THESIS U CALIFORNIA; YOUNG RA, 1986, 1986 P C COMP VIS PA, P564; YOUNG RA, 1985, J OPT SOC AM A, V2, pFV4	15	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1989	11	9					973	977		10.1109/34.35500	http://dx.doi.org/10.1109/34.35500			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AM008					2022-12-18	WOS:A1989AM00800007
J	LIN, Z; ATTIKIOUZEL, Y				LIN, Z; ATTIKIOUZEL, Y			TWO-DIMENSIONAL LINEAR PREDICTION MODEL-BASED DECORRELATION METHOD	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											LIN, Z (corresponding author), UNIV WESTERN AUSTRALIA,DEPT ELECT & ELECTR ENGN,NEDLANDS,WA 6009,AUSTRALIA.							AHUJA N, 1981, IEEE T PATTERN ANAL, V3, P1, DOI 10.1109/TPAMI.1981.4767045; BRODATZ P, 1956, TEXTURES PHOTOGRAPHI; CHELLAPPA R, 1985, IEEE T ACOUST SPEECH, V33, P959, DOI 10.1109/TASSP.1985.1164641; CHEN CH, 1982, 6TH P INT C PATT REC, P1074; CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341; DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P251, DOI 10.1109/TPAMI.1979.4766921; DESOUZA P, 1982, PATTERN RECOGN, V15, P471, DOI 10.1016/0031-3203(82)90025-5; FAUGERAS OD, 1980, IEEE T PATTERN ANAL, V2, P323, DOI 10.1109/TPAMI.1980.4767031; Galloway M, 1974, COMPUT GRAPHICS IMAG, V4, P172; HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; KASHYAP RL, 1986, IEEE T PATTERN ANAL, V8, P472, DOI 10.1109/TPAMI.1986.4767811; KASHYAP RL, 1982, PATTERN RECOGNITION, V1; MARAGOS PA, 1984, IEEE T ACOUST SPEECH, V32, P1213, DOI 10.1109/TASSP.1984.1164463; Pratt W. K., 1978, DIGITAL IMAGE PROCES; Rabiner L., 1978, DIGITAL PROCESSING S; RANGANATH S, 1985, IEEE T ACOUST SPEECH, V33, P280, DOI 10.1109/TASSP.1985.1164523; TAMARA H, 1978, IEEE T SYST MAN CYB, V8, P460; THERRIEN CW, 1986, P IEEE, V74, P532, DOI 10.1109/PROC.1986.13504; WAX M, 1983, IEEE T ACOUST SPEECH, V31; WESZKA JS, 1976, IEEE T SYST MAN CYB, V6, P269, DOI 10.1109/TSMC.1976.5408777; 1986, HDB PATTERN RECOGNIT	22	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1989	11	6					661	665		10.1109/34.24801	http://dx.doi.org/10.1109/34.24801			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	U6749		Green Published			2022-12-18	WOS:A1989U674900012
J	WU, XL; ROKNE, J				WU, XL; ROKNE, J			ON PROPERTIES OF DISCRETIZED CONVEX CURVES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											WU, XL (corresponding author), UNIV CALGARY,DEPT COMP SCI,CALGARY T2N 1N4,ALBERTA,CANADA.							BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025; Freeman H., 1974, Computing Surveys, V6, P57, DOI 10.1145/356625.356627; KIM CE, 1982, IEEE T PATTERN ANAL, V4, P149, DOI 10.1109/TPAMI.1982.4767221; KIM CE, 1982, IEEE T PATTERN ANAL, V4, P618, DOI 10.1109/TPAMI.1982.4767315; KULPA Z, 1979, COMPUT VISION GRAPH, V10, P348, DOI 10.1016/S0146-664X(79)80043-X; PREPARATA F, 1985, COMPUTATIONAL GEOMET, pCH3; REGIORI GB, 1972, 40322 NEW YORK U TEC, P46; ROBERTS AW, 1973, CONVEX FUNCTIONS, pCH1; RONSE C, 1987, BIBLIO DIGITAL COMPU; ROSENFELD A, 1982, AM MATH MON, V89, P230, DOI 10.2307/2320219; ROSENFELD A, 1979, AM MATH MON, V86, P621, DOI 10.2307/2321290; SANTALO LA, 1976, INTEGRAL GEOMETRY GE, P1; SKLANSKY J, 1970, PATTERN RECOGN, V2, P3, DOI 10.1016/0031-3203(70)90037-3; WU XL, 1987, COMPUT VISION GRAPH, V37, P331, DOI 10.1016/0734-189X(87)90041-7	14	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	1989	11	2					217	223		10.1109/34.16719	http://dx.doi.org/10.1109/34.16719			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	R9989					2022-12-18	WOS:A1989R998900012
J	CHANG, KH; WEE, WG				CHANG, KH; WEE, WG			A PLANNING-MODEL WITH PROBLEM ANALYSIS AND OPERATOR HIERARCHY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									UNIV CINCINNATI,DEPT ELECT & COMP ENGN,CINCINNATI,OH 45221	University System of Ohio; University of Cincinnati	CHANG, KH (corresponding author), AUBURN UNIV,DEPT COMP SCI & ENGN,AUBURN,AL 36849, USA.							CHANG KH, 1986, THESIS U CINCINNATI; COHEN PR, 1982, HDB ARTIFICIAL INTEL, V3, P515; DAWSON C, 1977, P IJCAI, V5, P465; FIKES RE, 1971, ARTIF INTELL, V2, P189, DOI 10.1016/0004-3702(71)90010-5; Friedland P. E., 1985, Journal of Automated Reasoning, V1, P161, DOI 10.1007/BF00244995; FRIEDLAND PE, 1979, THESIS STANFORD U ST; HAYESROTH B, 1979, P INT JOINT C ART IN, V6, P375; KIBLER D, 1981, P IJCAI, V7, P345; NILSSON NJ, 1980, PRINCIPLES ARTIFICIA, P321; SACERDOTI ED, 1974, ARTIF INTELL, V5, P115, DOI 10.1016/0004-3702(74)90026-5; SACERDOTI ED, 1975, P IJCAI, V4, P206; Stefik M. J., 1980, THESIS STANFORD U ST; TATE A, 1977, P IJCAI, V5, P888; WADINGGER R, 1981, READING ARTIFICIAL I, P250; WILKINS DE, 1984, ARTIF INTELL, V22, P269, DOI 10.1016/0004-3702(84)90053-5	15	3	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1988	10	5					672	675		10.1109/34.6775	http://dx.doi.org/10.1109/34.6775			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	Q4255					2022-12-18	WOS:A1988Q425500006
J	TANAKA, E; KOJIMA, Y				TANAKA, E; KOJIMA, Y			A HIGH-SPEED STRING CORRECTION METHOD USING A HIERARCHICAL FILE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											TANAKA, E (corresponding author), UTSUNOMIYA UNIV,FAC ENGN,DEPT INFORMAT SCI,UTSUNOMIYA,TOCHIGI 321,JAPAN.							Carroll JohnB., 1967, COMPUTATIONAL ANAL P; DAVIS P, 1983, AM HERITAGE DICT ENG; DIXON NR, 1976, IEEE T ACOUST SPEECH, V24, P137, DOI 10.1109/TASSP.1976.1162793; HALL PAV, 1980, COMPUT SURV, V12, P381, DOI 10.1145/356827.356830; Itahashi S., 1984, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ67D, P869; ITO T, 1982, T I ELECTRON COMMU D, V65, P1090; KANEKO T, 1983, IEEE T ACOUST SPEECH, V31, P1061, DOI 10.1109/TASSP.1983.1164211; Kruskal J.B., 1983, TIME WARPS STRING ED; KURITA T, 1984, J INFORM PROCESSING, V25, P831; LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845; OKUDA T, 1976, IEEE T COMPUT, V25, P172, DOI 10.1109/TC.1976.5009232; PETERSON JL, 1980, COMMUN ACM, V23, P676, DOI 10.1145/359038.359041; SAKOE H, 1971, J ACOUST SOC JAPAN, V29, P483; SANKOFF D, 1972, P NATL ACAD SCI USA, V69, P4, DOI 10.1073/pnas.69.1.4; Sellers P. H., 1974, Journal of Combinatorial Theory, Series A, V16, P253, DOI 10.1016/0097-3165(74)90050-8; SHINGHAL R, 1983, PATTERN RECOGN, V16, P261, DOI 10.1016/0031-3203(83)90030-4; SUGAMURA N, 1982, T IECE JAPAN D, V65, P1041; TANAKA E, 1976, IEEE T INFORM THEORY, V22, P156, DOI 10.1109/TIT.1976.1055532; Tanaka E., 1986, Transactions of the Information Processing Society of Japan, V27, P177; TANAKA E, 1986, PATTERN RECOGN, V19, P407, DOI 10.1016/0031-3203(86)90006-3; WAGNER RA, 1974, J ACM, V21, P168, DOI 10.1145/321796.321811	21	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1987	9	6					806	815		10.1109/TPAMI.1987.4767987	http://dx.doi.org/10.1109/TPAMI.1987.4767987			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	K6735	21869442				2022-12-18	WOS:A1987K673500008
J	STIRLING, WC; SWINDLEHURST, AL				STIRLING, WC; SWINDLEHURST, AL			DECISION-DIRECTED MULTIVARIATE EMPIRICAL BAYES CLASSIFICATION WITH NONSTATIONARY PRIORS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											STIRLING, WC (corresponding author), BRIGHAM YOUNG UNIV, DEPT ELECT ENGN, PROVO, UT 84602 USA.		Swindlehurst, Arnold Lee/J-4806-2017	Swindlehurst, Arnold Lee/0000-0002-0521-3107				[Anonymous], 1962, REV INT STAT I; DAVISSON LD, 1970, IEEE T INFORM THEORY, V16, P270, DOI 10.1109/TIT.1970.1054455; FREDRIKSEN A, 1972, IEEE T INFORM THEORY, V18, P607, DOI 10.1109/TIT.1972.1054887; GREGG WD, 1968, IEEE T INFORM THEORY, V14, P451, DOI 10.1109/TIT.1968.1054141; KATOPIS A, 1972, 3RD P ANN PITTSB C M, P473; KAZAKOS D, 1980, IEEE T INFORM THEORY, V26, P113, DOI 10.1109/TIT.1980.1056124; LUCKY RW, 1966, AT&T TECH J, V45, P255, DOI 10.1002/j.1538-7305.1966.tb00020.x; MCAULAY RJ, 1973, IEEE T AERO ELEC SYS, VAES9, P229, DOI 10.1109/TAES.1973.309772; MIDDLETON D, 1968, IEEE T INFORM THEORY, V14, P434, DOI 10.1109/TIT.1968.1054139; PATRICK EA, 1968, IEEE T INFORM THEORY, V14, P160, DOI 10.1109/TIT.1968.1054074; PATRICK EA, 1968, IEEE T INFORM THEORY, V14, P407, DOI 10.1109/TIT.1968.1054162; PATRICK EA, 1970, IEEE T COMPUT, VC 19, P197, DOI 10.1109/T-C.1970.222897; PETERS U, 1983, SIGNAL PROCESS, V2, P544; PROAKIS JG, 1964, IEEE T COMMUN SYST, VCS12, P54, DOI 10.1109/TCOM.1964.1088888; ROBBINS H, 1950, 2 P BERK S, P131; SAMUEL E, 1963, ANN MATH STAT, V34, P1079, DOI 10.1214/aoms/1177704032; SANO A, SIGNAL PROCESSING, V2, P547; SCHWARTZ SC, 1970, PROBL PEREDACHI INF, V6, P92; SCHWARTZ SC, 1977, 1977 P C DEC CONTR S, P1067; SCUDDER HJ, 1965, IEEE T INFORM THEORY, V11, P363, DOI 10.1109/tit.1965.1053799; SEGALL A, 1976, IEEE T INFORM THEORY, V22, P422, DOI 10.1109/TIT.1976.1055577; SEGALL A, 1976, IEEE T INFORM THEORY, V22, P275, DOI 10.1109/TIT.1976.1055559; STIRLING WC, 1987, IEEE T AUTOMAT CONTR, V32, P86, DOI 10.1109/TAC.1987.1104425; STIRLING WC, 1984, IEEE T AUTOMAT CONTR, V29, P928, DOI 10.1109/TAC.1984.1103404; STIRLING WC, 1983, THESIS STANFORD U ST; SWINDLEHURST AL, 1986, THESIS B YOUNG U PRO; TSYPKIN YZ, 1970, PROBL PEREDACHI INF, V6, P52; ULA N, 1977, IEEE T AUTOMAT CONTR, V22, P991, DOI 10.1109/TAC.1977.1101660; WONG AKC, 1977, IEEE T COMPUT, V26, P75, DOI 10.1109/TC.1977.5009277; YOUNG TY, 1972, IEEE T INFORM THEORY, V18, P671; [No title captured]; [No title captured]	33	3	5	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1987	9	5					644	660		10.1109/TPAMI.1987.4767959	http://dx.doi.org/10.1109/TPAMI.1987.4767959			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	J7393	21869423	Green Published, Green Submitted			2022-12-18	WOS:A1987J739300006
J	HOFFMAN, RL; JAIN, AK				HOFFMAN, RL; JAIN, AK			SPARSE DECOMPOSITIONS FOR EXPLORATORY PATTERN-ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									MICHIGAN STATE UNIV,DEPT COMP SCI,E LANSING,MI 48824	Michigan State University								DIGGLE PJ, 1979, BIOMETRICS, V35, P87, DOI 10.2307/2529938; GOSHTASBY A, 1985, IEEE T SYST MAN CYB, V15, P631, DOI 10.1109/TSMC.1985.6313439; HAAS HPA, 1980, 5TH P ICPR, P87; HE Q, 1982, SEP P INT C CHIN LAN; Rankin R.A., 1964, PACKING COVERING, V5414, P247; Ripley B. D., 1981, SPAT STAT-NETH; SMITH S, 1982, THESIS MICHIGAN STAT; SMITH SP, 1984, IEEE T PATTERN ANAL, V6, P73, DOI 10.1109/TPAMI.1984.4767477	8	3	3	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1987	9	4					551	560		10.1109/TPAMI.1987.4767942	http://dx.doi.org/10.1109/TPAMI.1987.4767942			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	H9088	21869412				2022-12-18	WOS:A1987H908800008
J	IHARA, J				IHARA, J			EXTENSION OF CONDITIONAL-PROBABILITY AND MEASURES OF BELIEF AND DISBELIEF IN A HYPOTHESIS BASED ON UNCERTAIN EVIDENCE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											IHARA, J (corresponding author), ELECTROTECH LAB,DIV ENERGY SYST,SYST ENGN SECT,1-1-4 UMEZONO,SAKURA,IBARAKI 305,JAPAN.							Adams JB, 1984, RULE BASED EXPERT SY, P263; ANDERSON JR, 1980, COGNITIVE PSYCHOL IT, pCH11; ASAI K, 1978, INTRO THEORY FUZZY S, P30; DUDA RO, 1976, AM FEDERATION INFORM, V45, P1075; Jeffrey R., 1983, LOGIC DECISION; LUKASIEWICZ J, 1970, J LUKASIEWICZ SELECT, P16; Popper Karl, 1972, OBJECTIVE KNOWLEDGE; ROZEBOOM WW, 1960, PSYCHOL BULL, V57, P416, DOI 10.1037/h0042040; SHORTLIFFE EH, 1976, COMPUTER BASED MED C, pCH4; ZADEH LA, 1968, J MATH ANAL APPL, V23, P421, DOI 10.1016/0022-247X(68)90078-4	10	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1987	9	4					561	568		10.1109/TPAMI.1987.4767943	http://dx.doi.org/10.1109/TPAMI.1987.4767943			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	H9088	21869413				2022-12-18	WOS:A1987H908800009
J	SCHALKOFF, RJ				SCHALKOFF, RJ			DYNAMIC IMAGERY MODELING AND MOTION ESTIMATION USING WEAK FORMULATIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											SCHALKOFF, RJ (corresponding author), CLEMSON UNIV,DEPT ELECT & COMP ENGN,CLEMSON,SC 29631, USA.							AIKITA K, 1984, PATTERN RECOGNITION, V17, P73; Ballard D.H., 1982, COMPUTER VISION; BRICE CR, 1970, ARTIF INTELL, V1, P205, DOI 10.1016/0004-3702(70)90008-1; BROWN CM, 1983, IEEE T PATTERN ANAL, V5, P493, DOI 10.1109/TPAMI.1983.4767428; FAN TI, 1979, AUG P IEEE C PATT RE, P638; FLINCHBAUGH BE, 1981, ARTIF INTELL, V17, P387, DOI 10.1016/0004-3702(81)90030-8; FREI W, 1977, IEEE T COMPUT, V26, P988, DOI 10.1109/TC.1977.1674733; GILBERT AL, 1980, IEEE T PATTERN ANAL, V2, P47, DOI 10.1109/TPAMI.1980.4766969; Hall E. L., 1979, COMPUTER IMAGE PROCE; HORN BK, 1980, MIT AI572 ART INT LA; JAIN AK, 1978, IEEE T AUTOMAT CONTR, V23, P817, DOI 10.1109/TAC.1978.1101881; JAIN R, 1981, COMPUTER, P12; LABUZ J, 1984, PATTERN RECOGN LETT, V2, P179, DOI 10.1016/0167-8655(84)90043-6; LEQTERS GR, 1982, IEEE T PATTERN ANAL, V4, P583; NAGEL HH, 1981, IMAGE SEQUENCE ANAL; NAGEL HH, 1982, PATTERN RECOGNIT OCT, P55; Panda D., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V252, P94; Ray W. H., 1978, DISTRIBUTED PARAMETE; ROACH JW, 1980, IEEE T PATTERN ANAL, V2, P554, DOI 10.1109/TPAMI.1980.6447703; SCHALKOFF RJ, 1982, IEEE T PATTERN ANAL, V4, P2, DOI 10.1109/TPAMI.1982.4767188; SCHALKOFF RJ, 1987, PATTERN RECOGNITION, V20; SCHALKOFF RJ, 1983, IMAGE VISION COMPUT, V1, P227; SCHALKOFF RJ, 1979, 3RD P INT COMP SOFTW, P504; SCHALKOFF RJ, 1982, 1982 P IEEE C PATT R, P119; SCHALKOFF RJ, P IEEE SOUTHEASTCON, P89; SCHALKOFF RJ, 1983, 1983 P IEEE COMP VIS, P232; SMITH EA, 1972, IEEE T COMPUT, V21; SNYDER WE, 1981, COMPUTER ANAL TI AUG; STOCKMAN G, 1982, IEEE T PATTERN ANAL, V4, P229, DOI 10.1109/TPAMI.1982.4767240; THOMPSON WB, 1981, COMPUTER, V14, P20, DOI 10.1109/C-M.1981.220559; WEBB JA, 1981, COMPUTER, V14, P40, DOI 10.1109/C-M.1981.220561	31	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1987	9	4					578	584		10.1109/TPAMI.1987.4767945	http://dx.doi.org/10.1109/TPAMI.1987.4767945			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	H9088	21869415				2022-12-18	WOS:A1987H908800011
J	BAILEY, T; COWLES, J				BAILEY, T; COWLES, J			A CONVEX-HULL INCLUSION TEST	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											BAILEY, T (corresponding author), UNIV WYOMING,DEPT COMP SCI,LARAMIE,WY 82071, USA.							ALLISON DCS, 1984, BIT, V24, P2, DOI 10.1007/BF01934510; Artin E., 1969, INTRO ALGEBRAIC TOPO; AVIS D, 1982, DISCRETE APPL MATH, V4, P81, DOI 10.1016/0166-218X(82)90065-8; BENTLEY JL, 1978, J ACM, V25, P536, DOI 10.1145/322092.322095; CHAND DR, 1970, J ACM, V17, P78, DOI 10.1145/321556.321564; Dijkstra E. W., 1976, DISCIPLINE PROGRAMMI; DOBKIN DP, 1980, THEOR COMPUT SCI, V11, P1, DOI 10.1016/0304-3975(80)90031-6; Eggleston HG., 1958, CONVEXITY; Graham R. L., 1972, Information Processing Letters, V1, P132, DOI 10.1016/0020-0190(72)90045-2; Grunbaum B., 1967, GRADUATE TEXTS MATH, V221; Hillier F.S., 1967, INTRO OPERATIONS RES; Jarvis R. A., 1973, Information Processing Letters, V2, P18, DOI 10.1016/0020-0190(73)90020-3; KARMARKAR N, 1984, 16TH ACM P S THEOR C; MCCALLUM D, 1979, INFORM PROCESS LETT, V9, P201, DOI 10.1016/0020-0190(79)90069-3; ORLOWSKI M, 1983, PATTERN RECOGN, V16, P579, DOI 10.1016/0031-3203(83)90074-2; PREPARATA FP, 1977, COMMUN ACM, V20, P87, DOI 10.1145/359423.359430; Sedgewick R., 1983, ALGORITHMS; SEIDEL R, 1981, 8114 U BRIT COL DEP; SKLANSKY J, 1972, IEEE T COMPUT, VC 21, P1355, DOI 10.1109/T-C.1972.223507; SMALE S, 1985, B AM MATH SOC, V13; SMITH SP, 1984, IEEE T PATTERN ANAL, V6, P73, DOI 10.1109/TPAMI.1984.4767477; SWART G, 1985, J ALGORITHM, V6, P17, DOI 10.1016/0196-6774(85)90017-3; TOUSSAINT GT, 1982, PATTERN RECOGN, V15, P23, DOI 10.1016/0031-3203(82)90057-7	23	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1987	9	2					312	316		10.1109/TPAMI.1987.4767904	http://dx.doi.org/10.1109/TPAMI.1987.4767904			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	G1633	21869400				2022-12-18	WOS:A1987G163300011
J	BELFORTE, G; BONA, B; TEMPO, R				BELFORTE, G; BONA, B; TEMPO, R			CONDITIONAL ALLOCATION AND STOPPING RULES IN BAYESIAN PATTERN-RECOGNITION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											BELFORTE, G (corresponding author), POLITECN TORINO, DIPARTIMENTO AUTOMAT & INFORMAT, CORSO DUCA DEGLI ABRUZZI 24, I-10129 TURIN, ITALY.			BONA, Basilio/0000-0002-4924-9144				BELFORTE G, 1982, 2ND P C MATH SERV MA; BELFORTE G, 1980, 4TH P ANN S COMP APP; BELFORTE G, 1981, LECTURE NOTES MED IN, V9; BELFORTE G, 1981, FEB P INT S MOD ID C; BELFORTE G, 1979, 2ND P INT C INF SCI; CHEN CH, 1973, STATISTICAL PATTERN; COVER TM, 1977, IEEE T SYST MAN CYB, V7, P657, DOI 10.1109/TSMC.1977.4309803; Devijver PA, 1982, PATTERN RECOGNITION; Fu K.S., 1974, MATH SCI ENG; Fu K. S., 1968, SEQUENTIAL METHODS P, V240, P241; FUKUNAGA K, 1973, IEEE T INFORM THEORY, V19, P434, DOI 10.1109/TIT.1973.1055049; FUKUNAGA K, 1972, INTRO STATISTICAL PA; HABBEMA JDF, 1978, METHOD INFORM MED, V17, P217; HERMANS J, 1976, MANUAL ALLOC DISCRIM; HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102; KANAL L, 1974, IEEE T INFORM THEORY, V20, P697, DOI 10.1109/TIT.1974.1055306; KANAL LN, 1979, IEEE T PATTERN ANAL, V1, P193, DOI 10.1109/TPAMI.1979.4766905; KITTLER J, 1982, IEEE T PATTERN ANAL, V4, P215, DOI 10.1109/TPAMI.1982.4767229; MANTEL N, 1970, TECHNOMETRICS, V12, P621, DOI 10.2307/1267207; MARILL T, 1963, IEEE T INFORM THEORY, V9, P11, DOI 10.1109/TIT.1963.1057810; MILANESE M, 1980, 4TH P ANN S COMP APP; MOBASSERI BG, 1979, IEEE T SYST MAN CYB, V9, P660; Murthy VK, 1966, MULTIVARIATE ANAL, P43; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; TOUSSAINT GT, 1974, IEEE T INFORM THEORY, V20, P472, DOI 10.1109/TIT.1974.1055260; VANCAMPENHOUT JM, 1978, IEEE T SYST MAN CYB, V8, P390, DOI 10.1109/TSMC.1978.4309980; ZADEH LA, 1975, INFORM SCIENCES, V8, P301, DOI 10.1016/0020-0255(75)90046-8; ZADEH LA, 1975, INFORM SCIENCES, V9, P43, DOI 10.1016/0020-0255(75)90017-1; ZADEH LA, 1975, INFORM SCIENCES, V8, P199, DOI 10.1016/0020-0255(75)90046-8	29	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1986	8	4					502	511		10.1109/TPAMI.1986.4767814	http://dx.doi.org/10.1109/TPAMI.1986.4767814			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	C7400					2022-12-18	WOS:A1986C740000009
J	OGORMAN, L; SANDERSON, AC				OGORMAN, L; SANDERSON, AC			SOME EXTENSIONS OF THE CONVERGING SQUARES ALGORITHM FOR IMAGE FEATURE ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									CARNEGIE MELLON UNIV,INST ROBOT,PITTSBURGH,PA 15213; CARNEGIE MELLON UNIV,DEPT ELECT & COMP ENGN,PITTSBURGH,PA 15213	Carnegie Mellon University; Carnegie Mellon University	OGORMAN, L (corresponding author), AT&T BELL LABS,MURRAY HILL,NJ 07974, USA.							BAIRD ML, 1977, GMR2502 GM RES LABS; BASSEVILLE M, 1981, IEEE T ACOUST SPEECH, V29, P24, DOI 10.1109/TASSP.1981.1163523; DEBRUYN DSV, 1968, CUMULATIVE SUM TESTS; JOHNSON CR, 1984, THESIS CARNEGIEMELLO; Kittler J., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P287; Moravec H., 1977, P 5 INT JOINT C ART, VVolume 1, P584; OGORMAN L, 1985, IEEE T BIO-MED ENG, V32, P696, DOI 10.1109/TBME.1985.325587; OGORMAN L, 1984, IEEE T PATTERN ANAL, V6, P280, DOI 10.1109/TPAMI.1984.4767520; OGORMAN L, 1985, IEEE T PATTERN ANAL, V7, P326, DOI 10.1109/TPAMI.1985.4767661; PAGE ES, 1954, BIOMETRIKA, V41, P100, DOI 10.2307/2333009; SEGEN J, 1980, IEEE T INFORM THEORY, V26, P249, DOI 10.1109/TIT.1980.1056151; Woodward R.H., 1964, CUMULATIVE SUM TECHN	12	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1986	8	4					520	524		10.1109/TPAMI.1986.4767816	http://dx.doi.org/10.1109/TPAMI.1986.4767816			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	C7400					2022-12-18	WOS:A1986C740000011
J	DEGANO, P; SIROVICH, F				DEGANO, P; SIROVICH, F			AN EVALUATION BASED THEOREM PROVER	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									IST ELABORAZ INFORMAZ,PISA,ITALY		DEGANO, P (corresponding author), UNIV PISA,DEPARTIMENTO INFORMAT,CORSO ITALIA 40,I-56100 PISA,ITALY.							Aubin R., 1979, Theoretical Computer Science, V9, P329, DOI 10.1016/0304-3975(79)90034-3; AUBIN R, 1976, THESIS U EDINBURGH E; Boyer R.S., 1979, COMPUTATIONAL LOGIC; BOYER RS, 1975, J ACM, V22, P129, DOI 10.1145/321864.321875; BOYER RS, 1977, 5TH P INT JOINT C AR, P511; BROTZ DK, 1974, STANCS74443 STANF U; BURSTALL R, 1974, P IFIP C INF PROC, P308; BURSTALL RM, 1969, COMPUT J, V12, P41, DOI 10.1093/comjnl/12.1.41; DEGANO P, 1979, 6TH P INT JOINT C AR, P208; Huet G., 1980, 21st Annual Symposium on Foundations of Computer Science, P96, DOI 10.1109/SFCS.1980.37; Knuth D. E., 1970, COMPUTATIONAL PROBLE, P263; MUSSER DR, 1977, 6TH P TEX C COMP SYS; PETERSON GE, 1981, J ACM, V28, P233, DOI 10.1145/322248.322251; [No title captured]	14	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	1					70	79		10.1109/TPAMI.1985.4767619	http://dx.doi.org/10.1109/TPAMI.1985.4767619			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ABF09	21869241				2022-12-18	WOS:A1985ABF0900006
J	GU, T; DUBUISSON, B				GU, T; DUBUISSON, B			A LOOSE-PATTERN PROCESS APPROACH TO CLUSTERING FUZZY DATA SETS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											GU, T (corresponding author), UNIV TECHNOL COMPIEGNE, F-60206 COMPIEGNE, FRANCE.							Bezdek J.C., 1973, FUZZY MATH PATTERN C; Bezdek J.C., 2013, PATTERN RECOGN, DOI 10.1007/978-1-4757-0450-1; Bezdek James C., 1974, J CYBERNETICS, V3, P58, DOI [DOI 10.1080/01969727308546047, 10.1080/01969727308546047]; BEZDEK JC, 1977, P M N AM SOC GEN SYS, P347; Dunn J.C., 1973, J CYBERNETICS, V3, P32, DOI [10.1080/ 01969727308546046, DOI 10.1080/01969727308546046]; DUNN JC, 1974, IEEE T SYST MAN CYB, V4, P130; GU T, 1981, AUG IEEE COMP SOC PA; GU T, 1982, SCI SINICA A, V25, P1220; HELLMAN ME, 1970, IEEE T SYST SCI CYB, VSSC6, P179, DOI 10.1109/TSSC.1970.300339; RUSPINI EH, 1970, INFORM SCIENCES, V2, P319, DOI 10.1016/S0020-0255(70)80056-1; RUSPINI EH, 1969, INFORM CONTROL, V15, P22, DOI 10.1016/S0019-9958(69)90591-9; RUSPINI EH, 1973, INFORM SCIENCES, V6, P273, DOI 10.1016/0020-0255(73)90043-1; Tou JT, 1974, PATTERN RECOGN; ZADEH LA, 1973, IEEE T SYST MAN CYB, VSMC3, P28, DOI 10.1109/TSMC.1973.5408575; ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X; ZADEH LA, 1977, CLASSIFICATION CLUST	16	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	3					366	372		10.1109/TPAMI.1985.4767669	http://dx.doi.org/10.1109/TPAMI.1985.4767669			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AFM44	21869275				2022-12-18	WOS:A1985AFM4400015
J	SINDEN, FW				SINDEN, FW			SHAPE INFORMATION FROM ROTATED SCANS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											SINDEN, FW (corresponding author), AT&T BELL LABS,MURRAY HILL,NJ 07974, USA.		Rohlf, F J/A-8710-2008					FREEMAN H, 1978, APR IEEE COMP SOC WO; KENDALL MG, 1963, GRIFFINS STATISTICAL, V10; KLINGER A, 1971, IEEE T COMPUT, VC 20, P1014, DOI 10.1109/T-C.1971.223397; KLINGER A, 1971, COMMUN ACM, V14, P21; MALLOWS CL, 1970, J APPL PROBAB, V7, P240, DOI 10.2307/3212164; MILLER GL, COMPCON 83 CRYSTAL C; MOORE DJH, 1974, PATTERN RECOGN, V6, P149, DOI 10.1016/0031-3203(74)90018-1; NICHOL DG, 1982, OPT COMMUN, V43, P168, DOI 10.1016/0030-4018(82)90338-8; NOVIKOFF ABJ, 1962, PRINCIPLES SELF ORGA, P347; PAVLIDIS T, 1980, IEEE T PATTERN ANAL, V2, P301, DOI 10.1109/TPAMI.1980.4767029; PAVLIDIS T, 1978, COMPUT VISION GRAPH, V7, P243, DOI 10.1016/0146-664X(78)90115-6; Rosenfeld Azriel, 1976, DIGITAL PICTURE PROC, V2, P8; TENERY G, 1963, IEEE T MIL ELECTRON, V7, P196; WONG E, 1969, METHODOLOGIES PATTER, P535; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949	15	3	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	6					726	730		10.1109/TPAMI.1985.4767731	http://dx.doi.org/10.1109/TPAMI.1985.4767731			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ATG05	21869313				2022-12-18	WOS:A1985ATG0500013
J	ZABELE, GS; KOPLOWITZ, J				ZABELE, GS; KOPLOWITZ, J			FOURIER ENCODING OF CLOSED PLANAR BOUNDARIES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											ZABELE, GS (corresponding author), CLARKSON UNIV,DEPT ELECT & COMP ENGN,POTSDAM,NY 13676, USA.							AMBLER AP, 1975, ARTIF INTELL, V6, P129, DOI 10.1016/0004-3702(75)90006-5; Berger T, 1971, RATE DISTORTION THEO; CHELLAPPA R, 1984, IEEE T PATTERN ANAL, V6, P102, DOI 10.1109/TPAMI.1984.4767482; COSGRIFF RL, 1960, AD254792; DUDANI SA, 1962, IEEE T COMPUT, V26, P179; GALLAGHER RG, 1968, INFORMATION THEORY R; GRAHAM DN, 1967, PR INST ELECTR ELECT, V55, P336, DOI 10.1109/PROC.1967.5490; GRANLUND GH, 1972, IEEE T COMPUT, VC 21, P195, DOI 10.1109/TC.1972.5008926; HUANG TS, 1977, TREE7710 PURD U SCH; KASHYAP RL, 1981, IEEE T INFORM THEORY, V27, P627, DOI 10.1109/TIT.1981.1056390; MAUERSBERGER W, 1981, IEEE T INFORM THEORY, V27; MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548; Oosterlinck A., 1976, 3rd International Joint Conference on Pattern Recognition, P334; Papoulis A., 2002, PROBABILITY RANDOM V; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; Pratt W. K., 1978, DIGITAL IMAGE PROCES; RICHARD CW, 1974, IEEE T SYST MAN CYB, VSMC4, P371, DOI 10.1109/TSMC.1974.5408458; ROSEN C, 1974, EXPLORATORY RES ADV; SCHEIBER WF, 1972, PICTURE BANDWIDTH CO, P443; Schultheiss P., 1963, IEEE T COMMUN SYST, V11, P289; WALLACE T, COMPUT GRAPHICS IMAG; WOOD RC, 1969, IEEE T INFORM THEORY, V5, P248; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949	23	3	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	1					98	102		10.1109/TPAMI.1985.4767623	http://dx.doi.org/10.1109/TPAMI.1985.4767623			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ABF09	21869245				2022-12-18	WOS:A1985ABF0900010
J	CULLINGFORD, RE; PAZZANI, MJ				CULLINGFORD, RE; PAZZANI, MJ			WORD-MEANING SELECTION IN MULTIPROCESS LANGUAGE UNDERSTANDING PROGRAMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									MITRE CORP,ARTIFICIAL INTELLIGENCE TECHNOL GRP,BEDFORD,MA 01730	MITRE Corporation	CULLINGFORD, RE (corresponding author), UNIV CONNECTICUT,DEPT ELECT ENGN & COMP SCI,STORRS,CT 06268, USA.			Pazzani, Michael/0000-0002-4240-7349				BIRNBAUM L, 1979, 168 YAL U DEP COMP S; BOBROW DG, 1977, ARTIFICIAL INTELL, V8; BROWN JS, 1975, REPRESENTATION UNDER; CARBONELL JG, 1979, 150 YAL U DEP COMP S; CHARNIAK E, 1972, MIT AITR266 REP; Chomsky N., 1965, ASPECTS THEORY SYNTA; CULLINGFORD R, 1982, OCT P IEEE INT C CYB, P444; CULLINGFORD R, 1978, 116 YAL U DEP COMP S; CULLINGFORD R E, 1979, Discourse Processes, V2, P319; CULLINGFORD RE, 1981, IEEE T SYST MAN CYB, V11, P52, DOI 10.1109/TSMC.1981.4308578; CULLINGFORD RE, 1982, IEEE T SYST MAN CYB, V12, P168, DOI 10.1109/TSMC.1982.4308801; CULLINGFORD RE, 1980, P INT C CYBERN SOC C; DeJong G., 1979, 158 YAL U DEP COMP S; GINSPARG JM, 1978, 78671 STANF U DEP CO; GOLDMAN N, 1975, CONCEPTUAL INFORMATI; HIRST G, 1982, 1982 P NAT C ART INT, P95; Hobbs J. R., 1976, 761 CIT U NEW YORK D; HOBBS JR, 1979, COGNITIVE PSYCHOL, V3; KATZ JJ, 1963, LANGUAGE, V39, P170, DOI 10.2307/411200; KIRBY R, 1977, TR546 U MAR DEP COMP; Marcus M. P., 1980, THEORY SYNTACTIC REC; MCDERMOTT J, 1978, PATTERN DIRECTED INF; MEEHAN J, 1976, 74 YAL U DEP COMP SC; NASHWEBBER BL, 1978, THEORETICAL ISSUES R; Newell A, 1972, HUMAN PROBLEM SOLVIN; Norman Donald, 1975, EXPLORATIONS COGNITI; RIEGER C, 1976, ARTIFICIAL INTELL, V7; RIESBECK C, 1975, CONCEPTUAL INFORMATI; RIESBECK CK, 1978, PATTERN DIRECTED INF; RITCHIE D, 1974, COMMUN ASS COMPU JUL; Schank R., 1977, SCRIPTS PLANS GOALS; SCHANK RC, 1972, COGNITIVE PSYCHOL, V3, P552, DOI 10.1016/0010-0285(72)90022-9; SCHANK RC, 1978, RECENT ADV PSYCHOL L; SCHANK RC, 1975, CONCEPTUAL INFORMATI; SCHMIDT C, 1976, P C AI SIMULATION BE; SMALL S, 1980, 954 U MAR DEP COMP S; Turing A.M., 1950, MIND, V49, P433; Wilensky Robert, 1978, 140 YAL U DEP COMP S; WILKS Y, 1975, ARTIF INTELL, V6, P53, DOI 10.1016/0004-3702(75)90016-8; Winograd Terry, 1972, UNDERSTANDING NATURA; WOODS W, 1970, COMMUN ASS COMPUT MA, V13	41	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	4					493	509		10.1109/TPAMI.1984.4767554	http://dx.doi.org/10.1109/TPAMI.1984.4767554			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	SY289	21869217				2022-12-18	WOS:A1984SY28900010
J	SHIPMAN, AL; BITMEAD, RR; ALLEN, GH				SHIPMAN, AL; BITMEAD, RR; ALLEN, GH			DIFFUSE EDGE FITTING AND FOLLOWING - A LOCATION-ADAPTIVE APPROACH	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									AUSTRALIAN NATL UNIV,INST ADV STUDIES,DEPT SYST ENGN,CANBERRA,ACT 2600,AUSTRALIA	Australian National University	SHIPMAN, AL (corresponding author), JAMES COOK UNIV N QUEENSLAND,DEPT ELECT & ELECTR ENGN,TOWNSVILLE,QLD 4811,AUSTRALIA.							BASSEVILLE M, 1981, IEEE T ACOUST SPEECH, V29, P24, DOI 10.1109/TASSP.1981.1163523; DANIELS RW, INTRO NUMERICAL METH; Heuckel M., 1973, J ASSOC COMPUT MACH, V20, P634; HEUCKEL MH, 1971, J ASSOC COMPUT MACH, V18, P113; JACOBUS CJ, 1981, IEEE T PATTERN ANAL, V3, P581, DOI 10.1109/TPAMI.1981.4767149; KITCHEN L, 1981, IEEE T SYST MAN CYB, V11, P597, DOI 10.1109/TSMC.1981.4308758; Niemann H, 1981, PATTERN ANAL; Pavlidis T., 1977, STRUCTURAL PATTERN R; Pratt W. K., 1978, DIGITAL IMAGE PROCES; RAMER EU, 1975, IEEE T CIRCUITS SYST, VCA22, P363, DOI 10.1109/TCS.1975.1084044; Roberts L, 1965, MACHINE PERCEPTION 3; VANDERBRUG GJ, 1978, IEEE T SYST MAN CYB, V8, P768	12	3	4	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	1					96	102		10.1109/TPAMI.1984.4767481	http://dx.doi.org/10.1109/TPAMI.1984.4767481			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SB213	21869171				2022-12-18	WOS:A1984SB21300012
J	SLAGLE, JR; GAYNOR, MW; HALPERN, EJ				SLAGLE, JR; GAYNOR, MW; HALPERN, EJ			AN INTELLIGENT CONTROL STRATEGY FOR COMPUTER CONSULTATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									BLOOMSBURG UNIV, BLOOMSBURG, PA 17815 USA	Pennsylvania State System of Higher Education (PASSHE); Bloomsburg University of Pennsylvania	SLAGLE, JR (corresponding author), USN, RES LAB, CTR APPL RES ARTIFICIAL INTELLIGENCE, WASHINGTON, DC 20375 USA.							DUDA RO, 1979, COMPUTER BASED CONSU; PEDNAULT EPD, 1981, ARTIF INTELL, V16, P213, DOI 10.1016/0004-3702(81)90011-4; Shortliffe E.H., 2012, COMPUTER BASED MED C; Shortliffe EH., 1975, MATH BIOSCI, V23, P351, DOI [10.1016/0025-5564(75)90047-4, DOI 10.1016/0025-5564(75)90047-4]; SLAGLE JR, 1971, INFORM SCIENCES, V3, P315, DOI 10.1016/S0020-0255(71)80013-0; SLAGLE JR, 1968, J ACM, V15, P85, DOI 10.1145/321439.321444; SLAGLE JR, 1971, COMMUN ACM, V14, P91, DOI 10.1145/362515.362560; SLAGLE JR, 1982, NRL4847 MEM REP; SLAGLE JR, 1965, P IFIP C, V2, P323; SLAGLE JR, 1971, ARTIFICIAL INTELLIGE; VANMELLE W, 1979, AUG P IJCAI 79 TOK, P923; WEISS SM, 1978, ARTIF INTELL, V11, P145, DOI 10.1016/0004-3702(78)90015-2; [No title captured]	13	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	2					129	136		10.1109/TPAMI.1984.4767498	http://dx.doi.org/10.1109/TPAMI.1984.4767498			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SF591	21869178				2022-12-18	WOS:A1984SF59100001
J	TEJWANI, YJ; JONES, RA				TEJWANI, YJ; JONES, RA			ON THE DETECTION OF PEAKS AND VALLEYS USING THE LOCAL DESCRIPTORS METHOD	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											TEJWANI, YJ (corresponding author), UNIV ARKANSAS,DEPT ELECT ENGN,FAYETTEVILLE,AR 72701, USA.							[Anonymous], 1978, COURSE DIFFERENTIAL; DAVIS LS, 1977, IEEE T COMPUT, V26, P236, DOI 10.1109/TC.1977.1674812; Duda R.O., 1973, J ROYAL STAT SOC SER; HERMANN R, 1968, DIFFERENTIAL GEOMETR; JOHNSTON E, 1973, IEEE T COMPUT, V22, P875; KASHYAP RL, 1981, IEEE T INFORM THEORY, V27, P627, DOI 10.1109/TIT.1981.1056390; WALLACE TP, 1981, IEEE T PATTERN ANAL, V3, P310, DOI 10.1109/TPAMI.1981.4767104	7	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	5					629	632		10.1109/TPAMI.1984.4767576	http://dx.doi.org/10.1109/TPAMI.1984.4767576			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TM813	21869231				2022-12-18	WOS:A1984TM81300008
J	BAIRD, ML				BAIRD, ML			GAGESIGHT - A COMPUTER VISION SYSTEM FOR AUTOMATIC INSPECTION OF INSTRUMENT GAUGES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									GM CORP,RES LABS,WARREN,MI 48090	General Motors								BAIRD M, 1976, 3RD P INT JOINT C PA; BAIRD ML, 1978, IEEE T SYST MAN CYB, V8, P133; BAIRD ML, 1982, MAY C REC WORKSH IND; BAIRD ML, 1976, GMR2124 DEP COMP SCI; BAIRD ML, 1982, GMR3959 DEP COMP SCI; HATTORI M, 1978, AUG P JAP SOC MECH E; LANGDON DA, 1983, 1983 P INT C EXP SOC; NAGOYA, 2ND TECH C RAT PROD, P81; 1977, HIGH SPEED FLEXIBLE	9	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	6					618	621		10.1109/TPAMI.1983.4767451	http://dx.doi.org/10.1109/TPAMI.1983.4767451			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RV488	21869148				2022-12-18	WOS:A1983RV48800008
J	STRACKEE, J; NAGELKERKE, NJD				STRACKEE, J; NAGELKERKE, NJD			ON CLOSING THE FOURIER DESCRIPTOR PRESENTATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											STRACKEE, J (corresponding author), UNIV AMSTERDAM,MED PHYS LAB,AMSTERDAM,NETHERLANDS.		Rohlf, F J/A-8710-2008					LANCZOS C, 1961, APPLIED ANAL, P240; PAVLIDES T, 1977, STRUCTURAL PATTERN R, P158; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949	3	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	6					660	661		10.1109/TPAMI.1983.4767457	http://dx.doi.org/10.1109/TPAMI.1983.4767457			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RV488	21869154				2022-12-18	WOS:A1983RV48800014
J	KEDEM, B				KEDEM, B			SOME GRAPHICAL CONSIDERATIONS IN TIME-SERIES ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											KEDEM, B (corresponding author), UNIV MARYLAND,DEPT MATH,COLLEGE PK,MD 20742, USA.							BENIGER JR, 1978, AM STAT, V32, P1, DOI 10.2307/2683467; FIENBERG SE, 1979, AM STAT, V33, P165, DOI 10.2307/2683729; KEDEM B, 1981, J MATH PHYS, V22, P456, DOI 10.1063/1.524930; KEDEM B, 1981, BIOMETRIKA, V68, P551, DOI 10.1093/biomet/68.2.551; KEDEM B, 1980, BINARY TIME SERIES; KEDEM B, UNPUB ANN STATIST	6	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	5					493	499		10.1109/TPAMI.1982.4767293	http://dx.doi.org/10.1109/TPAMI.1982.4767293			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PG894	21869068				2022-12-18	WOS:A1982PG89400005
J	ULLMANN, JR				ULLMANN, JR			DISCRETE OPTIMIZATION BY RELATIONAL CONSTRAINT SATISFACTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											ULLMANN, JR (corresponding author), UNIV SHEFFIELD,DEPT COMP SCI,SHEFFIELD S10 2TN,S YORKSHIRE,ENGLAND.							Bellman R., 1962, APPL DYNAMIC PROGRAM; BERTELE V, 1969, J MATH ANAL APPLICAT, V27, P565; BEVERIDGE GSG, 1970, OPTIMIZATION THEORY, pCH13; BOLTYANSKII VG, 1978, OPTIMAL CONTROL DISC, P46; BRIOSCHI F, 1970, OPER RES, V18, P66, DOI 10.1287/opre.18.1.66; FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602; HARALICK RM, 1980, IEEE T PATTERN ANAL, V2, P193, DOI 10.1109/TPAMI.1980.4767007; HARALICK RM, 1980, ARTIF INTELL, V14, P263, DOI 10.1016/0004-3702(80)90051-X; MCGREGOR JJ, 1979, INFORM SCIENCES, V19, P229, DOI 10.1016/0020-0255(79)90023-9; ULLMANN JR, 1976, J ACM, V23, P31, DOI 10.1145/321921.321925; ULLMANN JR, 1979, C SERIES, V44, P50	11	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	5					544	551		10.1109/TPAMI.1982.4767300	http://dx.doi.org/10.1109/TPAMI.1982.4767300			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	PG894	21869075				2022-12-18	WOS:A1982PG89400012
J	BOGNER, RE				BOGNER, RE			PATTERN-RECOGNITION VIA OBSERVATION CORRELATIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									BELL TEL LABS INC,DEPT ACOUST RES,MURRAY HILL,NJ 07974	AT&T; Nokia Corporation; Nokia Bell Labs								BOGNER RE, 1981, IEEE T ACOUST SPEECH, V29, P1, DOI 10.1109/TASSP.1981.1163505; JESORSKY P, 1978, SPEECH COMMUNICATION; LI KP, 1974, J ACOUST SOC AM, V55, P833, DOI 10.1121/1.1914608; MAKHOUL J, 1975, P IEEE, V63, P561, DOI 10.1109/PROC.1975.9792; SAMBUR MR, 1976, IEEE T ACOUST SPEECH, V24, P283, DOI 10.1109/TASSP.1976.1162826	5	3	4	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	2					128	133		10.1109/TPAMI.1981.4767070	http://dx.doi.org/10.1109/TPAMI.1981.4767070			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN968	21868927				2022-12-18	WOS:A1981MN96800002
J	KATSULAI, H; ARIMIZU, N				KATSULAI, H; ARIMIZU, N			EVALUATION OF IMAGE FIDELITY BY MEANS OF THE FIDELOGRAM AND LEVEL MEAN-SQUARE ERROR	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											KATSULAI, H (corresponding author), CHIBA UNIV,FAC MED,DEPT RADIOL,CHIBA 280,JAPAN.							Andrews H.C., 1970, COMPUTER TECHNIQUES; BORN M, 1970, PRINCIPLES OPTICS; HALL EL, 1971, IEEE T COMPUT, VC 20, P1032, DOI 10.1109/T-C.1971.223399; HORN BK, 1978, COMMUN ASS COMPUT MA, V21; KATSULAI H, 1979, IEEE T NUCL SCI, V26; Pratt W. K., 1978, DIGITAL IMAGE PROCES; ROSENFELD A, 1969, PICTURE PROCESSING C; SCHADE OH, 1964, APPL OPT, V3	8	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	3					337	347		10.1109/TPAMI.1981.4767107	http://dx.doi.org/10.1109/TPAMI.1981.4767107			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN969	21868955				2022-12-18	WOS:A1981MN96900011
J	MERO, L				MERO, L			AN OPTIMAL LINE FOLLOWING ALGORITHM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											MERO, L (corresponding author), HUNGARIAN ACAD SCI,INST COMP & AUTOMAT,H-1361 BUDAPEST 5,HUNGARY.		Mero, Laszlo/I-4931-2017	Mero, Laszlo/0000-0001-6906-3673				FREEMAN H, 1977, IEEE T COMPUT, V26, P297, DOI 10.1109/TC.1977.1674825; HUECKEL MH, 1973, J ACM, V20, P634, DOI 10.1145/321784.321791; HUECKEL MH, 1971, J ACM, V18, P113, DOI 10.1145/321623.321635; HUMMEL RA, 1979, COMPUT VISION GRAPH, V9, P40, DOI 10.1016/0146-664X(79)90081-9; MARTELLI A, 1973, 1ST P 17CPR WASH; MARTELLI A, 1972, COMPUTER GRAPHICS IM, V1, P169, DOI DOI 10.1016/S0146-664X(72)80013-3; MERO L, 1975, 4TH P INT JOINT C AR, P650; MERO L, 1978, P AISBGI C ARTIFICIA, P189; MONTANARI U, 1971, COMMUN ACM, V14, P335, DOI 10.1145/362588.362594; PERKINS WA, 1973, J COMPUT GRAPHICS IM, V2, P355; RAMER EU, 1975, IEEE T CIRCUITS SYST, VCA22, P363, DOI 10.1109/TCS.1975.1084044; SHAW GB, 1979, COMPUT VISION GRAPH, V9, P135, DOI 10.1016/0146-664X(79)90053-4; VAMOS T, 1979, 6TH P IJCAI TOK, P920; VAMOS T, 1977, IND OBJECT MACHINE P, pCH10	14	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	5					593	598		10.1109/TPAMI.1981.4767151	http://dx.doi.org/10.1109/TPAMI.1981.4767151			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MQ358	21868977				2022-12-18	WOS:A1981MQ35800009
J	OSTREM, JS; FALCONER, DG				OSTREM, JS; FALCONER, DG			A DIFFERENTIAL OPERATOR TECHNIQUE FOR RESTORING DEGRADED SIGNALS AND IMAGES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									SRI INT, CTR ARTIFICIAL INTELL GE KE, MENLO PK, CA 94025 USA	SRI International	OSTREM, JS (corresponding author), SRI INT, BIOENGN RES CTR, MENLO PK, CA 94025 USA.							ANDREWS HC, 1974, DIGITAL IMAGE RESTOR; Bracewell R, 1986, PENTAGRAM NOTATION C, V2nd, P192; Brigham E. O., 1974, FAST FOURIER TRANSFO; DERIUGIN NG, 1957, TELECOMMUNICATIONS, P1; ELIAS P, 1952, J OPT SOC AM, V42, P127, DOI 10.1364/JOSA.42.000127; HARRIS JL, 1966, J OPT SOC AM, V56, P569, DOI 10.1364/JOSA.56.000569; HELSTROM CW, 1967, J OPT SOC AM, V57, P297, DOI 10.1364/JOSA.57.000297; HUNT BR, 1975, P IEEE, V63, P693, DOI 10.1109/PROC.1975.9801; HUNT BR, 1973, 6 P HAW INT C SYST S, P154; MCGLAMER.BL, 1967, J OPT SOC AM, V57, P293, DOI 10.1364/JOSA.57.000293; PAPOULIS A, 1965, PROBABILITY RANDOM V, P267; Pratt W. K., 1978, DIGITAL IMAGE PROCES; PRATT WK, 1972, IEEE T COMPUT, VC 21, P636, DOI 10.1109/T-C.1972.223567; Rosenfeld Azriel, 1976, DIGITAL PICTURE PROC, V2, P8; SCHREIBER WF, 1978, P IEEE, V66, P1640, DOI 10.1109/PROC.1978.11172; SLEPIAN D, 1967, AT&T TECH J, V46, P2353, DOI 10.1002/j.1538-7305.1967.tb02461.x; WIENER N, 1966, EXTRAPOLATION INTERP	17	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	3					278	284		10.1109/TPAMI.1981.4767100	http://dx.doi.org/10.1109/TPAMI.1981.4767100			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN969	21868948				2022-12-18	WOS:A1981MN96900004
J	WU, AY; DUBITZKI, T; ROSENFELD, A				WU, AY; DUBITZKI, T; ROSENFELD, A			PARALLEL COMPUTATION OF CONTOUR PROPERTIES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											WU, AY (corresponding author), UNIV MARYLAND,CTR COMP SCI,COMP VIS LAB,COLLEGE PK,MD 20742, USA.							COLE SN, 1969, IEEE T COMPUT, VC 18, P349, DOI 10.1109/T-C.1969.222663; DYER CR, 1978, 710 U MAR DEP COMP S; Freeman H., 1974, Computing Surveys, V6, P57, DOI 10.1145/356625.356627; MOORE FR, 1968, INFORM CONTROL, V12, P212, DOI 10.1016/S0019-9958(68)90309-4; ROSENFELD A, 1974, IEEE T COMPUT, VC 23, P1264, DOI 10.1109/T-C.1974.223845; Rosenfeld A., 1979, COMPUTER SCI APPL MA; RUTKOWSKI WS, 1978, 623 U MAR DEP COMP S; SMITH AR, 1971, 12 ANN S SWITCH AUT, P144; Smith III A.R., 1972, J COMPUT SYST SCI, V6, P233, DOI DOI 10.1016/S0022-0000(72)80004-7; WU A, 1979, INFORM CONTROL, V42, P305, DOI 10.1016/S0019-9958(79)90288-2; WU A, 1979, 730 U MAR DEP COMP S	12	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	3					331	337		10.1109/TPAMI.1981.4767106	http://dx.doi.org/10.1109/TPAMI.1981.4767106			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN969	21868954				2022-12-18	WOS:A1981MN96900010
J	AGGARWAL, JK; BADLER, NI				AGGARWAL, JK; BADLER, NI			MOTION AND TIME-VARYING IMAGERY - PREFACE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									UNIV PENN,DEPT COMP & INFORMAT SCI,PHILADELPHIA,PA 19104	University of Pennsylvania	AGGARWAL, JK (corresponding author), UNIV TEXAS,AUSTIN,TX 78712, USA.								0	3	3	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	6					493	493						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KS962					2022-12-18	WOS:A1980KS96200001
J	COOK, PN; COOK, LT; BATNITZKY, S; LEE, KR; ANDERSON, WH; DWYER, SJ				COOK, PN; COOK, LT; BATNITZKY, S; LEE, KR; ANDERSON, WH; DWYER, SJ			VOLUME AND SURFACE-AREA ESTIMATES USING TOMOGRAPHIC DATA	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											COOK, PN (corresponding author), UNIV KANSAS,MED CTR,DEPT DIAGNOST RADIOL,KANSAS CITY,KS 66103, USA.							BARTRUM R J JR, 1974, Journal of Clinical Ultrasound, V2, P281, DOI 10.1002/jcu.1870020404; BRINKLEY J F, 1978, Ultrasound in Medicine and Biology, V4, P317, DOI 10.1016/0301-5629(78)90020-0; FUCHS H, 1977, COMMUN ACM, V20, P693, DOI 10.1145/359842.359846; MOSKOWITZ PS, 1980, RADIOLOGY, V134, P61, DOI 10.1148/radiology.134.1.7350636; Rasmussen S. N., 1979, Medical Imaging Techniques. A Comparison, P175; WALSER RL, 1977, J COMPUT ASSIST TOMO, V1, P117, DOI 10.1097/00004728-197701000-00014	6	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	5					478	479		10.1109/TPAMI.1980.6592370	http://dx.doi.org/10.1109/TPAMI.1980.6592370			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KW185					2022-12-18	WOS:A1980KW18500012
J	KIM, CE; STRINTZIS, MG				KIM, CE; STRINTZIS, MG			HIGH-SPEED MULTIDIMENSIONAL CONVOLUTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											KIM, CE (corresponding author), UNIV PITTSBURGH,DEPT ELECT ENGN,PITTSBURGH,PA 15261, USA.							AGARWAL RC, 1974, IEEE T ACOUST SPEECH, VSP22, P87, DOI 10.1109/TASSP.1974.1162555; AGARWAL RC, 1975, P IEEE, V63, P550, DOI 10.1109/PROC.1975.9791; AGARWAL RC, 1977, IEEE T ACOUST SPEECH, V25, P392, DOI 10.1109/TASSP.1977.1162981; COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354; JAGADEESAN M, 1970, 13TH P MIDW S CIRC T; KALBA D, 1977, IEEE T ACOUST SPEECH, V25, P281; KIM CE, 1978, 1978 P IEEE COMP SOC; RABINER L, 1975, THEORY APPLICATIONS; RADER CM, 1968, PR INST ELECTR ELECT, V56, P1107, DOI 10.1109/PROC.1968.6477; RADER CM, 1975, IEEE T CIRCUITS SYST, VCA22, P575, DOI 10.1109/TCS.1975.1084073; SILVERMAN HF, 1977, IEEE T ACOUST SPEECH, V25, P152, DOI 10.1109/TASSP.1977.1162924; SINGLETO.RC, 1969, IEEE T ACOUST SPEECH, VAU17, P93, DOI 10.1109/TAU.1969.1162042; STOCKHAM TG, 1966, 1966 SPRING JOINT CO, V28, P229; WINOGRAD S, 1976, P NATL ACAD SCI USA, V73, P1005, DOI 10.1073/pnas.73.4.1005	14	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	3					269	273		10.1109/TPAMI.1980.4767017	http://dx.doi.org/10.1109/TPAMI.1980.4767017			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JR843	21868903				2022-12-18	WOS:A1980JR84300010
J	AGUI, T; NAGAHASHI, H				AGUI, T; NAGAHASHI, H			CODING METHOD OF CHINESE-CHARACTERS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											AGUI, T (corresponding author), TOKYO INST TECHNOL,IMAGING SCI & ENGN LAB,MIDORI KU,YOKOHAMA 227,JAPAN.							AGUI T, 1979, IEEE T PATTERN ANAL, V1, P20, DOI 10.1109/TPAMI.1979.4766872; AGUI T, 1977, J I ELECTRON COMM JD, V60, P1109; AGUI T, 1977, J I ELECTRON COM DEC; FU KS, 1975, SYNTACTIC METHODS PA; FU KS, 1977, SYNTACTIC PATTERN RE, P95; HAMBLIN CL, 1962, COMPUT J, V3, P210; RANKIN BK, 1965, THESIS U PENNSYLVANI; RANKIN BK, 1966, NBS296 TECH NOT; STALLINGS W, 1975, COMPUT HUMANITIES, V9, P13, DOI 10.1007/BF02404316	9	3	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	4					333	341		10.1109/TPAMI.1979.4766941	http://dx.doi.org/10.1109/TPAMI.1979.4766941			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HV227	21868867				2022-12-18	WOS:A1979HV22700001
J	PHILIP, J				PHILIP, J			DIGITAL IMAGE AND SPECTRUM RESTORATION BY QUADRATIC PROGRAMMING AND BY MODIFIED FOURIER TRANSFORMATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											PHILIP, J (corresponding author), ROYAL INST TECHNOL, DEPT MATH, S-10044 STOCKHOLM 70, SWEDEN.							Andrews H.C., 1977, DIGITAL IMAGE RESTOR; BURG JP, 1967, 37TH ANN SOC EXPL GE; COOLEY JW, 1967, IEEE T ACOUST SPEECH, VAU15, P79, DOI 10.1109/TAU.1967.1161904; FRIEDEN BR, 1972, J OPT SOC AM, V62, P511, DOI 10.1364/JOSA.62.000511; HARRIS JL, 1966, J OPT SOC AM, V56, P569, DOI 10.1364/JOSA.56.000569; HELSTROM CW, 1967, J OPT SOC AM, V57, P297, DOI 10.1364/JOSA.57.000297; HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044; HOU HS, 1977, IEEE T COMPUT, V26, P856, DOI 10.1109/TC.1977.1674934; Huang TS, 1979, PICTURE PROCESSING D, V2nd; HUNT BR, 1971, IEEE T ACOUST SPEECH, VAU19, P285, DOI 10.1109/TAU.1971.1162202; HUNT BR, 1973, IEEE T COMPUT, VC 22, P805, DOI 10.1109/TC.1973.5009169; LEMKE CE, 1965, MANAGE SCI, V11, P681, DOI 10.1287/mnsc.11.7.681; Luenberger D. G., 1969, OPTIMIZATION VECTOR; MACADAM DP, 1970, J OPT SOC AM, V60, P1617, DOI 10.1364/JOSA.60.001617; NASHED MZ, 1976, GENERALIZED INVERSES; PHILIP J, 1963, J MATH ANAL APPL, V7, P327; PHILIP J, 1973, APR P IEEE, V61, P468; PHILIP J, 1966, Z WAHRSCHEINLICHKEIT, V5, P55; PHILLIPS DL, 1962, J ACM, V9, P84, DOI 10.1145/321105.321114; Rosenfeld Azriel, 1976, DIGITAL PICTURE PROC, V2, P8; Rust BW, 1972, MATH PROGRAMMING NUM; SONDHI MM, 1972, PR INST ELECTR ELECT, V60, P842, DOI 10.1109/PROC.1972.8783; TWOMEY S, 1963, J ACM, V10, P97, DOI 10.1145/321150.321157; WOLFE P, 1959, ECONOMETRICA, V27, P382, DOI 10.2307/1909468	24	3	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	4					385	399		10.1109/TPAMI.1979.4766947	http://dx.doi.org/10.1109/TPAMI.1979.4766947			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HV227	21868873				2022-12-18	WOS:A1979HV22700007
J	Agresti, G; Schafer, H; Sartor, P; Incesu, Y; Zanuttigh, P				Agresti, Gianluca; Schaefer, Henrik; Sartor, Piergiorgio; Incesu, Yalcin; Zanuttigh, Pietro			Unsupervised Domain Adaptation of Deep Networks for ToF Depth Refinement	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Noise reduction; Distortion; Frequency modulation; Frequency-domain analysis; Deep learning; Thermal sensors; Adversarial machine learning; Time-of-flight; depth; denoising; deep learning; unsupervised domain adaptation; adversarial learning		Depth maps acquired with ToF cameras have a limited accuracy due to the high noise level and to the multi-path interference. Deep networks can be used for refining ToF depth, but their training requires real world acquisitions with ground truth, which is complex and expensive to collect. A possible workaround is to train networks on synthetic data, but the domain shift between the real and synthetic data reduces the performances. In this paper, we propose three approaches to perform unsupervised domain adaptation of a depth denoising network from synthetic to real data. These approaches are respectively acting at the input, at the feature and at the output level of the network. The first approach uses domain translation networks to transform labeled synthetic ToF data into a representation closer to real data, that is then used to train the denoiser. The second approach tries to align the network internal features related to synthetic and real data. The third approach uses an adversarial loss, implemented with a discriminator trained to recognize the ground truth statistic, to train the denoiser on unlabeled real data. Experimental results show that the considered approaches are able to outperform other state-of-the-art techniques and achieve superior denoising performances.	[Agresti, Gianluca; Schaefer, Henrik; Sartor, Piergiorgio; Incesu, Yalcin] Sony R&D Ctr, Europe Stuttgart Lab 1, D-70327 Stuttgart, Germany; [Zanuttigh, Pietro] Univ Padua, I-35122 Padua, Italy	University of Padua	Agresti, G (corresponding author), Sony R&D Ctr, Europe Stuttgart Lab 1, D-70327 Stuttgart, Germany.	gianluca.agresti@sony.com; henrik.schaefer@sony.com; piergiorgio.sartor@sony.com; yalcin.incesu@sony.com; zanuttigh@dei.unipd.it			Sony Europe B.V.	Sony Europe B.V.	The research for this paper was funded by Sony Europe B.V. Gianluca Agresti, Henrik Schaefer, Piergiorgio Sartor and Yalcin Incesu are Sony Europe B.V. employees. Moreover, part of the work was carried out by Gianluca Agresti during his PhD, funded by Sony Europe B.V.	Achar S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073686; Agresti G, 2019, PROC CVPR IEEE, P5569, DOI 10.1109/CVPR.2019.00573; Agresti G, 2019, INFORM FUSION, V49, P161, DOI 10.1016/j.inffus.2018.11.006; Agresti G, 2019, LECT NOTES COMPUT SC, V11131, P410, DOI 10.1007/978-3-030-11015-4_30; Agresti G, 2017, IEEE INT CONF COMP V, P697, DOI 10.1109/ICCVW.2017.88; Agresti Gianluca, 2018, P EUR C COMP VIS ECC, P0; Bhandari Ayush, 2014, IEEE Sensors 2014. Proceedings, P614, DOI 10.1109/ICSENS.2014.6985073; Bhandari A, 2014, OPT LETT, V39, P1705, DOI 10.1364/OL.39.001705; Blender Swap, BLEND SWAP WEBS; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Buratto E, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21061962; Can Pu, 2019, Arxiv, DOI arXiv:1803.06657; Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542; Eichhardt I, 2017, MACH VISION APPL, V28, P267, DOI 10.1007/s00138-017-0831-9; Freedman D, 2014, LECT NOTES COMPUT SC, V8689, P234, DOI 10.1007/978-3-319-10590-1_16; Fuchs Stefan, 2013, Computer Vision Systems. 9th International Conference, ICVS 2013. Proceedings: LNCS 7963, P31, DOI 10.1007/978-3-642-39402-7_4; Fuchs Stefan, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3583, DOI 10.1109/ICPR.2010.874; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo Q, 2018, LECT NOTES COMPUT SC, V11205, P381, DOI 10.1007/978-3-030-01246-5_23; Gupta M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3152155; Gupta M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2735702; Gutierrez-Barragan F, 2019, PROC CVPR IEEE, P1566, DOI 10.1109/CVPR.2019.00166; Hansard Miles, 2012, TIME FLIGHT CAMERAS, P2; Hoffman J, 2018, PR MACH LEARN RES, V80; Jimenez D, 2014, IMAGE VISION COMPUT, V32, P1, DOI 10.1016/j.imavis.2013.10.008; Jung J, 2015, IEEE T PATTERN ANAL, V37, P1501, DOI 10.1109/TPAMI.2014.2363827; Katrolia JS, 2021, VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL. 4: VISAPP, P353, DOI 10.5220/0010252403530361; Kingma D.P, P 3 INT C LEARNING R; Kushida T., 2020, IPSJ T COMPUT VIS AP, V12, P1; Lange R, 2000, P SOC PHOTO-OPT INS, V3965, P177, DOI 10.1117/12.385434; Lefloch Damien, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P3, DOI 10.1007/978-3-642-44964-2_1; Lenzen Frank, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P337; Lindner M, 2010, COMPUT VIS IMAGE UND, V114, P1318, DOI 10.1016/j.cviu.2009.11.002; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Marco J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130884; Meister S., 2013, P VIS MOD VIS, P33; Morerio P., 2018, PROC INT C LEARN REP; Poggi M, 2020, IEEE SENS J, V20, P1411, DOI 10.1109/JSEN.2019.2946591; Qiu D, 2019, IEEE I CONF COMP VIS, P9993, DOI 10.1109/ICCV.2019.01009; Ren ZZ, 2018, PROC CVPR IEEE, P762, DOI [10.1109/CVPR.2018.00086, 10.1109/CVPR.2018.00104]; Roy S, 2019, PROC CVPR IEEE, P9463, DOI 10.1109/CVPR.2019.00970; Schafer H, 2014, OPT EXPRESS, V22, P29835, DOI 10.1364/OE.22.029835; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Son K, 2016, IEEE INT CONF ROBOT, P3390, DOI 10.1109/ICRA.2016.7487515; Su SC, 2018, PROC CVPR IEEE, P6383, DOI 10.1109/CVPR.2018.00668; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; The Blender Foundation, BLEND WEBS; Toldo M, 2020, TECHNOLOGIES, V8, DOI 10.3390/technologies8020035; Tonioni A, 2020, IEEE T PATTERN ANAL, V42, P2396, DOI 10.1109/TPAMI.2019.2940948; Whyte R, 2014, IEEE SENSOR; Whyte R, 2015, OPT ENG, V54, DOI 10.1117/1.OE.54.11.113109; Yan Chen, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2246, DOI 10.1109/WACV45572.2020.9093594; Zanuttigh Pietro, 2016, TIME OF FLIGHT STRUC; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	57	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9195	9208		10.1109/TPAMI.2021.3123843	http://dx.doi.org/10.1109/TPAMI.2021.3123843			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34714740				2022-12-18	WOS:000880661400046
J	Bello, JLG; Kim, M				Bello, Juan Luis Gonzalez; Kim, Munchurl			Self-Supervised Deep Monocular Depth Estimation With Ambiguity Boosting	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Boosting; Cameras; Estimation; Kernel; Task analysis; Image reconstruction; Monocular depth estimation; self-supervised learning; ambiguity boosting	STEREO	We propose a novel two-stage training strategy with ambiguity boosting for the self-supervised learning of single view depths from stereo images. Our proposed two-stage learning strategy first aims to obtain a coarse depth prior by training an auto-encoder network for a stereoscopic view synthesis task. This prior knowledge is then boosted and used to self-supervise the model in the second stage of training in our novel ambiguity boosting loss. Our ambiguity boosting loss is a confidence-guided type of data augmentation loss that improves the accuracy and consistency of generated depth maps under several transformations of the single-image input. To show the benefits of the proposed two-stage training strategy with boosting, our two previous depth estimation (DE) networks, one with t-shaped adaptive kernels and the other with exponential disparity volumes, are extended with our new learning strategy, referred to as DBoosterNet-t and DBoosterNet-e, respectively. Our self-supervised DBoosterNets are competitive, and in some cases even better, compared to the most recent supervised SOTA methods, and are remarkably superior to the previous self-supervised methods for monocular DE on the challenging KITTI dataset. We present intensive experimental results, showing the efficacy of our method for the self-supervised monocular DE task.	[Bello, Juan Luis Gonzalez; Kim, Munchurl] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Kim, M (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.	juanluisgb@kaist.ac.kr; mkimee@kaist.ac.kr		Kim, Munchurl/0000-0003-0146-5419	Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [2017-0-00419]	Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2017-0-00419, Intelligent High Realistic Visual Processing for Smart Broadcasting Media, 100%).	Abrams A, 2012, LECT NOTES COMPUT SC, V7573, P357, DOI 10.1007/978-3-642-33709-3_26; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bello JLG, 2019, IEEE IMAGE PROC, P474, DOI 10.1109/ICIP.2019.8803827; Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Eigen D, 2014, ADV NEUR IN, V27; Facil JM, 2019, PROC CVPR IEEE, P11818, DOI 10.1109/CVPR.2019.01210; Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052; Garg R, 2019, IEEE I CONF COMP VIS, P7627, DOI 10.1109/ICCV.2019.00772; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Geirhos R, 2020, NAT MACH INTELL, V2, P665, DOI 10.1038/s42256-020-00257-z; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gonzalez-Miguel J, 2020, PARASITOLOGY, V147, P729, DOI 10.1017/S0031182020000268; GonzalezBello Juan Luis, 2020, P ADV NEUR INF PROC, V33, P12626; Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907; Guizilini V, 2021, PROC CVPR IEEE, P11073, DOI 10.1109/CVPR46437.2021.01093; Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256; Guizilini Vitor, 2020, INT C LEARN REPR; Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30; Gur S, 2019, PROC CVPR IEEE, P7675, DOI 10.1109/CVPR.2019.00787; Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232; Horry Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P225, DOI 10.1145/258734.258854; Im S., 2019, P INT C LEARN REPR M, P1; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Lai HY, 2019, PROC CVPR IEEE, P1890, DOI 10.1109/CVPR.2019.00199; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lee J. H., 2019, ARXIV; Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350; Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97; Luo Y, 2018, PROC CVPR IEEE, P155, DOI 10.1109/CVPR.2018.00024; Malik AS, 2008, PATTERN RECOGN, V41, P2200, DOI 10.1016/j.patcog.2007.12.014; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37; Pillai S, 2019, IEEE INT CONF ROBOT, P9250, DOI 10.1109/ICRA.2019.8793621; Pilzer A, 2019, PROC CVPR IEEE, P9760, DOI 10.1109/CVPR.2019.01000; Poggi M, 2018, INT CONF 3D VISION, P324, DOI 10.1109/3DV.2018.00045; Ranftl R, 2016, PROC CVPR IEEE, P4058, DOI 10.1109/CVPR.2016.440; Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Saxena Ashutosh, 2006, ADV NEURAL INFORM PR, P1, DOI [DOI 10.1109/TPAMI.2015.2505283A, 10.1109/TPAMI.2015.2505283a]; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003; Tucker R, 2020, PROC CVPR IEEE, P548, DOI 10.1109/CVPR42600.2020.00063; Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012; van Dijk T, 2019, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2019.00227; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang R, 2019, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR.2019.00570; Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826; Watson J, 2019, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2019.00225; Wong A, 2019, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR.2019.00579; Woodford O. J., 2007, PROC BRIT MACH VIS C, P1120; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51; Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578; Zhang Y, 2019, PROC CVPR IEEE, P5853, DOI 10.1109/CVPR.2019.00601; Zhou JS, 2019, IEEE I CONF COMP VIS, P6871, DOI 10.1109/ICCV.2019.00697; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	64	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9131	9149		10.1109/TPAMI.2021.3124079	http://dx.doi.org/10.1109/TPAMI.2021.3124079			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34727025				2022-12-18	WOS:000880661400042
J	Charte, D; Charte, F; Herrera, F				Charte, David; Charte, Francisco; Herrera, Francisco			Reducing Data Complexity Using Autoencoders With Class-Informed Loss Functions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Complexity theory; Feature extraction; Measurement; Shape; Support vector machines; Data models; Transforms; Autoencoders; dimension reduction; data complexity	DIMENSIONALITY REDUCTION; FEATURE-SELECTION	Available data in machine learning applications is becoming increasingly complex, due to higher dimensionality and difficult classes. There exists a wide variety of approaches to measuring complexity of labeled data, according to class overlap, separability or boundary shapes, as well as group morphology. Many techniques can transform the data in order to find better features, but few focus on specifically reducing data complexity. Most data transformation methods mainly treat the dimensionality aspect, leaving aside the available information within class labels which can be useful when classes are somehow complex. This paper proposes an autoencoder-based approach to complexity reduction, using class labels in order to inform the loss function about the adequacy of the generated variables. This leads to three different new feature learners, Scorer, Skaler and Slicer. They are based on Fisher's discriminant ratio, the Kullback-Leibler divergence and least-squares support vector machines, respectively. They can be applied as a preprocessing stage for a binary classification problem. A thorough experimentation across a collection of 27 datasets and a range of complexity and classification metrics shows that class-informed autoencoders perform better than 4 other popular unsupervised feature extraction techniques, especially when the final objective is using the data for a classification task.	[Charte, David; Herrera, Francisco] Univ Granada, Comp Sci & AI Dept, Granada 18071, Spain; [Charte, Francisco] Univ Jaen, Comp Sci Dept, Jaen 23071, Spain; [Herrera, Francisco] King Abdulaziz Univ, Fac Comp & Informat Technol, Jeddah 21589, Saudi Arabia	University of Granada; Universidad de Jaen; King Abdulaziz University	Charte, D (corresponding author), Univ Granada, Comp Sci & AI Dept, Granada 18071, Spain.	fdavidcl@ugr.es; fcharte@ujaen.es; herrera@decsai.ugr.es		Charte Luque, Francisco David/0000-0002-4830-9512	Spanish Ministry of Science under the FPU Program [FPU17/04069]; Spanish Ministry of Science project [PID2020-119478GB-I00, PID2019-107793GB-I00]; Andalusian Excellence project [P18-FR-4961]; project DeepSCOP Ayudas Fundacion BBVA a Equipos de Investigacion Cientifica en Big Data 2018	Spanish Ministry of Science under the FPU Program; Spanish Ministry of Science project(Spanish Government); Andalusian Excellence project; project DeepSCOP Ayudas Fundacion BBVA a Equipos de Investigacion Cientifica en Big Data 2018	David Charte was supported by the Spanish Ministry of Science under the FPU Program (Ref. FPU17/04069). Francisco Charte was supported by the Spanish Ministry of Science project PID2019-107793GB-I00/AEI/10.13039/501100011033. Francisco Herrera was supported by the Spanish Ministry of Science project PID2020-119478GB-I00 and the Andalusian Excellence project P18-FR-4961. This work was supported by the project DeepSCOP Ayudas Fundacion BBVA a Equipos de Investigacion Cientifica en Big Data 2018.	Aggarwal C.C, 2015, OUTLIER ANAL, P237; Aggarwal CC, 2001, LECT NOTES COMPUT SC, V1973, P420; Ayesha S, 2020, INFORM FUSION, V59, P44, DOI 10.1016/j.inffus.2020.01.005; Ballard D.H., 1987, P 6 NAT C ART INT, V647, P279; Basu M, 2006, DATA COMPLEXITY PATT; Bengio Y., 2012, P ICML WORKSH UNS TR, P17, DOI DOI 10.1109/83.902291; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; BORG I., 2005, MODERN MULTIDIMENSIO, P207; Charte D, 2020, NEUROCOMPUTING, V404, P93, DOI 10.1016/j.neucom.2020.04.057; Charte D, 2018, INFORM FUSION, V44, P78, DOI 10.1016/j.inffus.2017.12.007; Dada EG, 2019, HELIYON, V5, DOI 10.1016/j.heliyon.2019.e01802; Deo RC, 2015, CIRCULATION, V132, P1920, DOI 10.1161/CIRCULATIONAHA.115.001593; Fisher RA, 1938, ANN EUGENIC, V8, P376, DOI 10.1111/j.1469-1809.1938.tb02189.x; Garcia S, 2015, INTEL SYST REF LIBR, V72, P1, DOI 10.1007/978-3-319-10247-4; Goldberger J, 2004, ADV NEURAL INF PROCE, V17, P513, DOI DOI 10.5555/2976040.2976105; Ho TK, 2002, IEEE T PATTERN ANAL, V24, P289, DOI 10.1109/34.990132; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jolliffe I., 1986, SPRINGER SERIES STAT; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kingma DP, 2014, ADV NEUR IN, V27; Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852; Lorena AC, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3347711; Suarez JL, 2021, NEUROCOMPUTING, V425, P300, DOI 10.1016/j.neucom.2020.08.017; Manukyan A, 2016, J MACH LEARN RES, V17; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Ng, 2011, CS294A LECT NOTES, V72, P1; Pascual-Triana JD, 2021, KNOWL INF SYST, V63, P1961, DOI 10.1007/s10115-021-01577-1; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Quinlan J. R., 1993, C4 5 PROGRAMS MACHIN; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang L, 2008, IEEE T PATTERN ANAL, V30, P1534, DOI 10.1109/TPAMI.2007.70799; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Weiss G. M., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P558; Xiong H, 2006, IEEE T KNOWL DATA EN, V18, P304, DOI 10.1109/TKDE.2006.46; Yang YM, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P42, DOI 10.1145/312624.312647; Zhang YS, 2013, NEUROCOMPUTING, V101, P32, DOI 10.1016/j.neucom.2012.06.036	45	2	2	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9549	9560		10.1109/TPAMI.2021.3127698	http://dx.doi.org/10.1109/TPAMI.2021.3127698			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34767504	Green Submitted			2022-12-18	WOS:000880661400070
J	Fan, WT; Yang, L; Bouguila, N				Fan, Wentao; Yang, Lin; Bouguila, Nizar			Unsupervised Grouped Axial Data Modeling via Hierarchical Bayesian Nonparametric Models With Watson Distributions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Data models; Bayes methods; Analytical models; Mixture models; Modeling; Tools; Task analysis; Axial data; Watson distribution; hierarchical nonparametric Bayesian model; hierarchical Dirichlet process; hierarchical Pitman-Yor process; variational Bayes; gene clustering; depth image	VARIATIONAL INFERENCE; TRANSCRIPTIONAL PROGRAM; DIRICHLET	This paper aims at proposing an unsupervised hierarchical nonparametric Bayesian framework for modeling axial data (i.e., observations are axes of direction) that can be partitioned into multiple groups, where each observation within a group is sampled from a mixture of Watson distributions with an infinite number of components that are allowed to be shared across different groups. First, we propose a hierarchical nonparametric Bayesian model for modeling grouped axial data based on the hierarchical Pitman-Yor process mixture model of Watson distributions. Then, we demonstrate that by setting the discount parameters of the proposed model to 0, another hierarchical nonparametric Bayesian model based on hierarchical Dirichlet process can be derived for modeling axial data. To learn the proposed models, we systematically develop a closed-form optimization algorithm based on the collapsed variational Bayes (CVB) inference. Furthermore, to ensure the convergence of the proposed learning algorithm, an annealing mechanism is introduced to the framework of CVB inference, leading to an averaged collapsed variational Bayes inference strategy. The merits of the proposed models for modeling grouped axial data are demonstrated through experiments on both synthetic data and real-world applications involving gene expression data clustering and depth image analysis.	[Fan, Wentao; Yang, Lin] Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361021, Peoples R China; [Bouguila, Nizar] Concordia Univ, Concordia Inst Informat Syst Engn CIISE, Montreal, PQ H3G 1T7, Canada	Huaqiao University; Concordia University - Canada	Fan, WT (corresponding author), Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361021, Peoples R China.	fwt@hqu.edu.cn; 19014083028@stu.hqu.edu.cn; nizar.bouguila@concordia.ca			National Natural Science Foundation of China [61876068]; Natural Science Foundation of Fujian Province [2018J01094]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Fujian Province(Natural Science Foundation of Fujian Province)	This work was supported in part by the National Natural Science Foundation of China under Grant 61876068 and in part by the Natural Science Foundation of Fujian Province under Grant 2018J01094.	[Anonymous], 2010, BAYESIAN NONPARAMETR; Attias H, 2000, ADV NEUR IN, V12, P209; Banerjee A, 2005, J MACH LEARN RES, V6, P1345; Bishop C. M., 2006, PATTERN RECOGN, DOI DOI 10.1117/1.2819119.ARNING; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Chatzis SP, 2012, IEEE T NEUR NET LEAR, V23, P1862, DOI 10.1109/TNNLS.2012.2217986; Chu S, 1998, SCIENCE, V282, P699, DOI 10.1126/science.282.5389.699; Dang HTV, 2010, INT CONF ACOUST SPEE, P241, DOI 10.1109/ICASSP.2010.5495994; DeRisi JL, 1997, SCIENCE, V278, P680, DOI 10.1126/science.278.5338.680; Dhillon IS, 2003, BIOINFORMATICS, V19, P1612, DOI 10.1093/bioinformatics/btg209; Dhillon IS, 2001, MACH LEARN, V42, P143, DOI 10.1023/A:1007612920971; Fan WT, 2020, IEEE T NEUR NET LEAR, V31, P3193, DOI 10.1109/TNNLS.2019.2938830; Fan WT, 2019, IEEE ACCESS, V7, P83600, DOI 10.1109/ACCESS.2019.2924651; Fan WT, 2019, IEEE T NEUR NET LEAR, V30, P1683, DOI 10.1109/TNNLS.2018.2872986; Fan WT, 2017, IEEE T NEUR NET LEAR, V28, P2048, DOI 10.1109/TNNLS.2016.2574500; Fan WT, 2017, INT CONF ACOUST SPEE, P2771, DOI 10.1109/ICASSP.2017.7952661; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Fuse T, 2017, IEEE T INTELL TRANSP, V18, P3083, DOI 10.1109/TITS.2017.2674684; Hasnat MA, 2014, INT C PATT RECOG, P214, DOI 10.1109/ICPR.2014.46; Hu WM, 2018, IEEE T PATTERN ANAL, V40, P2355, DOI 10.1109/TPAMI.2017.2756039; Ishiguro K, 2017, J MACH LEARN RES, V18; Iyer VR, 1999, SCIENCE, V283, P83, DOI 10.1126/science.283.5398.83; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kurihara K, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2796; Ley C., 2018, APPL DIRECTIONAL STA; Lin Yang, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12306), P17, DOI 10.1007/978-3-030-60639-8_2; Mardia K.V., 2000, DIRECTIONAL STAT; Mardia KV, 1999, J ROY STAT SOC B, V61, P913, DOI 10.1111/1467-9868.00210; Pitman J, 1997, ANN PROBAB, V25, P855; Platanios EA, 2014, IEEE T PATTERN ANAL, V36, P888, DOI 10.1109/TPAMI.2013.183; Sato I., 2012, PROC 18 ACM SIGKDD I, P105; Sato I., 2012, PROC 29 INT C MACH L, P763; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Shatkay H, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P317; Shen X, 2007, PROC 11 INT C ARTIF, P35; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Souden M, 2013, INT CONF ACOUST SPEE, P111, DOI 10.1109/ICASSP.2013.6637619; Spellman PT, 1998, MOL BIOL CELL, V9, P3273, DOI 10.1091/mbc.9.12.3273; Sra S, 2013, J MULTIVARIATE ANAL, V114, P256, DOI 10.1016/j.jmva.2012.08.010; Sudderth E. B., 2008, P ADV NEUR INF PROC, P1585; Taghia J, 2016, IEEE T PATTERN ANAL, V38, P1886, DOI 10.1109/TPAMI.2015.2498935; Taghia J, 2014, IEEE T PATTERN ANAL, V36, P1701, DOI 10.1109/TPAMI.2014.2306426; Teh Y. W., 2007, P ADV NEUR INF PROC, P1353; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Teh YW, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P985; Wang C., 2011, J MACH LEARN RES, V15, P752; Wang LM, 2013, EURASIP J BIOINFORM, DOI 10.1186/1687-4153-2013-5	48	2	2	3	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9654	9668		10.1109/TPAMI.2021.3128271	http://dx.doi.org/10.1109/TPAMI.2021.3128271			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34784270				2022-12-18	WOS:000880661400078
J	Fisch, M; Clark, R				Fisch, Martin; Clark, Ronald			Orientation Keypoints for 6D Human Pose Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Pose estimation; Joints; Bones; Kinematics; Solid modeling; Training; Computer vision; pose estimation; pose tracking; 6D estimation		Most realtime human pose estimation approaches are based on detecting joint positions. Using the detected joint positions, the yaw and pitch of the limbs can be computed. However, the roll along the limb, which is critical for application such as sports analysis and computer animation, cannot be computed as this axis of rotation remains unobserved. In this paper we therefore introduce orientation keypoints, a novel approach for estimating the full position and rotation of skeletal joints, using only single-frame RGB images. Inspired by how motion-capture systems use a set of point markers to estimate full bone rotations, our method uses virtual markers to generate sufficient information to accurately infer rotations with simple post processing. The rotation predictions improve upon the best reported mean error for joint angles by 48% and achieves 93% accuracy across 15 bone rotations. The method also improves the current state-of-the-art results for joint positions by 14% as measured by MPJPE on the principle dataset, and generalizes well to in-the-wild datasets.	[Fisch, Martin; Clark, Ronald] Imperial Coll London, Dept Comp, London SW7 2BX, England	Imperial College London	Fisch, M (corresponding author), Imperial Coll London, Dept Comp, London SW7 2BX, England.	martin@pose-ai.com; ronald.clark@imperial.ac.uk						Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751; Alex Krizhevsky, 2012, Arxiv, DOI arXiv:1207.0580; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Brau E, 2016, INT CONF 3D VISION, P582, DOI 10.1109/3DV.2016.84; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115; Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742; Choutas Vasileios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P20, DOI 10.1007/978-3-030-58607-2_2; Drover D, 2019, LECT NOTES COMPUT SC, V11132, P78, DOI 10.1007/978-3-030-11018-5_7; elyob, ELYOB; Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494; Glorot X., 2011, P 14 INT C ART INT S, P315; Habibie I, 2019, PROC CVPR IEEE, P10897, DOI 10.1109/CVPR.2019.01116; Hao Jiang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1674, DOI 10.1109/ICPR.2010.414; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500; Iqbal U, 2020, PROC CVPR IEEE, P5242, DOI 10.1109/CVPR42600.2020.00529; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Kingma D, 2014, COMPUTER SCI; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117; Kocabas Muhammed, 2021, INT C COMP VIS ICCV, P11127; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Lassner C., 2017, PROC IEEE C COMPUT V, P5059; LEE HJ, 1985, COMPUT VISION GRAPH, V30, P148, DOI 10.1016/0734-189X(85)90094-5; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Levinson Jake, 2020, ARXIV200614616, P8; Li JF, 2021, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR46437.2021.00339; Lin K., 2021, PROC IEEE INT C COMP; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Luo Chenxu, 2018, ARXIV181104989, P92; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mehta D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392410; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nibali A, 2019, IEEE WINT CONF APPL, P1477, DOI 10.1109/WACV.2019.00162; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Park S, 2016, LECT NOTES COMPUT SC, V9915, P156, DOI 10.1007/978-3-319-49409-8_15; Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Pavllo Dario, 2018, BRIT MACH VIS C BMVC, P299; Pons-Moll G, 2015, INT J COMPUT VISION, V113, P163, DOI 10.1007/s11263-015-0818-9; Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Sun Yu, 2021, P IEEE CVF INT C COM, P11179, DOI DOI 10.1109/ICCV48922.2021.01099; Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Yoshiyasu Y, 2019, LECT NOTES COMPUT SC, V11364, P485, DOI 10.1007/978-3-030-20870-7_30; Zhang HW, 2022, IEEE T PATTERN ANAL, V44, P2610, DOI 10.1109/TPAMI.2020.3042341; Zhou XW, 2019, IEEE T PATTERN ANAL, V41, P901, DOI 10.1109/TPAMI.2018.2816031; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51; Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17; Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589	74	2	2	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10145	10158		10.1109/TPAMI.2021.3136136	http://dx.doi.org/10.1109/TPAMI.2021.3136136			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34914582	Green Submitted			2022-12-18	WOS:000880661400111
J	Oksuz, K; Cam, BC; Kalkan, S; Akbas, E				Oksuz, Kemal; Cam, Baris Can; Kalkan, Sinan; Akbas, Emre			One Metric to Measure Them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Location awareness; Visualization; Codes; Measurement uncertainty; Detectors; Object detection; Robustness; Localisation recall precision; average precision; panoptic quality; object detection; keypoint detection; instance segmentation; panoptic segmentation; performance metric; threshold		Despite being widely used as a performance measure for visual detection tasks, Average Precision (AP) is limited in (i) reflecting localisation quality, (ii) interpretability and (iii) robustness to the design choices regarding its computation, and its applicability to outputs without confidence scores. Panoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation (Kirillov et al., 2019), does not suffer from these limitations but is limited to panoptic segmentation. In this paper, we propose Localisation Recall Precision (LRP) Error as the average matching error of a visual detector computed based on both its localisation and classification qualities for a given confidence score threshold. LRP Error, initially proposed only for object detection by Oksuz et al. (2018), does not suffer from the aforementioned limitations and is applicable to all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as the minimum LRP Error obtained over confidence scores to evaluate visual detectors and obtain optimal thresholds for deployment. We provide a detailed comparative analysis of LRP Error with AP and PQ, and use nearly 100 state-of-the-art visual detectors from seven visual detection tasks (i.e. object detection, keypoint detection, instance segmentation, panoptic segmentation, visual relationship detection, zero-shot detection and generalised zero-shot detection) using ten datasets to empirically show that LRP Error provides richer and more discriminative information than its counterparts. Code available at: https://github.com/kemaloksuz/LRP-Error.	[Oksuz, Kemal; Cam, Baris Can; Kalkan, Sinan; Akbas, Emre] Middle East Tech Univ METU, Dept Comp Engn, TR-06800 Ankara, Turkey	Middle East Technical University	Oksuz, K (corresponding author), Middle East Tech Univ METU, Dept Comp Engn, TR-06800 Ankara, Turkey.	kemal.oksuz@metu.edu.tr; can.cam@metu.edu.tr; skalkan@metu.edu.tr; eakbas@metu.edu.tr		Akbas, Emre/0000-0002-3760-6722	Scientific and Technological Research Council of Turkey (TUBITAK) [117E054, 120E494]; TUBITAK 2211-A Scholarship; BAGEP Award of the Science Academy, Turkey	Scientific and Technological Research Council of Turkey (TUBITAK)(Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK)); TUBITAK 2211-A Scholarship(Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK)); BAGEP Award of the Science Academy, Turkey	This work was supported by the Scientific and Technological Research Council of Turkey (TUBITAK) under Grants 117E054 and 120E494. Dr. Oksuz is supported by the TUBITAK 2211-A Scholarship and Dr. Kalkan by the BAGEP Award of the Science Academy, Turkey.	[Anonymous], TENSORFLOW OBJECT DE; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Bolya Daniel, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P558, DOI 10.1007/978-3-030-58580-8_33; BOURGEOIS F, 1971, COMMUN ACM, V14, P802, DOI 10.1145/362919.362945; Buyu Li, 2019, Arxiv, DOI arXiv:1906.07155; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen KA, 2021, IEEE T PATTERN ANAL, V43, P3782, DOI 10.1109/TPAMI.2020.2991457; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, ADV NEUR IN, V29; Doll~ar P., 2021, ARXIV; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; Hall D, 2020, IEEE WINT CONF APPL, P1020, DOI 10.1109/WACV45572.2020.9093599; He K., 2017, IEEE INT C COMP VIS, P2961; He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300; Heqian Qiu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13185, DOI 10.1109/CVPR42600.2020.01320; Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25; Hongkai Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P260, DOI 10.1007/978-3-030-58555-6_16; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Jitendra Malik, 2015, Arxiv, DOI arXiv:1505.04474; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Kirillov Alexander, 2020, CVPR; Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z; Li BY, 2019, AAAI CONF ARTIF INTE, P8577; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754; Lu YY, 2017, IEEE I CONF COMP VIS, P2363, DOI 10.1109/ICCV.2017.257; Oksuz K., 2020, ADV NEURAL INF PROCE, V33, P15534; Oksuz K, 2018, LECT NOTES COMPUT SC, V11211, P521, DOI 10.1007/978-3-030-01234-2_31; Oksuz K, 2018, IET RADAR SONAR NAV, V12, P373, DOI 10.1049/iet-rsn.2017.0390; Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091; Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075; Rossi L, 2021, INT C PATT RECOG, P2203; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schuhmacher D, 2008, IEEE T SIGNAL PROCES, V56, P3447, DOI 10.1109/TSP.2008.920469; Shao S, 2019, IEEE I CONF COMP VIS, P8429, DOI 10.1109/ICCV.2019.00852; Tamura M, 2021, PROC CVPR IEEE, P10405, DOI 10.1109/CVPR46437.2021.01027; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308; Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310; Wu Yuxin, DETECTRON2; Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975; Zhang S, 2021, EDUC MANAG ADM LEAD, V49, P768, DOI 10.1177/1741143220915925; Zhang XS, 2019, ADV NEUR IN, V32; Zheng Y., 2020, PROC ASIAN C COMPUT, P107	55	2	2	3	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9446	9463		10.1109/TPAMI.2021.3130188	http://dx.doi.org/10.1109/TPAMI.2021.3130188			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34813471	Green Submitted			2022-12-18	WOS:000880661400063
J	Yang, L; Han, JW; Zhao, T; Lin, TW; Zhang, DW; Chen, JX				Yang, Le; Han, Junwei; Zhao, Tao; Lin, Tianwei; Zhang, Dingwen; Chen, Jianxin			Background-Click Supervision for Temporal Action Localization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Location awareness; Annotations; Proposals; Task analysis; Costs; Hidden Markov models; Supervised learning; Temporal action localization; background-click supervision; weakly supervised learning		Weakly supervised temporal action localization aims at learning the instance-level action pattern from the video-level labels, where a significant challenge is action-context confusion. To overcome this challenge, one recent work builds an action-click supervision framework. It requires similar annotation costs but can steadily improve the localization performance when compared to the conventional weakly supervised methods. In this paper, by revealing that the performance bottleneck of the existing approaches mainly comes from the background errors, we find that a stronger action localizer can be trained with labels on the background video frames rather than those on the action frames. To this end, we convert the action-click supervision to the background-click supervision and develop a novel method, called BackTAL. Specifically, BackTAL implements two-fold modeling on the background video frames, i.e., the position modeling and the feature modeling. In position modeling, we not only conduct supervised learning on the annotated video frames but also design a score separation module to enlarge the score differences between the potential action frames and backgrounds. In feature modeling, we propose an affinity module to measure frame-specific similarities among neighboring frames and dynamically attend to informative neighbors when calculating temporal convolution. Extensive experiments on three benchmarks are conducted, which demonstrate the high performance of the established BackTAL and the rationality of the proposed background-click supervision.	[Yang, Le; Han, Junwei; Zhao, Tao; Zhang, Dingwen] Northwestern Polytech Univ, BRAIN Lab, Xian 710060, Shaanxi, Peoples R China; [Lin, Tianwei] Baidu VIS, Beijing 100085, Peoples R China; [Chen, Jianxin] Beijing Univ Chinese Med, Beijing 100029, Peoples R China	Northwestern Polytechnical University; Beijing University of Chinese Medicine	Han, JW; Zhang, DW (corresponding author), Northwestern Polytech Univ, BRAIN Lab, Xian 710060, Shaanxi, Peoples R China.	nwpuyangle@gmail.com; junweihan2010@gmail.com; 841991610@qq.com; lintianwei01@baidu.com; zhangdingwen2006yyy@gmail.com; cjx@bucm.edu.cn	Zhang, Dingwen/S-9447-2017	Zhang, Dingwen/0000-0001-8369-8886	Key-Area Research and Development Program of Guangdong Province [2019B010110001]; National Natural Science Foundation of China [62136007, 61876140, U21B20481010927]	Key-Area Research and Development Program of Guangdong Province; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the Key-Area Research and Development Program of Guangdong Province under Grant 2019B010110001 and in part by the National Natural Science Foundation of China under Grants 62136007, 61876140, and U21B20481010927.	Alwassel H, 2018, LECT NOTES COMPUT SC, V11207, P264, DOI 10.1007/978-3-030-01219-9_16; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Bilen H., WEAKLY SUPERVISED OB; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Choe J, 2020, PROC CVPR IEEE, P3130, DOI 10.1109/CVPR42600.2020.00320; Ci H, 2018, LECT NOTES COMPUT SC, V11215, P524, DOI 10.1007/978-3-030-01252-6_31; Damen D., 2014, PROC BRIT MACH VIS C, V2, P1; Tran D, 2014, IEEE T PATTERN ANAL, V36, P404, DOI 10.1109/TPAMI.2013.137; Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25; Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65; Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392; Gong G., 2020, PROC IEEE C COMPUT V, P9819; Guo GY, 2021, PROC CVPR IEEE, P7399, DOI 10.1109/CVPR46437.2021.00732; Harley AW, 2017, IEEE I CONF COMP VIS, P5048, DOI 10.1109/ICCV.2017.539; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637; Jain M, 2020, PROC CVPR IEEE, P1168, DOI 10.1109/CVPR42600.2020.00125; Jiang Y.-G., THUMOS CHALLENGE ACT; Ko KE, 2018, ENG APPL ARTIF INTEL, V67, P226, DOI 10.1016/j.engappai.2017.10.001; Kuehne H, 2020, IEEE T PATTERN ANAL, V42, P765, DOI 10.1109/TPAMI.2018.2884469; Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854; Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320; Li Shi-Jie, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3021756; Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343; Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139; Liu NA, 2020, IEEE T IMAGE PROCESS, V29, P6438, DOI 10.1109/TIP.2020.2988568; Liu Y, 2019, PROC CVPR IEEE, P3599, DOI 10.1109/CVPR.2019.00372; Liu ZY, 2021, AAAI CONF ARTIF INTE, V35, P2242; Liu ZY, 2021, AAAI CONF ARTIF INTE, V35, P2233; Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400; Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043; Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27; Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17; Moltisanti D, 2019, PROC CVPR IEEE, P9907, DOI 10.1109/CVPR.2019.01015; Moniruzzaman M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2166, DOI 10.1145/3394171.3413687; Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Paszke A, 2019, ADV NEUR IN, V32; Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35; Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706; Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560; Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109; Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Soomro K, 2019, IEEE T PATTERN ANAL, V41, P459, DOI 10.1109/TPAMI.2018.2797266; Su R, 2021, IEEE T PATTERN ANAL, V43, P4477, DOI 10.1109/TPAMI.2020.2997860; Tang YS, 2021, IEEE T PATTERN ANAL, V43, P3138, DOI 10.1109/TPAMI.2020.2980824; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Xu HJ, 2019, IEEE T PATTERN ANAL, V41, P2319, DOI 10.1109/TPAMI.2019.2921539; Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617; Xu M., 2020, CVPR, P10156; Xu YL, 2019, AAAI CONF ARTIF INTE, P9070; Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486; Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562; Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313; Zhang Jing, 2020, CVPR; Zhang XY, 2019, AAAI CONF ARTIF INTE, P9227; Zhao B, 2020, IEEE T NEUR NET LEAR, V31, P3989, DOI 10.1109/TNNLS.2019.2951680; Zhao H, 2019, IEEE I CONF COMP VIS, P8667, DOI 10.1109/ICCV.2019.00876; Zhao T, 2021, INT J COMPUT VISION, V129, P2474, DOI 10.1007/s11263-021-01473-9; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317; Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43	71	2	2	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9814	9829		10.1109/TPAMI.2021.3132058	http://dx.doi.org/10.1109/TPAMI.2021.3132058			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34855585	Green Submitted			2022-12-18	WOS:000880661400090
J	Cheng, MM; Gao, SH; Borji, A; Tan, YQ; Lin, Z; Wang, M				Cheng, Ming-Ming; Gao, Shang-Hua; Borji, Ali; Tan, Yong-Qiang; Lin, Zheng; Wang, Meng			A Highly Efficient Model to Study the Semantics of Salient Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Salient object detection; efficient saliency prediction; semantics	CONVOLUTIONAL NEURAL-NETWORK	CNN-based salient object detection (SOD) methods achieve impressive performance. However, the way semantic information is encoded in them and whether they are category-agnostic is less explored. One major obstacle in studying these questions is the fact that SOD models are built on top of the ImageNet pre-trained backbones which may cause information leakage and feature redundancy. To remedy this, here we first propose an extremely light-weight holistic model tied to the SOD task that can be freed from classification backbones and trained from scratch, and then employ it to study the semantics of SOD models. With the holistic network and representation redundancy reduction by a novel dynamic weight decay scheme, our model has only 100K parameters, similar to 0:2% of parameters of large models, and performs on par with SOTA on popular SOD benchmarks. Using CSNet, we find that a) SOD and classification methods use different mechanisms, b) SOD models are category insensitive, c) ImageNet pre-training is not necessary for SOD training, and d) SOD models require far fewer parameters than the classification models. The source code is publicly available at https://mmcheng.net/sod100k/.	[Cheng, Ming-Ming; Gao, Shang-Hua; Tan, Yong-Qiang; Lin, Zheng] Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin 300350, Peoples R China; [Borji, Ali] Primer Technol Inc, San Francisco, CA 94111 USA; [Wang, Meng] Hefei Univ Technol, Hefei 230009, Peoples R China	Nankai University; Hefei University of Technology	Cheng, MM (corresponding author), Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin 300350, Peoples R China.	cmm@nankai.edu.cn; shgao@mail.nankai.edu.cn; aliborji@gmail.com; yogitan@outlook.com; frazer.linzheng@gmail.com; eric.mengwang@gmail.com	Cheng, Ming-Ming/A-2527-2009	Cheng, Ming-Ming/0000-0001-5550-8758; Lin, Zheng/0000-0002-8057-4949; Tan, Yong-Qiang/0000-0002-0481-6715; Borji, Ali/0000-0001-8198-0335	Major Project for New Generation of AI [2018AAA0100400]; NSFC [61620106008]; S&T Innovation Project from the Chinese Ministry of Education; Fundamental Research Funds for the Central Universities (Nankai University) [63213090]; Tianjin Natural Science Foundation [18ZXZNGX00110]	Major Project for New Generation of AI; NSFC(National Natural Science Foundation of China (NSFC)); S&T Innovation Project from the Chinese Ministry of Education; Fundamental Research Funds for the Central Universities (Nankai University); Tianjin Natural Science Foundation(Natural Science Foundation of Tianjin)	This work was supported in part by the Major Project for New Generation of AI under Grant 2018AAA0100400, in part by the NSFC under Grant 61620106008, in part by the S&T Innovation Project from the Chinese Ministry of Education, in part by the Fundamental Research Funds for the Central Universities (Nankai University, 63213090), and in part by the Tianjin Natural Science Foundation under Grant 18ZXZNGX00110. Ming-Ming Cheng and Shang-Hua Gao are joint first authors. A preliminary version of this work has been presented in the ECCV 2020 [19].	Abhishek Chaurasia, 2016, Arxiv, DOI arXiv:1606.02147; Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Ali Borji, 2019, Arxiv, DOI arXiv:1803.09860; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Andrew Howard, 2019, Arxiv, DOI arXiv:1905.02244; [Anonymous], 2013, COMPUT SCI; Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833; Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9; Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15; Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470; Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353; Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7; Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193; Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev.neuro.18.1.193; Dongsheng Ruan, 2019, Arxiv, DOI arXiv:1909.03834; Fan D.-P., 2018, PROC EUR C COMPUT VI; Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172; Gao SH, 2021, PROC CVPR IEEE, P8665, DOI 10.1109/CVPR46437.2021.00856; Gao SH, 2021, PROC CVPR IEEE, P16800, DOI 10.1109/CVPR46437.2021.01653; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hong S, 2015, PR MACH LEARN RES, V37, P597; Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874; Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688; Hou QB, 2018, ADV NEUR IN, V31; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Ioffe S., 2015, P 32 INT C MACH LEAR; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304; Kingma D.P., 2015, INT C LEARN REPR ICL; KROGH A, 1992, ADV NEUR IN, V4, P950; Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78; Li G, 2019, PROC BRIT MACH VIS C; Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34; Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li H, 2016, PROC INT C LEARN REP; Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x; Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941; Mordvintsev A, 2015, INCEPTIONISM GOING D; Movahedi Vida, 2010, P IEEE C COMP VIS PA, P49, DOI DOI 10.1109/CVPRW.2010.5543739; Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735; Rui Zhang, 2019, Arxiv, DOI arXiv:1811.08201; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42; Gao SH, 2022, Arxiv, DOI arXiv:2106.03149; Simonyan K., 2015, INT C LEARN REPR ICL; Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584; Tan MX, 2019, PR MACH LEARN RES, V97; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3; Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404; Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938; Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50; Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330; Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang XC, 2019, COMPUT VIS MEDIA, V5, P193, DOI 10.1007/s41095-019-0131-6; Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411; Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943; Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20; Zeng Y, 2019, IEEE I CONF COMP VIS, P7233, DOI 10.1109/ICCV.2019.00733; Zhang G, 2019, PROC INT C LEARN REP; Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x; Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187; Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081; Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887; Zhao K, 2022, IEEE T PATTERN ANAL, V44, P4793, DOI 10.1109/TPAMI.2021.3077129; Zhao K, 2019, IEEE I CONF COMP VIS, P8848, DOI 10.1109/ICCV.2019.00894; Zhao X., 2020, ECCV, P35, DOI [DOI 10.1007/978-3-030-58536-5_3, 10.1007/978-3-030-58536-5_3]; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360	102	2	2	14	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8006	8021		10.1109/TPAMI.2021.3107956	http://dx.doi.org/10.1109/TPAMI.2021.3107956			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34437058				2022-12-18	WOS:000864325900053
J	Joo, K; Kim, P; Hebert, M; Kweon, IS; Kim, HJ				Joo, Kyungdon; Kim, Pyojin; Hebert, Martial; Kweon, In So; Kim, Hyoun Jin			Linear RGB-D SLAM for Structured Environments	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Linear SLAM; manhattan world; atlanta world; RGB-D image; Bayesian filtering; scene understanding	ODOMETRY; WORLD	We propose a new linear RGB-D simultaneous localization and mapping (SLAM) formulation by utilizing planar features of the structured environments. The key idea is to understand a given structured scene and exploit its structural regularities such as the Manhattan world. This understanding allows us to decouple the camera rotation by tracking structural regularities, which makes SLAM problems free from being highly nonlinear. Additionally, it provides a simple yet effective cue for representing planar features, which leads to a linear SLAM formulation. Given an accurate camera rotation, we jointly estimate the camera translation and planar landmarks in the global planar map using a linear Kalman filter. Our linear SLAM method, called L-SLAM, can understand not only the Manhattan world but the more general scenario of the Atlanta world, which consists of a vertical direction and a set of horizontal directions orthogonal to the vertical direction. To this end, we introduce a novel tracking-by-detection scheme that infers the underlying scene structure by Atlanta representation. With efficient Atlanta representation, we formulate a unified linear SLAM framework for structured environments. We evaluate L-SLAM on a synthetic dataset and RGB-D benchmarks, demonstrating comparable performance to other state-of-the-art SLAM methods without using expensive nonlinear optimization. We assess the accuracy of L-SLAM on a practical application of augmented reality.	[Joo, Kyungdon] Ulsan Natl Inst Sci & Technol, Artificial Intelligence Grad Sch, Dept Comp Sci & Engn, Ulsan 44919, South Korea; [Kim, Pyojin] Sookmyung Womens Univ, Dept Mech Syst Engn, Seoul 04310, South Korea; [Hebert, Martial] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Hebert, Martial] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea; [Kim, Hyoun Jin] Seoul Natl Univ, Sch Mech & Aerosp Engn, Seoul 08826, South Korea	Ulsan National Institute of Science & Technology (UNIST); Sookmyung Women's University; Carnegie Mellon University; Carnegie Mellon University; Korea Advanced Institute of Science & Technology (KAIST); Seoul National University (SNU)	Kim, P (corresponding author), Sookmyung Womens Univ, Dept Mech Syst Engn, Seoul 04310, South Korea.	kdjoo369@gmail.com; pjinkim@sookmyung.ac.kr; hebert@cs.cmu.edu; iskweon77@kaist.ac.kr; hjinkim@snu.ac.kr			Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [2020-0-01336]; National Research Foundation of Korea (NRF) - Korea government (MSIT) [NRF-2021R1F1A1061397, NRF-2021R1C1C1005723]; Sookmyung Women's University Research Grants [1-2003-2015]	Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); National Research Foundation of Korea (NRF) - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Sookmyung Women's University Research Grants	The work of Kyungdon Joo was supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2020-0-01336, Artificial Intelligence Graduate School Program (UNIST)) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2021R1C1C1005723). This work of Pyojin Kim was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2021R1F1A1061397) and Sookmyung Women's University Research Grants (1-2003-2015).	Bailey T, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P3562, DOI 10.1109/IROS.2006.281644; Camposeco F, 2015, IEEE INT CONF ROBOT, P5219, DOI 10.1109/ICRA.2015.7139926; Carlone L, 2015, IEEE INT CONF ROBOT, P4597, DOI 10.1109/ICRA.2015.7139836; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Coughlan J.M., 1999, P ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349; Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54; Gee A. P., 2007, BMVC, P1; Gee AP, 2008, IEEE T ROBOT, V24, P980, DOI 10.1109/TRO.2008.2004641; Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054; Hartley R., 2011, P IEEE C COMP VIS PA, P3041, DOI DOI 10.1109/CVPR.2011.5995745; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Hassner T, 2014, MACH VISION APPL, V25, P971, DOI 10.1007/s00138-013-0571-4; Jia C, 2014, IEEE T SIGNAL PROCES, V62, P3293, DOI 10.1109/TSP.2014.2325795; Jia ZY, 2013, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2013.8; Joo K, 2020, IEEE INT CONF ROBOT, P1077, DOI 10.1109/ICRA40945.2020.9196561; Joo K, 2020, IEEE T PATTERN ANAL, V42, P2656, DOI 10.1109/TPAMI.2019.2909863; Joo K, 2018, PROC CVPR IEEE, P5726, DOI 10.1109/CVPR.2018.00600; Kaess M, 2015, IEEE INT CONF ROBOT, P4605, DOI 10.1109/ICRA.2015.7139837; Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650; Kim P., 2017, P BMVC, P7; Kim P, 2018, PROC CVPR IEEE, P4673, DOI 10.1109/CVPR.2018.00491; Kim P, 2018, LECT NOTES COMPUT SC, V11208, P350, DOI 10.1007/978-3-030-01225-0_21; Kim P, 2018, IEEE INT CONF ROBOT, P7247; Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607; Le PH, 2017, IEEE INT C INT ROBOT, P4944; Li HA, 2019, IEEE INT CONF ROBOT, P2412, DOI 10.1109/ICRA.2019.8793716; Li Y., 2020, ARXIV; Li YY, 2020, IEEE ROBOT AUTOM LET, V5, P6583, DOI 10.1109/LRA.2020.3015456; Lin Y, 2018, J FIELD ROBOT, V35, P23, DOI 10.1002/rob.21732; Lu Y, 2015, IEEE I CONF COMP VIS, P3934, DOI 10.1109/ICCV.2015.448; Ma LN, 2016, IEEE INT CONF ROBOT, P1285, DOI 10.1109/ICRA.2016.7487260; Mart ~inez-Carranza J., 2010, P BRIT MACH VIS C, P1; Ming Hsiao, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5110, DOI 10.1109/ICRA.2017.7989597; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Prisacariu V. A., 2017, ARXIV; Rameau F, 2016, IEEE T VIS COMPUT GR, V22, P2395, DOI 10.1109/TVCG.2016.2593768; Schindler G, 2004, PROC CVPR IEEE, P203; Schops T, 2019, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2019.00022; Servant F., 2008, 2008 19 INT C PATTER, P1; Simon D., 2006, OPTIMAL STATE ESTIMA, DOI DOI 10.1002/0470045345.CH11; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Strasdat H, 2010, IEEE INT CONF ROBOT, P2657, DOI 10.1109/ROBOT.2010.5509636; Straub J, 2018, IEEE T PATTERN ANAL, V40, P235, DOI 10.1109/TPAMI.2017.2662686; Straub J, 2015, IEEE INT C INT ROBOT, P1913, DOI 10.1109/IROS.2015.7353628; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Taketomi T., 2017, IBSJ T COMPUT VIS AP, V9, P16, DOI 10.1186/s41074-017-0027-2; Taylor Camillo J., 2013, 2012 Robotics: Science and Systems, P401; Wang R, 2017, IEEE I CONF COMP VIS, P3923, DOI 10.1109/ICCV.2017.421; Weingarten J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P3062, DOI 10.1109/IROS.2006.282245; Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Yang M. Y., 2010, TRIGGP201001 U BONN; Yang ST, 2019, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2019.00348; Yang SC, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1222, DOI 10.1109/IROS.2016.7759204; Zhou Y, 2017, LECT NOTES COMPUT SC, V10115, P3, DOI 10.1007/978-3-319-54193-8_1; Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219; Zou DP, 2019, IEEE T ROBOT, V35, P999, DOI 10.1109/TRO.2019.2915140	64	2	2	21	33	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8403	8419		10.1109/TPAMI.2021.3106820	http://dx.doi.org/10.1109/TPAMI.2021.3106820			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34428135				2022-12-18	WOS:000864325900079
J	Liu, WZ; Salzmann, M; Fua, P				Liu, Weizhe; Salzmann, Mathieu; Fua, Pascal			Counting People by Estimating People Flows	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Video sequences; Feature extraction; Pattern analysis; Optical imaging; Computer architecture; Annotations; Crowd counting; temporal consistency; surveillance	CROWD	Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames. In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it allows us to exploit the correlation between people flow and optical flow to further improve the results. We also show that leveraging people conservation constraints in both a spatial and temporal manner makes it possible to train a deep crowd counting model in an active learning setting with much fewer annotations. This significantly reduces the annotation cost while still leading to similar performance to the full supervision case.	[Liu, Weizhe; Salzmann, Mathieu; Fua, Pascal] Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Liu, WZ (corresponding author), Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland.	weizhe.liu@epfl.ch; mathieu.salzmann@epfl.ch; pascal.fua@epfl.ch			Swiss National Science Foundation	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This work was supported in part by the Swiss National Science Foundation.	Andriyenko A, 2010, LECT NOTES COMPUT SC, V6311, P466, DOI 10.1007/978-3-642-15549-9_34; Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465; Beluch WH, 2018, PROC CVPR IEEE, P9368, DOI 10.1109/CVPR.2018.00976; Ben Shitrit H, 2014, IEEE T PATTERN ANAL, V36, P1614, DOI 10.1109/TPAMI.2013.210; Ben Shitrit H, 2011, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2011.6126235; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Butt Asad A., 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P163, DOI 10.1007/978-3-642-37431-9_13; Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191; Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625; Chintala S., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1701.07875; Collins RT, 2012, PROC CVPR IEEE, P1744, DOI 10.1109/CVPR.2012.6247870; Dicle C, 2013, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2013.286; Fang YY, 2019, IEEE INT CON MULTI, P814, DOI 10.1109/ICME.2019.00145; Gal Y, 2016, PR MACH LEARN RES, V48; Gijsberts A, 2014, IEEE T NEUR SYS REH, V22, P735, DOI 10.1109/TNSRE.2014.2303394; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guangshuai Gao, 2020, Arxiv, DOI arXiv:2003.12783; He ZY, 2016, IEEE T IMAGE PROCESS, V25, P3698, DOI 10.1109/TIP.2016.2570553; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33; Jaderberg M, 2015, ADV NEUR IN, V28; Kang D., 2018, P BMVC, P89; Kang D, 2017, ADV NEUR IN, V30; Laptev I, 2007, COMPUT VIS IMAGE UND, V108, P207, DOI 10.1016/j.cviu.2006.11.023; Lempitsky V., 2010, NIPS, V23, P1324; Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120; Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192; Liang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P164, DOI 10.1007/978-3-030-58607-2_10; Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131; Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545; Liu JC, 2013, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2013.239; Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849; Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186; Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334; Liu WZ, 2019, IEEE INT C INT ROBOT, P244, DOI 10.1109/IROS40897.2019.8967852; Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524; Liu XL, 2019, IEEE T PATTERN ANAL, V41, P1862, DOI 10.1109/TPAMI.2019.2899857; Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799; Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Loy CC, 2013, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2013.270; Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Nater F., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1737, DOI 10.1109/ICCVW.2011.6130459; Onoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17; Sam DB, 2019, AAAI CONF ARTIF INTE, P8868; Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830; Sam DB, 2018, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2018.00381; Schroder G, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P7; Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550; Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745; Shi XJ, 2015, ADV NEUR IN, V28; Shi ZL, 2019, IEEE I CONF COMP VIS, P4199, DOI 10.1109/ICCV.2019.00430; Shi ZL, 2018, PROC CVPR IEEE, P5382, DOI 10.1109/CVPR.2018.00564; Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P212, DOI 10.1007/978-3-030-58621-8_13; Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109; Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206; Sinha S, 2019, IEEE I CONF COMP VIS, P5971, DOI 10.1109/ICCV.2019.00607; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Suurballe J. W., 1974, Networks, V4, P125, DOI 10.1002/net.3230040204; Vogel C, 2011, IEEE I CONF COMP VIS, P1291, DOI 10.1109/ICCV.2011.6126381; von Borstel M, 2016, LECT NOTES COMPUT SC, V9905, P365, DOI 10.1007/978-3-319-46448-0_22; Walach E, 2016, LECT NOTES COMPUT SC, V9906, P660, DOI 10.1007/978-3-319-46475-6_41; Wan J, 2019, IEEE I CONF COMP VIS, P1130, DOI 10.1109/ICCV.2019.00122; Wan J, 2019, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2019.00416; Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269; Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839; Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI 10.1007/s11263-020-01365-4; Ren WH, 2020, Arxiv, DOI arXiv:2007.09509; Weizhe Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P723, DOI 10.1007/978-3-030-58555-6_43; Xiyang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P241, DOI 10.1007/978-3-030-58586-0_15; Yan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P242, DOI 10.1007/978-3-030-58555-6_15; Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104; Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443; Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689; Zhang A, 2019, IEEE I CONF COMP VIS, P5713, DOI 10.1109/ICCV.2019.00581; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684; Zhang L, 2021, IEEE T PATTERN ANAL, V43, P982, DOI 10.1109/TPAMI.2019.2943860; Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70; Zhao MM, 2019, PROC CVPR IEEE, P12728, DOI 10.1109/CVPR.2019.01302; Zhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P565, DOI 10.1007/978-3-030-58565-5_34	92	2	2	8	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8151	8166		10.1109/TPAMI.2021.3102690	http://dx.doi.org/10.1109/TPAMI.2021.3102690			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34351854	Green Submitted			2022-12-18	WOS:000864325900063
J	Medley, D; Santiago, C; Nascimento, JC				Medley, Daniela; Santiago, Carlos; Nascimento, Jacinto C.			CyCoSeg: A Cyclic Collaborative Framework for Automated Medical Image Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Collaboration; Shape; Deformable models; Semantics; Three-dimensional displays; Standards; Segmentation; image processing and computer vision; semantic networks; machine learning	LEFT-VENTRICULAR SEGMENTATION; ACTIVE SHAPE MODELS; APPEARANCE; RESOURCE; TRACKING; NODULES; HEART	Deep neural networks have been tremendously successful at segmenting objects in images. However, it has been shown they still have limitations on challenging problems such as the segmentation of medical images. The main reason behind this lower success resides in the reduced size of the object in the image. In this paper we overcome this limitation through a cyclic collaborative framework, CyCoSeg. The proposed framework is based on a deep active shape model (D-ASM), which provides prior information about the shape of the object, and a semantic segmentation network (SSN). These two models collaborate to reach the desired segmentation by influencing each other: SSN helps D-ASM identify relevant keypoints in the image through an Expectation Maximization formulation, while D-ASM provides a segmentation proposal that guides the SSN. This cycle is repeated until both models converge. Extensive experimental evaluation shows CyCoSeg boosts the performance of the baseline models, including several popular SSNs, while avoiding major architectural modifications. The effectiveness of our method is demonstrated on the left ventricle segmentation on two benchmark datasets, where our approach achieves one of the most competitive results in segmentation accuracy. Furthermore, its generalization is demonstrated for lungs and kidneys segmentation in CT scans.	[Medley, Daniela; Santiago, Carlos; Nascimento, Jacinto C.] Inst Super Tecn, Inst Sistemas & Robot, P-1049001 Lisbon, Portugal	Universidade de Lisboa; Instituto Superior Tecnico	Medley, D (corresponding author), Inst Super Tecn, Inst Sistemas & Robot, P-1049001 Lisbon, Portugal.	daniela.medley@tecnico.ulisboa.pt; carlos.santiago@tecnico.ulisboa.pt; jan@isr.ist.utl.pt	Nascimento, Jacinto/B-6128-2009	Nascimento, Jacinto/0000-0001-7468-5127; Medley, Daniela/0000-0002-5377-9076	LARSyS-FCT [UIDB/50009/2020];  [PD/BD/150628/2020]	LARSyS-FCT(Portuguese Foundation for Science and Technology); 	This work was supported in part by the LARSyS -FCT Project UIDB/50009/2020 and in part by the PhD Program under Grant PD/BD/150628/2020. The Titan Xp used in this research was donated by the NVIDIA Corporation.	Alan L. Yuille, 2016, Arxiv, DOI arXiv:1412.7062; American College of Cardiology Foundation Task Force on Expert Consensus Documents, 2010, J Am Coll Cardiol, V55, P2614, DOI [10.1161/CIR.0b013e3181d44a8f, 10.1016/j.jacc.2009.11.011]; Andreopoulos A, 2008, MED IMAGE ANAL, V12, P335, DOI 10.1016/j.media.2007.12.003; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204; Avendi MR, 2016, MED IMAGE ANAL, V30, P108, DOI 10.1016/j.media.2016.01.005; Bai WJ, 2015, MED IMAGE ANAL, V19, P98, DOI 10.1016/j.media.2014.09.005; Baumgartner CF, 2018, LECT NOTES COMPUT SC, V10663, P111, DOI 10.1007/978-3-319-75541-0_12; Ben Covington, 2018, Arxiv, DOI arXiv:1803.07703; Bernard O, 2018, IEEE T MED IMAGING, V37, P2514, DOI 10.1109/TMI.2018.2837502; Brosch T, 2016, IEEE T MED IMAGING, V35, P1229, DOI 10.1109/TMI.2016.2528821; Calisto MB, 2020, NEURAL NETWORKS, V126, P76, DOI 10.1016/j.neunet.2020.03.007; Chen H, 2016, AAAI CONF ARTIF INTE, P1160; Christian Rupprecht, 2016, Arxiv, DOI arXiv:1607.05074; Cocosco CA, 2008, J MAGN RESON IMAGING, V28, P366, DOI 10.1002/jmri.21451; Constantinides C, 2012, IEEE ENG MED BIO, P3207, DOI 10.1109/EMBC.2012.6346647; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Florian Schroff, 2017, Arxiv, DOI arXiv:1706.05587; Grinias E, 2018, LECT NOTES COMPUT SC, V10663, P91, DOI 10.1007/978-3-319-75541-0_10; Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu HF, 2019, NEUROCOMPUTING, V347, P139, DOI 10.1016/j.neucom.2019.02.008; Hu HF, 2013, MAGN RESON IMAGING, V31, P575, DOI 10.1016/j.mri.2012.10.004; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang S, 2011, J DIGIT IMAGING, V24, P598, DOI 10.1007/s10278-010-9315-4; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; Isensee F, 2019, LECT NOTES COMPUT SC, V11384, P234, DOI 10.1007/978-3-030-11726-9_21; Isensee F, 2018, LECT NOTES COMPUT SC, V10663, P120, DOI 10.1007/978-3-319-75541-0_13; Jaderberg M, 2015, ADV NEUR IN, V28; Jang Y, 2018, LECT NOTES COMPUT SC, V10663, P161, DOI 10.1007/978-3-319-75541-0_17; Jolly M. P., 2009, MIDAS J CARDIAC MR L, V4, P59; Katouzian A, 2006, 2006 28TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOLS 1-15, P4643; Khened M, 2019, MED IMAGE ANAL, V51, P21, DOI 10.1016/j.media.2018.10.004; Khened M, 2018, LECT NOTES COMPUT SC, V10663, P140, DOI 10.1007/978-3-319-75541-0_15; Lee HY, 2010, IEEE T BIO-MED ENG, V57, P905, DOI 10.1109/TBME.2009.2014545; Lekadir K, 2007, IEEE T MED IMAGING, V26, P212, DOI 10.1109/TMI.2006.889726; Lima JAC, 2004, J AM COLL CARDIOL, V44, P1164, DOI 10.1016/j.jacc.2004.06.033; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin X, 2006, LECT NOTES COMPUT SC, V4190, P728; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Liu H, 2012, ACAD RADIOL, V19, P723, DOI 10.1016/j.acra.2012.02.011; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lynch M, 2006, COMPUT BIOL MED, V36, P389, DOI 10.1016/j.compbiomed.2005.01.005; Medley DO, 2020, IEEE T IMAGE PROCESS, V29, P2380, DOI 10.1109/TIP.2019.2948728; Medrano-Gracia P, 2013, LECT NOTES COMPUT SC, V7945, P433, DOI 10.1007/978-3-642-38899-6_51; Mehta R, 2017, I S BIOMED IMAGING, P437, DOI 10.1109/ISBI.2017.7950555; Mitchell SC, 2001, IEEE T MED IMAGING, V20, P415, DOI 10.1109/42.925294; OBrien S., 2009, P MICCAI WORKSH CARD, P1; Painchaud N, 2019, LECT NOTES COMPUT SC, V11765, P632, DOI 10.1007/978-3-030-32245-8_70; Patravali J, 2018, LECT NOTES COMPUT SC, V10663, P130, DOI 10.1007/978-3-319-75541-0_14; Pednekar A, 2006, IEEE T BIO-MED ENG, V53, P1425, DOI 10.1109/TBME.2006.873684; Petitjean C, 2011, MED IMAGE ANAL, V15, P169, DOI 10.1016/j.media.2010.12.004; Poudel RPK, 2017, LECT NOTES COMPUT SC, V10129, P83, DOI 10.1007/978-3-319-52280-7_8; Queiros S, 2014, MED IMAGE ANAL, V18, P1115, DOI 10.1016/j.media.2014.06.001; Radau P., 2009, P MICCAI 2009 WORKSH, V49, DOI DOI 10.54294/G80RUO; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Rister B., 2019, CT ORG CT VOLUMES MU; Ronneberger O., 2015, P INT C MED IM COMP; Santiago C, 2018, COMPUT METH PROG BIO, V154, P9, DOI 10.1016/j.cmpb.2017.10.028; Santiago C, 2016, IEEE IMAGE PROC, P4112, DOI 10.1109/ICIP.2016.7533133; Schaerer J, 2010, MED IMAGE ANAL, V14, P738, DOI 10.1016/j.media.2010.05.009; Setio AAA, 2017, MED IMAGE ANAL, V42, P1, DOI 10.1016/j.media.2017.06.015; Suinesiaputra A, 2014, MED IMAGE ANAL, V18, P50, DOI 10.1016/j.media.2013.09.001; Tavakoli V, 2013, COMPUT VIS IMAGE UND, V117, P966, DOI 10.1016/j.cviu.2012.11.017; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Tran PV, 2016, FULLY CONVOLUTIONAL; Ngo TA, 2017, MED IMAGE ANAL, V35, P159, DOI 10.1016/j.media.2016.05.009; Uzumcu M, 2006, INVEST RADIOL, V41, P52, DOI 10.1097/01.rli.0000194070.88432.24; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wolterink JM, 2018, LECT NOTES COMPUT SC, V10663, P101, DOI 10.1007/978-3-319-75541-0_11; Wu YW, 2013, COMPUT VIS IMAGE UND, V117, P990, DOI 10.1016/j.cviu.2012.12.008; Xia FT, 2016, LECT NOTES COMPUT SC, V9909, P648, DOI 10.1007/978-3-319-46454-1_39; Yang X, 2018, LECT NOTES COMPUT SC, V10663, P152, DOI 10.1007/978-3-319-75541-0_16; Yu QH, 2018, PROC CVPR IEEE, P8280, DOI 10.1109/CVPR.2018.00864; Yuan TC, 2018, INT C PATT RECOG, P3838, DOI 10.1109/ICPR.2018.8545390; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhou Y., 2016, MICCAI, DOI 10.1007/978-3-319-66182-7; Zotti C, 2019, IEEE J BIOMED HEALTH, V23, P1119, DOI 10.1109/JBHI.2018.2865450	83	2	2	7	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8167	8182		10.1109/TPAMI.2021.3113077	http://dx.doi.org/10.1109/TPAMI.2021.3113077			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34529562				2022-12-18	WOS:000864325900064
J	Sanakoyeu, A; Ma, PC; Tschernezki, V; Ommer, B				Sanakoyeu, Artsiom; Ma, Pingchuan; Tschernezki, Vadim; Ommer, Bjorn			Improving Deep Metric Learning by Divide and Conquer	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Training; Training data; Visualization; Prototypes; Learning systems; Image retrieval; Deep metric learning; image retrieval; similarity learning; representation learning; computer vision; deep learning	SIMILARITY; ALGORITHM	Deep metric learning (DML) is a cornerstone of many computer vision applications. It aims at learning a mapping from the input domain to an embedding space, where semantically similar objects are located nearby and dissimilar objects far from another. The target similarity on the training data is defined by user in form of ground-truth class labels. However, while the embedding space learns to mimic the user-provided similarity on the training data, it should also generalize to novel categories not seen during training. Besides user-provided groundtruth training labels, a lot of additional visual factors (such as viewpoint changes or shape peculiarities) exist and imply different notions of similarity between objects, affecting the generalization on the images unseen during training. However, existing approaches usually directly learn a single embedding space on all available training data, struggling to encode all different types of relationships, and do not generalize well. We propose to build a more expressive representation by jointly splitting the embedding space and the data hierarchically into smaller sub-parts. We successively focus on smaller subsets of the training data, reducing its variance and learning a different embedding subspace for each data subset. Moreover, the subspaces are learned jointly to cover not only the intricacies, but the breadth of the data as well. Only after that, we build the final embedding from the subspaces in the conquering stage. The proposed algorithm acts as a transparent wrapper that can be placed around arbitrary existing DML methods. Our approach significantly improves upon the state-of-the-art on image retrieval, clustering, and re-identification tasks evaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop Clothes, and PKU VehicleID datasets.	[Sanakoyeu, Artsiom; Ma, Pingchuan; Tschernezki, Vadim; Ommer, Bjorn] Heidelberg Univ, Heidelberg Collaboratory Image Proc, D-69117 Heidelberg, Germany; [Sanakoyeu, Artsiom; Ma, Pingchuan; Tschernezki, Vadim; Ommer, Bjorn] Heidelberg Univ, Interdisciplinary Ctr Sci Comp, D-69117 Heidelberg, Germany	Ruprecht Karls University Heidelberg; Ruprecht Karls University Heidelberg	Sanakoyeu, A (corresponding author), Heidelberg Univ, Heidelberg Collaboratory Image Proc, D-69117 Heidelberg, Germany.; Sanakoyeu, A (corresponding author), Heidelberg Univ, Interdisciplinary Ctr Sci Comp, D-69117 Heidelberg, Germany.	Artsiom.Sanakoyeu@iwr.uni-heidelberg.de; Pingchuan.Ma@iwr.uni-heidelberg.de; Vadim.Tschernezki@iwr.uni-heidelberg.de; Bjorn.Ommer@iwr.uni-heidelberg.de			German Research Foundation (DFG) [421703927]; German Federal Ministry BMWi	German Research Foundation (DFG)(German Research Foundation (DFG)); German Federal Ministry BMWi	This work was supported in part by the German Research Foundation (DFG) under project 421703927 and the German Federal Ministry BMWi within the project "KI Absicherung".	Aziere N, 2019, PROC CVPR IEEE, P7291, DOI 10.1109/CVPR.2019.00747; Bai Y, 2017, IEEE INT CON MULTI, P1452, DOI 10.1109/ICME.2017.8019371; Bautista M. A., 2016, ADV NEURAL INFORM PR, V29, P3846; Bautista MA, 2017, PROC CVPR IEEE, P1923, DOI 10.1109/CVPR.2017.208; Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bohne J, 2018, PATTERN RECOGN, V75, P315, DOI 10.1016/j.patcog.2017.04.002; Buchler U, 2018, LECT NOTES COMPUT SC, V11219, P797, DOI 10.1007/978-3-030-01267-0_47; Cakir F, 2019, PROC CVPR IEEE, P1861, DOI 10.1109/CVPR.2019.00196; Caron M, 2019, IEEE I CONF COMP VIS, P2959, DOI 10.1109/ICCV.2019.00305; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005; Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294; Elyor Kodirov, 2019, Arxiv, DOI arXiv:1911.09976; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17; Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graham W. Taylor, 2017, Arxiv, DOI arXiv:1708.04552; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hermans Alexander, 2017, ARXIV, P1; Hershey JR, 2016, INT CONF ACOUST SPEE, P31, DOI 10.1109/ICASSP.2016.7471631; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Huang C, 2016, PROC CVPR IEEE, P5175, DOI 10.1109/CVPR.2016.559; Iscen A, 2018, PROC CVPR IEEE, P7642, DOI 10.1109/CVPR.2018.00797; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jian Guo, 2015, Arxiv, DOI arXiv:1506.07224; Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; Karras Timo Aila Samuli Laine Tero, 2018, PROC INT C LEARN REP, Patent No. [1710.10196, 171010196]; Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330; Kim W, 2018, LECT NOTES COMPUT SC, V11205, P760, DOI 10.1007/978-3-030-01246-5_45; Kingma DP, 2015, INT C LEARN REPR ICL; Koenecke A, 2020, LECT NOTES ARTIF INT, V11985, P16, DOI 10.1007/978-3-030-37720-5_2; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Krueger KA, 2009, COGNITION, V110, P380, DOI 10.1016/j.cognition.2008.11.014; Lin XD, 2018, LECT NOTES COMPUT SC, V11219, P714, DOI 10.1007/978-3-030-01267-0_42; Liu WY, 2016, PR MACH LEARN RES, V48; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Lukas J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Matiisen T, 2020, IEEE T NEUR NET LEAR, V31, P3732, DOI 10.1109/TNNLS.2019.2934906; McInnes L., 2018, J OPEN SOURCE SOFTW, DOI [DOI 10.21105/JOSS.00861, 10.21105/joss.00861]; Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47; Musgrave Kevin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P681, DOI 10.1007/978-3-030-58595-2_41; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925; Opitz M, 2017, IEEE I CONF COMP VIS, P5199, DOI 10.1109/ICCV.2017.555; Paszke A, 2019, ADV NEUR IN, V32; Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655; Rippel O., 2016, PROC 4 INT C LEARN R; Roth K, 2020, PROC CVPR IEEE, P6567, DOI 10.1109/CVPR42600.2020.00660; Roth K, 2019, IEEE I CONF COMP VIS, P7999, DOI 10.1109/ICCV.2019.00809; Roth Karsten, 2020, INT C MACH LEARN, P8242; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sanakoyeu A, 2019, PROC CVPR IEEE, P471, DOI 10.1109/CVPR.2019.00056; Sanakoyeu A, 2018, PATTERN RECOGN, V78, P331, DOI 10.1016/j.patcog.2018.01.036; Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129; SANGER TD, 1994, IEEE T ROBOTIC AUTOM, V10, P323, DOI 10.1109/70.294207; Saxena S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P369, DOI 10.1109/ICCVW.2015.56; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22; Sohn K, 2016, ADV NEUR IN, V29; Song HO, 2017, PROC CVPR IEEE, P2206, DOI 10.1109/CVPR.2017.237; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; St Amand J, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1097, DOI 10.1145/3097983.3098153; Steinbach M., 2000, KDD WORKSH TEXT MIN, P109; Su C, 2015, IEEE I CONF COMP VIS, P3739, DOI 10.1109/ICCV.2015.426; Suh Y, 2019, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2019.00742; Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049; Tadmor O., 2016, P ADV NEURAL INFORM, P1396; Tan RB, 2019, IEEE I CONF COMP VIS, P10372, DOI 10.1109/ICCV.2019.01047; Tuama A, 2016, IEEE INT WORKS INFOR; Ustinova E, 2016, ADV NEUR IN, V29; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Veit A, 2017, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2017.193; Wah C., 2011, TECH REP; Wang GC, 2017, IEEE I CONF COMP VIS, P2831, DOI 10.1109/ICCV.2017.306; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Wang XS, 2022, IEEE T PATTERN ANAL, V44, P5414, DOI [10.1109/TPAMI.2021.3068449, 10.1109/CVPR.2019.00535]; Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Xuan H, 2018, LECT NOTES COMPUT SC, V11220, P751, DOI 10.1007/978-3-030-01270-0_44; Xuan H, 2020, IEEE WINT CONF APPL, P2463, DOI 10.1109/WACV45572.2020.9093432; Yan ZC, 2015, IEEE I CONF COMP VIS, P2740, DOI 10.1109/ICCV.2015.314; Yu BS, 2019, IEEE I CONF COMP VIS, P6499, DOI 10.1109/ICCV.2019.00659; Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zhai Andrew, 2019, BMVC; Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485; Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016	106	2	2	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8306	8320		10.1109/TPAMI.2021.3113270	http://dx.doi.org/10.1109/TPAMI.2021.3113270			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34529564	Green Submitted			2022-12-18	WOS:000864325900073
J	Son, S; Kim, J; Lai, WS; Yang, MH; Lee, KM				Son, Sanghyun; Kim, Jaeha; Lai, Wei-Sheng; Yang, Ming-Hsuan; Lee, Kyoung Mu			Toward Real-World Super-Resolution via Adaptive Downsampling Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Training; Superresolution; Image reconstruction; Unsupervised learning; Degradation; Adaptation models; Image super-resolution; image downsampling; unsupervised learning		Most image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs that are constructed by a predetermined operation, e.g., bicubic downsampling. As existing methods typically learn an inverse mapping of the specific function, they produce blurry results when applied to real-world images whose exact formulation is different and unknown. Therefore, several methods attempt to synthesize much more diverse LR samples or learn a realistic downsampling model. However, due to restrictive assumptions on the downsampling process, they are still biased and less generalizable. This study proposes a novel method to simulate an unknown downsampling process without imposing restrictive prior knowledge. We propose a generalizable low-frequency loss (LFL) in the adversarial training framework to imitate the distribution of target LR images without using any paired examples. Furthermore, we design an adaptive data loss (ADL) for the downsampler, which can be adaptively learned and updated from the data during the training loops. Extensive experiments validate that our downsampling model can facilitate existing SR methods to perform more accurate reconstructions on various synthetic and real-world examples than the conventional approaches.	[Son, Sanghyun; Kim, Jaeha; Lee, Kyoung Mu] Seoul Natl Univ, Dept ECE, Seoul 08826, South Korea; [Son, Sanghyun; Kim, Jaeha; Lee, Kyoung Mu] Seoul Natl Univ, ASRI, Seoul 08826, South Korea; [Lai, Wei-Sheng; Yang, Ming-Hsuan] Google, Mountain View, CA 94043 USA	Seoul National University (SNU); Seoul National University (SNU); Google Incorporated	Lee, KM (corresponding author), Seoul Natl Univ, Dept ECE, Seoul 08826, South Korea.; Lee, KM (corresponding author), Seoul Natl Univ, ASRI, Seoul 08826, South Korea.	thstkdgus35@snu.ac.kr; hyjkim2@snu.ac.kr; wslai@google.com; minghsuan@google.com; kyoungmu@snu.ac.kr	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	IITP, Korea Government [2021-0-01343]	IITP, Korea Government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea)	This work was supported in part by the IITP, Korea Government under Grant 2021-0-01343 [Artificial Intelligence Graduate School Program (Seoul National University)]. Sanghyun Son and Jaeha Kimcontributed equally to thiswork.	Abu Hussein S, 2020, PROC CVPR IEEE, P1425, DOI 10.1109/CVPR42600.2020.00150; Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150; Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16; Alec Radford, 2016, Arxiv, DOI arXiv:1511.06434; Andrea Vedaldi, 2017, Arxiv, DOI arXiv:1607.08022; Andreas Lugmayr, 2019, Arxiv, DOI arXiv:1909.09629; Bahat Y, 2020, PROC CVPR IEEE, P2713, DOI 10.1109/CVPR42600.2020.00279; Bell-Kligler S, 2019, ADV NEUR IN, V32; Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12; Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12; Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318; Changqing Zhang, 2018, Arxiv, DOI arXiv:1812.04240; Chen C, 2019, PROC CVPR IEEE, P1652, DOI 10.1109/CVPR.2019.00175; Cornillere V, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356575; Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; Hu Z, 2012, LECT NOTES COMPUT SC, V7576, P59, DOI 10.1007/978-3-642-33715-4_5; Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082; Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304; Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32; Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Maeda S, 2020, PROC CVPR IEEE, P288, DOI 10.1109/CVPR42600.2020.00037; Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47; Michaeli T, 2013, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2013.121; Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371; Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428; Rad MS, 2019, IEEE I CONF COMP VIS, P2710, DOI 10.1109/ICCV.2019.00280; Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wang YF, 2018, IEEE COMPUT SOC CONF, P977, DOI 10.1109/CVPRW.2018.00131; Wei Pengxu, 2020, EUR C COMP VIS, P101, DOI DOI 10.1007/978-3-030-58598-3_7; Wonkyung Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P465, DOI 10.1007/978-3-030-58586-0_28; Wronski B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323024; Xiaotong Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P272, DOI 10.1007/978-3-030-58542-6_17; Xu XY, 2019, PROC CVPR IEEE, P1723, DOI 10.1109/CVPR.2019.00182; Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259; Yu-Syuan Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12493, DOI 10.1109/CVPR42600.2020.01251; Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang Kai, 2021, ARXIV; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319; Zhang XN, 2019, PROC CVPR IEEE, P3757, DOI 10.1109/CVPR.2019.00388; Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhou RF, 2019, IEEE I CONF COMP VIS, P2433, DOI 10.1109/ICCV.2019.00252; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	63	2	2	6	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8657	8670		10.1109/TPAMI.2021.3106790	http://dx.doi.org/10.1109/TPAMI.2021.3106790			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34428134	Green Submitted			2022-12-18	WOS:000864325900095
J	Wu, JB; Xu, CL; Han, X; Zhou, DQ; Zhang, ML; Li, HZ; Tan, KC				Wu, Jibin; Xu, Chenglin; Han, Xiao; Zhou, Daquan; Zhang, Malu; Li, Haizhou; Tan, Kay Chen			Progressive Tandem Learning for Pattern Recognition With Deep Spiking Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Neurons; Task analysis; Training; Pattern recognition; Biological neural networks; Learning systems; Encoding; Deep spiking neural network; ANN-to-SNN conversion; spike-based learning; large-scale object recognition; speech separation; efficient neuromorphic inference	COMMUNICATION; SPEAKER	Spiking neural networks (SNNs) have shown clear advantages over traditional artificial neural networks (ANNs) for low latency and high computational efficiency, due to their event-driven nature and sparse communication. However, the training of deep SNNs is not straightforward. In this paper, we propose a novel ANN-to-SNN conversion and layer-wise learning framework for rapid and efficient pattern recognition, which is referred to as progressive tandem learning. By studying the equivalence between ANNs and SNNs in the discrete representation space, a primitive network conversion method is introduced that takes full advantage of spike count to approximate the activation value of ANN neurons. To compensate for the approximation errors arising from the primitive network conversion, we further introduce a layer-wise learning method with an adaptive training scheduler to fine-tune the network weights. The progressive tandem learning framework also allows hardware constraints, such as limited weight precision and fan-in connections, to be progressively imposed during training. The SNNs thus trained have demonstrated remarkable classification and regression capabilities on large-scale object recognition, image reconstruction, and speech separation tasks, while requiring at least an order of magnitude reduced inference time and synaptic operations than other state-of-the-art SNN implementations. It, therefore, opens up a myriad of opportunities for pervasive mobile and embedded devices with a limited power budget.	[Wu, Jibin; Han, Xiao; Zhou, Daquan; Zhang, Malu; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore; [Xu, Chenglin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore; [Xu, Chenglin] Nanyang Technol Univ, Temasek Labs NTU, Singapore 639798, Singapore; [Li, Haizhou] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China; [Tan, Kay Chen] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China	National University of Singapore; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Chinese University of Hong Kong, Shenzhen; Hong Kong Polytechnic University	Zhang, ML (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.	jibin.wu@u.nus.edu; xuchenglin@ntu.edu.sg; e0269084@u.nus.edu; daquan.zhou@u.nus.edu; maluzhang@nus.edu.sg; haizhou.li@nus.edu.sg; kctan@polyu.edu.hk		zhang, malu/0000-0002-2345-0974; Wu, Jibin/0000-0003-0135-4188; Li, Haizhou/0000-0001-9158-9401	A*STAR under RIE2020 Advanced Manufacturing and Engineering Domain (AME) Programmatic [A1687b0033]; IAF; A*STAR; SOITEC; NXP; National University of Singapore under FD-fAbrICS: Joint Lab for FD-SOI Always-on Intelligent & Connected Systems [I2001E0053]; Zhejiang Lab [2019KC0AB02]; China Postdoctoral Science Foundation [2020M680148]; Zhejiang Lab's International Talent Found for Young Professionals	A*STAR under RIE2020 Advanced Manufacturing and Engineering Domain (AME) Programmatic(Agency for Science Technology & Research (A*STAR)); IAF; A*STAR(Agency for Science Technology & Research (A*STAR)); SOITEC; NXP; National University of Singapore under FD-fAbrICS: Joint Lab for FD-SOI Always-on Intelligent & Connected Systems; Zhejiang Lab; China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Zhejiang Lab's International Talent Found for Young Professionals	This work was supported in part by A*STAR under RIE2020 Advanced Manufacturing and Engineering Domain (AME) Programmatic under Grant (A1687b0033, Project Title: Spiking Neural Networks), in part by the IAF, A*STAR, SOITEC, NXP, and by the National University of Singapore under FD-fAbrICS: Joint Lab for FD-SOI Always-on Intelligent & Connected Systems under Grant I2001E0053. The work of Jibin Wu was supported by the Zhejiang Lab under Grant 2019KC0AB02. The work of Malu Zhang was supported in part by the China Postdoctoral Science Foundation under Grant 2020M680148 and in part by the Zhejiang Lab's International Talent Found for Young Professionals.	Alexander G. Anderson, 2017, Arxiv, DOI arXiv:1705.07199; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Perez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71; Bellec G, 2018, ADV NEUR IN, V31; Byunggook Na, 2019, Arxiv, DOI arXiv:1903.06530; Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3; Chris Eliasmith, 2016, Arxiv, DOI arXiv:1611.05141; Christian Szegedy, 2015, Arxiv, DOI arXiv:1502.03167; Daniel Soudry, 2016, Arxiv, DOI arXiv:1602.02830; Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng S., 2021, P INT C LEARN REPR; Diehl PU, 2015, IEEE IJCNN; Gang Pan, 2020, Arxiv, DOI arXiv:1805.01352; Garofolo J., 1993, CSR I WSJ0 COMPLETE, DOI [10.35111/ewkm-cg47, DOI 10.35111/EWKM-CG47]; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gopalakrishnan Srinivasan, 2020, Arxiv, DOI arXiv:2005.01807; Gu PJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1366; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hershey JR, 2016, INT CONF ACOUST SPEE, P31, DOI 10.1109/ICASSP.2016.7471631; Hirschberg J, 2015, SCIENCE, V349, P261, DOI 10.1126/science.aaa8685; Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094; Hu Y, 2008, IEEE T AUDIO SPEECH, V16, P229, DOI 10.1109/TASL.2007.911054; Huizi Mao, 2016, Arxiv, DOI arXiv:1510.00149; Isomura T, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004643; Kaya EM, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0101; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laughlin SB, 2003, SCIENCE, V301, P1870, DOI 10.1126/science.1089662; Le Roux J, 2019, INT CONF ACOUST SPEE, P626, DOI 10.1109/ICASSP.2019.8683855; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119; Li  J., 2015, ROBUST AUTOMATIC SPE; Luo Y, 2019, IEEE-ACM T AUDIO SPE, V27, P1256, DOI 10.1109/TASLP.2019.2915167; Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642; Mesgarani N, 2012, NATURE, V485, P233, DOI 10.1038/nature11020; Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595; Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8; Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774; Rao W, 2019, INTERSPEECH, P1273, DOI 10.21437/Interspeech.2019-1410; Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023; Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2; Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682; Sell G, 2018, INTERSPEECH, P2808, DOI 10.21437/Interspeech.2018-1893; Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095; Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y; Shrestha SB, 2018, ADV NEUR IN, V31; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; van den Oord A, 2017, ADV NEUR IN, V30; van den Oord Aaron, 2016, ARXIV, DOI DOI 10.48550/ARXIV.1609.03499; Wang DeLiang, 2017, IEEE Spectr, V54, P32, DOI 10.1109/MSPEC.2017.7864754; Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655; Wu JB, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3095724; Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311; Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331; Xiong W, 2017, IEEE-ACM T AUDIO SPE, V25, P2410, DOI 10.1109/TASLP.2017.2756440; Xu CL, 2020, IEEE-ACM T AUDIO SPE, V28, P1370, DOI 10.1109/TASLP.2020.2987429; Xu Y, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1219; Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086	62	2	2	11	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7824	7840		10.1109/TPAMI.2021.3114196	http://dx.doi.org/10.1109/TPAMI.2021.3114196			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34546918	Green Submitted, hybrid			2022-12-18	WOS:000864325900041
J	Yang, L; Xu, M; Guo, YC; Deng, X; Gao, FY; Guan, ZY				Yang, Li; Xu, Mai; Guo, Yichen; Deng, Xin; Gao, Fangyuan; Guan, Zhenyu			Hierarchical Bayesian LSTM for Head Trajectory Prediction on Omnidirectional Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Trajectory; Head; Magnetic heads; Hidden Markov models; Predictive models; Bayes methods; Computational modeling; Omnidirectional images; head trajectory; hierarchical Bayesian inference	SCANPATH ESTIMATION; SALIENCY ESTIMATION; VISUAL-ATTENTION; EYE-MOVEMENT; MODEL	When viewing omnidirectional images (ODIs), viewers can access different viewports via head movement (HM), which sequentially forms head trajectories in spatial-temporal domain. Thus, head trajectories play a key role in modeling human attention on ODIs. In this paper, we establish a large-scale dataset collecting 21,600 head trajectories on 1,080 ODIs. By mining our dataset, we find two important factors influencing head trajectories, i.e., temporal dependency and subject-specific variance. Accordingly, we propose a novel approach integrating hierarchical Bayesian inference into long short-term memory (LSTM) network for head trajectory prediction on ODIs, which is called HiBayes-LSTM. In HiBayes-LSTM, we develop a mechanism of Future Intention Estimation (FIE), which captures the temporal correlations from previous, current and estimated future information, for predicting viewport transition. Additionally, a training scheme called Hierarchical Bayesian inference (HBI) is developed for modeling inter-subject uncertainty in HiBayes-LSTM. For HBI, we introduce a joint Gaussian distribution in a hierarchy, to approximate the posterior distribution over network weights. By sampling subject-specific weights from the approximated posterior distribution, our HiBayes-LSTM approach can yield diverse viewport transition among different subjects and obtain multiple head trajectories. Extensive experiments validate that our HiBayes-LSTM approach significantly outperforms 9 state-of-the-art approaches for trajectory prediction on ODIs, and then it is successfully applied to predict saliency on ODIs.	[Yang, Li; Xu, Mai; Guo, Yichen] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China; [Deng, Xin; Gao, Fangyuan; Guan, Zhenyu] Beihang Univ, Sch Cyber Sci & Technol, Beijing 100191, Peoples R China	Beihang University; Beihang University	Xu, M (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.	LiYang2018@buaa.edu.cn; MaiXu@buaa.edu.cn; 16711024@buaa.edu.cn; cindydeng@buaa.edu.cn; fygao0808@buaa.edu.cn; guanzhenyu@buaa.edu.cn		Guo, Yichen/0000-0003-3164-9139; GUAN, zhenyu/0000-0002-3959-338X; Gao, Fangyuan/0000-0002-1225-2067; Yang, Li/0000-0002-1889-3113	NSFC [61922009, 61876013, 62050175, 62001016]; Beijing Natural Science Foundation [JQ20020]	NSFC(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work was supported by the NSFC Projects under Grants 61922009, 61876013, 62050175, and 62001016, and by Beijing Natural Science Foundation under Grant JQ20020.	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; [Anonymous], US; Assens M, 2018, SIGNAL PROCESS-IMAGE, V69, P8, DOI 10.1016/j.image.2018.06.006; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821; Battisti F, 2018, SIGNAL PROCESS-IMAGE, V69, P53, DOI 10.1016/j.image.2018.03.008; Bays PM, 2012, J VISION, V12, DOI 10.1167/12.8.8; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Boccignone G, 2004, PHYSICA A, V331, P207, DOI 10.1016/j.physa.2003.09.011; Boccignone G., 2016, ARXIV; Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13; Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212; Cristino F, 2010, BEHAV RES METHODS, V42, P692, DOI 10.3758/BRM.42.3.692; De Abreu A, 2017, INT WORK QUAL MULTIM; de la Fuente YS, 2017, MULTIMED TOOLS APPL, V76, P5631, DOI 10.1007/s11042-016-4097-4; Dewhurst R, 2012, BEHAV RES METHODS, V44, P1079, DOI 10.3758/s13428-012-0212-2; Dong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P323, DOI 10.1007/978-3-030-58604-1_20; Engbert R, 2015, J VISION, V15, DOI 10.1167/15.1.14; Fang YM, 2018, SIGNAL PROCESS-IMAGE, V69, P1, DOI 10.1016/j.image.2018.07.009; Fortunato M., 2017, ARXIV; Gal Y, 2016, ADV NEUR IN, V29; Gal Y, 2016, PR MACH LEARN RES, V48; Gan Z, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P321, DOI 10.18653/v1/P17-1030; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hu B, 2017, 2017 51ST ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS); Huang MK, 2018, IEEE T IMAGE PROCESS, V27, P6039, DOI 10.1109/TIP.2018.2865089; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jiang M, 2016, IEEE T NEUR NET LEAR, V27, P1241, DOI 10.1109/TNNLS.2015.2496306; Joshi A, 2017, PROC CVPR IEEE, P455, DOI 10.1109/CVPR.2017.56; Kingma DP, 2015, INT C LEARN REPR ICL; Kootstra G, 2011, COGN COMPUT, V3, P223, DOI 10.1007/s12559-010-9089-5; Laumann F., 2018, ARXIV; Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86; Le Meur O, 2017, IEEE T IMAGE PROCESS, V26, P4777, DOI 10.1109/TIP.2017.2722238; Le Meur O, 2016, VISION RES, V121, P72, DOI 10.1016/j.visres.2016.01.005; Le Meur O, 2015, VISION RES, V116, P152, DOI 10.1016/j.visres.2014.12.026; Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006; Lee DK, 1999, NAT NEUROSCI, V2, P375, DOI 10.1038/7286; Lee TS, 2000, ADV NEUR IN, V12, P834; Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581; Liu HY, 2013, IEEE I CONF COMP VIS, P3232, DOI 10.1109/ICCV.2013.401; Muller M, 2007, INFORM RETRIEVAL MUS, V2; NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4; Ng K.-T., 2001, PROC INT C IMAGE PRO, P82; Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218; Salvucci D. D., 2000, P 2000 S EYE TRACKIN, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]; Sharma P., 2013, INT J COMPUT APPL, V61, P26; Simonyan K., 2015, INT C LEARN REPR ICL; Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599; Startsev M, 2018, SIGNAL PROCESS-IMAGE, V69, P43, DOI 10.1016/j.image.2018.03.013; Sun WJ, 2021, IEEE T PATTERN ANAL, V43, P2101, DOI 10.1109/TPAMI.2019.2956930; Sun XS, 2014, IEEE T IMAGE PROCESS, V23, P4649, DOI 10.1109/TIP.2014.2337758; Ngo T, 2017, IEEE IMAGE PROC, P3435; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tsotsos JK, 2016, J EYE MOVEMENT RES, V9, DOI 10.16910/jemr.9.5.2; Upenik Evgeniy, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P73, DOI 10.1109/ICMEW.2017.8026231; Vaswani A, 2017, ADV NEUR IN, V30; Wang W, 2011, PROC CVPR IEEE, P441, DOI 10.1109/CVPR.2011.5995423; Wang YX, 2017, COGN PROCESS, V18, P87, DOI 10.1007/s10339-016-0781-6; Weisstein E. W., MOORE NEIGHBORHOOD; Wloka C, 2018, PROC CVPR IEEE, P3184, DOI 10.1109/CVPR.2018.00336; Wu Y, 2017, IEEE INT CON MULTI, P529, DOI 10.1109/ICME.2017.8019456; Xia C, 2019, IEEE T IMAGE PROCESS, V28, P3502, DOI 10.1109/TIP.2019.2897966; Xu M, 2022, IEEE T PATTERN ANAL, V44, P2198, DOI 10.1109/TPAMI.2020.3028509; Xu M, 2021, IEEE T IMAGE PROCESS, V30, P2087, DOI 10.1109/TIP.2021.3050861; Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783; Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010	68	2	2	5	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7563	7580		10.1109/TPAMI.2021.3117019	http://dx.doi.org/10.1109/TPAMI.2021.3117019			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34596534				2022-12-18	WOS:000864325900024
J	Yang, WH; Tan, RT; Feng, JS; Wang, SQ; Cheng, B; Liu, JY				Yang, Wenhan; Tan, Robby T.; Feng, Jiashi; Wang, Shiqi; Cheng, Bin; Liu, Jiaying			Recurrent Multi-Frame Deraining: Combining Physics Guidance and Adversarial Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Rain; Degradation; Physics; Computational modeling; Cameras; Image color analysis; Estimation; Multi-frame; video rain removal; physics recovery guidance; adversarial learning	RAIN; REMOVAL	Existing video rain removal methods mainly focus on rain streak removal and are solely trained based on the synthetic data, which neglect more complex degradation factors, e.g., rain accumulation, and the prior knowledge in real rain data. Thus, in this paper, we build a more comprehensive rain model with several degradation factors and construct a novel two-stage video rain removal method that combines the power of synthetic videos and real data. Specifically, a novel two-stage progressive network is proposed: recovery guided by a physics model, and further restoration by adversarial learning. The first stage performs an inverse recovery process guided by our proposed rain model. An initially estimated background frame is obtained based on the input rain frame. The second stage employs adversarial learning to refine the result, i.e., recovering the overall color and illumination distributions of the frame, the background details that are failed to be recovered in the first stage, and removing the artifacts generated in the first stage. Furthermore, we also introduce a more comprehensive rain model that includes degradation factors, e.g., occlusion and rain accumulation, which appear in real scenes yet ignored by existing methods. This model, which generates more realistic rain images, will train and evaluate our models better. Extensive evaluations on synthetic and real videos show the effectiveness of our method in comparisons to the state-of-the-art methods. Our datasets, results and code are available at: https://github.com/flyywh/Recurrent-Multi-Frame-Deraining.	[Yang, Wenhan; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China; [Tan, Robby T.] Natl Univ Singapore, Yale NUS Coll, Dept Elect & Comp Engn, Singapore, Singapore; [Feng, Jiashi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore; [Cheng, Bin] Beijing Acad Artificial Intelligence, Machine Learning Grp, Beijing 100081, Peoples R China; [Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China	City University of Hong Kong; National University of Singapore; Yale NUS College; National University of Singapore; Peking University	Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.	wyang34@cityu.edu.hk; robby.tan@nus.edu.sg; elefjia@nus.edu.sg; shiqwang@cityu.edu.hk; chengbin@baai.ac.cn; liujiaying@pku.edu.cn			National Key Research and Development Program of China [2018AAA 0102702]; Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China [61772043, 62022038, 62022002]; National Research Foundation Singapore under its AI Singapore Programme [AISG-100E-2019-035]; Hong Kong RGC ECS [21211018]; GRF [11203220];  [MOE2019-T2-1-130]	National Key Research and Development Program of China; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Research Foundation Singapore under its AI Singapore Programme(National Research Foundation, Singapore); Hong Kong RGC ECS; GRF; 	This work was supported by the National Key Research and Development Program of China under Grant 2018AAA 0102702, the Fundamental Research Funds for the Central Universities, the National Natural Science Foundation of China under Contract No. 61772043, No. 62022038, and No. 62022002, the National Research Foundation Singapore under its AI Singapore Programme (Award Number: [AISG-100E-2019-035]), the Hong Kong RGC ECS under Grant 21211018, GRF under Grant 11203220. This is a research achievement of Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). The work of Robby T. Tan was supported by MOE2019-T2-1-130.	Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2; Bolun Cai, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9917, P315, DOI 10.1007/978-3-319-48896-7_31; Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7; Chen J, 2018, PROC CVPR IEEE, P6286, DOI 10.1109/CVPR.2018.00658; De-An Huang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P164, DOI 10.1109/ICME.2012.92; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802; Garg K, 2005, IEEE I CONF COMP VIS, P1067; Garg K, 2006, ACM T GRAPHIC, V25, P996, DOI 10.1145/1141911.1141985; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821; Hu YY, 2018, IEEE DATA COMPR CONF, P413, DOI 10.1109/DCC.2018.00066; Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759; Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522; Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512; Jiang TX, 2017, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2017.301; Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057; Kim JH, 2015, IEEE T IMAGE PROCESS, V24, P2658, DOI 10.1109/TIP.2015.2428933; Li BY, 2018, AAAI CONF ARTIF INTE, P7016; Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951; Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Li ZW, 2015, PROC CVPR IEEE, P4988, DOI 10.1109/CVPR.2015.7299133; Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341; Liu JY, 2019, IEEE T IMAGE PROCESS, V28, P699, DOI 10.1109/TIP.2018.2869722; Loong-Fah Cheong, 2017, Arxiv, DOI arXiv:1712.06830; Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388; Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263; Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406; Ren WH, 2017, PROC CVPR IEEE, P2838, DOI 10.1109/CVPR.2017.303; Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178; Ronneberger O., 2015, P INT C MED IM COMP; Santhaseelan V, 2015, INT J COMPUT VISION, V112, P71, DOI 10.1007/s11263-014-0759-8; Starik S, 2003, TEXTURE WORKSHOP, V2, P406; Sun SH, 2014, IEEE IMAGE PROC, P4482, DOI 10.1109/ICIP.2014.7025909; Tripathi AK, 2012, IET IMAGE PROCESS, V6, P181, DOI 10.1049/iet-ipr.2010.0547; Tripathi AK, 2011, IETE J RES, V57, P82, DOI 10.4103/0377-2063.78382; Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wei W, 2017, IEEE I CONF COMP VIS, P2535, DOI 10.1109/ICCV.2017.275; Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2; Yang WH, 2019, PROC CVPR IEEE, P1661, DOI 10.1109/CVPR.2019.00176; Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183; Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; Zhang XP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P461, DOI 10.1109/ICME.2006.262572; Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276	55	2	2	6	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8569	8586		10.1109/TPAMI.2021.3083076	http://dx.doi.org/10.1109/TPAMI.2021.3083076			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34029186				2022-12-18	WOS:000864325900090
J	Yu, BS; Tao, DC				Yu, Baosheng; Tao, Dacheng			Heatmap Regression via Randomized Rounding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Heating systems; Location awareness; Semantics; Quantization (signal); Pose estimation; Numerical models; Faces; Semantic landmark localization; heatmap regression; quantization error; randomized rounding	FACE ALIGNMENT; NETWORK	Heatmap regression has become the mainstream methodology for deep learning-based semantic landmark localization, including in facial landmark localization and human pose estimation. Though heatmap regression is robust to large variations in pose, illumination, and occlusion in unconstrained settings, it usually suffers from a sub-pixel localization problem. Specifically, considering that the activation point indices in heatmaps are always integers, quantization error thus appears when using heatmaps as the representation of numerical coordinates. Previous methods to overcome the sub-pixel localization problem usually rely on high-resolution heatmaps. As a result, there is always a trade-off between achieving localization accuracy and computational cost, where the computational complexity of heatmap regression depends on the heatmap resolution in a quadratic manner. In this paper, we formally analyze the quantization error of vanilla heatmap regression and propose a simple yet effective quantization system to address the sub-pixel localization problem. The proposed quantization system induced by the randomized rounding operation 1) encodes the fractional part of numerical coordinates into the ground truth heatmap using a probabilistic approach during training; and 2) decodes the predicted numerical coordinates from a set of activation points during testing. We prove that the proposed quantization system for heatmap regression is unbiased and lossless. Experimental results on popular facial landmark localization datasets (WFLW, 300W, COFW, and AFLW) and human pose estimation datasets (MPII and COCO) demonstrate the effectiveness of the proposed method for efficient and accurate semantic landmark localization. Code is available at http://github.com/baoshengyu/H3R.	[Yu, Baosheng; Tao, Dacheng] Univ Sydney, Camperdown, NSW 2006, Australia; [Tao, Dacheng] JD Explore Acad, Beijing 101111, Peoples R China	University of Sydney	Tao, DC (corresponding author), Univ Sydney, Camperdown, NSW 2006, Australia.; Tao, DC (corresponding author), JD Explore Acad, Beijing 101111, Peoples R China.	baosheng.yu@sydney.edu.au; dacheng.tao@gmail.com			ARC project [FL-170100117]	ARC project(Australian Research Council)	The work of Baosheng Yu was supported by the ARC project under Grant FL-170100117.	Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.596; Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64; Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23; Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204; Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012; Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742; Chen YC, 2017, IEEE I CONF COMP VIS, P4511, DOI 10.1109/ICCV.2017.482; Dapogny A, 2019, IEEE I CONF COMP VIS, P6892, DOI 10.1109/ICCV.2019.00699; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047; Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kostinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Korte B., 2012, COMBINATORIAL OPTIMI, V2; Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254; Kumar Abhinav, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8233, DOI 10.1109/CVPR42600.2020.00826; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Levine S, 2016, J MACH LEARN RES, V17; Li W., 2019, ARXIV; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Luvizon DC, 2019, COMPUT GRAPH-UK, V85, P15, DOI 10.1016/j.cag.2019.09.002; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Merget D, 2018, PROC CVPR IEEE, P781, DOI 10.1109/CVPR.2018.00088; Miao X, 2018, PROC CVPR IEEE, P5040, DOI 10.1109/CVPR.2018.00529; Naruniec J, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14062; Newell A, 2017, ADV NEUR IN, V30; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nibali A., 2018, ARXIV; Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Paszke A, 2019, ADV NEUR IN, V32; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Qian SJ, 2019, IEEE I CONF COMP VIS, P10152, DOI 10.1109/ICCV.2019.01025; RAGHAVAN P, 1987, COMBINATORICA, V7, P365, DOI 10.1007/BF02579324; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Ronneberger O., 2015, P INT C MED IM COMP; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Saxena A, 2008, INT J ROBOT RES, V27, P157, DOI 10.1177/0278364907087172; Sinha A, 2016, PROC CVPR IEEE, P4150, DOI 10.1109/CVPR.2016.450; Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tai Y, 2019, AAAI CONF ARTIF INTE, P8893; Thewlis J, 2017, IEEE I CONF COMP VIS, P3229, DOI 10.1109/ICCV.2017.348; Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262; Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056; Tompson J.J., 2014, ADV NEURAL INF PROCE, P1799, DOI DOI 10.5555/2968826.2969027; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Valle R, 2018, LECT NOTES COMPUT SC, V11218, P609, DOI 10.1007/978-3-030-01264-9_36; Valle R, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102846; Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346; Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265; Wang XY, 2019, IEEE I CONF COMP VIS, P6970, DOI 10.1109/ICCV.2019.00707; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu WY, 2017, IEEE COMPUT SOC CONF, P2096, DOI 10.1109/CVPRW.2017.261; Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253; Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103; Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zou X, 2019, IEEE I CONF COMP VIS, P141, DOI 10.1109/ICCV.2019.00023	86	2	2	7	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8276	8289		10.1109/TPAMI.2021.3103980	http://dx.doi.org/10.1109/TPAMI.2021.3103980			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34379589	Green Submitted			2022-12-18	WOS:000864325900071
J	Zhang, YJ; Wang, CY; Maybank, SJ; Tao, DC				Zhang, Youjian; Wang, Chaoyue; Maybank, Stephen J.; Tao, Dacheng			Exposure Trajectory Recovery From Motion Blur	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Trajectory; Dynamics; Estimation; Cameras; Kernel; Task analysis; Image restoration; Motion blur; exposure trajectory recovery; motion-aware image deblurring; video extraction from a single blurry image	SHAKEN	Motion blur in dynamic scenes is an important yet challenging research topic. Recently, deep learning methods have achieved impressive performance for dynamic scene deblurring. However, the motion information contained in a blurry image has yet to be fully explored and accurately formulated because: (i) the ground truth of dynamic motion is difficult to obtain; (ii) the temporal ordering is destroyed during the exposure; and (iii) the motion estimation from a blurry image is highly ill-posed. By revisiting the principle of camera exposure, motion blur can be described by the relative motions of sharp content with respect to each exposed position. In this paper, we define exposure trajectories, which represent the motion information contained in a blurry image and explain the causes of motion blur. A novel motion offset estimation framework is proposed to model pixel-wise displacements of the latent sharp image at multiple timepoints. Under mild constraints, our method can recover dense, (non-)linear exposure trajectories, which significantly reduce temporal disorder and ill-posed problems. Finally, experiments demonstrate that the recovered exposure trajectories not only capture accurate and interpretable motion information from a blurry image, but also benefit motion-aware image deblurring and warping-based video extraction tasks. Codes are available on https://github.com/yjzhang96/Motion-ETR.	[Zhang, Youjian; Wang, Chaoyue; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2006, Australia; [Maybank, Stephen J.] Birkbeck Coll, Dept Comp Sci & Informat Syst, London WC1E 7HX, England	University of Sydney; University of London; Birkbeck University London	Wang, CY; Tao, DC (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2006, Australia.	yzha0535@uni.sydney.edu.au; chaoyue.wang@sydney.edu.au; sjmaybank@dcs.bbk.ac.uk; dacheng.tao@sydney.edu.au		Wang, Chaoyue/0000-0002-9002-1029; Zhang, Youjian/0000-0002-5255-9258	Australian Research Council [FL-170100117, IH-180100002, IC-190100031]	Australian Research Council(Australian Research Council)	This work was supported by the Australian Research Council Projects under Grants FL-170100117, IH-180100002, and IC-190100031.	Aizenberg I, 2006, ADV SOFT COMP, P441, DOI 10.1007/3-540-34783-6_45; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Ben-Ezra M, 2004, IEEE T PATTERN ANAL, V26, P689, DOI 10.1109/TPAMI.2004.1; Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14; Chen HJ, 2018, IEEE INT CONF COMPUT; Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405; Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13; Jia JY, 2007, PROC CVPR IEEE, P453; Jin MG, 2018, PROC CVPR IEEE, P6334, DOI 10.1109/CVPR.2018.00663; Khare C., 2011, IMAGE, V2, P25; Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392; Kim TH, 2014, PROC CVPR IEEE, P2766, DOI 10.1109/CVPR.2014.348; Kingma D.P, P 3 INT C LEARNING R; Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308; Liu PD, 2020, IEEE ROBOT AUTOM LET, V5, P2475, DOI 10.1109/LRA.2020.2972873; Li M, 2021, Arxiv, DOI arXiv:2105.04104; Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804; Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244; Pan JS, 2016, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2016.56; Pan LY, 2019, PROC CVPR IEEE, P6027, DOI 10.1109/CVPR.2019.00619; Purohit K, 2020, AAAI CONF ARTIF INTE, V34, P11882; Purohit K, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00699; Qiu JY, 2019, PROC CVPR IEEE, P8485, DOI 10.1109/CVPR.2019.00869; Ramakrishnan S, 2017, IEEE INT CONF COMP V, P2993, DOI 10.1109/ICCVW.2017.353; Ren WQ, 2017, IEEE I CONF COMP VIS, P1086, DOI 10.1109/ICCV.2017.123; Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Tai YW, 2011, IEEE T PATTERN ANAL, V33, P1603, DOI 10.1109/TPAMI.2010.222; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Wang CY, 2019, IEEE T EVOLUT COMPUT, V23, P921, DOI 10.1109/TEVC.2019.2895748; Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247; Whyte O, 2014, INT J COMPUT VISION, V110, P185, DOI 10.1007/s11263-014-0727-3; Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Yuan Y, 2020, PROC CVPR IEEE, P3552, DOI 10.1109/CVPR42600.2020.00361; Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613; Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267; Zhang KH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P384, DOI 10.1145/3394171.3413929; Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865; Zheng SC, 2013, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2013.185	58	2	2	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7490	7504		10.1109/TPAMI.2021.3116135	http://dx.doi.org/10.1109/TPAMI.2021.3116135			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34582347	Green Accepted, Green Submitted			2022-12-18	WOS:000864325900019
J	Akram-Ali-Hammouri, Z; Fernandez-Delgado, M; Cernadas, E; Barro, S				Akram-Ali-Hammouri, Ziad; Fernandez-Delgado, Manuel; Cernadas, Eva; Barro, Senen			Fast Support Vector Classification for Large-Scale Problems	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Classification; large-scale datasets; support vector machine; closed-form training; model selection	INSTANCE SELECTION; OPTIMIZATION; KERNEL; PARAMETER; MACHINES	The support vector machine (SVM) is a very important machine learning algorithm with state-of-the-art performance on many classification problems. However, on large datasets it is very slow and requires much memory. To solve this defficiency, we propose the fast support vector classifier (FSVC) that includes: 1) an efficient closed-form training free of any numerical iterative procedure; 2) a small collection of class prototypes that avoids to store in memory an excessive number of support vectors; and 3) a fast method that selects the spread of the radial basis function kernel directly from data, without classifier execution nor iterative hyper-parameter tuning. The memory requirements of FSVC are very low, spending in average only 6.10(-7) sec. per pattern, input and class, and processing datasets up to 31 millions of patterns, 30,000 inputs and 131 classes in less than 1.5 hours (less than 3 hours with only 2GB of RAM). In average, the FSVC is 10 times faster, requires 12 times less memory and achieves 4.7 percent more performance than Liblinear, that fails on the 4 largest datasets by lack of memory, being 100 times faster and achieving only 6.7 percent less performance than Libsvm. The time spent by FSVC only depends on the dataset size and thus it can be accurately estimated for new datasets, while Libsvm or Liblinear are much slower on "difficult" datasets, even if they are small. The FSVC adjusts its requirements to the available memory, classifying large datasets in computers with limited memory. Code for the proposed algorithm in the Octave scientific programming language is provided.(1)	[Akram-Ali-Hammouri, Ziad; Fernandez-Delgado, Manuel; Cernadas, Eva; Barro, Senen] Univ Santiago de Compostela CiTIUS, Ctr Singular Invest Tecnol Intelixentes, Santiago De Compostela 15782, Spain		Fernandez-Delgado, M (corresponding author), Univ Santiago de Compostela CiTIUS, Ctr Singular Invest Tecnol Intelixentes, Santiago De Compostela 15782, Spain.	ziad.akram@rai.usc.es; manuel.fernandez.delgado@usc.es; eva.cernadas@usc.es; senen.barro@usc.es		Hammouri, Ziad/0000-0002-6916-886X	Xunta de Galicia [2019-2022 ED431G-2019/04]; European RegionalDevelopment Fund (ERDF)	Xunta de Galicia(Xunta de GaliciaEuropean Commission); European RegionalDevelopment Fund (ERDF)(European Commission)	This work was supported in part by the Xunta de Galicia through Accreditation 2019-2022 ED431G-2019/04) and in part by the European RegionalDevelopment Fund (ERDF).	Anant Raj, 2015, Arxiv, DOI arXiv:1407.5599; Bhavsar H., 2012, INT J ADV RES COMPUT, V1, P185; Boyang Li, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1784, DOI 10.1109/IJCNN.2009.5178618; Cano JR, 2003, IEEE T EVOLUT COMPUT, V7, P561, DOI 10.1109/TEVC.2003.819265; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen GX, 2008, PACIIA: 2008 PACIFIC-ASIA WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION, VOLS 1-3, PROCEEDINGS, P6; Fernandez-Delgado M, 2014, NEURAL NETWORKS, V50, P60, DOI 10.1016/j.neunet.2013.11.002; Friedrichs F, 2005, NEUROCOMPUTING, V64, P107, DOI 10.1016/j.neucom.2004.11.022; Hazan Elad, 2011, ADV NEURAL INFORM PR, V24, P1233; Kapp MN, 2012, APPL SOFT COMPUT, V12, P2550, DOI 10.1016/j.asoc.2012.04.001; Liang X, 2013, PATTERN RECOGN LETT, V34, P1203, DOI 10.1016/j.patrec.2013.03.015; Lin KM, 2003, IEEE T NEURAL NETWOR, V14, P1449, DOI 10.1109/TNN.2003.820828; Liu ZL, 2014, J ALGORITHMS COMPUT, V8, P163; Menezes MVF, 2019, PATTERN RECOGN LETT, V128, P1, DOI 10.1016/j.patrec.2019.08.001; Nalepa J, 2019, ARTIF INTELL REV, V52, P857, DOI 10.1007/s10462-017-9611-1; Pan XL, 2019, IEEE T NEUR NET LEAR, V30, P2263, DOI 10.1109/TNNLS.2018.2879800; Perez-Ortiz M, 2016, NEURAL PROCESS LETT, V44, P491, DOI 10.1007/s11063-015-9471-0; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Rosales-Perez A, 2017, IEEE T EVOLUT COMPUT, V21, P863, DOI 10.1109/TEVC.2017.2688863; Schleif FM, 2017, PATTERN RECOGN, V71, P187, DOI 10.1016/j.patcog.2017.06.003; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shawe-Taylor J, 2011, NEUROCOMPUTING, V74, P3609, DOI 10.1016/j.neucom.2011.06.026; Tharwat A, 2017, PATTERN RECOGN LETT, V93, P13, DOI 10.1016/j.patrec.2016.10.007; Tsang IW, 2005, J MACH LEARN RES, V6, P363; Verbiest N, 2016, APPL SOFT COMPUT, V38, P10, DOI 10.1016/j.asoc.2015.09.006; Wang WJ, 2003, NEUROCOMPUTING, V55, P643, DOI 10.1016/S0925-2312(02)00632-X; Wang Z, 2014, NEURAL COMPUT APPL, V24, P755, DOI 10.1007/s00521-012-1278-6	28	2	2	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6184	6195		10.1109/TPAMI.2021.3085969	http://dx.doi.org/10.1109/TPAMI.2021.3085969			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34077354				2022-12-18	WOS:000853875300026
J	Bai, Y; Liu, J; Lou, YH; Wang, C; Duan, LY				Bai, Yan; Liu, Jun; Lou, Yihang; Wang, Ce; Duan, Ling-yu			Disentangled Feature Learning Network and a Comprehensive Benchmark for Vehicle Re-Identification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Vehicle re-identification; vehicle dataset; disentangled learning		Vehicle Re-Identification (ReID) is of great significance for public security and intelligent transportation. Large and comprehensive datasets are crucial for the development of vehicle ReID in model training and evaluation. However, existing datasets in this field have limitations in many aspects, including the constrained capture conditions, limited variation of vehicle appearances, and small scale of training and test set, etc. Hence, a new, large, and challenging benchmark for vehicle ReID is urgently needed. In this paper, we propose a large vehicle ReID dataset, called VERI-Wild 2.0, containing 825,042 images. It is captured using a city-scale surveillance camera system, consisting of 274 cameras covering a very large area over 200 km(2). Specifically, the samples in our dataset present very rich appearance diversities thanks to the long time span collecting settings, unconstrained capturing viewpoints, various illumination conditions, diversified background environments, and different weather conditions. Furthermore, to facilitate more practical benchmarking, we define a challenging and large test set containing about 400K vehicle images that do not have any camera overlap with the training set. VERI-Wild 2.0 is expected to be able to facilitate the design, adaptation, development, and evaluation of different types of learning models for vehicle ReID. Besides, we also design a new method for vehicle ReID. We observe that orientation is a crucial factor for feature matching in vehicle ReID. To match vehicle pairs captured from similar orientations, the learned features are expected to capture specific detailed differential information for discriminating the visually similar yet different vehicles. In contrast, features are desired to capture the orientation invariant common information when matching samples captured from different orientations. Thus a novel disentangled feature learning network (DFNet) is proposed. It explicitly considers the orientation information for vehicle ReID, and concurrently learns the orientation specific and orientation common features that thus can be adaptively exploited via an adaptive matching scheme when dealing with matching pairs from similar or different orientations. The comprehensive experimental results show the effectiveness of our proposed method.	[Bai, Yan; Wang, Ce; Duan, Ling-yu] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China; [Liu, Jun] Singapore Univ Technol & Design, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore; [Lou, Yihang] Huawei Technol Co Ltd, GoTen AI Lab, Intelligent Vis Dept, Beijing 100085, Peoples R China; [Duan, Ling-yu] Peng Cheng Lab, Shenzhen 518066, Peoples R China	Peking University; Singapore University of Technology & Design; Huawei Technologies; Peng Cheng Laboratory	Duan, LY (corresponding author), Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.	yanbai@pku.edu.cn; jun_liu@sutd.edu.sg; louyihang1@huawei.com; wce@pku.edu.cn; lingyu@pku.edu.cn			National Natural Science Foundation of China [62088102, U1611461]; PKU-NTU Joint Research Institute (JRI) - Ng Teng Fong Charitable Foundation	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); PKU-NTU Joint Research Institute (JRI) - Ng Teng Fong Charitable Foundation	This work was supported in part by the National Natural Science Foundation of China under Grants 62088102 and U1611461, and in part by the PKU-NTU Joint Research Institute (JRI) sponsored by a donation from the Ng Teng Fong Charitable Foundation.	Achille A, 2018, J MACH LEARN RES, V19; Alfasly S, 2019, IEEE ACCESS, V7, P162605, DOI 10.1109/ACCESS.2019.2948965; Ali Farhadi, 2016, Arxiv, DOI arXiv:1612.08242; Bai Y, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P474; Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240; Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bulan O, 2017, IEEE T INTELL TRANSP, V18, P2351, DOI 10.1109/TITS.2016.2639020; Chen H., 2019, P CVPR WORKSH, P184; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Christian Szegedy, 2015, Arxiv, DOI arXiv:1502.03167; Dimitris Metaxas, 2017, Arxiv, DOI arXiv:1612.03242; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Eom Chanho, 2019, ADV NEURAL INFORM PR, P5297; Fei Shen, 2020, Arxiv, DOI arXiv:2005.14684; Ge YX, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo HY, 2019, IEEE T IMAGE PROCESS, V28, P4328, DOI 10.1109/TIP.2019.2910408; Guo HY, 2018, AAAI CONF ARTIF INTE, P6853; Hao Zhang, 2018, Arxiv, DOI arXiv:1704.02510; He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jun-cheng Chen, 2020, Arxiv, DOI arXiv:2004.06271; Kanaci A., 2017, BMVC AMMDS WORKSHOP, V2, P772; Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623; Kingma D. P, 2014, ARXIV13126114; Kumar R, 2019, IEEE IJCNN; Ledig Christian, 2017, ARXIV; Liang Zheng, 2017, Arxiv, DOI arXiv:1701.07717; Liu XB, 2020, IEEE T IMAGE PROCESS, V29, P2638, DOI 10.1109/TIP.2019.2950796; Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53; Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222; Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335; Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112; Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190; Ma L., 2017, ARXIV; Ma Liqian, 2017, P NEUR INF PROC SYST, P405; Mathieu M, 2016, ADV NEUR IN, V29; Peng X, 2017, IEEE I CONF COMP VIS, P1632, DOI 10.1109/ICCV.2017.180; Peri Neehar, 2020, PROC IEEECVF C COMPU, P622; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sochor J, 2016, PROC CVPR IEEE, P3006, DOI 10.1109/CVPR.2016.328; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030; Tang Z, 2019, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2019.00900; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wojke N, 2017, IEEE IMAGE PROC, P3645; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yan K, 2017, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2017.68; Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023; Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94; Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679; Zhou Y, 2018, IEEE WINT CONF APPL, P653, DOI 10.1109/WACV.2018.00077	65	2	2	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6854	6871		10.1109/TPAMI.2021.3099253	http://dx.doi.org/10.1109/TPAMI.2021.3099253			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34310289				2022-12-18	WOS:000853875300072
J	Ben Baruch, E; Keller, Y				Ben Baruch, Elad; Keller, Yosi			Joint Detection and Matching of Feature Points in Multimodal Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Detectors; Computer architecture; Measurement; Image sensors; Image edge detection; Training; Deep learning; multisensor images; image matching; feature points detection	REGISTRATION	In this work, we propose a novel Convolutional Neural Network (CNN) architecture for the joint detection and matching of feature points in images acquired by different sensors using a single forward pass. The resulting feature detector is tightly coupled with the feature descriptor, in contrast to classical approaches (SIFT, etc.), where the detection phase precedes and differs from computing the descriptor. Our approach utilizes two CNN subnetworks, the first being a Siamese CNN and the second, consisting of dual non-weight-sharing CNNs. This allows simultaneous processing and fusion of the joint and disjoint cues in the multimodal image patches. The proposed approach is experimentally shown to outperform contemporary state-of-the-art schemes when applied to multiple datasets of multimodal images. It is also shown to provide repeatable feature points detections across multi-sensor images, outperforming state-of-the-art detectors. To the best of our knowledge, it is the first unified approach for the detection and matching of such images.	[Ben Baruch, Elad] Ben Gurion Univ Negev, Fac Engn, IL-84105 Beer Sheva, Israel; [Keller, Yosi] Bar Ilan Univ, Fac Engn, IL-5290002 Ramat Gan, Israel	Ben Gurion University; Bar Ilan University	Keller, Y (corresponding author), Bar Ilan Univ, Fac Engn, IL-5290002 Ramat Gan, Israel.	eladbb1@gmail.com; yosi.keller@gmail.com						Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Aguilera C, 2012, SENSORS-BASEL, V12, P12661, DOI 10.3390/s120912661; Aguilera CA, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17040873; Aguilera CA, 2016, IEEE COMPUT SOC CONF, P267, DOI 10.1109/CVPRW.2016.40; Aguilera CA, 2015, IEEE IMAGE PROC, P178, DOI 10.1109/ICIP.2015.7350783; Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Broder AZ, 2000, LECT NOTES COMPUT SC, V1848, P1; Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637; Chen J, 2009, PROG NAT SCI-MATER, V19, P643, DOI 10.1016/j.pnsc.2008.06.029; Choy CB, 2016, ADV NEUR IN, V29; Coifman R. R., 2006, PROC SPIE INTELL INT, P34; Quan D, 2019, LECT NOTES COMPUT SC, V11362, P115, DOI 10.1007/978-3-030-20890-5_8; Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828; En S, 2018, IEEE IMAGE PROC, P3024, DOI 10.1109/ICIP.2018.8451804; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Hasan M, 2012, INT GEOSCI REMOTE SE, P2348, DOI 10.1109/IGARSS.2012.6351023; Hossain M. T., 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P197, DOI 10.1109/DICTA.2011.40; Irani M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P959, DOI 10.1109/ICCV.1998.710832; Keller Y, 2006, IEEE T PATTERN ANAL, V28, P794, DOI 10.1109/TPAMI.2006.100; Kim S, 2015, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR.2015.7298822; Kwon YP, 2016, IEEE IMAGE PROC, P310, DOI 10.1109/ICIP.2016.7532369; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; LI H, 1995, IEEE T IMAGE PROCESS, V4, P320, DOI 10.1109/83.366480; Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma R., 2010, P ACM INT C IM VID R, P228, DOI [10.1145/1816041.1816076, DOI 10.1145/1816041.1816076]; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mishchuk A., 2017, P INT C NEUR INF PRO, P4829; Quan D, 2019, IEEE I CONF COMP VIS, P3017, DOI 10.1109/ICCV.2019.00311; Razakarivony S, 2016, J VIS COMMUN IMAGE R, V34, P187, DOI 10.1016/j.jvcir.2015.11.002; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Shechtman E, 2007, PROC CVPR IEEE, P1744; Simeoni O, 2019, PROC CVPR IEEE, P11643, DOI 10.1109/CVPR.2019.01192; Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603; Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Vaquero D, 2010, PROC SPIE, V7798, DOI 10.1117/12.862419; Viola P, 1997, INT J COMPUT VISION, V24, P137, DOI 10.1023/A:1007958904918; Wang S, 2019, IEEE I CONF COMP VIS, P4811, DOI 10.1109/ICCV.2019.00491; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	47	2	2	7	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6585	6593		10.1109/TPAMI.2021.3092289	http://dx.doi.org/10.1109/TPAMI.2021.3092289			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34166186	Green Submitted			2022-12-18	WOS:000853875300053
J	Christiansen, R; Pfister, N; Jakobsen, ME; Gnecco, N; Peters, J				Christiansen, Rune; Pfister, Niklas; Jakobsen, Martin Emil; Gnecco, Nicola; Peters, Jonas			A Causal Framework for Distribution Generalization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Distribution generalization; causality; worst-case risk; distributional robustness; invariance; domain adaptation	INFERENCE	We consider the problem of predicting a response Y from a set of covariates X when test- and training distributions differ. Since such differences may have causal explanations, we consider test distributions that emerge from interventions in a structural causal model, and focus on minimizing the worst-case risk. Causal regression models, which regress the response on its direct causes, remain unchanged under arbitrary interventions on the covariates, but they are not always optimal in the above sense. For example, for linear models and bounded interventions, alternative solutions have been shown to be minimax prediction optimal. We introduce the formal framework of distribution generalization that allows us to analyze the above problem in partially observed nonlinear models for both direct interventions on X and interventions that occur indirectly via exogenous variables A. It takes into account that, in practice, minimax solutions need to be identified from data. Our framework allows us to characterize under which class of interventions the causal function is minimax optimal. We prove sufficient conditions for distribution generalization and present corresponding impossibility results. We propose a practical method, NILE, that achieves distribution generalization in a nonlinear IV setting with linear extrapolation. We prove consistency and present empirical results.	[Christiansen, Rune; Pfister, Niklas; Jakobsen, Martin Emil; Peters, Jonas] Univ Copenhagen, Dept Math Sci, DK-1165 Copenhagen, Denmark; [Gnecco, Nicola] Univ Geneva, Res Ctr Stat, CH-1205 Geneva, Switzerland	University of Copenhagen; University of Geneva	Christiansen, R (corresponding author), Univ Copenhagen, Dept Math Sci, DK-1165 Copenhagen, Denmark.	krunechristiansen@math.ku.dk; np@math.ku.dk; m.jakobsen@math.ku.dk; nicola.gnecco@unige.ch; jonas.peters@math.ku.dk		Peters, Jonas/0000-0002-1487-7511; Gnecco, Nicola/0000-0002-0044-5208	VILLUM FONDEN [18968]; Carlsberg Foundation	VILLUM FONDEN(Villum Fonden); Carlsberg Foundation(Carlsberg Foundation)	The authors thank Thomas Kneib for helpful discussions and two anonymous reviewers for valuable comments. RC and JP were supported by a research grant (18968) from VILLUM FONDEN; MEJ and JP were supported by the Carlsberg Foundation.	Amemiya T, 1974, J ECONOMETRICS, P105, DOI DOI 10.1016/0304-4076(74)90033-5; Amir Rahmati, 2018, Arxiv, DOI arXiv:1707.08945; Arjovsky Martin, 2019, ARXIV, DOI DOI 10.48550/ARXIV.1907.02893; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bagnell J. A., 2005, P 20 NAT C ART INT, P714; Bartlett P. L., 2008, PROC 21 ANN C LEARN, P335; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Bickel S, 2009, J MACH LEARN RES, V10, P2137; Blanchet J, 2019, WINT SIMUL C PROC, P3740, DOI 10.1109/WSC40007.2019.9004785; Bollen K. A, 1989, STRUCTURAL EQUATIONS; Bowden R. J., 1985, INSTRUMENTALVARIABLE; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen XH, 2018, QUANT ECON, V9, P39, DOI 10.3982/QE722; Christian Szegedy, 2015, Arxiv, DOI arXiv:1412.6572; Darolles S, 2011, ECONOMETRICA, V79, P1541, DOI 10.3982/ECTA6539; El Ghaoui L., 2003, TECH REP UCBCSD 03 1; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Fahrmeir L., 2013, REGRESSION, DOI DOI 10.1007/978-3-642-34333-9; Greene H.W., 2003, ECONOMETRIC ANAL, V5th; Haavelmo T, 1943, ECONOMETRICA, V11, P1, DOI 10.2307/1905714; Hall A.R., 2005, GEN METHOD MOMENTS; Hartford J, 2017, PR MACH LEARN RES, V70; Heinze-Deml C, 2021, MACH LEARN, V110, P303, DOI 10.1007/s10994-020-05924-1; Horowitz JL, 2011, ECONOMETRICA, V79, P347, DOI 10.3982/ECTA8662; Hoyer PO, 2008, INT J APPROX REASON, V49, P362, DOI 10.1016/j.ijar.2008.02.006; Hu Z., 2013, OPTIM ONLINE TECH RE; Jakobsen M. E., 2020, ARXIV; Janzing D., 2009, PROC 25 C UNCERTAINT, P249; Kim S.-J., 2006, ADV NEURAL INFORM PR, P659; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Magliacane S, 2018, ADV NEUR IN, V31; Mariano R. S., 2001, COMPANION THEORETICA, P122; Meinshausen N, 2018, 2018 IEEE DATA SCIENCE WORKSHOP (DSW), P6; Meinshausen N, 2015, ANN STAT, V43, P1801, DOI 10.1214/15-AOS1325; Muandet K., 2013, INT C MACH LEARN, P10; Newey WK, 2003, ECONOMETRICA, V71, P1565, DOI 10.1111/1468-0262.00459; Pal D., 2010, INT C ART INT STAT, P129; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Peters J, 2017, ADAPT COMPUT MACH LE; Peters J, 2014, J MACH LEARN RES, V15, P2009; Pfister N., 2019, ARXIV; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Racine Jeffrey S, 2020, CRAN; Rojas-Carulla M, 2018, J MACH LEARN RES, V19; Rostamizadeh A., 2009, ADV NEURAL INFORM PR, P1041; Rothenhausler D, 2021, J R STAT SOC B, V83, P215, DOI 10.1111/rssb.12398; Sani N, 2020, PR MACH LEARN RES, V124, P949; Saunders C., 1998, RIDGE REGRESSION LEA; Sch_olkopf B., 2012, P ICML, P1255; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0; Singh R., 2019, PROC ADV NEURAL INF, V32; Sinha A., 2018, ICLR; Sugiyama M., 2005, PROC WORKSHOP INF BA, P21; Sugiyama M., 2008, NIPS, P1433; Theil H., 1958, EC FORECASTS POLICY; Volpi R, 2018, PROC CVPR IEEE, P5495, DOI 10.1109/CVPR.2018.00576; Wooldridge JM, 2010, ECONOMETRIC ANALYSIS OF CROSS SECTION AND PANEL DATA, 2ND EDITION, P3; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647	66	2	2	5	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6614	6630		10.1109/TPAMI.2021.3094760	http://dx.doi.org/10.1109/TPAMI.2021.3094760			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34232865	Green Submitted			2022-12-18	WOS:000853875300058
J	Corbiere, C; Thome, N; Saporta, A; Vu, TH; Cord, M; Perez, P				Corbiere, Charles; Thome, Nicolas; Saporta, Antoine; Vu, Tuan-Hung; Cord, Matthieu; Perez, Patrick			Confidence Estimation via Auxiliary Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Estimation; Neural networks; Semantics; Predictive models; Uncertainty; Training; Confidence estimation; uncertainty; deep neural networks; classification with reject option; misclassification detection; failure prediction; self-training; pseudo-labeling; unsupervised domain adaptation; semantic image segmentation	NEURAL-NETWORKS; CLASSIFICATION	Reliably quantifying the confidence of deep neural classifiers is a challenging yet fundamental requirement for deploying such models in safety-critical applications. In this paper, we introduce a novel target criterion for model confidence, namely the true class probability (TCP). We show that TCP offers better properties for confidence estimation than standard maximum class probability (MCP). Since the true class is by essence unknown at test time, we propose to learn TCP criterion from data with an auxiliary model, introducing a specific learning scheme adapted to this context. We evaluate our approach on the task of failure prediction and of self-training with pseudo-labels for domain adaptation, which both necessitate effective confidence estimates. Extensive experiments are conducted for validating the relevance of the proposed approach in each task. We study various network architectures and experiment with small and large datasets for image classification and semantic segmentation. In every tested benchmark, our approach outperforms strong baselines.	[Corbiere, Charles; Thome, Nicolas] Conservatoire Natl Arts & Metiers, CEDRIC, F-75003 Paris, France; [Saporta, Antoine; Cord, Matthieu] Sorbonne Univ, LIP6, F-75006 Paris, France; [Vu, Tuan-Hung; Perez, Patrick] Valeoai Univ, F-75008 Paris, France	heSam Universite; Conservatoire National Arts & Metiers (CNAM); Institut Polytechnique de Paris; UDICE-French Research Universities; Sorbonne Universite	Corbiere, C (corresponding author), Conservatoire Natl Arts & Metiers, CEDRIC, F-75003 Paris, France.	charles.corbiere@cnam.fr; nicolas.thome@cnam.fr; antoine.saporta@lip6.fr; tuan-hung.vu@valeo.com; matthieu.cord@lip6.fr; patrick.perez@valeo.com						Amodei D., 2016, CLIN ORTHOP RELAT R; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217; Blatz J, 2004, COL 2004 P 20 INT C, P315; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Chang WL, 2019, PROC CVPR IEEE, P1900, DOI 10.1109/CVPR.2019.00200; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; Corbi`ere Charles, 2019, ADV NEUR IN, P2902; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5; El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605; Gal Y, 2016, PR MACH LEARN RES, V48; Ganin Y, 2016, J MACH LEARN RES, V17; Geifman Y, 2019, PR MACH LEARN RES, V97; Geifman Y, 2017, ADV NEUR IN, V30; Geifman Yonatan, 2019, INT C LEARN REPR; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Graham W. Taylor, 2018, Arxiv, DOI arXiv:1802.04865; Grandvalet Y., 2005, CAP, P529; Guo CA, 2017, PR MACH LEARN RES, V70; Hecker S, 2018, IEEE INT VEH SYM, P1792; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hoffman J, 2018, PR MACH LEARN RES, V80; Janai J, 2020, FOUND TRENDS COMPUT, V12, P1, DOI 10.1561/0600000079; Jiang H, 2018, ADV NEUR IN, V31; Kendall A., 2017, P BMVC; Krizhevsky A., 2009, THESIS TORONTO U TOR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lakshminarayanan B, 2017, ADV NEUR IN, V30; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee D.-H, 2013, WORKSH CHALL REPR LE, V3, P1; Li Q, 2019, INT CONF ACOUST SPEE, P6755, DOI 10.1109/ICASSP.2019.8683488; Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Linda Ondrej, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1827, DOI 10.1109/IJCNN.2009.5178592; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Maddox WJ, 2019, ADV NEUR IN, V32; Mikolov T., 2013, 1 INT LEARN REPR ICL; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mohapatra P, 2018, PROC CVPR IEEE, P3693, DOI 10.1109/CVPR.2018.00389; Nam JG, 2019, RADIOLOGY, V290, P218, DOI 10.1148/radiol.2018180237; Neal R. M., 2012, BAYESIAN LEARNING NE; Netzer Y., 2011, P C NEUR INF PROC SY; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Neumann, 2018, NIPS WORKSH MACH LEA; Paszke A, 2019, ADV NEUR IN, V32; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Saporta A.., 2020, PROC IEEE C COMPUT V; Szegedy C., 2014, ICLR 2014; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262; Yu D, 2011, IEEE T AUDIO SPEECH, V19, P2461, DOI 10.1109/TASL.2011.2141988; Zaragoza H.., 1998, PROC 7 INT C INF PRO; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_; Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608	68	2	2	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6043	6055		10.1109/TPAMI.2021.3085983	http://dx.doi.org/10.1109/TPAMI.2021.3085983			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34086561	Green Submitted			2022-12-18	WOS:000853875300017
J	Fatras, K; Damodaran, BB; Lobry, S; Flamary, R; Tuia, D; Courty, N				Fatras, Kilian; Damodaran, Bharath Bhushan; Lobry, Sylvain; Flamary, Remi; Tuia, Devis; Courty, Nicolas			Wasserstein Adversarial Regularization for Learning With Label Noise	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Noise measurement; Training; Entropy; Neural networks; Task analysis; Smoothing methods; Semantics; Label noise; optimal transport; wasserstein distance; adversarial regularization		Noisy labels often occur in vision datasets, especially when they are obtained from crowdsourcing or Web scraping. We propose a new regularization method, which enables learning robust classifiers in presence of noisy data. To achieve this goal, we propose a new adversarial regularization scheme based on the Wasserstein distance. Using this distance allows taking into account specific relations between classes by leveraging the geometric properties of the labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a selective regularization, which promotes smoothness of the classifier between some classes, while preserving sufficient complexity of the decision boundary between others. We first discuss how and why adversarial regularization can be used in the context of noise and then show the effectiveness of our method on five datasets corrupted with noisy labels: in both benchmarks and real datasets, WAR outperforms the state-of-the-art competitors.	[Fatras, Kilian; Courty, Nicolas] Univ Bretagne Sud, IRISA, INRIA, CNRS,UMR 6074, F-56321 Lorient, France; [Damodaran, Bharath Bhushan] InterDigital R&D, F-35510 Rennes, France; [Lobry, Sylvain] Univ Paris, LIPADE, F-75006 Paris, France; [Flamary, Remi] Ecole Polytech, CMAP, F-91120 Palaiseau, France; [Tuia, Devis] Ecole Polytech Fed Lausanne, CH-1950 Sion, Switzerland	Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; Universite Paris Cite; Institut Polytechnique de Paris; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Fatras, K (corresponding author), Univ Bretagne Sud, IRISA, INRIA, CNRS,UMR 6074, F-56321 Lorient, France.	kilian.fatras@irisa.fr; Bharath.Damodaran@interdigital.com; sylvain.lobry@u-paris.fr; remi.amary@polytechnique.edu; devis.tuia@epfl.ch; ncourty@irisa.fr		Flamary, Remi/0000-0002-4212-6627	French National Research Agency (ANR) [ANR-19-P3IA-0002, OATMIL ANR-17-CE23-0012]; Chair "Challenging Technology for Responsible Energy"; Fondation de l'Ecole polytechnique - TOTAL; 3rd Programme d'Investissements d'Avenir [ANR-18-EUR-0006-02]	French National Research Agency (ANR)(French National Research Agency (ANR)); Chair "Challenging Technology for Responsible Energy"; Fondation de l'Ecole polytechnique - TOTAL; 3rd Programme d'Investissements d'Avenir(French National Research Agency (ANR))	This work was supported in part by projects OATMIL ANR-17-CE23-0012 and 3IA Cote d'Azur Investments ANR-19-P3IA-0002 of French National Research Agency (ANR). This work was supported by 3rd Programme d'Investissements d'Avenir [ANR-18-EUR-0006-02]. This action benefited from the support of the Chair "Challenging Technology for Responsible Energy" led by l'X -Ecole polytechnique and the Fondation de l'Ecole polytechnique, sponsored by TOTAL. Kilian Fatras and Bharath Bhushan Damodaran are contributed equally.	Altschuler J, 2017, ADV NEUR IN, V30; Arpit D, 2017, PR MACH LEARN RES, V70; Biggio B., 2011, PROC ACML, P97; Brodley CE, 1999, J ARTIF INTELL RES, V11, P131, DOI 10.1613/jair.606; Chen PF, 2019, PR MACH LEARN RES, V97; Choi K, 2018, IEEE TETCI, V2, P139, DOI 10.1109/TETCI.2017.2771298; Cristianini N, 2002, ADV NEUR IN, V14, P649; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, J MACH LEARN RES, V15, P533; Damodaran BB, 2020, COMPUT VIS IMAGE UND, V191, DOI 10.1016/j.cviu.2019.102863; Damodaran BB, 2018, LECT NOTES COMPUT SC, V11208, P467, DOI 10.1007/978-3-030-01225-0_28; Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5; Elvis Dohmatob, 2019, Arxiv, DOI arXiv:1906.11567; Fergus Rob, 2014, PROC INT C LEARN REP; Frogner Charlie, 2015, ADV NEURAL INF PROCE, V2, P2053; Genevay A, 2018, PR MACH LEARN RES, V84; Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919; Goodfellow I. J., 2014, ARXIV14126572; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; Han Xiao, 2017, Arxiv, DOI arXiv:1708.07747; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendrycks D, 2018, ADV NEUR IN, V31; Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374; Huang G., 2016, PROC NEURAL INF PROC, P4869; Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571; Li RL, 2019, J MACH LEARN RES, V20; Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211; Li ZG, 2007, IEEE I CONF COMP VIS, P1823; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Lu J, 2018, PR MACH LEARN RES, V80; Luise G., 2018, PROC INT C NEURAL IN, P5864; Ma XJ, 2018, PR MACH LEARN RES, V80; Menon AK, 2015, PR MACH LEARN RES, V37, P125; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Misra I, 2016, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2016.320; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Mroueh Y., 2018, P INT C LEARN REPR; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Paszke A., 2017, PROC INT C NEURAL IN; Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Reedman SE, 2016, PHYS OCCUP THER PEDI, V36, P292, DOI 10.3109/01942638.2015.1040576; Ren MY, 2018, PR MACH LEARN RES, V80; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133; Shafie AS, 2018, INT CONF INFORM RETR, P107; Shaham U, 2018, NEUROCOMPUTING, V307, P195, DOI 10.1016/j.neucom.2018.04.027; Song H, 2019, PR MACH LEARN RES, V97; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; van den Oord A, 2016, PR MACH LEARN RES, V48; van Rooyen B, 2015, ADV NEUR IN, V28; Vandat A, 2017, ADV NEUR IN, V30; Volpi M, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301377; Wang F, 2018, LECT NOTES COMPUT SC, V11213, P780, DOI 10.1007/978-3-030-01240-3_47; Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906; Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Xu YY, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010144; Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718; Yu XR, 2019, PR MACH LEARN RES, V97; Yuan JS, 2008, PROC CVPR IEEE, P55, DOI 10.1109/CVPR.2008.4587348; Zhang Chiyuan, 2016, ARXIV161103530; Zhang ZL, 2018, ADV NEUR IN, V31	67	2	2	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7296	7306		10.1109/TPAMI.2021.3094662	http://dx.doi.org/10.1109/TPAMI.2021.3094662			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34232864				2022-12-18	WOS:000853875300099
J	Guo, MT; Hou, JH; Jin, J; Chen, J; Chau, LP				Guo, Mantang; Hou, Junhui; Jin, Jing; Chen, Jie; Chau, Lap-Pui			Deep Spatial-Angular Regularization for Light Field Imaging, Denoising, and Super-Resolution	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Apertures; Noise reduction; Cameras; Sensors; Reconstruction algorithms; Imaging; Light field; coded aperture; deep learning; optimization; observation model; denoising; spatial super-resolution; depth	PERFORMANCE; ENGINE; CNN	Coded aperture is a promising approach for capturing the 4-D light field (LF), in which the 4-D data are compressively modulated into 2-D coded measurements that are further decoded by reconstruction algorithms. The bottleneck lies in the reconstruction algorithms, resulting in rather limited reconstruction quality. To tackle this challenge, we propose a novel learning-based framework for the reconstruction of high-quality LFs from acquisitions via learned coded apertures. The proposed method incorporates the measurement observation into the deep learning framework elegantly to avoid relying entirely on data-driven priors for LF reconstruction. Specifically, we first formulate the compressive LF reconstruction as an inverse problem with an implicit regularization term. Then, we construct the regularization term with a deep efficient spatial-angular separable convolutional sub-network in the form of local and global residual learning to comprehensively explore the signal distribution free from the limited representation ability and inefficiency of deterministic mathematical modeling. Furthermore, we extend this pipeline to LF denoising and spatial super-resolution, which could be considered as variants of coded aperture imaging equipped with different degradation matrices. Extensive experimental results demonstrate that the proposed methods outperform state-of-the-art approaches to a significant extent both quantitatively and qualitatively, i.e., the reconstructed LFs not only achieve much higher PSNR/SSIM but also preserve the LF parallax structure better on both real and synthetic LF benchmarks. The code will be publicly available at https://github.com/MantangGuo/DRLF.	[Guo, Mantang; Hou, Junhui; Jin, Jing] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China; [Chen, Jie] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China; [Chau, Lap-Pui] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore	City University of Hong Kong; Hong Kong Baptist University; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Hou, JH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.	mantanguo2-c@my.cityu.edu.hk; jh.hou@cityu.edu.hk; jingjin25-c@my.cityu.edu.hk; chenjie@comp.hkbu.edu.hk; elpchau@ntu.edu.sg		Hou, Junhui/0000-0003-3431-2021; JIN, Jing/0000-0002-6584-8438; GUO, Mantang/0000-0002-3882-4238	Hong Kong Research Grants Council [9048123 (CityU 21211518), 9042820 (CityU 11219019)]; Basic Research General Program of Shenzhen Municipality [JCYJ20190808183003968]	Hong Kong Research Grants Council(Hong Kong Research Grants Council); Basic Research General Program of Shenzhen Municipality	The authors would like to thank the associate editor and anonymous reviewers for their valuable comments to improve the manuscript's quality, the authors of [21] for sharing their source codes, and the authors of [16] for the suggestion on counting the running time of their codes. This work was supported in part by the Hong Kong Research Grants Coun-cil under grants 9048123 (CityU 21211518) and 9042820 (CityU 11219019), and in part by the Basic Research General Program of Shenzhen Municipalityunder grants JCYJ20190808183003968.	Alain M, 2017, IEEE INT WORKSH MULT; Ashok A, 2010, PROC SPIE, V7690, DOI 10.1117/12.852738; Babacan SD, 2012, IEEE T IMAGE PROCESS, V21, P4746, DOI 10.1109/TIP.2012.2210237; Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168; Chen J, 2018, IEEE SIGNAL PROC LET, V25, P1403, DOI 10.1109/LSP.2018.2861212; Chen J, 2018, IEEE T IMAGE PROCESS, V27, P4889, DOI 10.1109/TIP.2018.2839524; Chen J, 2017, IEEE T CIRC SYST VID, V27, P855, DOI 10.1109/TCSVT.2015.2513485; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dansereau DG, 2013, PROC SPIE, V8657, DOI 10.1117/12.2002239; Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610; Farrugia RA, 2017, IEEE J-STSP, V11, P1058, DOI 10.1109/JSTSP.2017.2747127; Gupta M, 2017, IEEE COMPUT SOC CONF, P1277, DOI 10.1109/CVPRW.2017.168; Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2; Inagaki Y, 2018, LECT NOTES COMPUT SC, V11211, P431, DOI 10.1007/978-3-030-01234-2_26; Jin J, 2022, IEEE T PATTERN ANAL, V44, P1819, DOI 10.1109/TPAMI.2020.3026039; Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251; Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359; [李针英 Li Zhenying], 2013, [高分子通报, Polymer Bulletin], P1; Liang CK, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360654; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Lytro, 2016, US; Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324; Mantang Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P278, DOI 10.1007/978-3-030-58536-5_17; Marwah K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461914; Miandji E, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269980; Mitra K., 2012, P IEEECVF C COMPUTER, P22; Nabati O, 2018, IEEE INT CONF COMPUT; Nagahara H, 2010, LECT NOTES COMPUT SC, V6316, P337, DOI 10.1007/978-3-642-15567-3_25; Ng R., 2006, DIGITAL LIGHT FIELD, P1; RayTrix, 3D LIGHT FIELD CAMER; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Rossi M, 2018, IEEE T IMAGE PROCESS, V27, P4207, DOI 10.1109/TIP.2018.2828983; Sepas-Moghaddam A., 2016, PROC 3DTV C TRUE VIS, P1; Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323; Solomon O, 2020, IEEE T MED IMAGING, V39, P1051, DOI 10.1109/TMI.2019.2941271; Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246; Sunder, 2016, STANFORD LYTRO LIGHT; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Vadathya AK, 2020, IEEE T COMPUT IMAG, V6, P304, DOI 10.1109/TCI.2019.2948780; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Wang TC, 2016, IEEE T PATTERN ANAL, V38, P2170, DOI 10.1109/TPAMI.2016.2515615; Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147; Wanner S, 2012, LECT NOTES COMPUT SC, V7576, P608, DOI 10.1007/978-3-642-33715-4_44; Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259; Wu GC, 2017, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR.2017.178; Yagi Y, 2017, IEEE IMAGE PROC, P3031; Yang Y, 2016, ADV NEUR IN, V29; Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236; Yeung HWF, 2018, LECT NOTES COMPUT SC, V11210, P138, DOI 10.1007/978-3-030-01231-1_9; Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333; Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17; Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang S, 2019, PROC CVPR IEEE, P11038, DOI 10.1109/CVPR.2019.01130; Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521	64	2	2	12	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6094	6110		10.1109/TPAMI.2021.3087485	http://dx.doi.org/10.1109/TPAMI.2021.3087485			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34101585				2022-12-18	WOS:000853875300020
J	Parashar, S; Pizarro, D; Bartoli, A				Parashar, Shaifali; Pizarro, Daniel; Bartoli, Adrien			Robust Isometric Non-Rigid Structure-From-Motion	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Three-dimensional displays; Shape; Measurement; Mathematical model; Tensors; Strain; 3D computer vision; NRSfM; robustness; isometry	SHAPE	Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object from keypoint correspondences established between monocular 2D images. Current NRSfM methods lack statistical robustness, which is the ability to cope with correspondence errors. This prevents one to use automatically established correspondences, which are prone to errors, thereby strongly limiting the scope of NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by exploiting isometry. Step (i) computes the optical flow from correspondences, step (ii) reconstructs each 3D point's normal vector using multiple reference images and integrates them to form surfaces with the best reference and step (iii) rejects the 3D points that break isometry in their local neighborhood. Importantly, each step is designed to discard or flag erroneous correspondences. Our contributions include the robustification of optical flow by warp estimation, new fast analytic solutions to local normal reconstruction and their robustification, and a new scale-independent measure of 3D local isometric coherence. Experimental results show that our robust NRSfM method consistently outperforms existing methods on both synthetic and real datasets.	[Parashar, Shaifali] Ecole Polytech Fed Lausanne, CVLAB, CH-1015 Lausanne, Switzerland; [Pizarro, Daniel] Univ Alcala, GEINTRA, Alcald De Henares 28801, Spain; [Bartoli, Adrien] Univ Clermont Auvergne, EnCoV, CNRS, Inst Pascal, F-63000 Clermont Ferrand, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Universidad de Alcala; Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)	Parashar, S (corresponding author), Ecole Polytech Fed Lausanne, CVLAB, CH-1015 Lausanne, Switzerland.	shaifali.parashar@gmail.com; Dani.Pizarro@gmail.com; bartoli@gmail.com			Spanish Ministry of Economy, Industry and Competitiveness under Project ARTEMISA [TIN2016-80939-R]	Spanish Ministry of Economy, Industry and Competitiveness under Project ARTEMISA	This work was supported by the Spanish Ministry of Economy, Industry and Competitiveness under Project ARTEMISA (TIN2016-80939-R).	Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Akhter I., 2009, ADV NEURAL INFORM PR, P1; AKRITAS AG, 1993, FIBONACCI QUART, V31, P325; [Anonymous], 2014, AGISOFT PHOTOSCAN PR; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; BASU S., 2006, ALGORITHMS REAL ALGE; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Brunet F, 2011, INT J COMPUT VISION, V93, P33, DOI 10.1007/s11263-010-0407-x; Chhatkuli A., 2014, BRIT MACH VIS C, P1; Chhatkuli A, 2018, PROC EUR C COMPUT VI, P776; Chhatkuli A, 2018, IEEE T PATTERN ANAL, V40, P2428, DOI 10.1109/TPAMI.2017.2762669; Collins T, 2015, 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P116, DOI 10.1109/ISMAR.2015.35; Cox David A, 2006, USING ALGEBRAIC GEOM; Dai YC, 2014, INT J COMPUT VISION, V107, P101, DOI 10.1007/s11263-013-0684-2; Ngo DT, 2015, IEEE I CONF COMP VIS, P2273, DOI 10.1109/ICCV.2015.262; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Del Bue A., 2004, PROC C COMPUT VIS PA, P8; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; Fayad J., 2009, PROC BRIT MACH VIS C, P1; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Garg R, 2013, INT J COMPUT VISION, V104, P286, DOI 10.1007/s11263-012-0607-7; Golyanik V, 2017, IEEE WINT CONF APPL, P282, DOI 10.1109/WACV.2017.38; Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319; Hartley R, 2008, LECT NOTES COMPUT SC, V5302, P276, DOI 10.1007/978-3-540-88682-2_22; Henrion D, 2003, ACM T MATH SOFTWARE, V29, P165, DOI 10.1145/779359.779363; Ji P, 2017, IEEE I CONF COMP VIS, P929, DOI 10.1109/ICCV.2017.106; Kukelova Z, 2012, IEEE T PATTERN ANAL, V34, P1381, DOI 10.1109/TPAMI.2011.230; Lee M, 2016, PROC CVPR IEEE, P4670, DOI 10.1109/CVPR.2016.505; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730; Parashar S, 2020, IEEE T PATTERN ANAL, V42, P3011, DOI 10.1109/TPAMI.2019.2920821; Parashar S, 2018, LECT NOTES COMPUT SC, V11205, P259, DOI 10.1007/978-3-030-01246-5_16; Parashar S, 2018, IEEE T PATTERN ANAL, V40, P2442, DOI 10.1109/TPAMI.2017.2760301; Parashar S, 2016, PROC CVPR IEEE, P4679, DOI 10.1109/CVPR.2016.506; Pilet J, 2008, INT J COMPUT VISION, V76, P109, DOI 10.1007/s11263-006-0017-9; Pizarro D, 2016, INT J COMPUT VISION, V119, P93, DOI 10.1007/s11263-016-0882-9; Tran QH, 2012, LECT NOTES COMPUT SC, V7575, P274, DOI 10.1007/978-3-642-33765-9_20; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Salzmann M, 2011, IEEE T PATTERN ANAL, V33, P931, DOI 10.1109/TPAMI.2010.158; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; Taylor J, 2010, PROC CVPR IEEE, P2761, DOI 10.1109/CVPR.2010.5540002; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Varol A, 2012, PROC CVPR IEEE, P2248, DOI 10.1109/CVPR.2012.6247934; Varol A, 2009, IEEE I CONF COMP VIS, P1811, DOI 10.1109/ICCV.2009.5459403; Vicente S, 2012, LECT NOTES COMPUT SC, V7574, P426, DOI 10.1007/978-3-642-33712-3_31	45	2	2	4	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6409	6423		10.1109/TPAMI.2021.3089923	http://dx.doi.org/10.1109/TPAMI.2021.3089923			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34133273	Green Submitted			2022-12-18	WOS:000853875300041
J	Wang, JH; Jiang, JM				Wang, Jinghua; Jiang, Jianmin			Learning Across Tasks for Zero-Shot Domain Adaptation From a Single Source Domain	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Domain adaptation; zero-shot learning; adversarial learning; generative adversarial networks; domain generalization		Domain adaptation techniques learn transferable knowledge from a source domain to a target domain and train models that generalize well in the target domain. Unfortunately, a majority of the existing techniques are only applicable to scenarios that the target-domain data in the task of interest is available for training, yet this is not often true in practice. In general, human beings are experts in generalization across domains. For example, a baby can easily identify the bear from a clipart image after learning this category of animal from the photo images. To reduce the gap between the generalization ability of human and that of machines, we propose a new solution to the challenging zero-shot domain adaptation (ZSDA) problem, where only a single source domain is available and the target domain for the task of interest is not accessible. Inspired by the observation that the knowledge about domain correlation can improve our generalization ability, we explore the correlation between source domain and target domain in an irrelevant knowledge task (K-task), where dual-domain samples are available. We denote the task of interest as the question task (Q-task) and synthesize its non-accessible target-domain as such that these two tasks have the shared domain correlation. In order to realize our idea, we introduce a new network structure, i.e., conditional coupled generative adversarial networks (CoCoGAN), by extending the coupled generative adversarial networks (CoGAN) into a conditioning model. With a pair of coupling GANs, our CoCoGAN is able to capture the joint distribution of data samples across two domains and two tasks. For CoCoGAN training in a ZSDA task, we introduce three supervisory signals, i.e., semantic relationship consistency across domains, global representation alignment across tasks, and alignment consistency across domains. Experimental results demonstrate that our method can learn a suitable model for the non-accessible target domain and outperforms the existing state of the arts in both image classification and semantic segmentation.	[Wang, Jinghua; Jiang, Jianmin] Shenzhen Univ, Res Inst Future Media Comp, Coll Comp Sci & Software Engn, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518060, Guangdong, Peoples R China	Shenzhen University	Jiang, JM (corresponding author), Shenzhen Univ, Res Inst Future Media Comp, Coll Comp Sci & Software Engn, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518060, Guangdong, Peoples R China.	wang.jh@szu.edu.cn; jianmin.jiang@szu.edu.cn		Wang, Jinghua/0000-0002-2629-1198	Natural Science Foundation China (NSFC) [62032015, 61620106008, 61802266]	Natural Science Foundation China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported by Natural Science Foundation China (NSFC) under Grants 62032015, 61620106008, and 61802266.	Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Atsutoshi Kumagai, 2018, Arxiv, DOI arXiv:1807.02927; Aytar Y, 2018, IEEE T PATTERN ANAL, V40, P2303, DOI 10.1109/TPAMI.2017.2753232; Balaji Y, 2018, ADV NEUR IN, V31; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233; Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072; Chen M., 2012, ARXIV12064683; Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217; Csurka G, 2017, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-58347-1; Ding ZM, 2018, IEEE T IMAGE PROCESS, V27, P304, DOI 10.1109/TIP.2017.2758199; Dou Q, 2019, ADV NEUR IN, V32; French G., 2018, PROC INT C LEARN REP; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Gretton A., 2009, COVARIATE SHIFT LOCA; Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301; Han Xiao, 2017, Arxiv, DOI arXiv:1708.07747; Handa A, 2016, PROC CVPR IEEE, P4077, DOI 10.1109/CVPR.2016.442; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Hu SB, 2020, PR MACH LEARN RES, V115, P292; Ilse Maximilian, 2020, MED IMAGING DEEP LEA, P322; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kihyuk Sohn, 2019, Arxiv, DOI arXiv:1803.00068; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Laine S., 2016, INT C LEARN REPR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058; Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38; Liang J, 2019, IEEE T PATTERN ANAL, V41, P1027, DOI 10.1109/TPAMI.2018.2832198; Liu KC, 2019, IEEE I CONF COMP VIS, P7344, DOI 10.1109/ICCV.2019.00744; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136; Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685; Long MS, 2015, PR MACH LEARN RES, V37, P97; Mancini M, 2021, IEEE T PATTERN ANAL, V43, P485, DOI 10.1109/TPAMI.2019.2933829; McCormac J, 2017, IEEE I CONF COMP VIS, P2697, DOI 10.1109/ICCV.2017.292; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Muandet Krikamol, 2013, ABS13012115 ARXIV, V28, pI10; Niu L, 2015, IEEE I CONF COMP VIS, P4193, DOI 10.1109/ICCV.2015.477; Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234; Peng KC, 2018, LECT NOTES COMPUT SC, V11215, P793, DOI 10.1007/978-3-030-01252-6_47; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271; Radford A., 2015, ARXIV PREPR ARXIV151; Rahman MM, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107124; Ramirez PZ, 2019, IEEE I CONF COMP VIS, P8109, DOI 10.1109/ICCV.2019.00820; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Saito K, 2017, PR MACH LEARN RES, V70; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Sener Ozan, 2016, ADV NEURAL INFORM PR, P2; Shankar Shiv, 2018, P INT C LEARN REPR I; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sohn K, 2017, IEEE I CONF COMP VIS, P5917, DOI 10.1109/ICCV.2017.630; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Tarvainen Antti, 2017, CORR, Vabs/1703; Tatsuya Harada, 2019, Arxiv, DOI arXiv:1911.07661; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Wang H., 2019, PROC INT C LEARN REP; Wang JH, 2019, IEEE I CONF COMP VIS, P3374, DOI 10.1109/ICCV.2019.00347; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang YX, 2018, PROC CVPR IEEE, P5467, DOI 10.1109/CVPR.2018.00573; Xiang Gu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9098, DOI 10.1109/CVPR42600.2020.00912; Xu Z, 2014, LECT NOTES COMPUT SC, V8691, P628, DOI 10.1007/978-3-319-10578-9_41; Yang Y., 2015, PROC 1 INT WORKSHOP, V12, P1; Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13864, DOI 10.1109/CVPR42600.2020.01388; Zhou K., 2021, PROC INT C LEARN REP	85	2	2	8	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6264	6279		10.1109/TPAMI.2021.3088859	http://dx.doi.org/10.1109/TPAMI.2021.3088859			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34125667				2022-12-18	WOS:000853875300032
J	Wei, JW; Yang, Y; Xu, X; Zhu, XF; Shen, HT				Wei, Jiwei; Yang, Yang; Xu, Xing; Zhu, Xiaofeng; Shen, Heng Tao			Universal Weighting Metric Learning for Cross-Modal Retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Task analysis; Visualization; Learning systems; Image retrieval; Semantics; Loss measurement; Universal weighting metric learning; cross-modal retrieval; self-similarity polynomial loss; relative-similarity polynomial loss	IMAGE; TEXT	Cross-modal retrieval has recently attracted growing attention, which aims to match instances captured from different modalities. The performance of cross-modal retrieval methods heavily relies on the capability of metric learning to mine and weight the informative pairs. While various metric learning methods have been developed for unimodal retrieval tasks, the cross-modal retrieval tasks, however, have not been explored to its fullest extent. In this paper, we develop a universal weighting metric learning framework for cross-modal retrieval, which can effectively sample informative pairs and assign proper weight values to them based on their similarity scores so that different pairs favor different penalty strength. Based on this framework, we introduce two types of polynomial loss for cross-modal retrieval, self-similarity polynomial loss and relative-similarity polynomial loss. The former provides a polynomial function to associate the weight values with self-similarity scores, and the latter defines a polynomial function to associate the weight values with relative-similarity scores. Both self and relative-similarity polynomial loss can be freely applied to off-the-shelf methods and further improve their retrieval performance. Extensive experiments on two image-text retrieval datasets, three video-text retrieval datasets and one fine-grained image retrieval dataset demonstrate that our proposed method can achieve a noticeable boost in retrieval performance.	[Wei, Jiwei; Yang, Yang; Xu, Xing; Zhu, Xiaofeng; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China; [Wei, Jiwei; Yang, Yang; Xu, Xing; Zhu, Xiaofeng; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China	University of Electronic Science & Technology of China; University of Electronic Science & Technology of China	Yang, Y (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.	mathematic6@hotmail.com; dlyyang@hotmail.com; xing.xu@uestc.edu.cn; seanzhuxf@gmail.com; shenhengtao@hotmail.com			National Natural Science Foundation of China [71672022]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grant 71672022.	Chen David, 2011, P 49 ANN M ASS COMP, P190; Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693; Dey S, 2019, PROC CVPR IEEE, P2174, DOI 10.1109/CVPR.2019.00228; Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957; Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Faghri F., 2018, PROC BRIT MACH VIS C; Frome A., 2013, ADV NEURAL INFORM PR, P2121; Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139; Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17; Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144; Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645; Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330; Kim W, 2018, LECT NOTES COMPUT SC, V11205, P760, DOI 10.1007/978-3-030-01246-5_45; Kingma D.P., 2015, INT C LEARN REPR ICL; Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13; Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475; Li XR, 2021, IEEE T MULTIMEDIA, V23, P4351, DOI 10.1109/TMM.2020.3042067; Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180; Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869; Liu Chunxiao, 2020, P IEEE CVF C COMP VI, P10918, DOI DOI 10.1109/CVPR42600.2020.01093; Liu Y., 2019, PROC BRIT MACH VIS C; Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442; Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TNNLS.2020.2995708, 10.1109/TKDE.2020.2970050]; Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065; Song HO, 2017, PROC CVPR IEEE, P2206, DOI 10.1109/CVPR.2017.237; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077; Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208; Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643; Wah C., 2011, CALTECHUCSD BIRDS 20; Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326; Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921; Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875; Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642; Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516; Wei Jiwei, 2020, P IEEECVF C COMPUTER, P13005; Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677; Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571; Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597; Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI [10.1109/TCYB.2019.2928180, 10.1080/00207543.2019.1579935]; Xu X, 2022, IEEE T PATTERN ANAL, V44, P3030, DOI [10.1109/TPAMI.2020.3045530, 10.1109/TCYB.2020.3009004]; Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x; Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345; Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422; Young Peter, 2014, T ASSOC COMPUT LING, V2, P67; Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23; Zhang L, 2020, IEEE T CIRC SYST VID, V30, P3788, DOI 10.1109/TCSVT.2019.2943902; Zheng WZ, 2020, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR42600.2020.00303; Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016	62	2	2	9	21	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6534	6545		10.1109/TPAMI.2021.3088863	http://dx.doi.org/10.1109/TPAMI.2021.3088863			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34125668				2022-12-18	WOS:000853875300049
J	Yin, W; Liu, YF; Shen, CH				Yin, Wei; Liu, Yifan; Shen, Chunhua			Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Measurement; Surface reconstruction; Geometry; Estimation; Image reconstruction; Shape; Monocular depth estimation; 3D from single images; surface normal; virtual normal		Monocular depth prediction plays a crucial role in understanding 3D scene geometry. Although recent methods have achieved impressive progress in the evaluation metrics such as the pixel-wise relative error, most methods neglect the geometric constraints in the 3D space. In this work, we show the importance of the high-order 3D geometric constraints for depth prediction. By designing a loss term that enforces a simple geometric constraint, namely, virtual normal directions determined by randomly sampled three points in the reconstructed 3D space, we significantly improve the accuracy and robustness of monocular depth estimation. Importantly, the virtual normal loss can not only improve the performance of learning metric depth, but also disentangle the scale information and enrich the model with better shape information. Therefore, when not having access to absolute metric depth training data, we can use virtual normal to learn a robust affine-invariant depth generated on diverse scenes. Our experiments demonstrate state-of-the-art results of learning metric depth on NYU Depth-V2 and KITTI. From the high-quality predicted depth, we are now able to recover good 3D structures of the scene such as the point cloud and surface normal directly, eliminating the necessity of relying on additional models as was previously done. To demonstrate the excellent generalization capability of learning affine-invariant depth on diverse data with the virtual normal loss, we construct a large-scale and diverse dataset for training affine-invariant depth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five datasets with the zero-shot test setting. Code is available at: https://git.io/Depth.	[Yin, Wei; Liu, Yifan] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia; [Shen, Chunhua] Monash Univ, Clayton, Vic 3800, Australia	University of Adelaide; Monash University	Shen, CH (corresponding author), Monash Univ, Clayton, Vic 3800, Australia.	wei.yin@adelaide.edu.au; yifanliu0926@gmail.com; chunhua@me.com		Liu, Yifan/0000-0002-2746-8186				Andrea F. Daniele, 2019, Arxiv, DOI arXiv:1908.00463; Anelia Angelova, 2020, Arxiv, DOI arXiv:2010.16404; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bian J., 2019, P NEURAL INFORM PROC, P35; Cao YZH, 2018, IEEE T CIRC SYST VID, V28, P3174, DOI 10.1109/TCSVT.2017.2740321; Chakrabarti Ayan, 2016, NIPS; Chang Shu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P572, DOI 10.1007/978-3-030-58529-7_34; Changming Sun, 2020, Arxiv, DOI arXiv:2002.00569; Chen WF, 2020, PROC CVPR IEEE, P676, DOI 10.1109/CVPR42600.2020.00076; Chen Weifeng, 2016, NEURIPS, P2, DOI DOI 10.5555/3157096.3157178; Chen XT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P694; Cheng XJ, 2020, IEEE T PATTERN ANAL, V42, P2361, DOI [10.1109/TPAMI.2019.2947374, 10.5194/isprs-archives-XLII-3-W7-1-2019]; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Diaz R, 2019, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2019.00487; Eigen D, 2014, ADV NEUR IN, V27; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Fouhey DF, 2013, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2013.421; Fouhey DF, 2014, LECT NOTES COMPUT SC, V8694, P687, DOI 10.1007/978-3-319-10599-4_44; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hacohen G, 2019, PR MACH LEARN RES, V97; Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326; Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116; Ibraheem Alhashim, 2019, Arxiv, DOI arXiv:1812.11941; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Cho JH, 2019, Arxiv, DOI arXiv:1904.10230; Jiao JB, 2018, LECT NOTES COMPUT SC, V11219, P55, DOI 10.1007/978-3-030-01267-0_4; Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835; Klasing K, 2009, IEEE INT CONF ROBOT, P2011; Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Ladicky L, 2014, LECT NOTES COMPUT SC, V8693, P468, DOI 10.1007/978-3-319-10602-1_31; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35; Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715; Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365; Li RB, 2019, LECT NOTES COMPUT SC, V11364, P663, DOI 10.1007/978-3-030-20870-7_41; Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218; Liu BY, 2010, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2010.5539823; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97; Mendes RD, 2021, ROBOT AUTON SYST, V136, DOI 10.1016/j.robot.2020.103701; Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037; Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594; Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Saxena Ashutosh, 2006, ADV NEURAL INFORM PR, P1, DOI [DOI 10.1109/TPAMI.2015.2505283A, 10.1109/TPAMI.2015.2505283a]; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; van Dijk T, 2019, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2019.00227; Wang CY, 2019, INT CONF 3D VISION, P348, DOI 10.1109/3DV.2019.00046; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652; Weifeng Chen, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5597, DOI 10.1109/CVPR.2019.00575; Weinshall D, 2018, PR MACH LEARN RES, V80; Xian K, 2020, PROC CVPR IEEE, P608, DOI 10.1109/CVPR42600.2020.00069; Xian K, 2018, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2018.00040; Yin W, 2021, PROC CVPR IEEE, P204, DOI 10.1109/CVPR46437.2021.00027; Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	69	2	2	5	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7282	7295		10.1109/TPAMI.2021.3097396	http://dx.doi.org/10.1109/TPAMI.2021.3097396			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34270413	Green Submitted			2022-12-18	WOS:000853875300098
J	Fang, PF; Zhou, JM; Roy, SK; Ji, P; Petersson, L; Harandi, M				Fang, Pengfei; Zhou, Jieming; Roy, Soumava Kumar; Ji, Pan; Petersson, Lars; Harandi, Mehrtash			Attention in Attention Networks for Person Retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Task analysis; Visualization; Estimation; Benchmark testing; Training; Feature extraction; Attention in attention mechanism; person retrieval; pedestrian representation; convolutional neural network; second-order polynomial kernel; Gaussian kernel		This paper generalizes the Attention in Attention (AiA) mechanism, in P. Fang et al., 2019 by employing explicit mapping in reproducing kernel Hilbert spaces to generate attention values of the input feature map. The AiA mechanism models the capacity of building inter-dependencies among the local and global features by the interaction of inner and outer attention modules. Besides a vanilla AiA module, termed linear attention with AiA, two non-linear counterparts, namely, second-order polynomial attention and Gaussian attention, are also proposed to utilize the non-linear properties of the input features explicitly, via the second-order polynomial kernel and Gaussian kernel approximation. The deep convolutional neural network, equipped with the proposed AiA blocks, is referred to as Attention in Attention Network (AiA-Net). The AiA-Net learns to extract a discriminative pedestrian representation, which combines complementary person appearance and corresponding part features. Extensive ablation studies verify the effectiveness of the AiA mechanism and the use of non-linear features hidden in the feature map for attention design. Furthermore, our approach outperforms current state-of-the-art by a considerable margin across a number of benchmarks. In addition, state-of-the-art performance is also achieved in the video person retrieval task with the assistance of the proposed AiA blocks.	[Fang, Pengfei; Zhou, Jieming; Roy, Soumava Kumar; Petersson, Lars] Australian Natl Univ, Res Sch Elect Energy & Mat Engn, Canberra, ACT 2601, Australia; [Fang, Pengfei; Zhou, Jieming; Roy, Soumava Kumar; Petersson, Lars] Black Mt Labs, Data61 CSIRO, Canberra, ACT 2601, Australia; [Ji, Pan] OPPO US Res Ctr, Palo Alto, CA 94303 USA; [Harandi, Mehrtash] Monash Univ, Dept Elect & Comp Syst Engn, Clayton, Vic 3800, Australia; [Harandi, Mehrtash] Data61 CSIRO, Melbourne, Vic 3800, Australia	Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO); Monash University; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Fang, PF (corresponding author), Australian Natl Univ, Res Sch Elect Energy & Mat Engn, Canberra, ACT 2601, Australia.	Pengfei.Fang@anu.edu.au; Jieming.Zhou@anu.edu.au; Soumava.KumarRoy@anu.edu.au; peterji1990@gmail.com; lars.petersson@data61.csiro.au; mehrtash.harandi@monash.edu	Fang, Pengfei/GQB-2393-2022; Harandi, Mehrtash/D-6586-2018	Harandi, Mehrtash/0000-0002-6937-6300				Bai S, 2017, PROC CVPR IEEE, P3356, DOI 10.1109/CVPR.2017.358; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805; Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325; Evgeniya Ustinova, 2017, Arxiv, DOI arXiv:1512.05300; Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Fu Y, 2019, AAAI CONF ARTIF INTE, P8287; Fu Y, 2019, AAAI CONF ARTIF INTE, P8295; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Gheissari N., 2006, P IEEE C COMP VIS PA, V2, P1528, DOI DOI 10.1109/CVPR.2006.223; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Guoliang Kang, 2017, Arxiv, DOI arXiv:1708.04896; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535; Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Jaderberg M, 2015, ADV NEUR IN, V28; Jiyang Gao, 2018, Arxiv, DOI arXiv:1805.02104; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Kim J.-H., 2017, PROC INT C LEARN REP; Kingma D.P, P 3 INT C LEARNING R; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406; Li JN, 2022, IEEE T PATTERN ANAL, V44, P622, DOI 10.1109/TPAMI.2019.2929036; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762; Liu J, 2017, IEEE IMAGE PROC, P2309; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Maji S, 2009, IEEE I CONF COMP VIS, P40, DOI 10.1109/ICCV.2009.5459203; McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148; Ni X., 2020, PROC INT C PATTERN R; Paszke A., 2017, 31 C NEURAL INFORM P, P1, DOI DOI 10.1017/CB09781107707221.009; Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511; Pirsiavash H., 2009, P ADV NEUR INF PROC, P1482; Qian XL, 2020, IEEE T PATTERN ANAL, V42, P371, DOI 10.1109/TPAMI.2019.2928294; Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Roy SK, 2019, IEEE I CONF COMP VIS, P3046, DOI 10.1109/ICCV.2019.00314; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Jayasumana S, 2021, Arxiv, DOI arXiv:2012.09607; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen YT, 2018, PROC CVPR IEEE, P6886, DOI 10.1109/CVPR.2018.00720; Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30; Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun H, 2019, IEEE I CONF COMP VIS, P6736, DOI 10.1109/ICCV.2019.00684; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730; Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607; Vapnik V., 2000, NATURE STAT LEARNING; Vaswani A, 2017, ADV NEUR IN, V30; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23; Wang GC, 2019, AAAI CONF ARTIF INTE, P8933; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang Y, 2018, PROC CVPR IEEE, P8042, DOI 10.1109/CVPR.2018.00839; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xu J, 2020, P MACHINE LEARNING R, V119, P10617; Yan YC, 2016, LECT NOTES COMPUT SC, V9910, P701, DOI 10.1007/978-3-319-46466-4_42; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12; Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325; Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380	96	2	2	2	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4626	4641		10.1109/TPAMI.2021.3073512	http://dx.doi.org/10.1109/TPAMI.2021.3073512			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33856981				2022-12-18	WOS:000836666600014
J	Gao, HY; Ji, SW				Gao, Hongyang; Ji, Shuiwang			Graph U-Nets	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Topology; Feature extraction; Computer architecture; Neural networks; Logic gates; Lattices; Graph neural networks; pooling; un-pooling; attention; U-Net		We consider the problem of representation learning for graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied to image pixel-wise prediction tasks, similar methods are lacking for graph data. This is because pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling and unpooling operations. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values. We further propose the gUnpool layer as the inverse operation of the gPool layer. Based on our proposed methods, we develop an encoder-decoder model, known as the graph U-Nets. Experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models. Along this direction, we extend our methods by integrating attention mechanisms. Based on attention operators, we proposed attention-based pooling and unpooling layers, which can better capture graph topology information. The empirical results on graph classification tasks demonstrate the promising capability of our methods.	[Gao, Hongyang] Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA; [Ji, Shuiwang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA	Iowa State University; Texas A&M University System; Texas A&M University College Station	Ji, SW (corresponding author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.	hygao@iastate.edu; sji@tamu.edu			National Science Foundation [IIS-1908166, IIS-1908198]	National Science Foundation(National Science Foundation (NSF))	This work was supported by National Science Foundation under Grants IIS-1908166 and IIS-1908198.	Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Alippi C, 2019, Arxiv, DOI arXiv:1907.00481; Chepuri S. P., 2016, P SENS ARR MULT SIGN, P1; Dai HJ, 2016, PR MACH LEARN RES, V48; Defferrard M, 2016, ADV NEUR IN, V29; Dobson PD, 2003, J MOL BIOL, V330, P771, DOI 10.1016/S0022-2836(03)00628-4; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Estrach J. B., 2014, P 2 INT C LEARN REPR; Gao HY, 2019, PR MACH LEARN RES, V97; Gao HY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1416, DOI 10.1145/3219819.3219947; Gao HY, 2017, IEEE DATA MINING, P871, DOI 10.1109/ICDM.2017.107; Gilmer J, 2017, PR MACH LEARN RES, V70; Hamilton WL, 2017, ADV NEUR IN, V30; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jegou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156; Joan Bruna, 2015, Arxiv, DOI arXiv:1506.05163; Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062; Kersting Kristian, 2016, BENCHMARK DATA SETS; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lee J, 2019, PR MACH LEARN RES, V97; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Niepert M, 2016, PR MACH LEARN RES, V48; Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732; Ronneberger O., 2015, P INT C MED IM COMP; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Velickovi Petar, 2017, ARXIV171010903; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xu K., 2018, PROC INT C LEARN REP; Yanardag P., 2015, ADV NEURAL INFORM PR, P2134; Yang Z, 2016, PR MACH LEARN RES, V48; Ying R, 2018, ADV NEUR IN, V31; Yu F., 2016, P ICLR 2016; Yuan H., 2020, PROC 8 INT C LEARN R; Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438; Zhao H, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4069; Zitnik M, 2017, BIOINFORMATICS, V33, pI190, DOI 10.1093/bioinformatics/btx252	51	2	2	15	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4948	4960		10.1109/TPAMI.2021.3081010	http://dx.doi.org/10.1109/TPAMI.2021.3081010			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33999813				2022-12-18	WOS:000836666600035
J	Gecer, B; Ploumpis, S; Kotsia, I; Zafeiriou, S				Gecer, Baris; Ploumpis, Stylianos; Kotsia, Irene; Zafeiriou, Stefanos			Fast-GANFIT: Generative Adversarial Network for High Fidelity 3D Face Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Three-dimensional displays; Shape; Face recognition; Solid modeling; Generative adversarial networks; Cost function; 3d face reconstruction; high-quality texture; identity preservation; 3DMM; regression; fitting; 3D morphable models	MORPHABLE MODEL; SHAPE	A lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of deep convolutional neural networks (DCNNs). In the recent works, the texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction is still not capable of modeling facial texture with high-frequency details. In this paper, we take a radically different approach and harness the power of generative adversarial networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful facial texture prior from a large-scale 3D texture dataset. Then, we revisit the original 3D Morphable Models (3DMMs) fitting making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. In order to be robust towards initialisation and expedite the fitting process, we propose a novel self-supervised regression based approach. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.	[Gecer, Baris; Ploumpis, Stylianos; Kotsia, Irene; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London SW7 2BX, England	Imperial College London	Gecer, B (corresponding author), Imperial Coll London, Dept Comp, London SW7 2BX, England.	b.gecer@imperial.ac.uk; s.ploumpis@imperial.ac.uk; drkotsia@gmail.com; s.zafeiriou@imperial.ac.uk			EPSRC Fellowship DEFORM [EP/S010203/1]; Google Faculty Award	EPSRC Fellowship DEFORM(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google Faculty Award(Google Incorporated)	The work of Stefanos Zafeiriou was supported by the EPSRC Fellowship DEFORM (EP/S010203/1) and a Google Faculty Award. The authors would like to thank Vasileios Triantafyllou for his valuable contributions on the visualizations in Figs. 1 and 5.	Bagdanov A. D., 2011, BEHAV UNDERSTANDING, P79; Bas A., 2016, AS C COMP VIS, P377; BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Cole F, 2017, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2017.361; Dai H, 2017, IEEE I CONF COMP VIS, P3104, DOI 10.1109/ICCV.2017.335; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741; Deng JK, 2018, IEEE INT CONF AUTOMA, P399, DOI 10.1109/FG.2018.00064; Diederik P Kingma, 2014, Arxiv, DOI arXiv:1312.6114; Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208; Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493; Gecer Baris, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P415, DOI 10.1007/978-3-030-58526-6_25; Gecer B, 2021, PROC CVPR IEEE, P7624, DOI 10.1109/CVPR46437.2021.00754; Gecer B, 2018, LECT NOTES COMPUT SC, V11215, P230, DOI 10.1007/978-3-030-01252-6_14; Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125; Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874; Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu GS, 2017, PATTERN RECOGN, V67, P366, DOI 10.1016/j.patcog.2017.02.007; Huang G., 2012, ADV NEURAL INFORM PR, P764; Ian Goodfellow, 2017, Arxiv, DOI arXiv:1701.00160; Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697; Jolliffe I., 2011, INT ENCY STAT SCI, P1094, DOI [DOI 10.1007/978-3-642-04898-2_455, 10.1007/978-3-642-04898-2_455, DOI 10.1007/978-3-642-04898-2]; Karras Tero, 2018, INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084; Lombardi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201401; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Parkhi Omkar M., 2015, BRIT MACH VIS C; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Piotraschke M, 2016, PROC CVPR IEEE, P3418, DOI 10.1109/CVPR.2016.372; Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56; Romdhani S, 2005, PROC CVPR IEEE, P986; Romdhani S, 2002, LECT NOTES COMPUT SC, V2353, P3; Saito S, 2017, PROC CVPR IEEE, P2326, DOI 10.1109/CVPR.2017.250; Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Slossberg R., 2019, LNCS, V11131, P498, DOI [10.1007/978-3-030-11015-4_36, DOI 10.1007/978-3-030-11015-4_36]; Smith W. A. P., 2020, PROC IEEE CVF C COMP, P5011; Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107; Tewari A, 2020, IEEE T PATTERN ANAL, V42, P357, DOI 10.1109/TPAMI.2018.2876842; Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270; Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153; Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122; Tran L, 2021, IEEE T PATTERN ANAL, V43, P157, DOI 10.1109/TPAMI.2019.2927975; Wilber MJ, 2017, IEEE I CONF COMP VIS, P1211, DOI 10.1109/ICCV.2017.136; WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9; Yamaguchi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201364; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	61	2	2	5	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4879	4893		10.1109/TPAMI.2021.3084524	http://dx.doi.org/10.1109/TPAMI.2021.3084524			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	34043505	Green Submitted			2022-12-18	WOS:000836666600031
J	Liu, S; Hui, TR; Huang, SF; Wei, YC; Li, B; Li, GB				Liu, Si; Hui, Tianrui; Huang, Shaofei; Wei, Yunchao; Li, Bo; Li, Guanbin			Cross-Modal Progressive Comprehension for Referring Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Feature extraction; Cognition; Visualization; Semantics; Task analysis; Linguistics; Referring segmentation; progressive comprehension; graph reasoning; multimodal feature fusion		Given a natural language expression and an image/video, the goal of referring segmentation is to produce the pixel-level masks of the entities described by the subject of the expression. Previous approaches tackle this problem by implicit feature interaction and fusion between visual and linguistic modalities in a one-stage manner. However, human tends to solve the referring problem in a progressive manner based on informative words in the expression, i.e., first roughly locating candidate entities and then distinguishing the target one. In this paper, we propose a cross-modal progressive comprehension (CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I (Image) module and a CMPC-V (Video) module to improve referring image and video segmentation models. For image data, our CMPC-I module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the target entity as well as suppress other irrelevant ones by spatial graph reasoning. For video data, our CMPC-V module further exploits action words based on CMPC-I to highlight the correct entity matched with the action cues by temporal graph reasoning. In addition to the CMPC, we also introduce a simple yet effective Text-Guided Feature Exchange (TGFE) module to integrate the reasoned multimodal features corresponding to different levels in the visual backbone under the guidance of textual information. In this way, multi-level features can communicate with each other and be mutually refined based on the textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or video version referring segmentation frameworks and our frameworks achieve new state-of-the-art performances on four referring image segmentation benchmarks and three referring video segmentation benchmarks respectively. Our code is available at https://github.com/spyflying/CMPC-Refseg.	[Liu, Si; Li, Bo] Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China; [Hui, Tianrui; Huang, Shaofei] Chinese Acad Sci, Inst Informat Engn, Beijing 100864, Peoples R China; [Hui, Tianrui; Huang, Shaofei] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China; [Wei, Yunchao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China; [Li, Guanbin] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Guangdong, Peoples R China; [Li, Guanbin] Pazhou Lab, Guangzhou 510335, Guangdong, Peoples R China	Beihang University; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Beijing Jiaotong University; Sun Yat Sen University; Pazhou Lab	Li, GB (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Guangdong, Peoples R China.	liusi@buaa.edu.cn; huitianrui@iie.ac.cn; huangshaofei@iie.ac.cn; wychao1987@gmail.com; boli@buaa.edu.cn; liguanbin@mail.sysu.edu.cn			National Natural Science Foundation of China [61876177, 61976250, U1811463]; Beijing Natural Science Foundation [4202034]; Fundamental Research Funds for the Central Universities, Zhejiang Lab [2019KD0AB04]; Guangdong Basic and Applied Basic Research Foundation [2020B1515020048]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Fundamental Research Funds for the Central Universities, Zhejiang Lab; Guangdong Basic and Applied Basic Research Foundation	This work was supported in part by the National Natural Science Foundation of China under Grants 61876177, 61976250, and U1811463 Beijing Natural Science Foundation (4202034), Fundamental Research Funds for the Central Universities, Zhejiang Lab (No. 2019KD0AB04) and Guangdong Basic and Applied Basic Research Foundation under Grant No.2020B1515020048.	Alan L. Yuille, 2016, Arxiv, DOI arXiv:1412.7062; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Bo Li, 2020, Arxiv, DOI arXiv:1909.07072; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chandra S, 2017, IEEE I CONF COMP VIS, P5113, DOI 10.1109/ICCV.2017.546; Chen DJ, 2019, IEEE I CONF COMP VIS, P7453, DOI 10.1109/ICCV.2019.00755; Chen JB, 2018, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR.2018.00909; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052; Dingcheng Yue, 2018, Arxiv, DOI arXiv:1809.03327; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Florian Schroff, 2017, Arxiv, DOI arXiv:1706.05587; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039; Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7; Huang Shaofei, 2020, P IEEE CVF C COMP VI, P10488, DOI DOI 10.1109/CVPR42600.2020.01050; Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787; Kingma D.P, P 3 INT C LEARNING R; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602; Li Y, 2018, ADV NEUR IN, V31; Li ZY, 2017, PROC CVPR IEEE, P7350, DOI 10.1109/CVPR.2017.777; Liang X., 2018, NEURIPS, P1858; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Ma CY, 2019, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR.2019.00689; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39; Max Welling, 2017, Arxiv, DOI arXiv:1609.02907; Ming-Hsuan Yang, 2019, Arxiv, DOI arXiv:1910.04748; Ning K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P948; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480; Seo S., 2020, PROC EUR C COMPUT VI, P1; Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3; Shi XJ, 2015, ADV NEUR IN, V28; Shipeng Yan, 2019, Arxiv, DOI arXiv:1905.11634; TANENHAUS MK, 1995, SCIENCE, V268, P1632, DOI 10.1126/science.7777863; Wang H, 2019, IEEE I CONF COMP VIS, P3938, DOI 10.1109/ICCV.2019.00404; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wang X, 2019, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2019.00679; Xu CL, 2015, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2015.7298839; Yang SB, 2021, IEEE T PATTERN ANAL, V43, P2765, DOI 10.1109/TPAMI.2020.2973983; Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474; Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427; Yang Sibei, 2020, P IEEE CVF C COMP VI, P9952; Ye LW, 2020, IEEE T MULTIMEDIA, V22, P3224, DOI 10.1109/TMM.2020.2971171; Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Yu WH, 2019, PETROL SCI TECHNOL, V37, P1338, DOI 10.1080/10916466.2019.1581813; Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660	62	2	2	5	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4761	4775		10.1109/TPAMI.2021.3079993	http://dx.doi.org/10.1109/TPAMI.2021.3079993			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33983880	Green Submitted			2022-12-18	WOS:000836666600023
J	Mikes, S; Haindl, M				Mikes, Stanislav; Haindl, Michal			Texture Segmentation Benchmark	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Benchmark testing; Heuristic algorithms; Image color analysis; Satellites; Lighting; Time measurement; Benchmark; image segmentation; texture segmentation; (Un)supervised segmentation; segmentation criteria; scale; rotation and illumination invariants	COLOR IMAGE SEGMENTATION; UNSUPERVISED SEGMENTATION; MODEL	The Prague texture segmentation data-generator and benchmark (mosaic.utia.cas.cz) is a web-based service designed to mutually compare and rank (recently nearly 200) different static and dynamic texture and image segmenters, to find optimal parametrization of a segmenter and support the development of new segmentation and classification methods. The benchmark verifies segmenter performance characteristics on potentially unlimited monospectral, multispectral, satellite, and bidirectional texture function (BTF) data using an extensive set of over forty prevalent criteria. It also enables us to test for noise robustness and scale, rotation, or illumination invariance. It can be used in other applications, such as feature selection, image compression, query by pictorial example, etc. The benchmark's functionalities are demonstrated in evaluating several examples of leading previously published unsupervised and supervised image segmentation algorithms. However, they are used to illustrate the benchmark functionality and not review the recent image segmentation state-of-the-art.	[Mikes, Stanislav; Haindl, Michal] Czech Acad Sci, Inst Informat Theory & Automat, Prague 11720, Czech Republic	Czech Academy of Sciences; Institute of Information Theory & Automation of the Czech Academy of Sciences	Haindl, M (corresponding author), Czech Acad Sci, Inst Informat Theory & Automat, Prague 11720, Czech Republic.	xaos@utia.cas.cz; haindl@utia.cas.cz			Czech Science Foundation Project GACR [19-12340S]	Czech Science Foundation Project GACR	This work was supported by the Czech Science Foundation Project GACR under Grant 19-12340S	Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130; [Anonymous], 2002, THESIS U CALIFORNIA; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bampis CG, 2016, IEEE IMAGE PROC, P1254; Belongie S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P675, DOI 10.1109/ICCV.1998.710790; Borsotti M, 1998, PATTERN RECOGN LETT, V19, P741, DOI 10.1016/S0167-8655(98)00052-X; Chabrier S, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/96306; Cheng HD, 2001, PATTERN RECOGN, V34, P2259, DOI 10.1016/S0031-3203(00)00149-7; Christoudias CM, 2002, INT C PATT RECOG, P150, DOI 10.1109/ICPR.2002.1047421; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Correia P, 2000, IEEE IMAGE PROC, P308, DOI 10.1109/ICIP.2000.900956; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985; Dharmagunawardhana C, 2014, IMAGE VISION COMPUT, V32, P884, DOI 10.1016/j.imavis.2014.07.002; DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409; Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535; Everingham M, 2002, LECT NOTES COMPUT SC, V2353, P34; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Filip J., 2013, ADV COMPUTER VISION; FOWLKES EB, 1983, J AM STAT ASSOC, V78, P553, DOI 10.2307/2288117; Fred ALN, 2003, PROC CVPR IEEE, P128; Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27; GeoEye Inc., 2009, GEOEYE PROD GUID V1; Haindl M., 2009, UTIA BIDIRECTIONAL T; Haindl M, 2007, LECT NOTES ARTIF INT, V4694, P33; Haindl M, 2007, LECT NOTES COMPUT SC, V4673, P987; Haindl M, 2016, PATTERN RECOGN, V57, P136, DOI 10.1016/j.patcog.2016.03.003; Haindl M, 2015, LECT NOTES COMPUT SC, V9256, P261, DOI 10.1007/978-3-319-23192-1_22; Haindl M, 2013, LECT NOTES COMPUT SC, V8047, P433, DOI 10.1007/978-3-642-40261-6_52; HARALICK RM, 1985, COMPUT VISION GRAPH, V29, P100, DOI 10.1016/S0734-189X(85)90153-7; Hoang MA, 2005, SIGNAL PROCESS, V85, P265, DOI 10.1016/j.sigpro.2004.10.009; Hoover A, 1996, IEEE T PATTERN ANAL, V18, P673, DOI 10.1109/34.506791; Huang Q, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC53; Huang Y, 2019, NEUROCOMPUTING, V349, P31, DOI 10.1016/j.neucom.2019.04.021; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; Ilea DE, 2011, PATTERN RECOGN, V44, P2479, DOI 10.1016/j.patcog.2011.03.005; Jaccard P., 1912, NEW PHYTOL, V11, P37, DOI [DOI 10.1111/J.1469-8137.1912.TB05611.X, 10.1111/j.1469-8137.1912.tb05611.x]; Jiang XY, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/35909; Kiechle M, 2018, IEEE T IMAGE PROCESS, V27, P1994, DOI 10.1109/TIP.2018.2792904; Larsen, 1999, P 5 ACM SIGKDD INT C, P16, DOI [DOI 10.1145/312129.312186, 10.1145/312129.312186]; LEE SU, 1990, COMPUT VISION GRAPH, V52, P171, DOI 10.1016/0734-189X(90)90053-X; LEVINE MD, 1985, IEEE T PATTERN ANAL, V7, P155, DOI 10.1109/TPAMI.1985.4767640; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lucchese L., 2001, PINSA-A (Proceedings of the Indian National Science Academy) Part A (Physical Sciences), V67, P207; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Meila M., 2005, P 22 INT C MACH LEAR, P577; Mevenkamp N., 2016, P IEEE WINT C APPL C, P1; Mikes S, 2015, IEEE J-STARS, V8, P2240, DOI 10.1109/JSTARS.2015.2416656; Mobahi H, 2011, INT J COMPUT VISION, V95, P86, DOI 10.1007/s11263-011-0444-0; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854; PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J; Panagiotakis C, 2011, IEEE T IMAGE PROCESS, V20, P2276, DOI 10.1109/TIP.2011.2114893; Patino F. C., 2010, THESIS U POLITECNICA; Paul F. Whelan, 2017, Arxiv, DOI arXiv:1703.05230; Payet N, 2013, IEEE T PATTERN ANAL, V35, P1066, DOI 10.1109/TPAMI.2012.194; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Peteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009; Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481406; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; SAHOO PK, 1988, COMPUT VISION GRAPH, V41, P233, DOI 10.1016/0734-189X(88)90022-9; Sattler M., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P167; Scarpa G, 2007, LECT NOTES COMPUT SC, V4522, P303; Shaffrey C. W., 2002, PROC ADV CONCEPTS IN; Sharma M, 2001, ANZIIS 2001: PROCEEDINGS OF THE SEVENTH AUSTRALIAN AND NEW ZEALAND INTELLIGENT INFORMATION SYSTEMS CONFERENCE, P231; Shewchuk J.R., 1996, WORKSH APPL COMP GEO, V1148, DOI DOI 10.1007/BFB0014497; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Srubar S, 2014, LECT NOTES COMPUT SC, V8466, P113, DOI 10.1007/978-3-319-07148-0_11; Stewart IT, 2009, HYDROL PROCESS, V23, P78, DOI 10.1002/hyp.7128; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Unnikrishnan R., 2005, P IEEE COMP SOC C CO, P34, DOI DOI 10.1109/CVPR.2005.390; Wagner Silke, 2007, COMP CLUSTERINGS OVE; WALLACE DL, 1983, J AM STAT ASSOC, V78, P569, DOI 10.2307/2288118; Wang MR, 2017, SIGNAL PROCESS-IMAGE, V56, P28, DOI 10.1016/j.image.2017.04.007; Wang XF, 2015, IEEE T IMAGE PROCESS, V24, P1399, DOI 10.1109/TIP.2015.2397313; Yuan J., 2013, OSUCISRC113TR0; Yuan JY, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P482, DOI 10.1109/IJCNN.2011.6033260; Zhang H, 2004, P SOC PHOTO-OPT INS, V5307, P38; Zhang H, 2008, COMPUT VIS IMAGE UND, V110, P260, DOI 10.1016/j.cviu.2007.08.003; Zhang YJ, 2001, ISSPA 2001: SIXTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOLS 1 AND 2, PROCEEDINGS, P148, DOI 10.1109/ISSPA.2001.949797; Zhang YJ, 1996, PATTERN RECOGN, V29, P1335, DOI 10.1016/0031-3203(95)00169-7; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544	86	2	2	10	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5647	5663		10.1109/TPAMI.2021.3075916	http://dx.doi.org/10.1109/TPAMI.2021.3075916			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33905324				2022-12-18	WOS:000836666600081
J	Rondon, MFR; Sassatelli, L; Aparicio-Pardo, R; Precioso, F				Rondon, Miguel Fabian Romero; Sassatelli, Lucile; Aparicio-Pardo, Ramon; Precioso, Frederic			TRACK: A New Method From a Re-Examination of Deep Architectures for Head Motion Prediction in 360 degrees Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Modeling and prediction; virtual reality; neural nets; machine learning; kinematics and dynamics		We consider predicting the user's head motion in 360 degrees videos, with 2 modalities only: the past user's positions and the video content (not knowing other users' traces). We make two main contributions. First, we re-examine existing deep-learning approaches for this problem and identify hidden flaws from a thorough root-cause analysis. Second, from the results of this analysis, we design a new proposal establishing state-of-the-art performance. First, re-assessing the existing methods that use both modalities, we obtain the surprising result that they all perform worse than baselines using the user's trajectory only. A root-cause analysis of the metrics, datasets and neural architectures shows in particular that (i) the content can inform the prediction for horizons longer than 2 to 3 sec. (existing methods consider shorter horizons), and that (ii) to compete with the baselines, it is necessary to have a recurrent unit dedicated to process the positions, but this is not sufficient. Second, from a re-examination of the problem supported with the concept of Structural-RNN, we design a new deep neural architecture, named TRACK. TRACK achieves state-of-the-art performance on all considered datasets and prediction horizons, outperforming competitors by up to 20 percent on focus-type videos and horizons 2-5 seconds. The entire framework (codes and datasets) is online and received an ACM reproducibility badge https://gitlab.com/miguelfromeror/head-motion-prediction	[Rondon, Miguel Fabian Romero; Sassatelli, Lucile; Aparicio-Pardo, Ramon; Precioso, Frederic] Univ Cote dAzur, CNRS, I3S, F-06900 Sophia Antipolis, France; [Sassatelli, Lucile] Inst Univ France, F-75005 Paris, France; [Precioso, Frederic] INRIA, F-06900 Sophia Antipolis, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur; Institut Universitaire de France; Inria	Sassatelli, L (corresponding author), Univ Cote dAzur, CNRS, I3S, F-06900 Sophia Antipolis, France.	miguel.romero-rondon@univ-cotedazur.fr; lucile.sassatelli@univ-cotedazur.fr; ramon.aparicio-pardo@univ-cotedazur.fr; frederic.precioso@univ-cotedazur.fr			French government, through the UCA JEDI; EUR DS4H Investments in the Future projects [ANR-15-IDEX-0001, ANR-17-EURE0004]; EU [951911]	French government, through the UCA JEDI; EUR DS4H Investments in the Future projects; EU(European Commission)	This work was supported in part by the French government, through the UCA JEDI and EUR DS4H Investments in the Future projects ANR-15-IDEX-0001 and ANR-17-EURE0004. This work was supported in part by the EU Horizon 2020 Project AI4Media, under contract no. 951911 (https://ai4media.eu/).A preliminary version of this work has been published in [1].	Aladagli A. D., 2017, 2017 INT C 3D IMMERS, P1, DOI DOI 10.1109/IC3D.2017.8251913; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Almquist M, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P258, DOI 10.1145/3204949.3204970; Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ban YX, 2018, IEEE INT CON MULTI; Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720; Blalock D, 2020, P MACHINE LEARNING S; Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215; David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139; Fan C., 2017, P 27 WORKSH NETW OP; Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807; Fu CW, 2009, IEEE T MULTIMEDIA, V11, P634, DOI 10.1109/TMM.2009.2017626; Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304; Global Industry Analysts Inc., 2020, VIRT REL GLOB MAR KE; Gutierrez J, 2018, INT WORK QUAL MULTIM, P171; Hristova H., 2018, 2018 IEEE 20 INT WOR, P1; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573; Li Y., 2018, P INT C COMM NETW CH, P501; Magliano JP, 2011, COGNITIVE SCI, V35, P1489, DOI 10.1111/j.1551-6709.2011.01202.x; Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497; Musgrave Kevin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P681, DOI 10.1007/978-3-030-58595-2_41; Nguyen A, 2019, PANOSALNET SOURCE CO; Park J, 2019, IEEE J EM SEL TOP C, V9, P149, DOI 10.1109/JETCAS.2019.2898622; Petrangeli S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P157, DOI 10.1109/AIVR.2018.00033; Qian F., 2016, P 5 WORKSHOP ALL THI, P1, DOI DOI 10.1145/2980055.2980056; Romero-Rondon M. F., 2020, PROC IEEE INT C IMAG, P2586; Romero-Rondon M. F., 2020, HEAD MOTION PREDICTI; Rondon Miguel Fabian Romero, 2020, MMSys '20: Proceedings of the 11th Multimedia Systems Conference, P279, DOI 10.1145/3339825.3394934; Rondon MFR, 2022, IEEE T PATTERN ANAL, V44, P5681, DOI 10.1109/TPAMI.2021.3070520; Rossi S., 2020, MMVE 20, P19, DOI [10.1145/3386293.3397115, DOI 10.1145/3386293.3397115]; Rossi S, 2019, INT CONF ACOUST SPEE, P4020, DOI 10.1109/ICASSP.2019.8683854; Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41; Simonyan K., 2015, P INT C LEARN REPR, P1, DOI DOI 10.48550/ARXIV.1409.1556; Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599; Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150; Sutskever I, 2014, ADV NEUR IN, V27; Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210; Xiao MB, 2018, IEEE INFOCOM SER, P953; Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783; Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559; Yang W, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P1129, DOI 10.1145/3331184.3331340; Yu Y, 2018, AAAI CONF ARTIF INTE, P7525	45	2	2	2	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5681	5699		10.1109/TPAMI.2021.3070520	http://dx.doi.org/10.1109/TPAMI.2021.3070520			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33819149	Green Submitted			2022-12-18	WOS:000836666600083
J	Shao, XH; Xing, JL; Lyu, JJ; Zhou, XD; Shi, Y; Maybank, S				Shao, Xiaohu; Xing, Junliang; Lyu, Jiangjing; Zhou, Xiangdong; Shi, Yu; Maybank, Steve			Robust Face Alignment via Deep Progressive Reinitialization and Adaptive Error-Driven Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Faces; Annotations; Face recognition; Training; Shape; Face detection; Computer architecture; Face alignment; regression model; deep architecture; supervised spatial transformer network; adaptive learning	LOCALIZATION; MULTIVIEW; NETWORK	Regression-based face alignment involves learning a series of mapping functions to predict the true landmarks from an initial estimation of the alignment. Most existing approaches focus on learning efficacious mapping functions from some feature representations to improve performance. The issues related to the initial alignment estimation and the final learning objective, however, receive less attention. This work proposes a deep regression architecture with progressive reinitialization and a new error-driven learning loss function to explicitly address the above two issues. Given an image with a rough face detection result, the full face region is first mapped by a supervised spatial transformer network to a normalized form and trained to regress coarse positions of landmarks. Then, different face parts are further respectively reinitialized to their own normalized states, followed by another regression sub-network to refine the landmark positions. To deal with the inconsistent annotations in existing training datasets, we further propose an adaptive landmark-weighted loss function. It dynamically adjusts the importance of different landmarks according to their learning errors during training without depending on any hyper-parameters manually set by trial and error. A high level of robustness to annotation inconsistencies is thus achieved. The whole deep architecture permits training from end to end, and extensive experimental analyses and comparisons demonstrate its effectiveness and efficiency. The source code, trained models, and experimental results are made available at https://github.com/shaoxiaohu/Face_Alignment_DPR.git.	[Shao, Xiaohu; Zhou, Xiangdong; Shi, Yu] Chinese Acad Sci, Chongqing Inst Green & Intelligent Technol, Chongqing 400714, Peoples R China; [Shao, Xiaohu; Zhou, Xiangdong; Shi, Yu] Univ Chinese Acad Sci, Chongqing Sch, Chongqing 400714, Peoples R China; [Xing, Junliang] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China; [Xing, Junliang] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China; [Lyu, Jiangjing] Alibaba Grp, Hangzhou 310052, Zhejiang, Peoples R China; [Maybank, Steve] Univ London, Birkbeck Coll, Dept Comp Sci & Informat Syst, London WC1E 7HX, England	Chinese Academy of Sciences; Chongqing Institute of Green & Intelligent Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Alibaba Group; University of London; Birkbeck University London	Xing, JL (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.	shaoxiaohu@cigit.ac.cn; jlxing@nlpr.ia.ac.cn; jiangjing.ljj@alibaba-inc.com; zhouxiangdong@cigit.ac.cn; shiyu@cigit.ac.cn; steve.maybank@bbk.ac.uk	Xing, Junliang/HGE-9630-2022	Xing, Junliang/0000-0001-6801-0510; Lyu, Jiangjing/0000-0002-6461-9553	National Key Research and Development Program of China [2018YFC0808303, 2019AAA010340X]; National Natural Science Foundation of China [61806185, 61802361, 62076238]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the Project from National Key Research and Development Program of China (Grants 2018YFC0808303 and 2019AAA010340X), National Natural Science Foundation of China under Grants 61806185, 61802361, and 62076238. Xiaohu Shao and Junliang Xing contributed equally to this work.	Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen Change Loy, 2015, Arxiv, DOI arXiv:1511.05049; Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8; Chen LS, 2019, IEEE I CONF COMP VIS, P6991, DOI 10.1109/ICCV.2019.00709; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cristinacce D., 2006, P BRIT MACH VIS C, V3, P929; Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024; Dapogny A, 2019, IEEE I CONF COMP VIS, P6892, DOI 10.1109/ICCV.2019.00699; Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525; Deng JK, 2019, IEEE T IMAGE PROCESS, V28, P3636, DOI 10.1109/TIP.2019.2899267; Deng JK, 2016, IMAGE VISION COMPUT, V47, P19, DOI 10.1016/j.imavis.2015.11.005; Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094; Dong XY, 2019, IEEE I CONF COMP VIS, P783, DOI 10.1109/ICCV.2019.00087; Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045; Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047; Erjin Zhou, 2015, Arxiv, DOI arXiv:1511.04901; Feng ZH, 2020, INT J COMPUT VISION, V128, P2126, DOI 10.1007/s11263-019-01275-0; Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238; Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392; Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306; Guler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280; Hara K, 2014, LECT NOTES COMPUT SC, V8690, P552, DOI 10.1007/978-3-319-10605-2_36; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Huang Gary B, 2014, 14003 U MASS AMH DEP, V14, P1; Jaderberg M, 2015, ADV NEUR IN, V28; Jia Y., 2014, ARXIV; Jiang X., 2020, P MACHINE LEARNING S, V2, P1, DOI 10.48550/arXiv.2002.12418; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Kostinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397; Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254; Kumar Abhinav, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8233, DOI 10.1109/CVPR42600.2020.00826; Liu ZW, 2019, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2019.00358; Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393; Qian SJ, 2019, IEEE I CONF COMP VIS, P10152, DOI 10.1109/ICCV.2019.01025; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Ren SQ, 2016, IEEE T IMAGE PROCESS, V25, P1233, DOI 10.1109/TIP.2016.2518867; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shi BG, 2018, IEEE T NEUR NET LEAR, V29, P183, DOI 10.1109/TNNLS.2016.2618340; Sun K., 2019, ARXIV, DOI DOI 10.48550/ARXIV.1904.04514; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Wang XY, 2019, IEEE I CONF COMP VIS, P6970, DOI 10.1109/ICCV.2019.00707; Wu WY, 2017, IEEE COMPUT SOC CONF, P2096, DOI 10.1109/CVPRW.2017.261; Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227; Wu Y, 2018, IEEE T PATTERN ANAL, V40, P3067, DOI 10.1109/TPAMI.2017.2787130; Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4; Xiao ST, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P986, DOI 10.1109/ICCVW.2015.130; Xing JL, 2018, IEEE T PATTERN ANAL, V40, P987, DOI 10.1109/TPAMI.2017.2697958; Xing JL, 2014, PROC CVPR IEEE, P1829, DOI 10.1109/CVPR.2014.236; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126; Yang H., 2015, P BRIT MACH VIS C; Yang H, 2014, IEEE SIGNAL PROC LET, V21, P1321, DOI 10.1109/LSP.2014.2333544; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P52, DOI 10.1007/978-3-319-46454-1_4; Yu X, 2020, IEEE T PATTERN ANAL, V42, P2148, DOI 10.1109/TPAMI.2019.2914039; Yu X, 2017, AAAI CONF ARTIF INTE, P4327; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58; Zhu ML, 2019, PROC CVPR IEEE, P3481, DOI 10.1109/CVPR.2019.00360; Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679; Zou X, 2019, IEEE I CONF COMP VIS, P141, DOI 10.1109/ICCV.2019.00023	78	2	2	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5488	5502		10.1109/TPAMI.2021.3073593	http://dx.doi.org/10.1109/TPAMI.2021.3073593			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33856985	Green Accepted			2022-12-18	WOS:000836666600070
J	Takahashi, K; Nobuhara, S				Takahashi, Kosuke; Nobuhara, Shohei			Structure of Multiple Mirror System From Kaleidoscopic Projections of Single 3D Point	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Mirrors; Cameras; Three-dimensional displays; Calibration; Two dimensional displays; Parameter estimation; Geometry; Kaleidoscopic imaging system; camera calibration; mirror	STEREO; DESIGN	This paper proposes a novel algorithm of discovering the structure of a kaleidoscopic imaging system that consists of multiple planar mirrors and a camera. The kaleidoscopic imaging system can be recognized as the virtual multi-camera system and has strong advantages in that the virtual cameras are strictly synchronized and have the same intrinsic parameters. In this paper, we focus on the extrinsic calibration of the virtual multi-camera system. The problems to be solved in this paper are two-fold. The first problem is to identify to which mirror chamber each of the 2D projections of mirrored 3D points belongs. The second problem is to estimate all mirror parameters, i.e., normals, and distances of the mirrors. The key contribution of this paper is to propose novel algorithms for these problems using a single 3D point of unknown geometry by utilizing a kaleidoscopic projection constraint, which is an epipolar constraint on mirror reflections. We demonstrate the performance of the proposed algorithm of chamber assignment and estimation of mirror parameters with qualitative and quantitative evaluations using synthesized and real data.	[Takahashi, Kosuke; Nobuhara, Shohei] Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan	Kyoto University	Takahashi, K (corresponding author), Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan.	t.kosuke.1210@gmail.com; nob@i.kyoto-u.ac.jp		Nobuhara, Shohei/0000-0002-3204-8696	JSPS Kakenhi [26240023, 18K19815]	JSPS Kakenhi(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported in part by the JSPS Kakenhi under Grant 26240023 and 18K19815.	Agrawal A, 2013, IEEE I CONF COMP VIS, P2368, DOI 10.1109/ICCV.2013.294; Bushnevskiy A, 2016, PROC CVPR IEEE, P3373, DOI 10.1109/CVPR.2016.367; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Delaunoy Amael, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P123, DOI 10.1109/3DV.2014.102; Forbes K, 2006, LECT NOTES COMPUT SC, V3952, P165; Fuchs M, 2013, COMPUT GRAPH FORUM, V32, P246, DOI 10.1111/cgf.12201; Furukawa Y, 2009, INT J COMPUT VISION, V84, P257, DOI 10.1007/s11263-009-0232-2; Gluckman J, 2001, INT J COMPUT VISION, V44, P65, DOI 10.1023/A:1011172403203; GOSHTASBY A, 1993, PATTERN RECOGN, V26, P923, DOI 10.1016/0031-3203(93)90058-5; Hartley R. I., 2000, MULTIPLE VIEW GEOMET; Hesch JA, 2010, SPRINGER TRAC ADV RO, V57, P285; Hesch JA, 2010, LECT NOTES COMPUT SC, V6314, P311, DOI 10.1007/978-3-642-15561-1_23; Huang P.-H., 2006, P IEEE COMP SOC C CO, P379; Ihrke I., 2012, PROC IEEE COMPUT SOC, P29; Inoshita C., 2013, IPSJ T COMPUT VIS AP, V5, P119; Kumar R., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587676; Lanman D, 2009, COMPUT VIS IMAGE UND, V113, P1107, DOI 10.1016/j.cviu.2009.03.016; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Levoy M, 2004, ACM T GRAPHIC, V23, P825, DOI 10.1145/1015706.1015806; Mariottini GL, 2010, LECT NOTES CONTR INF, V401, P3; Mukaigawa Y, 2011, LECT NOTES COMPUT SC, V6492, P336, DOI 10.1007/978-3-642-19315-6_26; Nene SA, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1087, DOI 10.1109/ICCV.1998.710852; Nitschke C, 2011, COMPUT VIS IMAGE UND, V115, P835, DOI 10.1016/j.cviu.2011.02.008; Nobuhara S, 2016, INT CONF 3D VISION, P342, DOI 10.1109/3DV.2016.43; Reshetouski Ilya, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P77, DOI 10.1007/978-3-642-44964-2_5; Reshetouski I, 2011, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2011.5995579; Reshtouski I, 2013, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2013.19; Rodrigues R, 2010, LECT NOTES COMPUT SC, V6314, P382, DOI 10.1007/978-3-642-15561-1_28; Scaramuzza Davide, 2006, IEEE INT C COMP VIS; Sen P, 2005, ACM T GRAPHIC, V24, P745, DOI 10.1145/1073204.1073257; Sturm P, 2006, LECT NOTES COMPUT SC, V3852, P21; Tagawa S, 2012, INT C PATT RECOG, P2181; Tahara T, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P153, DOI 10.1109/3DV.2015.25; Takahashi K., 2016, IPSJ T COMPUT VIS AP, V8, P20; Takahashi K, 2017, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2017.46; Takahashi K, 2012, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2012.6247783; Ying XH, 2013, IEEE T PATTERN ANAL, V35, P1206, DOI 10.1109/TPAMI.2012.195; Ying XH, 2010, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2010.5540088; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	39	2	2	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5602	5617		10.1109/TPAMI.2021.3070347	http://dx.doi.org/10.1109/TPAMI.2021.3070347			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33793398	Green Submitted			2022-12-18	WOS:000836666600078
J	Wang, J; Geng, X; Xue, H				Wang, Jing; Geng, Xin; Xue, Hui			Re-Weighting Large Margin Label Distribution Learning for Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Label distribution learning (LDL); classification; re-weighting; large margin; generalization		Label ambiguity has attracted quite some attention among the machine learning community. The latterly proposed Label Distribution Learning (LDL) can handle label ambiguity and has found wide applications in real classification problems. In the training phase, an LDL model is learned first. In the test phase, the top label(s) in the label distribution predicted by the learned LDL model is (are) then regarded as the predicted label(s). That is, LDL considers the whole label distribution in the training phase, but only the top label(s) in the test phase, which likely leads to objective inconsistency. To avoid such inconsistency, we propose a new LDL method Re-Weighting Large Margin Label Distribution Learning (RWLM-LDL). First, we prove that the expected L-1-norm loss of LDL bounds the classification error probability, and thus apply L-1-norm loss as the learning metric. Second, re-weighting schemes are put forward to alleviate the inconsistency. Third, large margin is introduced to further solve the inconsistency. The theoretical results are presented to showcase the generalization and discrimination of RWLM-LDL. Finally, experimental results show the statistically superior performance of RWLM-LDL against other comparing methods.	[Wang, Jing; Geng, Xin; Xue, Hui] Southeast Univ, Minist Educ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China; [Wang, Jing; Geng, Xin; Xue, Hui] Southeast Univ, Minist Educ, Key Lab Comp Network & Informat Integrat, Nanjing 211189, Peoples R China	Southeast University - China; Southeast University - China	Geng, X (corresponding author), Southeast Univ, Minist Educ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.; Geng, X (corresponding author), Southeast Univ, Minist Educ, Key Lab Comp Network & Informat Integrat, Nanjing 211189, Peoples R China.	wangjing91@seu.edu.cn; xgeng@seu.edu.cn; hxue@seu.edu.cn			National Key Research and Development Plan of China [2017YFB1002801]; National Natural Science Foundation of China [62076063, 62076062]; Collaborative Innovation Center of Novel Software Technology and Industrialization; Collaborative Innovation Center of Wireless Communications Technology	National Key Research and Development Plan of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Center of Novel Software Technology and Industrialization; Collaborative Innovation Center of Wireless Communications Technology	This work was supported by the National Key Research and Development Plan of China under Grant 2017YFB1002801, in part by the National Natural Science Foundation of China under Grants 62076063 and 62076062, in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization, and in part by the Collaborative Innovation Center of Wireless Communications Technology.	Ba J., 2017, P 3 INT C LEARN REPR; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Berger AL, 1996, COMPUT LINGUIST, V22, P39; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Chen MT, 2018, NEUROCOMPUTING, V320, P171, DOI 10.1016/j.neucom.2018.09.002; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Demsar J, 2006, J MACH LEARN RES, V7, P1; DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Elisseeff A, 2002, ADV NEUR IN, V14, P681; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gao BB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P712; Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998; Gao W, 2013, ARTIF INTELL, V199, P22, DOI 10.1016/j.artint.2013.03.001; Geng X, 2022, IEEE T PATTERN ANAL, V44, P1974, DOI 10.1109/TPAMI.2020.3029585; Geng X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3511; Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658; Geng X, 2014, PROC CVPR IEEE, P3742, DOI 10.1109/CVPR.2014.478; Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427; Jia XY, 2021, IEEE T KNOWL DATA EN, V33, P1619, DOI 10.1109/TKDE.2019.2943337; KAHN H, 1953, J OPER RES SOC AM, V1, P263, DOI 10.1287/opre.1.5.263; Li S, 2019, INT J COMPUT VISION, V127, P884, DOI 10.1007/s11263-018-1131-1; Liang LY, 2018, INT C PATT RECOG, P1598, DOI 10.1109/ICPR.2018.8546038; Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949; Mohri M., 2018, FDN MACHINE LEARNING; Shen W, 2017, ADV NEUR IN, V30; Snoek C.G., 2006, P 14 ANN ACM INT C M, P421, DOI DOI 10.1145/1180639.1180727; Vapnik V, 1964, AUTOMAT REM CONTR+, V25, P821; Wang J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3712; Wang J, 2019, AAAI CONF ARTIF INTE, P5256; Wu XZ, 2017, PR MACH LEARN RES, V70; Wu XP, 2019, IEEE I CONF COMP VIS, P10641, DOI 10.1109/ICCV.2019.01074; Xin Wen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P379, DOI 10.1007/978-3-030-58592-1_23; Xu N, 2021, IEEE T KNOWL DATA EN, V33, P1632, DOI 10.1109/TKDE.2019.2947040; Yang W., 2018, MATH PROBL ENG, P1; Yu JF, 2012, MATCH-COMMUN MATH CO, V67, P845; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhang ML, 2021, IEEE T KNOWL DATA EN, V33, P2057, DOI 10.1109/TKDE.2019.2951561; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang T, 2020, IEEE T KNOWL DATA EN, V32, P1143, DOI 10.1109/TKDE.2019.2897662; Zhu J, 2009, STAT INTERFACE, V2, P349	48	2	2	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5445	5459		10.1109/TPAMI.2021.3082623	http://dx.doi.org/10.1109/TPAMI.2021.3082623			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	34018929				2022-12-18	WOS:000836666600067
J	Wang, ZH; Ma, KD				Wang, Zhihua; Ma, Kede			Active Fine-Tuning From gMAD Examples Improves Blind Image Quality Assessment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computational modeling; Databases; Adaptation models; Training; Predictive models; Task analysis; Image quality; Blind image quality assessment; deep neural networks; gMAD competition; active learning	STATISTICS; INDEX	The research in image quality assessment (IQA) has a long history, and significant progress has been made by leveraging recent advances in deep neural networks (DNNs). Despite high correlation numbers on existing IQA datasets, DNN-based models may be easily falsified in the group maximum differentiation (gMAD) competition. Here we show that gMAD examples can be used to improve blind IQA (BIQA) methods. Specifically, we first pre-train a DNN-based BIQA model using multiple noisy annotators, and fine-tune it on multiple synthetically distorted images, resulting in a "top-performing" baseline model. We then seek pairs of images by comparing the baseline model with a set of full-reference IQA methods in gMAD. The spotted gMAD examples are most likely to reveal the weaknesses of the baseline, and suggest potential ways for refinement. We query human quality annotations for the selected images in a well-controlled laboratory environment, and further fine-tune the baseline on the combination of human-rated images from gMAD and existing databases. This process may be iterated, enabling active fine-tuning from gMAD examples for BIQA. We demonstrate the feasibility of our active learning scheme on a large-scale unlabeled image set, and show that the fine-tuned quality model achieves improved generalizability in gMAD, without destroying performance on previously seen databases.	[Wang, Zhihua; Ma, Kede] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China	City University of Hong Kong	Wang, ZH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.	zhihua.wang@my.cityu.edu.hk; kede.ma@cityu.edu.hk		Ma, Kede/0000-0001-8608-1128	National Natural Science Foundation of China [62071407]; CityU Start-up Grant [7200630]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CityU Start-up Grant	The authors would like to thank Dingquan Li for fruitful discussions throughout the development of this project, and Yiru Yao for coordinating the subjective experiments. This project was supported in part by the National Natural Science Foundation of China under Grant 62071407 and the CityU Start-up Grant 7200630.	AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784; [Anonymous], 2002, BT500 IR; [Anonymous], 2015, FINAL REPORT VIDEO Q; Balle J., 2015, ARXIV151106281; Balle Johannes, 2017, ICLR 2017; Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8; Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518; DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952; Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829; Kang L, 2015, IEEE IMAGE PROC, P2791, DOI 10.1109/ICIP.2015.7351311; Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819; Kingma D.P., 2015, INT C LEARN REPR, P1; Kong XF, 2018, INT J COMPUT VISION, V126, P537, DOI 10.1007/s11263-017-1054-2; Laparra V, 2016, ELECT IMAGING, V2016, P1, DOI DOI 10.2352/ISSN.2470-1173.2016.16.HVEI-103; Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105; Lin HH, 2019, INT WORK QUAL MULTIM; Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118; Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009; Ma KD, 2019, IEEE IMAGE PROC, P2344, DOI 10.1109/ICIP.2019.8803390; Ma Kede, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P851, DOI 10.1109/TPAMI.2018.2889948; Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888; Mallat S., 1999, WAVELET TOUR SIGNAL; MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250; Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003; Ming-Feng Tsai, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P383, DOI 10.1145/1277741.1277808; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325; Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954; Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042; Nielsen M.A., 2002, QUANTUM COMPUTATION; Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009; Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194; Ring M. B., 1994, THESIS; Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526; Settles Burr, 2009, TR1648 U WISC MAD DE, P2; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Sung K.-K, 1996, THESIS MIT CAMBRIDGE; Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288; Wang HL, 2020, IEEE T ROBOT, V36, P597, DOI [10.1109/TRO.2020.2967656, 10.1002/pc.25860]; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z, 2002, IEEE IMAGE PROC, P477; Wang Z., 2006, Modern Image Quality Assessment, DOI 10.2200/S00010ED1V01Y200508IVM003; Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297; Wang Z, 2008, J VISION, V8, DOI 10.1167/8.12.8; Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880; Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133; Yan QS, 2019, IEEE T IMAGE PROCESS, V28, P2200, DOI 10.1109/TIP.2018.2883741; Ye P, 2014, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2014.541; Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789; Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028; Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771; Zhang WX, 2020, IEEE IMAGE PROC, P111, DOI 10.1109/ICIP40778.2020.9191278	63	2	2	2	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4577	4590		10.1109/TPAMI.2021.3071759	http://dx.doi.org/10.1109/TPAMI.2021.3071759			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33830918	Green Submitted			2022-12-18	WOS:000836666600011
J	Xu, XY; Chen, H; Moreno-Noguer, F; Jeni, LA; De la Torre, F				Xu, Xiangyu; Chen, Hao; Moreno-Noguer, Francesc; Jeni, Laszlo A.; De la Torre, Fernando			3D Human Pose, Shape and Texture From Low-Resolution Images and Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Image resolution; Shape; Estimation; Training; Solid modeling; Videos; 3D human pose and shape; low-resolution; neural network; self-supervised learning; contrastive learning; texture estimation; video		3D human pose and shape estimation from monocular images has been an active research area in computer vision. Existing deep learning methods for this task rely on high-resolution input, which however, is not always available in many scenarios such as video surveillance and sports broadcasting. Two common approaches to deal with low-resolution images are applying super-resolution techniques to the input, which may result in unpleasant artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed method is able to learn 3D body pose and shape across different resolutions with one single model. The self-supervision loss enforces scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new losses provide robustness when learning in a weakly-supervised manner. Moreover, we extend the RSC-Net to handle low-resolution videos and apply it to reconstruct textured 3D pedestrians from low-resolution input. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images.	[Xu, Xiangyu; Jeni, Laszlo A.; De la Torre, Fernando] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Chen, Hao] Carnegie Mellon Univ, Elect & Comp Engn, Pittsburgh, PA 15213 USA; [Moreno-Noguer, Francesc] Inst Robot & Informat Ind CSIC UPC, Barcelona 08028, Spain	Carnegie Mellon University; Carnegie Mellon University; Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i Informatica Industrial (IRII); Universitat Politecnica de Catalunya	Xu, XY (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	xuxiangyu2014@gmail.com; hchen@cmu.edu; finoreno@iri.upc.edu; laszlojeni@cmu.edu; ftorre@cs.cmu.edu			National Science Foundation [RI-1617953]	National Science Foundation(National Science Foundation (NSF))	This work was partially supported by Facebook and the National Science Foundation under the grants RI-1617953.	Aaron van den Oord, 2019, Arxiv, DOI arXiv:1807.03748; Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238; Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Arjovsky M, 2017, PR MACH LEARN RES, V70; Ba J., 2017, P 3 INT C LEARN REPR; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Chen T, 2020, PR MACH LEARN RES, V119; Cheng Z., 2018, ASIAN C COMPUTER VIS, P605; Cho K., 2014, P 2014 C EMP METH NA, P1724; Dilip Krishnan, 2020, Arxiv, DOI arXiv:1906.05849; Doersch C., 2019, PROC ADV NEURAL INF, P12905; Eigen D, 2014, ADV NEUR IN, V27; Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greg Shakhnarovich, 2018, Arxiv, DOI arXiv:1803.11316; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Johnson S., 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12.CITESEER]; Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Laine Samuli, 2017, P INT C LEARN REPR I, P3; Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Nair V., 2010, ICML, P807; Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461; Neumann L, 2019, LECT NOTES COMPUT SC, V11363, P558, DOI 10.1007/978-3-030-20893-6_35; Nishibori K, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P115; Noh J, 2019, IEEE I CONF COMP VIS, P9724, DOI 10.1109/ICCV.2019.00982; Oh SM, 2011, PROC CVPR IEEE; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Pumarola A, 2019, IEEE I CONF COMP VIS, P2242, DOI [10.1109/ICCV.2019.00233, 10.1109/ICCV.2019.2019.00233]; Ravi Nikhila, 2020, PYTORCH3D; Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239; Simo-Serra E, 2013, PROC CVPR IEEE, P3634, DOI 10.1109/CVPR.2013.466; Simo-Serra E, 2017, INT J COMPUT VISION, V122, P388, DOI 10.1007/s11263-016-0941-2; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tan WM, 2018, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2018.00420; Tarvainen A, 2017, ADV NEUR IN, V30; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; Wang J, 2019, PROC CVPR IEEE, P11838, DOI 10.1109/CVPR.2019.01212; Wang ZY, 2016, PROC CVPR IEEE, P4792, DOI 10.1109/CVPR.2016.518; Wright J., HIGHDIMENSIONAL DATA; Xiangyu Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P284, DOI 10.1007/978-3-030-58545-7_17; Xu X., 2020, P INT C MACH LEARN, P10587; Xu XY, 2019, PROC CVPR IEEE, P1723, DOI 10.1109/CVPR.2019.00182; Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zhang JY, 2019, IEEE I CONF COMP VIS, P7113, DOI 10.1109/ICCV.2019.00721; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783	72	2	2	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4490	4504		10.1109/TPAMI.2021.3070002	http://dx.doi.org/10.1109/TPAMI.2021.3070002			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33788678	Green Published, Green Submitted			2022-12-18	WOS:000836666600006
J	Bejar, B; Dokmanic, I; Vidal, R				Bejar, Benjamin; Dokmanic, Ivan; Vidal, Rene			The Fastest l(1,infinity )Prox in the West	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Proximal operator; mixed norm; block sparsity	CLASSIFICATION; PROJECTION; CARCINOMAS; SIMPLEX; NORM	Proximal operators are of particular interest in optimization problems dealing with non-smooth objectives because in many practical cases they lead to optimization algorithms whose updates can be computed in closed form or very efficiently. A well-known example is the proximal operator of the vector l(1) norm, which is given by the soft-thresholding operator. In this paper we study the proximal operator of the mixed l(1,infinity) matrix norm and show that it can be computed in closed form by applying the well-known soft-thresholding operator to each column of the matrix. However, unlike the vector l(1) norm case where the threshold is constant, in the mixed l(1,infinity) norm case each column of the matrix might require a different threshold and all thresholds depend on the given matrix. We propose a general iterative algorithm for computing these thresholds, as well as two efficient implementations that further exploit easy to compute lower bounds for the mixed norm of the optimal solution. Experiments on large-scale synthetic and real data indicate that the proposed methods can be orders of magnitude faster than state-of-the-art methods.	[Bejar, Benjamin; Vidal, Rene] Johns Hopkins Univ, Dept Biomed Engn, Math Inst Data Sci, Baltimore, MD 21218 USA; [Dokmanic, Ivan] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL 61820 USA	Johns Hopkins University; University of Illinois System; University of Illinois Urbana-Champaign	Bejar, B (corresponding author), Johns Hopkins Univ, Dept Biomed Engn, Math Inst Data Sci, Baltimore, MD 21218 USA.	bbejar@jhu.edu; dokmanic@illinois.edu; rvidal@jhu.edu			 [NSF 1704458]		The authors would like to thank the anonymous reviewers for their suggestions and constructive comments that have certainly contributed to improve the quality of this paper. This work acknowledges financial support under grant NSF 1704458.	Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Bhattacharjee A, 2001, P NATL ACAD SCI USA, V98, P13790, DOI 10.1073/pnas.191502998; Boyd S, 2004, CONVEX OPTIMIZATION; Chau G, 2019, SIAM J IMAGING SCI, V12, P604, DOI 10.1137/18M1212525; Chau G, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4694; Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10; Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6; Dokmanic I, 2017, MOORE PENROSE 1; Dokmanic I, 2019, SIAM J MATRIX ANAL A, V40, P92, DOI 10.1137/17M1145409; Duchi J., 2008, PROC 25 INT C MACH L, P272; Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852; Fodor SPA, 1997, SCIENCE, V277, P393, DOI 10.1126/science.277.5324.393; MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Nutt CL, 2003, CANCER RES, V63, P1602; Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x; Palomar DP, 2005, IEEE T SIGNAL PROCES, V53, P686, DOI 10.1109/TSP.2004.840816; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Quattoni A., 2009, P 26 ANN INT C MACH, P857, DOI DOI 10.1145/1553374.1553484; Singh D, 2002, CANCER CELL, V1, P203, DOI 10.1016/S1535-6108(02)00030-2; Sra S, 2011, LECT NOTES ARTIF INT, V6913, P305, DOI 10.1007/978-3-642-23808-6_20; Su AI, 2001, CANCER RES, V61, P7388; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Yang K, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-228	26	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3858	3869		10.1109/TPAMI.2021.3059301	http://dx.doi.org/10.1109/TPAMI.2021.3059301			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33587698				2022-12-18	WOS:000805820500037
